{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8142dc0f-2c8c-4c72-bfe5-4997d2f54d2d",
   "metadata": {},
   "source": [
    "# Trace simulation playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673af8c1-3a98-4164-9501-1b8569af9086",
   "metadata": {},
   "source": [
    "### Initialize weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcbcdbc-877a-41e5-b508-a48b4eba6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2bbc2e",
   "metadata": {},
   "source": [
    "### import functions for simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3440228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d21834dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperfunctions import *\n",
    "from simulation_pipeline import *\n",
    "from dataprep_helperfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99ddb2",
   "metadata": {},
   "source": [
    "### import functions for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c84ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_helperfunctions import prepare_data_f_memory, prepare_dataset_from_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702cbaef-9bdb-477c-be1e-73abaacab330",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\begin{center}\n",
    "    \\begin{tabular}{lrrrrr}\\toprule\n",
    "        Dataset name & Num. cases & Max trace length & Truncation & Avg. trace length & Avg. case duration \\\\ \\midrule\n",
    "            Sepsis & 966 (83) & 185 & None &  \\textbf{18.51} & 22.96\\\\  \\cmidrule{1-6}\n",
    "            Helpdesk & 4362 (218) & 15 & None &  5.07 & 40.74\\\\ \\cmidrule{1-6}\n",
    "            Traffic fines & \\textbf{125815} (3800) & 20 & 10 & 4.25 & \\textbf{186.59}\\\\  \\cmidrule{1-6}\n",
    "            Hospital billing & 63645 (13880) & \\textbf{217} & 8 &  5.73 & 140.99\\\\ \\cmidrule{1-6}\n",
    "        \n",
    "    \\end{tabular}\n",
    "\\end{center}\n",
    "\\caption{\\textit{Dataset statistics (full data). Parenthesis is dropped cases due to censoring. Durations are measured in days.}}\n",
    "\\label{table:datasets}\n",
    "\\end{table}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3526631-75d0-4a80-8169-897a9006707f",
   "metadata": {},
   "source": [
    "# Project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bfeff24-6a3e-4b74-a85d-35d522b3c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"sim_play_ITX_test\" #name for weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2d9419",
   "metadata": {},
   "source": [
    "# Overall experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3441e260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learningrate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>y_transformation</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>gamma</th>\n",
       "      <th>...</th>\n",
       "      <th>process_type</th>\n",
       "      <th>process_memory</th>\n",
       "      <th>inter_arrival_time</th>\n",
       "      <th>process_stability_scale</th>\n",
       "      <th>resource_availability_p</th>\n",
       "      <th>resource_availability_n</th>\n",
       "      <th>resource_availability_m</th>\n",
       "      <th>activity_duration_lambda_range</th>\n",
       "      <th>repetition</th>\n",
       "      <th>RUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_units  num_blocks  epochs  batch_size  learningrate optimizer  \\\n",
       "0        50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "1        50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "2        50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "3        50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "4        50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "5        50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "6        50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "7        50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "8        50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "9        50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "10       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "11       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "12       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "13       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "14       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "15       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "16       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "17       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "18       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "19       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "20       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "21       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "22       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "23       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "24       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "25       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "26       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "27       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "28       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "29       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "30       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "31       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "32       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "33       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "34       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "35       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "36       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "37       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "38       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "39       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "40       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "41       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "42       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "43       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "44       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "45       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "46       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "47       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "48       50.0         1.0   100.0       128.0          0.01       SGD   \n",
       "49       50.0         1.0   100.0       256.0          0.01       SGD   \n",
       "\n",
       "    dropout y_transformation loss_function  gamma  ...  process_type  \\\n",
       "0       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "1       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "2       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "3       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "4       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "5       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "6       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "7       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "8       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "9       0.2         standard        MAE_td    0.0  ...        memory   \n",
       "10      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "11      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "12      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "13      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "14      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "15      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "16      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "17      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "18      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "19      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "20      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "21      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "22      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "23      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "24      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "25      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "26      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "27      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "28      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "29      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "30      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "31      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "32      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "33      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "34      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "35      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "36      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "37      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "38      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "39      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "40      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "41      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "42      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "43      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "44      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "45      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "46      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "47      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "48      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "49      0.2         standard        MAE_td    0.0  ...        memory   \n",
       "\n",
       "    process_memory  inter_arrival_time  process_stability_scale  \\\n",
       "0             15.0                 1.5                      0.1   \n",
       "1             15.0                 1.5                      0.1   \n",
       "2             15.0                 1.5                      0.1   \n",
       "3             15.0                 1.5                      0.1   \n",
       "4             15.0                 1.5                      0.1   \n",
       "5             15.0                 1.5                      0.1   \n",
       "6             15.0                 1.5                      0.1   \n",
       "7             15.0                 1.5                      0.1   \n",
       "8             15.0                 1.5                      0.1   \n",
       "9             15.0                 1.5                      0.1   \n",
       "10            15.0                 1.5                      0.1   \n",
       "11            15.0                 1.5                      0.1   \n",
       "12            15.0                 1.5                      0.1   \n",
       "13            15.0                 1.5                      0.1   \n",
       "14            15.0                 1.5                      0.1   \n",
       "15            15.0                 1.5                      0.1   \n",
       "16            15.0                 1.5                      0.1   \n",
       "17            15.0                 1.5                      0.1   \n",
       "18            15.0                 1.5                      0.1   \n",
       "19            15.0                 1.5                      0.1   \n",
       "20            15.0                 1.5                      0.1   \n",
       "21            15.0                 1.5                      0.1   \n",
       "22            15.0                 1.5                      0.1   \n",
       "23            15.0                 1.5                      0.1   \n",
       "24            15.0                 1.5                      0.1   \n",
       "25            15.0                 1.5                      0.1   \n",
       "26            15.0                 1.5                      0.1   \n",
       "27            15.0                 1.5                      0.1   \n",
       "28            15.0                 1.5                      0.1   \n",
       "29            15.0                 1.5                      0.1   \n",
       "30            15.0                 1.5                      0.1   \n",
       "31            15.0                 1.5                      0.1   \n",
       "32            15.0                 1.5                      0.1   \n",
       "33            15.0                 1.5                      0.1   \n",
       "34            15.0                 1.5                      0.1   \n",
       "35            15.0                 1.5                      0.1   \n",
       "36            15.0                 1.5                      0.1   \n",
       "37            15.0                 1.5                      0.1   \n",
       "38            15.0                 1.5                      0.1   \n",
       "39            15.0                 1.5                      0.1   \n",
       "40            15.0                 1.5                      0.1   \n",
       "41            15.0                 1.5                      0.1   \n",
       "42            15.0                 1.5                      0.1   \n",
       "43            15.0                 1.5                      0.1   \n",
       "44            15.0                 1.5                      0.1   \n",
       "45            15.0                 1.5                      0.1   \n",
       "46            15.0                 1.5                      0.1   \n",
       "47            15.0                 1.5                      0.1   \n",
       "48            15.0                 1.5                      0.1   \n",
       "49            15.0                 1.5                      0.1   \n",
       "\n",
       "   resource_availability_p resource_availability_n  resource_availability_m  \\\n",
       "0                     0.25                     3.0                    0.041   \n",
       "1                     0.25                     3.0                    0.041   \n",
       "2                     0.25                     3.0                    0.041   \n",
       "3                     0.25                     3.0                    0.041   \n",
       "4                     0.25                     3.0                    0.041   \n",
       "5                     0.25                     3.0                    0.041   \n",
       "6                     0.25                     3.0                    0.041   \n",
       "7                     0.25                     3.0                    0.041   \n",
       "8                     0.25                     3.0                    0.041   \n",
       "9                     0.25                     3.0                    0.041   \n",
       "10                    0.25                     3.0                    0.041   \n",
       "11                    0.25                     3.0                    0.041   \n",
       "12                    0.25                     3.0                    0.041   \n",
       "13                    0.25                     3.0                    0.041   \n",
       "14                    0.25                     3.0                    0.041   \n",
       "15                    0.25                     3.0                    0.041   \n",
       "16                    0.25                     3.0                    0.041   \n",
       "17                    0.25                     3.0                    0.041   \n",
       "18                    0.25                     3.0                    0.041   \n",
       "19                    0.25                     3.0                    0.041   \n",
       "20                    0.25                     3.0                    0.041   \n",
       "21                    0.25                     3.0                    0.041   \n",
       "22                    0.25                     3.0                    0.041   \n",
       "23                    0.25                     3.0                    0.041   \n",
       "24                    0.25                     3.0                    0.041   \n",
       "25                    0.25                     3.0                    0.041   \n",
       "26                    0.25                     3.0                    0.041   \n",
       "27                    0.25                     3.0                    0.041   \n",
       "28                    0.25                     3.0                    0.041   \n",
       "29                    0.25                     3.0                    0.041   \n",
       "30                    0.25                     3.0                    0.041   \n",
       "31                    0.25                     3.0                    0.041   \n",
       "32                    0.25                     3.0                    0.041   \n",
       "33                    0.25                     3.0                    0.041   \n",
       "34                    0.25                     3.0                    0.041   \n",
       "35                    0.25                     3.0                    0.041   \n",
       "36                    0.25                     3.0                    0.041   \n",
       "37                    0.25                     3.0                    0.041   \n",
       "38                    0.25                     3.0                    0.041   \n",
       "39                    0.25                     3.0                    0.041   \n",
       "40                    0.25                     3.0                    0.041   \n",
       "41                    0.25                     3.0                    0.041   \n",
       "42                    0.25                     3.0                    0.041   \n",
       "43                    0.25                     3.0                    0.041   \n",
       "44                    0.25                     3.0                    0.041   \n",
       "45                    0.25                     3.0                    0.041   \n",
       "46                    0.25                     3.0                    0.041   \n",
       "47                    0.25                     3.0                    0.041   \n",
       "48                    0.25                     3.0                    0.041   \n",
       "49                    0.25                     3.0                    0.041   \n",
       "\n",
       "    activity_duration_lambda_range  repetition  RUN  \n",
       "0                              0.5         1.0    0  \n",
       "1                              0.5         1.0    1  \n",
       "2                              0.5         1.0    2  \n",
       "3                              0.5         1.0    3  \n",
       "4                              0.5         1.0    4  \n",
       "5                              0.5         1.0    5  \n",
       "6                              0.5         1.0    6  \n",
       "7                              0.5         1.0    7  \n",
       "8                              0.5         1.0    8  \n",
       "9                              0.5         1.0    9  \n",
       "10                             0.5         1.0   10  \n",
       "11                             0.5         1.0   11  \n",
       "12                             0.5         1.0   12  \n",
       "13                             0.5         1.0   13  \n",
       "14                             0.5         1.0   14  \n",
       "15                             0.5         1.0   15  \n",
       "16                             0.5         1.0   16  \n",
       "17                             0.5         1.0   17  \n",
       "18                             0.5         1.0   18  \n",
       "19                             0.5         1.0   19  \n",
       "20                             0.5         1.0   20  \n",
       "21                             0.5         1.0   21  \n",
       "22                             0.5         1.0   22  \n",
       "23                             0.5         1.0   23  \n",
       "24                             0.5         1.0   24  \n",
       "25                             0.5         1.0   25  \n",
       "26                             0.5         1.0   26  \n",
       "27                             0.5         1.0   27  \n",
       "28                             0.5         1.0   28  \n",
       "29                             0.5         1.0   29  \n",
       "30                             0.5         1.0   30  \n",
       "31                             0.5         1.0   31  \n",
       "32                             0.5         1.0   32  \n",
       "33                             0.5         1.0   33  \n",
       "34                             0.5         1.0   34  \n",
       "35                             0.5         1.0   35  \n",
       "36                             0.5         1.0   36  \n",
       "37                             0.5         1.0   37  \n",
       "38                             0.5         1.0   38  \n",
       "39                             0.5         1.0   39  \n",
       "40                             0.5         1.0   40  \n",
       "41                             0.5         1.0   41  \n",
       "42                             0.5         1.0   42  \n",
       "43                             0.5         1.0   43  \n",
       "44                             0.5         1.0   44  \n",
       "45                             0.5         1.0   45  \n",
       "46                             0.5         1.0   46  \n",
       "47                             0.5         1.0   47  \n",
       "48                             0.5         1.0   48  \n",
       "49                             0.5         1.0   49  \n",
       "\n",
       "[50 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_settings = {\"num_units\":[50],\n",
    "                \"num_blocks\":[1],\n",
    "                \"epochs\":[100],#,75,100,200],\n",
    "                \"batch_size\":[128,256], #\n",
    "                \"learningrate\":[0.01], \n",
    "                \"optimizer\":[\"SGD\"], #\"Adam\", \"Nadam\", \"SGD\"\n",
    "                \"dropout\":[0.2],\n",
    "                \"y_transformation\":[\"standard\"], #\"log\",\"range\",\"standard\"\n",
    "                \"loss_function\":[\"MAE_td\"],#,\"MAE\"], #\"MAE\",\"MSE\",\"MAE_td\"\n",
    "                \n",
    "                \n",
    "                ######\n",
    "                # F_gamma * K.mean(K.abs(y_pred - y_true)) + F_beta * K.mean((K.abs(y_pred - y_true))/F_alpha*time_weights)\n",
    "                #####\n",
    "                \"gamma\":[0], #np.linspace(start=0.01, stop=1.0, num=5),#[1], #gamma * mean(abs(y_pred-y_true))\n",
    "                \n",
    "                \"beta\":np.linspace(start=0.25, stop=1.5, num=5), #[0.01, 0.02, 0.1], #beta * mean((abs(y_pred-y_true))/alpha*t)\n",
    "                \n",
    "                \"alpha\":np.linspace(start=0.25, stop=1.5, num=5),#[1], #beta * mean((abs(y_pred-y_true))/alpha*t)\n",
    "                \n",
    "                \"number_of_traces\":[100], #100 #5000\n",
    "                \"statespace_size\":[12],\n",
    "                \"process_entropy\":[\"med_entropy\"], #\"min_entropy\",\"med_entropy\",\"max_entropy\"\n",
    "                \"process_type\":[\"memory\"],#,\"memory\"],        #\"memoryless\",\"memory\"        \n",
    "                \"process_memory\":[15],\n",
    "                \n",
    "                        #lambda parameter of inter-arrival times\n",
    "                        \"inter_arrival_time\":[1.5], #1.5\n",
    "                \n",
    "                        #lambda parameter of process noise\n",
    "                        \"process_stability_scale\":[0.1],#0.1,\n",
    "                \n",
    "                        #probability of getting an agent\n",
    "                        \"resource_availability_p\":[0.25], #0.5\n",
    "                \n",
    "                        #number of agents   \n",
    "                        \"resource_availability_n\":[3],\n",
    "                \n",
    "                        #waiting time in days, when no agent is available\n",
    "                        \"resource_availability_m\":[0.041], \n",
    "                \n",
    "                        #variation between activity durations\n",
    "                        \"activity_duration_lambda_range\":[0.5],\n",
    "                \n",
    "                \"repetition\":[1]}\n",
    "\n",
    "\n",
    "# Generate a full factorial:\n",
    "df=build_full_fact(run_settings)#[0:2]\n",
    "\n",
    "\"\"\"\n",
    "Put the right labels back:\n",
    "\"\"\"\n",
    "\n",
    "# Lookup the values from the npy file\n",
    "variables = [\"process_entropy\",\n",
    "             \"process_type\",\n",
    "             \"optimizer\",\n",
    "             \"y_transformation\",\n",
    "             \"loss_function\"]\n",
    "\n",
    "if 'Name_fix' not in df:\n",
    "    df[\"Name_fix\"] = 0\n",
    "    for variable in variables:\n",
    "        for run in df.index:\n",
    "            idx = int(df[variable].loc[run])\n",
    "            value = run_settings[variable][idx]\n",
    "            df[variable].loc[run] = value\n",
    "    df[\"Name_fix\"] = 1\n",
    "\n",
    "if 'Name_fix' in df.columns:\n",
    "     df = df.drop(\"Name_fix\",axis=1)\n",
    "\n",
    "# Important variables\n",
    "df[\"RUN\"] = df.index #+ 1\n",
    "# df[\"Done\"] = 0\n",
    "# df[\"Failure\"] = 0\n",
    "\n",
    "#change types\n",
    "df.statespace_size = df.statespace_size.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15eac954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_D(statespace):\n",
    "    D=list(range(1, statespace + 1))\n",
    "    D = [\"S\"+str(s) for s in D]\n",
    "    return D\n",
    "\n",
    "def make_workweek(workweek):\n",
    "    if workweek == \"weekdays\":\n",
    "        # CLOSED HOURS FROM\n",
    "        W = [[0.001, #monday\n",
    "             1, #tuesday\n",
    "             2, #wednesday\n",
    "             3, #thursday\n",
    "             4, #friday\n",
    "             5],\n",
    "            # TO\n",
    "            [0.5, #monday\n",
    "             1.5, #tuesday\n",
    "             2.5, #wednesday\n",
    "             3.5, #thursday\n",
    "             4.5, #friday\n",
    "             7.5]]  #weekend-closed\n",
    "\n",
    "    if workweek == \"all-week\":\n",
    "        # CLOSED HOURS FROM\n",
    "        W = [[0.001,\n",
    "             1, \n",
    "             2, \n",
    "             3,\n",
    "             4, \n",
    "             5,\n",
    "             6,\n",
    "             7],\n",
    "            # TO\n",
    "            [0.5, #monday\n",
    "             1.5, #tuesday\n",
    "             2.5, #wednesday\n",
    "             3.5, #thursday\n",
    "             4.5, #friday\n",
    "             5.5, #saturday\n",
    "             6.5, #sunday\n",
    "             7.5]] \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634d541-d56e-484b-b9f2-cc952cdd87b7",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f161ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rt_model(data_objects, curr_settings):\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "    import time\n",
    "\n",
    "    def AE(y, y_hat):\n",
    "        AE = np.abs(y - y_hat)\n",
    "        return AE\n",
    "\n",
    "\n",
    "    # Aggregated metrics:\n",
    "\n",
    "    # MAE = np.mean(AE)\n",
    "    # MEP = np.mean(EP)\n",
    "\n",
    "    def ear(table, alpha=2):    \n",
    "        #absolute error\n",
    "        table[\"AE\"] = AE(table[\"y\"], table[\"y_pred\"])\n",
    "\n",
    "        #temporal decay\n",
    "        table[\"AE_td\"] = table[\"AE\"]/alpha*table[\"prefix_number\"]\n",
    "\n",
    "        # Calculate metrics:\n",
    "\n",
    "        # MAE:\n",
    "        MAE_td = np.mean(table[\"AE_td\"])\n",
    "\n",
    "        #Aggregated metrics:\n",
    "        table[\"MAE_td\"] = MAE_td\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "    def acc(table):\n",
    "        \"\"\"\n",
    "        Evaluates earliness alone\n",
    "\n",
    "        Input: \n",
    "            Inference table with relevant variables\n",
    "\n",
    "                T = \"num_events\"\n",
    "                t = \"event_number\"\n",
    "\n",
    "                case = \"caseid\"\n",
    "                y = \"y\"\n",
    "                y_pred = \"y_pred\"\n",
    "\n",
    "        Output:\n",
    "            EP, AE, MEP, MAE, MAEPE\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate metrics:\n",
    "\n",
    "        #Absolute error\n",
    "        table[\"AE\"] = AE(table[\"y\"], table[\"y_pred\"])\n",
    "\n",
    "        # MAE:\n",
    "        MAE = np.mean(table[\"AE\"])\n",
    "\n",
    "        #Aggregated metrics:\n",
    "        table[\"MAE\"] = MAE\n",
    "\n",
    "        return table\n",
    "\n",
    "\n",
    "\n",
    "    def Earliness(Inference_test, parallel=False, EAR=True, TS=True):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        \"\"\"\n",
    "        Earliness\n",
    "        \"\"\"\n",
    "        print(\"Earliness...\")\n",
    "        Inference_test = acc(Inference_test)\n",
    "        Inference_test = ear(Inference_test, alpha=1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        Time_sec = end_time - start_time\n",
    "        print(Time_sec)\n",
    "\n",
    "        return Inference_test\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluation\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Time information\n",
    "    time_taken = 0\n",
    "    now = datetime.now() # current date and time\n",
    "    timestamp = now.strftime(\"%Y/%m/%d, %H:%M:%S\")\n",
    "\n",
    "    if \"train_time\" in data_objects:\n",
    "        time_taken = data_objects[\"train_time\"]\n",
    "\n",
    "    ###################################################################\n",
    "\n",
    "    # get the data\n",
    "    Inference_test = data_objects[\"Inference_test\"]\n",
    "    model_params = data_objects[\"model_params\"]\n",
    "\n",
    "    ###################################################################\n",
    "    # Calculate earliness    \n",
    "    Inference_test = Earliness(Inference_test, EAR=True)\n",
    "    \n",
    "    ########## ACCURACY #######################\n",
    "\n",
    "    mae_test = np.mean(Inference_test[\"MAE\"])/(24.0*3600)\n",
    "    maetd_test = np.mean(Inference_test[\"MAE_td\"])/(24.0*3600)\n",
    "\n",
    "    # Print status:\n",
    "    print('_'*60)\n",
    "    print('Test MAE:     ', mae_test, ' (days)')\n",
    "    print(\"================================\"*3)\n",
    "    \n",
    "    ########### Generate a report on individual level ########\n",
    "    # Generate report with parameters of interest\n",
    "    results = {\"RUN\":0,\n",
    "               \"Time\":timestamp,\n",
    "               \"Traintime\":time_taken,\n",
    "               \"MAE\":mae_test,\n",
    "               \"MAE_td\":maetd_test}\n",
    "\n",
    "    report = dict(results, **model_params)\n",
    "    report = pd.DataFrame(report, index=[0])\n",
    "    # Storing individual perf\n",
    "    #report.to_csv(\"report.csv\",index=False)\n",
    "\n",
    "    # store report\n",
    "    data_objects[\"report\"] = report\n",
    "    \n",
    "    #fixed\n",
    "    events = 5\n",
    "\n",
    "    inf_results = Inference_test.loc[Inference_test.event_number < events+1]\n",
    "    inf_results.AE = inf_results.AE /(24.0*3600)\n",
    "\n",
    "    pivottable = pd.pivot_table(inf_results, \n",
    "                           values='AE',\n",
    "                        columns=['event_number'], aggfunc=np.mean)\n",
    "\n",
    "    newcols = []\n",
    "    for colno in range(0,len(pivottable.columns)):\n",
    "        colno = colno + 1\n",
    "        name = \"AE_\"+str(colno)+\"\"\n",
    "        newcols.append(name)\n",
    "\n",
    "    pivottable.columns = newcols\n",
    "    pivottable.index = [0]\n",
    "\n",
    "    # store prefix performance\n",
    "    data_objects[\"prefix_performance\"] = pivottable\n",
    "    \n",
    "    return data_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36996d8d-a7fd-49ec-8160-ebe2ce8d8e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f783fe43-098f-4859-93e4-53779efde40e",
   "metadata": {},
   "source": [
    "# Training imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3404d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras import metrics, regularizers\n",
    "#from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.optimizers import Nadam, Adam\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as Kc\n",
    "\n",
    "# TF2: Disable eager execution\n",
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\"\"\"\n",
    "import time\n",
    "from datetime import datetime\n",
    "from random import randrange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Nadam, Adam, SGD\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as Kc\n",
    "\n",
    "# TF2: Mixed precision\n",
    "\n",
    "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "#mixed_precision.set_policy('mixed_float16')\n",
    "\n",
    "# TF2: Disable eager execution\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Callback for tracking training time per epoch:\n",
    "class TimeHistory(Kc.Callback): #callbacks.\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d972b-75a8-47ca-aacb-0f888ffb74b4",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605ec065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_objects, modelparams, curr_settings):\n",
    "    #### Weights and biases ####\n",
    "    \n",
    "    from wandb.keras import WandbCallback\n",
    "    \n",
    "    #### Load the data ########\n",
    "    x_train, y_train = data_objects[\"x_train\"], data_objects[\"y_train\"]\n",
    "    x_test, y_test = data_objects[\"x_test\"], data_objects[\"y_test\"]\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "    # Transformations: Input\n",
    "    ##########################################################\n",
    "\n",
    "    #Standardize to mean 0, sd 1\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    #Transform Train:\n",
    "    n = x_train.shape[0]\n",
    "    t = x_train.shape[1]\n",
    "    k = x_train.shape[2]\n",
    "\n",
    "    x_train_2d = x_train.reshape(n,t*k)\n",
    "\n",
    "    sc.fit_transform(x_train_2d)\n",
    "    x_train_2d = sc.transform(x_train_2d)\n",
    "\n",
    "    x_train = x_train_2d.reshape(n,t,k)\n",
    "\n",
    "    #Transform Test:\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    n = x_test.shape[0]\n",
    "    t = x_test.shape[1]\n",
    "    k = x_test.shape[2]\n",
    "\n",
    "    x_test_2d = x_test.reshape(n,t*k)\n",
    "\n",
    "    sc.fit_transform(x_test_2d)\n",
    "    x_test_2d = sc.transform(x_test_2d)\n",
    "\n",
    "    x_test = x_test_2d.reshape(n,t,k)\n",
    "\n",
    "    ##########################################################\n",
    "    # Transformations: Target\n",
    "    ##########################################################\n",
    "    \n",
    "    y_transformation = curr_settings[\"y_transformation\"]\n",
    "        \n",
    "    #Normalize to mean 0, sd 1\n",
    "    if y_transformation == \"standard\":  \n",
    "        #Standardize\n",
    "        sc_train = StandardScaler()\n",
    "        sc_test = StandardScaler()\n",
    "        \n",
    "        #Transform Train:\n",
    "        sc_train.fit_transform(y_train)\n",
    "        y_train = sc_train.transform(y_train)\n",
    "        \n",
    "        #Transform Test:\n",
    "        sc_test.fit_transform(y_test)\n",
    "        y_test = sc_test.transform(y_test)\n",
    "        \n",
    "    # Transformations:\n",
    "    if y_transformation==\"range\":\n",
    "        \n",
    "        y_train_min = np.min(y_train)\n",
    "        y_train_max = np.max(y_train)\n",
    "        \n",
    "        y_test_min = np.min(y_test)\n",
    "        y_test_max = np.max(y_test)\n",
    "        \n",
    "        # Train data\n",
    "        y_train = (y_train -  y_train_min)/(y_test_max - y_train_min)\n",
    "        y_test = (y_test -  y_test_min)/(y_test_max - y_test_min)\n",
    "        \n",
    "        #Inverse range transform\n",
    "        #Inference_test[\"y_pred\"] = (Inference_test[\"y_pred\"] * (data_objects[\"y_test_max\"] - data_objects[\"y_test_min\"])) + data_objects[\"y_test_min\"]\n",
    "    \n",
    "                \n",
    "    if y_transformation==\"log\":\n",
    "        #Log-transform\n",
    "        y_train = np.log(1+y_train)\n",
    "        y_test = np.log(1+y_test)    \n",
    "        \n",
    "        # Inference data\n",
    "        #Inference_test[\"y\"] = np.log(1+Inference_test[\"y\"])\n",
    "        #Inference_test[\"y_t\"] = np.log(1+Inference_test[\"y_t\"])\n",
    "        \n",
    "        #Inverse log transform\n",
    "        #Inference_test[\"y_pred\"] = np.exp(#Inference_test[\"y_pred\"])-1\n",
    "    \n",
    "\n",
    "    # Get prefix log characteristics for calculating the loss:\n",
    "    max_prefix_length = x_train.shape[1]\n",
    "    n_obs = x_train.shape[0]\n",
    "    cases = int(n_obs/max_prefix_length)\n",
    "\n",
    "    ##########################################################\n",
    "    # Model Type/Architecture\n",
    "    ##########################################################\n",
    "    # Clear the TF graph\n",
    "    K.clear_session()\n",
    "\n",
    "    print(\"Input data shape:\",x_train.shape)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # Number of block layers\n",
    "    BLOCK_LAYERS = modelparams[\"BLOCK_LAYERS\"]\n",
    "\n",
    "    # Number of cells\n",
    "    FULLY_CONNECTED = modelparams[\"FULLY_CONNECTED\"]\n",
    "\n",
    "    #CNN related params\n",
    "    DROPOUT_RATE = modelparams[\"DROPOUT_RATE\"]\n",
    "\n",
    "\n",
    "    #####################################\n",
    "\n",
    "    input_dim = (x_train.shape[1], x_train.shape[2])\n",
    "    print(\"Input data shape:\",input_dim)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(FULLY_CONNECTED,  implementation=2, \n",
    "                         recurrent_dropout=DROPOUT_RATE, \n",
    "                         input_shape=input_dim,\n",
    "                         return_sequences=False))\n",
    "\n",
    "    model.add(Dense(1)) #, dtype='float32' #Only the softmax is adviced to be float32 \n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    ##########################################################\n",
    "    # Loss function\n",
    "    ##########################################################\n",
    "\n",
    "    F_lossfunction = modelparams[\"lossfunction\"]\n",
    "    F_epochs = modelparams[\"epochs\"]\n",
    "    F_batch_size = modelparams[\"batch_size\"]\n",
    "\n",
    "    F_alpha = modelparams[\"alpha\"]\n",
    "    F_beta = modelparams[\"beta\"]\n",
    "    F_gamma = modelparams[\"gamma\"]\n",
    "\n",
    "\n",
    "    if F_lossfunction == \"MAE_td\":\n",
    "        ## Generate the weight vector:\n",
    "        prefix_weights = []\n",
    "\n",
    "        for case in range(0,cases):\n",
    "            for event in range(1,max_prefix_length+1):\n",
    "                #weight = 1/(event+1)\n",
    "                #weight = (max_prefix_length-event)/max_prefix_length\n",
    "                weight = event\n",
    "                prefix_weights.append(weight)\n",
    "\n",
    "        ## Save the penalty vector\n",
    "        temporal_weight_vector = np.asarray(prefix_weights)\n",
    "\n",
    "        ## Specify the form of the loss\n",
    "        def MAE_td(time_weights, F_beta, F_alpha):\n",
    "\n",
    "            def mae_w(y_true, y_pred):\n",
    "\n",
    "                return F_gamma * K.mean(K.abs(y_pred - y_true), axis=-1) + F_beta * K.mean((K.abs(y_pred - y_true))/F_alpha*time_weights, axis=-1)\n",
    "\n",
    "            return mae_w\n",
    "\n",
    "        ## replace the loss with\n",
    "        loss_func = MAE_td(temporal_weight_vector, F_beta, F_alpha)\n",
    "\n",
    "\n",
    "\n",
    "    if F_lossfunction == \"MAE\":\n",
    "        loss_func = \"mae\"\n",
    "\n",
    "    if F_lossfunction == \"MSE\":\n",
    "        loss_func = \"mse\"\n",
    "        \n",
    "    ##########################################################\n",
    "    # Logging: AE_t\n",
    "    ##########################################################\n",
    "    ## Generate the weight vector:\n",
    "    \n",
    "    \n",
    "    def MAE_t1(cases, max_prefix_length, timestep=1):\n",
    "        timestep_weights = []\n",
    "\n",
    "        for case in range(0,cases):\n",
    "            for event in range(1,max_prefix_length+1):\n",
    "\n",
    "                if event == timestep:\n",
    "                    weight = 1\n",
    "                else:\n",
    "                    weight = 0\n",
    "\n",
    "                timestep_weights.append(weight)\n",
    "        \n",
    "\n",
    "            def mae_t1(y_true, y_pred):\n",
    "\n",
    "                return K.mean(K.abs(y_pred - y_true)*timestep_weights, axis=-1)\n",
    "\n",
    "            return mae_t1\n",
    "        \n",
    "    def MAE_t2(cases, max_prefix_length, timestep=2):\n",
    "        timestep_weights2 = []\n",
    "\n",
    "        for case in range(0,cases):\n",
    "            for event in range(1,max_prefix_length+1):\n",
    "\n",
    "                if event == timestep:\n",
    "                    weight = 1\n",
    "                else:\n",
    "                    weight = 0\n",
    "\n",
    "                timestep_weights2.append(weight)\n",
    "        \n",
    "\n",
    "            def mae_t2(y_true, y_pred):\n",
    "\n",
    "                return K.mean(K.abs(y_pred - y_true)*timestep_weights2, axis=-1)\n",
    "\n",
    "            return mae_t2\n",
    "\n",
    "    def MAE_t3(cases, max_prefix_length, timestep=3):\n",
    "        timestep_weights = []\n",
    "\n",
    "        for case in range(0,cases):\n",
    "            for event in range(1,max_prefix_length+1):\n",
    "\n",
    "                if event == timestep:\n",
    "                    weight = 1\n",
    "                else:\n",
    "                    weight = 0\n",
    "\n",
    "                timestep_weights.append(weight)\n",
    "        \n",
    "\n",
    "            def mae_t3(y_true, y_pred):\n",
    "\n",
    "                return K.mean(K.abs(y_pred - y_true)*timestep_weights, axis=-1)\n",
    "\n",
    "            return mae_t3\n",
    "    \n",
    "    ##########################################################\n",
    "    # Train assist\n",
    "    ##########################################################\n",
    "\n",
    "    ## Helper function to show mean predicted remaining time\n",
    "    def mean_pred(y_true, y_pred):\n",
    "        return K.mean(K.abs(y_pred), axis=-1)\n",
    "\n",
    "\n",
    "    time_callback = TimeHistory()\n",
    "\n",
    "#     early_stopping = EarlyStopping(patience=42)    #42 for the navarini paper\n",
    "\n",
    "#     #Reduce learning rate (same for both intial and final)\n",
    "#     lr_reducer = ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                    factor=0.5, \n",
    "#                                    patience=10, \n",
    "#                                    verbose=1, \n",
    "#                                    mode='auto', \n",
    "#                                    epsilon=0.0001, \n",
    "#                                    cooldown=0, \n",
    "#                                    min_lr=0)\n",
    "\n",
    "    # Optimizer\n",
    "    if modelparams[\"optimizer\"] == \"Adam\":\n",
    "        optimizer = Adam(learning_rate=modelparams[\"learningrate\"])\n",
    "\n",
    "    if modelparams[\"optimizer\"] == \"Nadam\":\n",
    "        optimizer = Nadam(learning_rate=modelparams[\"learningrate\"])\n",
    "\n",
    "    if modelparams[\"optimizer\"] == \"SGD\":\n",
    "        optimizer = SGD(learning_rate=modelparams[\"learningrate\"], momentum=0.9)\n",
    "\n",
    "        \n",
    "    print(\"y_train mean:\",np.mean(y_train))\n",
    "    print(\"x_train mean:\",np.mean(x_train))\n",
    "    print(\"y_test mean:\",np.mean(y_test))\n",
    "    print(\"x_test mean:\",np.mean(x_test))\n",
    "\n",
    "\n",
    "    model.compile(loss=loss_func, \n",
    "          optimizer=optimizer, \n",
    "          metrics=[\"mae\",\n",
    "                   mean_pred, \n",
    "                   MAE_t1(cases, max_prefix_length, timestep=1)])\n",
    "\n",
    "    # Store starttime\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=int(F_batch_size),\n",
    "                  callbacks=[#early_stopping,\n",
    "                             #model_checkpoint,\n",
    "                            #lr_reducer,\n",
    "                            #csv_logger, \n",
    "                            WandbCallback(),\n",
    "                            time_callback],\n",
    "              epochs=F_epochs,\n",
    "              verbose=1,\n",
    "              #validation_steps=128,\n",
    "              validation_split=0.2)\n",
    "\n",
    "\n",
    "    # REPLACE with best version of the model (checkpoint)\n",
    "    #     model = load_model(filename, compile=False)\n",
    "\n",
    "    # Store endtime\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store epoch times in already existing CSV-history\n",
    "    epoch_times = time_callback.times\n",
    "    \n",
    "    ########################################################\n",
    "    # PREDICT\n",
    "    ########################################################\n",
    "    \n",
    "    # Load inference tables\n",
    "    Inference_test = data_objects[\"Inference_test\"]   \n",
    "\n",
    "    # Load the model\n",
    "    #model = data_objects[\"model\"]\n",
    "\n",
    "    # Predict on inference table\n",
    "    y_pred = model.predict(x_test, verbose=1, batch_size=2048)\n",
    "    Inference_test[\"y_pred\"] = y_pred\n",
    "    \n",
    "    \n",
    "    #Log transform inverse  \n",
    "    if y_transformation == \"log\":    \n",
    "        Inference_test[\"y_pred\"] = np.exp(y_pred)-1\n",
    "        Inference_test[\"y_test\"] = np.exp(y_test)-1\n",
    "        print(\"log\"*155)\n",
    "        #Inference_test[\"y\"] = np.exp(Inference_test[\"y\"])-1\n",
    "\n",
    "    #Range transform inverse  \n",
    "    if y_transformation == \"range\":\n",
    "        Inference_test[\"y_pred\"] = (y_pred * (y_test_max - y_test_min)) + y_test_min\n",
    "        Inference_test[\"y_test\"] = (y_test * (y_test_max - y_test_min)) + y_test_min\n",
    "        print(\"range\"*155)\n",
    "        \n",
    "          \n",
    "    #Normalize to mean 0, sd 1\n",
    "    if y_transformation == \"standard\":  \n",
    "        #Standardize backtransform\n",
    "        \n",
    "        #Transform Train:\n",
    "        \n",
    "        #Transform Test:\n",
    "        y_pred = sc_test.inverse_transform(y_pred)\n",
    "        Inference_test[\"y_pred\"] = y_pred\n",
    "        \n",
    "        \n",
    "        y_test = sc_test.inverse_transform(y_test)\n",
    "        Inference_test[\"y_test\"] = y_test\n",
    "    \n",
    "  \n",
    "    ########################################################\n",
    "    # Store results\n",
    "    ########################################################\n",
    "    #     hist = pd.read_csv(csvfilename)\n",
    "    #     hist[\"duration_sec\"] = epoch_times\n",
    "    #     hist.to_csv(csvfilename,index=False)\n",
    "    #     epochs = len(hist)\n",
    "\n",
    "\n",
    "    #     ## Save model for later inference\n",
    "    #     model.save(filename)\n",
    "    #     print(\"Done. Model saved as: \", filename)\n",
    "\n",
    "    ## Store model in data object\n",
    "    data_objects[\"model\"] = model\n",
    "    data_objects[\"epoch_times\"] = epoch_times\n",
    "\n",
    "    ## Store the parameters in data object:\n",
    "    data_objects[\"model_params\"] = modelparams\n",
    "\n",
    "    #Store number of epochs\n",
    "    data_objects[\"epochs\"] = F_epochs\n",
    "\n",
    "    ## Log the full time for training:\n",
    "    time_sec = end_time - start_time\n",
    "\n",
    "    ## Store train time in data object\n",
    "    data_objects[\"train_time\"] = time_sec\n",
    "\n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e15e35-1f92-4511-bbd3-1424e413a419",
   "metadata": {},
   "source": [
    "# Freeze the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d42d481-fb46-40f9-95a6-4dba6539e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822\n",
      "Cases before dropping len=1: 100 cases 822 rows\n",
      "Cases after dropping len=1: 87 cases 809 rows\n",
      "Sorting by id, date (chronological order)\n",
      "Number of cases in log: 87\n",
      "longest trace is: 15\n",
      "Time format: 1970-01-05 20:21:53\n",
      "Std. format: %Y-%m-%d %H:%M:%S\n",
      "   id event  activity_no                 time        end_datetime\n",
      "0   1    S2            2  1970-01-05 20:21:53 1970-01-06 01:02:19\n",
      "1   1    S3            1  1970-01-05 21:44:42 1970-01-05 19:30:31\n",
      "2   1    S3            3  1970-01-06 12:00:00 1970-01-06 13:01:50\n",
      "3   1    S4            4  1970-01-06 13:46:20 1970-01-06 14:16:55\n",
      "4   1   S12            5  1970-01-06 14:57:03 1970-01-06 16:27:38\n",
      "mode: event\n",
      "**********************************************************************************************************************************************************************************************************************************************************\n",
      "=======================================\n",
      "Log starts at: 1970-01-05 12:00:00\n",
      "Last event starts at: 1970-01-11 23:09:54\n",
      "Train-test split happens at: 1970-01-09 12:00:00\n",
      "=======================================\n",
      "87\n",
      "87\n",
      "440\n",
      "=======================================\n",
      "============================\n",
      "Post-processing:\n",
      "============================\n",
      "\n",
      "dropping last event from each case\n",
      "before: 809\n",
      "after: 722\n",
      "data in X is the 15 last events, excluding the final event\n",
      "\n",
      "dropping vars:\n",
      "dropping last event category from y_a: ['y_a_t1_1']\n",
      "dropping vars from X:  ['y_timetofinish', 'y_timetonextev', 'next_activity', 'y_a_t1_1', 'y_a_t1_2', 'y_a_t1_3', 'y_a_t1_4', 'y_a_t1_5', 'y_a_t1_6', 'y_a_t1_7', 'y_a_t1_8', 'y_a_t1_9', 'y_a_t1_10', 'y_a_t1_11', 'y_a_t1_12', 'y_a_t1_13']\n",
      "\n",
      "\n",
      "Output length: 10830\n",
      "Generating case data\n",
      "done\n",
      "Making casestats..\n",
      "Done.\n",
      "merging..\n",
      "10830\n",
      "10830\n",
      "722\n",
      "722\n",
      "722\n",
      "722\n",
      "722\n",
      "722\n",
      "reshaping..\n",
      "Trainset size (with prefixes of  15 ): 646\n",
      "Testset size (with prefixes of  15 ): 76\n",
      "==========================================\n",
      "X: observations, timesteps, vars\n",
      "(646, 15, 18)\n",
      "y_train: observations, labels\n",
      "(646, 1)\n",
      "y_t_train: observations, labels\n",
      "(646, 1)\n",
      "y_a_train: observations, labels\n",
      "(646, 12)\n",
      "Inference train: 646\n",
      "Inference test:  76\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "settings for simulation\n",
    "\"\"\"\n",
    "# take the settings of the first run\n",
    "curr_settings = df.loc[0]\n",
    "\n",
    "SIM_SETTINGS = {\"save_eventlog\":0, #0 = no, 1 = yes... Standard destination: A:\\Process_sim\n",
    "\n",
    "            \"statespace_size\":make_D(int(curr_settings[\"statespace_size\"])),\n",
    "\n",
    "            \"number_of_traces\":int(curr_settings[\"number_of_traces\"]),  \n",
    "\n",
    "            \"process_entropy\":curr_settings[\"process_entropy\"],\n",
    "\n",
    "            \"process_type\":curr_settings[\"process_type\"],                \n",
    "\n",
    "            \"process_memory\":int(curr_settings[\"process_memory\"]),                \n",
    "\n",
    "                                #desired max number of steps:\n",
    "            \"process_settings\":{\"med_ent_e_steps\":5,\n",
    "                                # desired max number of possible transitions in P. \n",
    "                                # NOTE: This can maximally be the number of states, and should be higher than 2\n",
    "                                \"med_ent_n_transitions\":3,\n",
    "                                #max number of trials to find matrix with desired max steps\n",
    "                                \"med_ent_max_trials\":5},\n",
    "\n",
    "            #lambda parameter of inter-arrival times\n",
    "            \"time_settings\":{\"inter_arrival_time\":curr_settings[\"inter_arrival_time\"], \n",
    "                            \"process_stability_scale\":curr_settings[\"process_stability_scale\"],\n",
    "                            \"resource_availability_p\":curr_settings[\"resource_availability_p\"],   \n",
    "                            \"resource_availability_n\":int(curr_settings[\"resource_availability_n\"]),\n",
    "                            \"resource_availability_m\":curr_settings[\"resource_availability_m\"], \n",
    "                            \"activity_duration_lambda_range\":curr_settings[\"activity_duration_lambda_range\"],\n",
    "\n",
    "                            #time-unit for a full week: days = 7, hrs = 24*7, etc.\n",
    "                            \"Deterministic_offset_W\":make_workweek([\"weekdays\",\"all-week\"][1]),\n",
    "\n",
    "                            \"Deterministic_offset_u\":7},\n",
    "\n",
    "            \"run\":0}\n",
    "\n",
    "# generate the log\n",
    "log = Generate_eventlog(SIM_SETTINGS)\n",
    "print(len(log))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Prepare data for modelling\n",
    "\"\"\"\n",
    "# part one\n",
    "Input_data = prepare_data_f_memory(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "118415b8-19b0-4c43-867b-31dea8f98f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>activity</th>\n",
       "      <th>activity_no</th>\n",
       "      <th>y_acc_sum</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>z_t</th>\n",
       "      <th>h_t</th>\n",
       "      <th>b_t</th>\n",
       "      <th>q_t</th>\n",
       "      <th>s_t</th>\n",
       "      <th>v_t</th>\n",
       "      <th>arrival_datetime</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>start_day</th>\n",
       "      <th>start_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.812867</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.812867</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.906049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>1970-01-05 19:29:17</td>\n",
       "      <td>1970-01-05 21:44:42</td>\n",
       "      <td>1970-01-05 19:30:31</td>\n",
       "      <td>Monday</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>S2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.856145</td>\n",
       "      <td>0.812867</td>\n",
       "      <td>1.043279</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.035672</td>\n",
       "      <td>0.848539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230412</td>\n",
       "      <td>1970-01-05 19:30:31</td>\n",
       "      <td>1970-01-05 20:21:53</td>\n",
       "      <td>1970-01-06 01:02:19</td>\n",
       "      <td>Monday</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>S3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.399093</td>\n",
       "      <td>1.043279</td>\n",
       "      <td>1.542947</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051790</td>\n",
       "      <td>1.095069</td>\n",
       "      <td>0.404931</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>1970-01-06 01:02:19</td>\n",
       "      <td>1970-01-06 12:00:00</td>\n",
       "      <td>1970-01-06 13:01:50</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>S4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.994185</td>\n",
       "      <td>1.542947</td>\n",
       "      <td>1.595092</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030902</td>\n",
       "      <td>1.573849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>1970-01-06 13:01:50</td>\n",
       "      <td>1970-01-06 13:46:20</td>\n",
       "      <td>1970-01-06 14:16:55</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>S12</td>\n",
       "      <td>5</td>\n",
       "      <td>6.680053</td>\n",
       "      <td>1.595092</td>\n",
       "      <td>1.685868</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027867</td>\n",
       "      <td>1.622960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090776</td>\n",
       "      <td>1970-01-06 14:16:55</td>\n",
       "      <td>1970-01-06 14:57:03</td>\n",
       "      <td>1970-01-06 16:27:38</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>S2</td>\n",
       "      <td>6</td>\n",
       "      <td>8.929396</td>\n",
       "      <td>1.685868</td>\n",
       "      <td>2.249344</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.377947</td>\n",
       "      <td>2.104815</td>\n",
       "      <td>0.395185</td>\n",
       "      <td>0.168291</td>\n",
       "      <td>1970-01-06 16:27:38</td>\n",
       "      <td>1970-01-07 12:00:00</td>\n",
       "      <td>1970-01-07 05:59:03</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>S2</td>\n",
       "      <td>7</td>\n",
       "      <td>11.437220</td>\n",
       "      <td>2.249344</td>\n",
       "      <td>2.507823</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.245493</td>\n",
       "      <td>2.494836</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.253316</td>\n",
       "      <td>1970-01-07 05:59:03</td>\n",
       "      <td>1970-01-07 12:00:00</td>\n",
       "      <td>1970-01-07 12:11:15</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>S7</td>\n",
       "      <td>8</td>\n",
       "      <td>14.750515</td>\n",
       "      <td>2.507823</td>\n",
       "      <td>3.313296</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>2.535370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.805472</td>\n",
       "      <td>1970-01-07 12:11:15</td>\n",
       "      <td>1970-01-07 12:50:55</td>\n",
       "      <td>1970-01-08 07:31:08</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>S2</td>\n",
       "      <td>9</td>\n",
       "      <td>18.104837</td>\n",
       "      <td>3.313296</td>\n",
       "      <td>3.354322</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.090468</td>\n",
       "      <td>3.485764</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.026790</td>\n",
       "      <td>1970-01-08 07:31:08</td>\n",
       "      <td>1970-01-08 12:00:00</td>\n",
       "      <td>1970-01-08 08:30:13</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>S1</td>\n",
       "      <td>10</td>\n",
       "      <td>22.113876</td>\n",
       "      <td>3.354322</td>\n",
       "      <td>4.009040</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.228932</td>\n",
       "      <td>3.624254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654718</td>\n",
       "      <td>1970-01-08 08:30:13</td>\n",
       "      <td>1970-01-08 14:58:55</td>\n",
       "      <td>1970-01-09 00:13:01</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>S1</td>\n",
       "      <td>11</td>\n",
       "      <td>26.487354</td>\n",
       "      <td>4.009040</td>\n",
       "      <td>4.373477</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.208849</td>\n",
       "      <td>4.217888</td>\n",
       "      <td>0.282112</td>\n",
       "      <td>0.082326</td>\n",
       "      <td>1970-01-09 00:13:01</td>\n",
       "      <td>1970-01-09 12:00:00</td>\n",
       "      <td>1970-01-09 08:57:48</td>\n",
       "      <td>Friday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>S7</td>\n",
       "      <td>12</td>\n",
       "      <td>30.970861</td>\n",
       "      <td>4.373477</td>\n",
       "      <td>4.483507</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.071406</td>\n",
       "      <td>4.526883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110029</td>\n",
       "      <td>1970-01-09 08:57:48</td>\n",
       "      <td>1970-01-09 12:38:42</td>\n",
       "      <td>1970-01-09 11:36:14</td>\n",
       "      <td>Friday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>S11</td>\n",
       "      <td>13</td>\n",
       "      <td>35.988239</td>\n",
       "      <td>4.483507</td>\n",
       "      <td>5.017379</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.132659</td>\n",
       "      <td>4.698166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.533872</td>\n",
       "      <td>1970-01-09 11:36:14</td>\n",
       "      <td>1970-01-09 16:45:21</td>\n",
       "      <td>1970-01-10 00:25:01</td>\n",
       "      <td>Friday</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>S10</td>\n",
       "      <td>14</td>\n",
       "      <td>41.386363</td>\n",
       "      <td>5.017379</td>\n",
       "      <td>5.398123</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.003547</td>\n",
       "      <td>5.143926</td>\n",
       "      <td>0.356074</td>\n",
       "      <td>0.024670</td>\n",
       "      <td>1970-01-10 00:25:01</td>\n",
       "      <td>1970-01-10 12:00:00</td>\n",
       "      <td>1970-01-10 09:33:17</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>S3</td>\n",
       "      <td>15</td>\n",
       "      <td>47.077596</td>\n",
       "      <td>5.398123</td>\n",
       "      <td>5.691233</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>5.444925</td>\n",
       "      <td>0.055075</td>\n",
       "      <td>0.238036</td>\n",
       "      <td>1970-01-10 09:33:17</td>\n",
       "      <td>1970-01-10 12:00:00</td>\n",
       "      <td>1970-01-10 16:35:22</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>S11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994363</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.994363</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.024705</td>\n",
       "      <td>0.951142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108926</td>\n",
       "      <td>1970-01-05 21:15:01</td>\n",
       "      <td>1970-01-05 22:49:38</td>\n",
       "      <td>1970-01-05 23:51:52</td>\n",
       "      <td>Monday</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>S1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.418246</td>\n",
       "      <td>0.994363</td>\n",
       "      <td>1.423883</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>1.082613</td>\n",
       "      <td>0.417387</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>1970-01-05 23:51:52</td>\n",
       "      <td>1970-01-06 12:00:00</td>\n",
       "      <td>1970-01-06 10:10:23</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>S2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.101553</td>\n",
       "      <td>1.423883</td>\n",
       "      <td>1.683307</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.038492</td>\n",
       "      <td>1.462376</td>\n",
       "      <td>0.037624</td>\n",
       "      <td>0.221799</td>\n",
       "      <td>1970-01-06 10:10:23</td>\n",
       "      <td>1970-01-06 12:00:00</td>\n",
       "      <td>1970-01-06 16:23:57</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>S8</td>\n",
       "      <td>4</td>\n",
       "      <td>5.794958</td>\n",
       "      <td>1.683307</td>\n",
       "      <td>1.693405</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>1.689064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010098</td>\n",
       "      <td>1970-01-06 16:23:57</td>\n",
       "      <td>1970-01-06 16:32:15</td>\n",
       "      <td>1970-01-06 16:38:30</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>S5</td>\n",
       "      <td>5</td>\n",
       "      <td>7.522779</td>\n",
       "      <td>1.693405</td>\n",
       "      <td>1.727821</td>\n",
       "      <td>0.885437</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>1.700116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034416</td>\n",
       "      <td>1970-01-06 16:38:30</td>\n",
       "      <td>1970-01-06 16:48:10</td>\n",
       "      <td>1970-01-06 17:28:03</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caseid activity  activity_no  y_acc_sum         X         Y       z_t  \\\n",
       "0       0       S3            1   0.812867  0.812014  0.812867  0.812014   \n",
       "1       0       S2            2   1.856145  0.812867  1.043279  0.812014   \n",
       "2       0       S3            3   3.399093  1.043279  1.542947  0.812014   \n",
       "3       0       S4            4   4.994185  1.542947  1.595092  0.812014   \n",
       "4       0      S12            5   6.680053  1.595092  1.685868  0.812014   \n",
       "5       0       S2            6   8.929396  1.685868  2.249344  0.812014   \n",
       "6       0       S2            7  11.437220  2.249344  2.507823  0.812014   \n",
       "7       0       S7            8  14.750515  2.507823  3.313296  0.812014   \n",
       "8       0       S2            9  18.104837  3.313296  3.354322  0.812014   \n",
       "9       0       S1           10  22.113876  3.354322  4.009040  0.812014   \n",
       "10      0       S1           11  26.487354  4.009040  4.373477  0.812014   \n",
       "11      0       S7           12  30.970861  4.373477  4.483507  0.812014   \n",
       "12      0      S11           13  35.988239  4.483507  5.017379  0.812014   \n",
       "13      0      S10           14  41.386363  5.017379  5.398123  0.812014   \n",
       "14      0       S3           15  47.077596  5.398123  5.691233  0.812014   \n",
       "15      1      S11            1   0.994363  0.885437  0.994363  0.885437   \n",
       "16      1       S1            2   2.418246  0.994363  1.423883  0.885437   \n",
       "17      1       S2            3   4.101553  1.423883  1.683307  0.885437   \n",
       "18      1       S8            4   5.794958  1.683307  1.693405  0.885437   \n",
       "19      1       S5            5   7.522779  1.693405  1.727821  0.885437   \n",
       "\n",
       "      h_t       b_t       q_t       s_t       v_t    arrival_datetime  \\\n",
       "0   0.041  0.053035  0.906049  0.000000  0.000853 1970-01-05 19:29:17   \n",
       "1   0.000  0.035672  0.848539  0.000000  0.230412 1970-01-05 19:30:31   \n",
       "2   0.000  0.051790  1.095069  0.404931  0.094737 1970-01-06 01:02:19   \n",
       "3   0.000  0.030902  1.573849  0.000000  0.052145 1970-01-06 13:01:50   \n",
       "4   0.000  0.027867  1.622960  0.000000  0.090776 1970-01-06 14:16:55   \n",
       "5   0.041  0.377947  2.104815  0.395185  0.168291 1970-01-06 16:27:38   \n",
       "6   0.000  0.245493  2.494836  0.005164  0.253316 1970-01-07 05:59:03   \n",
       "7   0.000  0.027547  2.535370  0.000000  0.805472 1970-01-07 12:11:15   \n",
       "8   0.082  0.090468  3.485764  0.014236  0.026790 1970-01-08 07:31:08   \n",
       "9   0.041  0.228932  3.624254  0.000000  0.654718 1970-01-08 08:30:13   \n",
       "10  0.000  0.208849  4.217888  0.282112  0.082326 1970-01-09 00:13:01   \n",
       "11  0.082  0.071406  4.526883  0.000000  0.110029 1970-01-09 08:57:48   \n",
       "12  0.082  0.132659  4.698166  0.000000  0.533872 1970-01-09 11:36:14   \n",
       "13  0.123  0.003547  5.143926  0.356074  0.024670 1970-01-10 00:25:01   \n",
       "14  0.041  0.005802  5.444925  0.055075  0.238036 1970-01-10 09:33:17   \n",
       "15  0.041  0.024705  0.951142  0.000000  0.108926 1970-01-05 21:15:01   \n",
       "16  0.082  0.006251  1.082613  0.417387  0.012134 1970-01-05 23:51:52   \n",
       "17  0.000  0.038492  1.462376  0.037624  0.221799 1970-01-06 10:10:23   \n",
       "18  0.000  0.005757  1.689064  0.000000  0.010098 1970-01-06 16:23:57   \n",
       "19  0.000  0.006711  1.700116  0.000000  0.034416 1970-01-06 16:38:30   \n",
       "\n",
       "        start_datetime        end_datetime  start_day  start_hour  \n",
       "0  1970-01-05 21:44:42 1970-01-05 19:30:31     Monday          21  \n",
       "1  1970-01-05 20:21:53 1970-01-06 01:02:19     Monday          20  \n",
       "2  1970-01-06 12:00:00 1970-01-06 13:01:50    Tuesday          12  \n",
       "3  1970-01-06 13:46:20 1970-01-06 14:16:55    Tuesday          13  \n",
       "4  1970-01-06 14:57:03 1970-01-06 16:27:38    Tuesday          14  \n",
       "5  1970-01-07 12:00:00 1970-01-07 05:59:03  Wednesday          12  \n",
       "6  1970-01-07 12:00:00 1970-01-07 12:11:15  Wednesday          12  \n",
       "7  1970-01-07 12:50:55 1970-01-08 07:31:08  Wednesday          12  \n",
       "8  1970-01-08 12:00:00 1970-01-08 08:30:13   Thursday          12  \n",
       "9  1970-01-08 14:58:55 1970-01-09 00:13:01   Thursday          14  \n",
       "10 1970-01-09 12:00:00 1970-01-09 08:57:48     Friday          12  \n",
       "11 1970-01-09 12:38:42 1970-01-09 11:36:14     Friday          12  \n",
       "12 1970-01-09 16:45:21 1970-01-10 00:25:01     Friday          16  \n",
       "13 1970-01-10 12:00:00 1970-01-10 09:33:17   Saturday          12  \n",
       "14 1970-01-10 12:00:00 1970-01-10 16:35:22   Saturday          12  \n",
       "15 1970-01-05 22:49:38 1970-01-05 23:51:52     Monday          22  \n",
       "16 1970-01-06 12:00:00 1970-01-06 10:10:23    Tuesday          12  \n",
       "17 1970-01-06 12:00:00 1970-01-06 16:23:57    Tuesday          12  \n",
       "18 1970-01-06 16:32:15 1970-01-06 16:38:30    Tuesday          16  \n",
       "19 1970-01-06 16:48:10 1970-01-06 17:28:03    Tuesday          16  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86fec79-a5e9-4aee-87dd-0a953a781bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_units                                50.0\n",
       "num_blocks                                1.0\n",
       "epochs                                  100.0\n",
       "batch_size                              128.0\n",
       "learningrate                             0.01\n",
       "optimizer                                 SGD\n",
       "dropout                                   0.2\n",
       "y_transformation                     standard\n",
       "loss_function                          MAE_td\n",
       "gamma                                     0.0\n",
       "beta                                     0.25\n",
       "alpha                                    0.25\n",
       "number_of_traces                        100.0\n",
       "statespace_size                            12\n",
       "process_entropy                   med_entropy\n",
       "process_type                           memory\n",
       "process_memory                           15.0\n",
       "inter_arrival_time                        1.5\n",
       "process_stability_scale                   0.1\n",
       "resource_availability_p                  0.25\n",
       "resource_availability_n                   3.0\n",
       "resource_availability_m                 0.041\n",
       "activity_duration_lambda_range            0.5\n",
       "repetition                                1.0\n",
       "RUN                                         0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ef018-478d-443b-af82-c9018ba68bbd",
   "metadata": {},
   "source": [
    "## For-loop going over all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18e5ed3e-4c37-4d12-868f-59148bd1de4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmikeriess\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_153629-1ruiuytq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ruiuytq\" target=\"_blank\">royal-forest-51</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "512/516 [============================>.] - ETA: 0s - loss: 6.6234 - mae: 0.8279 - mean_pred: 0.3269 - mae_t1: 0.0552WARNING:tensorflow:From C:\\Users\\Mike\\anaconda3\\envs\\TF_GPU_2_3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "516/516 [==============================] - 0s 854us/sample - loss: 6.6144 - mae: 0.8268 - mean_pred: 0.3294 - mae_t1: 0.0551 - val_loss: 5.3332 - val_mae: 0.6666 - val_mean_pred: 1.0417 - val_mae_t1: 0.0444\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 5.5594 - mae: 0.6949 - mean_pred: 0.8988 - mae_t1: 0.0463 - val_loss: 4.2227 - val_mae: 0.5278 - val_mean_pred: 0.5691 - val_mae_t1: 0.0352\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 4.6282 - mae: 0.5785 - mean_pred: 0.5339 - mae_t1: 0.0386 - val_loss: 4.5033 - val_mae: 0.5629 - val_mean_pred: 0.6022 - val_mae_t1: 0.0375\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.5523 - mae: 0.5690 - mean_pred: 0.4960 - mae_t1: 0.0379 - val_loss: 4.8828 - val_mae: 0.6104 - val_mean_pred: 0.7200 - val_mae_t1: 0.0407\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 4.5664 - mae: 0.5708 - mean_pred: 0.6685 - mae_t1: 0.0381 - val_loss: 4.1270 - val_mae: 0.5159 - val_mean_pred: 0.8323 - val_mae_t1: 0.0344\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 215us/sample - loss: 4.0692 - mae: 0.5086 - mean_pred: 0.7331 - mae_t1: 0.0339 - val_loss: 3.7726 - val_mae: 0.4716 - val_mean_pred: 0.9742 - val_mae_t1: 0.0314\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 3.9312 - mae: 0.4914 - mean_pred: 0.8911 - mae_t1: 0.0328 - val_loss: 3.6114 - val_mae: 0.4514 - val_mean_pred: 0.7873 - val_mae_t1: 0.0301\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.7656 - mae: 0.4707 - mean_pred: 0.6797 - mae_t1: 0.0314 - val_loss: 3.6632 - val_mae: 0.4579 - val_mean_pred: 0.8328 - val_mae_t1: 0.0305\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.5781 - mae: 0.4473 - mean_pred: 0.8444 - mae_t1: 0.0298 - val_loss: 3.8352 - val_mae: 0.4794 - val_mean_pred: 0.7282 - val_mae_t1: 0.0320\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 3.9413 - mae: 0.4927 - mean_pred: 0.6036 - mae_t1: 0.0328 - val_loss: 3.8655 - val_mae: 0.4832 - val_mean_pred: 0.8413 - val_mae_t1: 0.0322\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.9838 - mae: 0.4980 - mean_pred: 0.9327 - mae_t1: 0.0332 - val_loss: 4.3293 - val_mae: 0.5412 - val_mean_pred: 0.9278 - val_mae_t1: 0.0361\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 3.4745 - mae: 0.4343 - mean_pred: 0.7528 - mae_t1: 0.0290 - val_loss: 4.0414 - val_mae: 0.5052 - val_mean_pred: 0.8059 - val_mae_t1: 0.0337\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.3681 - mae: 0.4210 - mean_pred: 0.8381 - mae_t1: 0.0281 - val_loss: 3.9997 - val_mae: 0.5000 - val_mean_pred: 0.9376 - val_mae_t1: 0.0333\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3354 - mae: 0.4169 - mean_pred: 0.7374 - mae_t1: 0.0278 - val_loss: 4.1857 - val_mae: 0.5232 - val_mean_pred: 0.8043 - val_mae_t1: 0.0349\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.3366 - mae: 0.4171 - mean_pred: 0.7252 - mae_t1: 0.0278 - val_loss: 4.0977 - val_mae: 0.5122 - val_mean_pred: 1.0010 - val_mae_t1: 0.0341\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 3.2395 - mae: 0.4049 - mean_pred: 0.7722 - mae_t1: 0.0270 - val_loss: 3.9183 - val_mae: 0.4898 - val_mean_pred: 0.7196 - val_mae_t1: 0.0327\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.1876 - mae: 0.3984 - mean_pred: 0.6313 - mae_t1: 0.0266 - val_loss: 3.7902 - val_mae: 0.4738 - val_mean_pred: 0.8763 - val_mae_t1: 0.0316\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9383 - mae: 0.3673 - mean_pred: 0.8098 - mae_t1: 0.0245 - val_loss: 3.6662 - val_mae: 0.4583 - val_mean_pred: 0.8725 - val_mae_t1: 0.0306\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 2.9246 - mae: 0.3656 - mean_pred: 0.7921 - mae_t1: 0.0244 - val_loss: 3.5103 - val_mae: 0.4388 - val_mean_pred: 0.8496 - val_mae_t1: 0.0293\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.9917 - mae: 0.3740 - mean_pred: 0.7909 - mae_t1: 0.0249 - val_loss: 3.5911 - val_mae: 0.4489 - val_mean_pred: 0.9187 - val_mae_t1: 0.0299\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.2538 - mae: 0.4067 - mean_pred: 0.9429 - mae_t1: 0.0271 - val_loss: 3.8846 - val_mae: 0.4856 - val_mean_pred: 0.6785 - val_mae_t1: 0.0324\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.5507 - mae: 0.4438 - mean_pred: 0.5399 - mae_t1: 0.0296 - val_loss: 3.9500 - val_mae: 0.4938 - val_mean_pred: 0.8522 - val_mae_t1: 0.0329\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.3692 - mae: 0.4212 - mean_pred: 0.9027 - mae_t1: 0.0281 - val_loss: 4.2587 - val_mae: 0.5323 - val_mean_pred: 1.0729 - val_mae_t1: 0.0355\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.1411 - mae: 0.3926 - mean_pred: 0.7720 - mae_t1: 0.0262 - val_loss: 4.2675 - val_mae: 0.5334 - val_mean_pred: 0.7341 - val_mae_t1: 0.0356\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.5030 - mae: 0.4379 - mean_pred: 0.7343 - mae_t1: 0.0292 - val_loss: 4.1823 - val_mae: 0.5228 - val_mean_pred: 0.9968 - val_mae_t1: 0.0349\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.2931 - mae: 0.4116 - mean_pred: 0.9139 - mae_t1: 0.0274 - val_loss: 3.9781 - val_mae: 0.4973 - val_mean_pred: 0.8549 - val_mae_t1: 0.0332\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 3.5513 - mae: 0.4439 - mean_pred: 0.7297 - mae_t1: 0.0296 - val_loss: 3.9095 - val_mae: 0.4887 - val_mean_pred: 0.6801 - val_mae_t1: 0.0326\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.3116 - mae: 0.4140 - mean_pred: 0.6616 - mae_t1: 0.0276 - val_loss: 3.6717 - val_mae: 0.4590 - val_mean_pred: 0.8962 - val_mae_t1: 0.0306\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0480 - mae: 0.3810 - mean_pred: 0.8021 - mae_t1: 0.0254 - val_loss: 3.7919 - val_mae: 0.4740 - val_mean_pred: 0.8377 - val_mae_t1: 0.0316\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.0640 - mae: 0.3830 - mean_pred: 0.7400 - mae_t1: 0.0255 - val_loss: 3.6393 - val_mae: 0.4549 - val_mean_pred: 0.9574 - val_mae_t1: 0.0303\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 3.1688 - mae: 0.3961 - mean_pred: 0.9263 - mae_t1: 0.0264 - val_loss: 3.4757 - val_mae: 0.4345 - val_mean_pred: 0.7932 - val_mae_t1: 0.0290\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.7202 - mae: 0.3400 - mean_pred: 0.7354 - mae_t1: 0.0227 - val_loss: 3.6272 - val_mae: 0.4534 - val_mean_pred: 0.8267 - val_mae_t1: 0.0302\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0118 - mae: 0.3765 - mean_pred: 0.7160 - mae_t1: 0.0251 - val_loss: 3.9447 - val_mae: 0.4931 - val_mean_pred: 0.7972 - val_mae_t1: 0.0329\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.0064 - mae: 0.3758 - mean_pred: 0.7376 - mae_t1: 0.0251 - val_loss: 3.8439 - val_mae: 0.4805 - val_mean_pred: 0.9130 - val_mae_t1: 0.0320\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.9146 - mae: 0.3643 - mean_pred: 0.7688 - mae_t1: 0.0243 - val_loss: 3.7261 - val_mae: 0.4658 - val_mean_pred: 0.7849 - val_mae_t1: 0.0311\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.6832 - mae: 0.3354 - mean_pred: 0.7019 - mae_t1: 0.0224 - val_loss: 3.7216 - val_mae: 0.4652 - val_mean_pred: 0.8869 - val_mae_t1: 0.0310\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5641 - mae: 0.3205 - mean_pred: 0.7835 - mae_t1: 0.0214 - val_loss: 3.7656 - val_mae: 0.4707 - val_mean_pred: 0.8170 - val_mae_t1: 0.0314\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6188 - mae: 0.3274 - mean_pred: 0.7101 - mae_t1: 0.0218 - val_loss: 3.8238 - val_mae: 0.4780 - val_mean_pred: 0.9591 - val_mae_t1: 0.0319\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9622 - mae: 0.3703 - mean_pred: 0.9126 - mae_t1: 0.0247 - val_loss: 3.5880 - val_mae: 0.4485 - val_mean_pred: 0.9438 - val_mae_t1: 0.0299\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 222us/sample - loss: 2.6798 - mae: 0.3350 - mean_pred: 0.7477 - mae_t1: 0.0223 - val_loss: 3.4220 - val_mae: 0.4278 - val_mean_pred: 0.8627 - val_mae_t1: 0.0285\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8540 - mae: 0.3567 - mean_pred: 0.8829 - mae_t1: 0.0238 - val_loss: 3.6164 - val_mae: 0.4520 - val_mean_pred: 0.7937 - val_mae_t1: 0.0301\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8607 - mae: 0.3576 - mean_pred: 0.6343 - mae_t1: 0.0238 - val_loss: 3.6893 - val_mae: 0.4612 - val_mean_pred: 0.7602 - val_mae_t1: 0.0307\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8445 - mae: 0.3556 - mean_pred: 0.8536 - mae_t1: 0.0237 - val_loss: 3.7588 - val_mae: 0.4698 - val_mean_pred: 1.0204 - val_mae_t1: 0.0313\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8147 - mae: 0.3518 - mean_pred: 0.8039 - mae_t1: 0.0235 - val_loss: 3.7798 - val_mae: 0.4725 - val_mean_pred: 0.7836 - val_mae_t1: 0.0315\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.5877 - mae: 0.3235 - mean_pred: 0.7784 - mae_t1: 0.0216 - val_loss: 4.1371 - val_mae: 0.5171 - val_mean_pred: 1.0475 - val_mae_t1: 0.0345\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.6181 - mae: 0.3273 - mean_pred: 0.8586 - mae_t1: 0.0218 - val_loss: 4.2168 - val_mae: 0.5271 - val_mean_pred: 0.7486 - val_mae_t1: 0.0351\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.1165 - mae: 0.3896 - mean_pred: 0.6230 - mae_t1: 0.0260 - val_loss: 3.9994 - val_mae: 0.4999 - val_mean_pred: 0.9047 - val_mae_t1: 0.0333\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.1927 - mae: 0.3991 - mean_pred: 0.9264 - mae_t1: 0.0266 - val_loss: 4.0286 - val_mae: 0.5036 - val_mean_pred: 1.0295 - val_mae_t1: 0.0336\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.5515 - mae: 0.3189 - mean_pred: 0.7923 - mae_t1: 0.0213 - val_loss: 3.9029 - val_mae: 0.4879 - val_mean_pred: 0.8780 - val_mae_t1: 0.0325\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7781 - mae: 0.3473 - mean_pred: 0.8193 - mae_t1: 0.0232 - val_loss: 4.0755 - val_mae: 0.5094 - val_mean_pred: 1.0061 - val_mae_t1: 0.0340\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 3.1017 - mae: 0.3877 - mean_pred: 0.8676 - mae_t1: 0.0258 - val_loss: 3.7937 - val_mae: 0.4742 - val_mean_pred: 0.8335 - val_mae_t1: 0.0316\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.8086 - mae: 0.3511 - mean_pred: 0.7276 - mae_t1: 0.0234 - val_loss: 3.6043 - val_mae: 0.4505 - val_mean_pred: 0.8542 - val_mae_t1: 0.0300\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 2.4357 - mae: 0.3045 - mean_pred: 0.8392 - mae_t1: 0.0203 - val_loss: 3.8277 - val_mae: 0.4785 - val_mean_pred: 0.9992 - val_mae_t1: 0.0319\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4886 - mae: 0.3111 - mean_pred: 0.8657 - mae_t1: 0.0207 - val_loss: 3.7086 - val_mae: 0.4636 - val_mean_pred: 0.8105 - val_mae_t1: 0.0309\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 2.4689 - mae: 0.3086 - mean_pred: 0.7162 - mae_t1: 0.0206 - val_loss: 3.3873 - val_mae: 0.4234 - val_mean_pred: 0.8691 - val_mae_t1: 0.0282\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3237 - mae: 0.2905 - mean_pred: 0.8082 - mae_t1: 0.0194 - val_loss: 3.8680 - val_mae: 0.4835 - val_mean_pred: 0.9994 - val_mae_t1: 0.0322\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.5336 - mae: 0.3167 - mean_pred: 0.8372 - mae_t1: 0.0211 - val_loss: 3.6464 - val_mae: 0.4558 - val_mean_pred: 0.8888 - val_mae_t1: 0.0304\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5862 - mae: 0.3233 - mean_pred: 0.8281 - mae_t1: 0.0216 - val_loss: 3.6448 - val_mae: 0.4556 - val_mean_pred: 0.7810 - val_mae_t1: 0.0304\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.7815 - mae: 0.3477 - mean_pred: 0.6630 - mae_t1: 0.0232 - val_loss: 3.9258 - val_mae: 0.4907 - val_mean_pred: 0.7292 - val_mae_t1: 0.0327\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5203 - mae: 0.3150 - mean_pred: 0.7424 - mae_t1: 0.0210 - val_loss: 3.7546 - val_mae: 0.4693 - val_mean_pred: 0.9298 - val_mae_t1: 0.0313\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.2092 - mae: 0.2761 - mean_pred: 0.8247 - mae_t1: 0.0184 - val_loss: 3.5226 - val_mae: 0.4403 - val_mean_pred: 0.8488 - val_mae_t1: 0.0294\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.2261 - mae: 0.2783 - mean_pred: 0.7814 - mae_t1: 0.0186 - val_loss: 3.7718 - val_mae: 0.4715 - val_mean_pred: 0.9997 - val_mae_t1: 0.0314\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4451 - mae: 0.3056 - mean_pred: 0.8919 - mae_t1: 0.0204 - val_loss: 3.8132 - val_mae: 0.4766 - val_mean_pred: 0.8149 - val_mae_t1: 0.0318\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5775 - mae: 0.3222 - mean_pred: 0.6670 - mae_t1: 0.0215 - val_loss: 4.0211 - val_mae: 0.5026 - val_mean_pred: 0.9027 - val_mae_t1: 0.0335\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.8111 - mae: 0.3514 - mean_pred: 0.9121 - mae_t1: 0.0234 - val_loss: 3.7293 - val_mae: 0.4662 - val_mean_pred: 0.8606 - val_mae_t1: 0.0311\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.7876 - mae: 0.3485 - mean_pred: 0.6621 - mae_t1: 0.0232 - val_loss: 3.9314 - val_mae: 0.4914 - val_mean_pred: 0.7595 - val_mae_t1: 0.0328\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.6399 - mae: 0.3300 - mean_pred: 0.8501 - mae_t1: 0.0220 - val_loss: 3.8079 - val_mae: 0.4760 - val_mean_pred: 1.0296 - val_mae_t1: 0.0317\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.7130 - mae: 0.3391 - mean_pred: 0.8570 - mae_t1: 0.0226 - val_loss: 3.8970 - val_mae: 0.4871 - val_mean_pred: 0.8200 - val_mae_t1: 0.0325\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.5972 - mae: 0.3246 - mean_pred: 0.8237 - mae_t1: 0.0216 - val_loss: 3.4023 - val_mae: 0.4253 - val_mean_pred: 0.9320 - val_mae_t1: 0.0284\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3778 - mae: 0.2972 - mean_pred: 0.7933 - mae_t1: 0.0198 - val_loss: 3.6351 - val_mae: 0.4544 - val_mean_pred: 0.9273 - val_mae_t1: 0.0303\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.3461 - mae: 0.2933 - mean_pred: 0.8530 - mae_t1: 0.0196 - val_loss: 3.7350 - val_mae: 0.4669 - val_mean_pred: 0.9740 - val_mae_t1: 0.0311\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2696 - mae: 0.2837 - mean_pred: 0.8007 - mae_t1: 0.0189 - val_loss: 3.9308 - val_mae: 0.4913 - val_mean_pred: 0.9044 - val_mae_t1: 0.0328\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.3649 - mae: 0.2956 - mean_pred: 0.8167 - mae_t1: 0.0197 - val_loss: 3.8861 - val_mae: 0.4858 - val_mean_pred: 0.9407 - val_mae_t1: 0.0324\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.7090 - mae: 0.3386 - mean_pred: 0.8195 - mae_t1: 0.0226 - val_loss: 4.0066 - val_mae: 0.5008 - val_mean_pred: 0.9034 - val_mae_t1: 0.0334\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.4306 - mae: 0.3038 - mean_pred: 0.8016 - mae_t1: 0.0203 - val_loss: 3.3162 - val_mae: 0.4145 - val_mean_pred: 0.8972 - val_mae_t1: 0.0276\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.3107 - mae: 0.2888 - mean_pred: 0.7886 - mae_t1: 0.0193 - val_loss: 3.4381 - val_mae: 0.4298 - val_mean_pred: 0.8662 - val_mae_t1: 0.0287\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.4430 - mae: 0.3054 - mean_pred: 0.7593 - mae_t1: 0.0204 - val_loss: 3.7163 - val_mae: 0.4645 - val_mean_pred: 0.9319 - val_mae_t1: 0.0310\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7826 - mae: 0.3478 - mean_pred: 0.8812 - mae_t1: 0.0232 - val_loss: 3.8132 - val_mae: 0.4766 - val_mean_pred: 1.0285 - val_mae_t1: 0.0318\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.5915 - mae: 0.3239 - mean_pred: 0.7876 - mae_t1: 0.0216 - val_loss: 3.9801 - val_mae: 0.4975 - val_mean_pred: 0.7335 - val_mae_t1: 0.0332\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2730 - mae: 0.2841 - mean_pred: 0.7299 - mae_t1: 0.0189 - val_loss: 3.7277 - val_mae: 0.4660 - val_mean_pred: 1.0318 - val_mae_t1: 0.0311\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.4333 - mae: 0.3042 - mean_pred: 0.9016 - mae_t1: 0.0203 - val_loss: 3.4596 - val_mae: 0.4325 - val_mean_pred: 0.8881 - val_mae_t1: 0.0288\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.1388 - mae: 0.2674 - mean_pred: 0.8082 - mae_t1: 0.0178 - val_loss: 3.6135 - val_mae: 0.4517 - val_mean_pred: 0.8747 - val_mae_t1: 0.0301\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.9672 - mae: 0.2459 - mean_pred: 0.7843 - mae_t1: 0.0164 - val_loss: 3.4531 - val_mae: 0.4316 - val_mean_pred: 0.8768 - val_mae_t1: 0.0288\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 2.1539 - mae: 0.2692 - mean_pred: 0.7974 - mae_t1: 0.0179 - val_loss: 3.5482 - val_mae: 0.4435 - val_mean_pred: 0.9243 - val_mae_t1: 0.0296\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1416 - mae: 0.2677 - mean_pred: 0.8192 - mae_t1: 0.0178 - val_loss: 3.8372 - val_mae: 0.4796 - val_mean_pred: 0.8989 - val_mae_t1: 0.0320\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.4175 - mae: 0.3022 - mean_pred: 0.7653 - mae_t1: 0.0201 - val_loss: 4.1788 - val_mae: 0.5223 - val_mean_pred: 0.8562 - val_mae_t1: 0.0348\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.2195 - mae: 0.4024 - mean_pred: 0.7977 - mae_t1: 0.0268 - val_loss: 3.7467 - val_mae: 0.4683 - val_mean_pred: 0.8156 - val_mae_t1: 0.0312\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.9164 - mae: 0.3645 - mean_pred: 0.7551 - mae_t1: 0.0243 - val_loss: 3.4754 - val_mae: 0.4344 - val_mean_pred: 0.8378 - val_mae_t1: 0.0290\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.5561 - mae: 0.3195 - mean_pred: 0.8301 - mae_t1: 0.0213 - val_loss: 3.6741 - val_mae: 0.4593 - val_mean_pred: 0.9858 - val_mae_t1: 0.0306\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.4367 - mae: 0.3046 - mean_pred: 0.8577 - mae_t1: 0.0203 - val_loss: 3.7085 - val_mae: 0.4636 - val_mean_pred: 0.8139 - val_mae_t1: 0.0309\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.8077 - mae: 0.3510 - mean_pred: 0.7027 - mae_t1: 0.0234 - val_loss: 3.9182 - val_mae: 0.4898 - val_mean_pred: 0.8128 - val_mae_t1: 0.0327\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 3.0099 - mae: 0.3762 - mean_pred: 0.7709 - mae_t1: 0.0251 - val_loss: 4.0043 - val_mae: 0.5005 - val_mean_pred: 1.0363 - val_mae_t1: 0.0334\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 373us/sample - loss: 2.4441 - mae: 0.3055 - mean_pred: 0.8363 - mae_t1: 0.0204 - val_loss: 3.7550 - val_mae: 0.4694 - val_mean_pred: 0.8707 - val_mae_t1: 0.0313\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.5910 - mae: 0.3239 - mean_pred: 0.7346 - mae_t1: 0.0216 - val_loss: 3.5463 - val_mae: 0.4433 - val_mean_pred: 0.8939 - val_mae_t1: 0.0296\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1381 - mae: 0.2673 - mean_pred: 0.8361 - mae_t1: 0.0178 - val_loss: 3.3747 - val_mae: 0.4218 - val_mean_pred: 0.8832 - val_mae_t1: 0.0281\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0613 - mae: 0.2577 - mean_pred: 0.7537 - mae_t1: 0.0172 - val_loss: 3.5652 - val_mae: 0.4456 - val_mean_pred: 0.8927 - val_mae_t1: 0.0297\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.0672 - mae: 0.2584 - mean_pred: 0.8584 - mae_t1: 0.0172 - val_loss: 3.5644 - val_mae: 0.4456 - val_mean_pred: 0.9759 - val_mae_t1: 0.0297\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.0431 - mae: 0.2554 - mean_pred: 0.7888 - mae_t1: 0.0170 - val_loss: 3.4151 - val_mae: 0.4269 - val_mean_pred: 0.8763 - val_mae_t1: 0.0285\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0022 - mae: 0.2503 - mean_pred: 0.7602 - mae_t1: 0.0167 - val_loss: 3.4632 - val_mae: 0.4329 - val_mean_pred: 0.9484 - val_mae_t1: 0.0289\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0310 - mae: 0.2539 - mean_pred: 0.8230 - mae_t1: 0.0169 - val_loss: 3.5939 - val_mae: 0.4492 - val_mean_pred: 0.9188 - val_mae_t1: 0.0299\n",
      "Earliness...\n",
      "0.0019714832305908203\n",
      "____________________________________________________________\n",
      "Test MAE:      0.32421598415858116  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.run.join() is deprecated, please use wandb.run.finish().\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▂▃▃▃▃▃▂▂▂▂▂▂▂▃▂▁▂▁▂▂▂▁▂▂▂▁▁▃▂▂▂▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▄▃▃▂▃▃▃▃▃▂▂▂▂▂▂▂▃▂▁▂▁▂▂▂▁▂▂▂▁▁▃▂▂▂▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▄▃▃▂▃▃▃▃▃▂▂▂▂▂▂▂▃▂▁▂▁▂▂▂▁▂▂▂▁▁▃▂▂▂▁▁</td></tr><tr><td>mean_pred</td><td>▁▃▆▅█▇▆▆███▅█▆▅█▇▆▇▆▇▇▆▅▇▅▇▇▆▇▆▆▆▆▆▇▆▆▇▇</td></tr><tr><td>val_loss</td><td>█▅▂▂▄▃▃▂▃▄▃▂▁▃▂▂▂▂▄▃▂▂▃▃▁▃▂▁▃▃▂▃▂▂▂▂▃▂▂▂</td></tr><tr><td>val_mae</td><td>█▅▂▂▄▃▃▂▃▄▃▂▁▃▂▂▂▂▄▃▂▂▃▃▁▃▂▁▃▃▂▃▂▂▂▂▃▂▂▂</td></tr><tr><td>val_mae_t1</td><td>█▅▂▂▄▃▃▂▃▄▃▂▁▃▂▂▂▂▄▃▂▂▃▃▁▃▂▁▃▃▂▃▂▂▂▂▃▂▂▂</td></tr><tr><td>val_mean_pred</td><td>█▁▇▄▆▆▃▅▂█▅▅▄▆▅▆▄▄▃▅▄▄▇▃▅▅▇▆▅▅▆▃▅▆▄▇▇▅▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.40387</td></tr><tr><td>AE_2</td><td>0.32445</td></tr><tr><td>AE_3</td><td>0.2936</td></tr><tr><td>MAE</td><td>0.32422</td></tr><tr><td>best_epoch</td><td>74</td></tr><tr><td>best_val_loss</td><td>3.31622</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.03104</td></tr><tr><td>mae</td><td>0.25388</td></tr><tr><td>mae_t1</td><td>0.01693</td></tr><tr><td>mean_pred</td><td>0.82296</td></tr><tr><td>val_loss</td><td>3.59395</td></tr><tr><td>val_mae</td><td>0.44924</td></tr><tr><td>val_mae_t1</td><td>0.02995</td></tr><tr><td>val_mean_pred</td><td>0.9188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">royal-forest-51</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ruiuytq\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ruiuytq</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_153629-1ruiuytq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154039-35ig8b2z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35ig8b2z\" target=\"_blank\">lilac-night-52</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 702us/sample - loss: 7.6612 - mae: 0.9577 - mean_pred: 0.2045 - mae_t1: 0.0638 - val_loss: 4.3750 - val_mae: 0.5469 - val_mean_pred: 0.5204 - val_mae_t1: 0.0365\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 5.2573 - mae: 0.6572 - mean_pred: 0.7119 - mae_t1: 0.0438 - val_loss: 4.3359 - val_mae: 0.5420 - val_mean_pred: 1.1296 - val_mae_t1: 0.0361\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 5.3745 - mae: 0.6718 - mean_pred: 1.0896 - mae_t1: 0.0448 - val_loss: 3.6233 - val_mae: 0.4529 - val_mean_pred: 0.7709 - val_mae_t1: 0.0302\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 133us/sample - loss: 4.0752 - mae: 0.5094 - mean_pred: 0.6878 - mae_t1: 0.0340 - val_loss: 4.2820 - val_mae: 0.5353 - val_mean_pred: 0.5797 - val_mae_t1: 0.0357\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 4.3707 - mae: 0.5463 - mean_pred: 0.5249 - mae_t1: 0.0364 - val_loss: 4.1339 - val_mae: 0.5167 - val_mean_pred: 0.8216 - val_mae_t1: 0.0344\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 4.0367 - mae: 0.5046 - mean_pred: 0.7981 - mae_t1: 0.0336 - val_loss: 3.8854 - val_mae: 0.4857 - val_mean_pred: 0.9755 - val_mae_t1: 0.0324\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 3.9805 - mae: 0.4976 - mean_pred: 0.8286 - mae_t1: 0.0332 - val_loss: 4.1728 - val_mae: 0.5216 - val_mean_pred: 0.6767 - val_mae_t1: 0.0348\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.2938 - mae: 0.5367 - mean_pred: 0.5770 - mae_t1: 0.0358 - val_loss: 4.3540 - val_mae: 0.5443 - val_mean_pred: 0.5576 - val_mae_t1: 0.0363\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 4.2852 - mae: 0.5357 - mean_pred: 0.4929 - mae_t1: 0.0357 - val_loss: 3.6698 - val_mae: 0.4587 - val_mean_pred: 0.8544 - val_mae_t1: 0.0306\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.0136 - mae: 0.5017 - mean_pred: 0.8678 - mae_t1: 0.0334 - val_loss: 4.2936 - val_mae: 0.5367 - val_mean_pred: 1.0967 - val_mae_t1: 0.0358\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.2950 - mae: 0.5369 - mean_pred: 0.9404 - mae_t1: 0.0358 - val_loss: 3.9051 - val_mae: 0.4881 - val_mean_pred: 0.7824 - val_mae_t1: 0.0325\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.7431 - mae: 0.4679 - mean_pred: 0.6797 - mae_t1: 0.0312 - val_loss: 4.3646 - val_mae: 0.5456 - val_mean_pred: 0.8145 - val_mae_t1: 0.0364\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.7557 - mae: 0.4695 - mean_pred: 0.7336 - mae_t1: 0.0313 - val_loss: 4.3070 - val_mae: 0.5384 - val_mean_pred: 0.8602 - val_mae_t1: 0.0359\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 3.6311 - mae: 0.4539 - mean_pred: 0.7621 - mae_t1: 0.0303 - val_loss: 4.1894 - val_mae: 0.5237 - val_mean_pred: 0.7966 - val_mae_t1: 0.0349\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.6046 - mae: 0.4506 - mean_pred: 0.6939 - mae_t1: 0.0300 - val_loss: 4.1226 - val_mae: 0.5153 - val_mean_pred: 0.7616 - val_mae_t1: 0.0344\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 3.5830 - mae: 0.4479 - mean_pred: 0.7084 - mae_t1: 0.0299 - val_loss: 4.0151 - val_mae: 0.5019 - val_mean_pred: 0.9972 - val_mae_t1: 0.0335\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 3.8747 - mae: 0.4843 - mean_pred: 0.9764 - mae_t1: 0.0323 - val_loss: 3.9993 - val_mae: 0.4999 - val_mean_pred: 1.0358 - val_mae_t1: 0.0333\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.6848 - mae: 0.4606 - mean_pred: 0.9304 - mae_t1: 0.0307 - val_loss: 3.7058 - val_mae: 0.4632 - val_mean_pred: 0.8615 - val_mae_t1: 0.0309\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6113 - mae: 0.4514 - mean_pred: 0.8166 - mae_t1: 0.0301 - val_loss: 3.6660 - val_mae: 0.4582 - val_mean_pred: 0.9417 - val_mae_t1: 0.0305\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.5449 - mae: 0.4431 - mean_pred: 0.8741 - mae_t1: 0.0295 - val_loss: 4.1114 - val_mae: 0.5139 - val_mean_pred: 1.0058 - val_mae_t1: 0.0343\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.8631 - mae: 0.4829 - mean_pred: 0.9157 - mae_t1: 0.0322 - val_loss: 3.9823 - val_mae: 0.4978 - val_mean_pred: 0.9570 - val_mae_t1: 0.0332\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.6830 - mae: 0.4604 - mean_pred: 0.8674 - mae_t1: 0.0307 - val_loss: 4.3548 - val_mae: 0.5443 - val_mean_pred: 0.6928 - val_mae_t1: 0.0363\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.9764 - mae: 0.4971 - mean_pred: 0.5817 - mae_t1: 0.0331 - val_loss: 4.7535 - val_mae: 0.5942 - val_mean_pred: 0.5900 - val_mae_t1: 0.0396\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.8737 - mae: 0.4842 - mean_pred: 0.5984 - mae_t1: 0.0323 - val_loss: 4.1391 - val_mae: 0.5174 - val_mean_pred: 0.9692 - val_mae_t1: 0.0345\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.9716 - mae: 0.4964 - mean_pred: 0.9632 - mae_t1: 0.0331 - val_loss: 4.1391 - val_mae: 0.5174 - val_mean_pred: 0.9970 - val_mae_t1: 0.0345\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.6485 - mae: 0.4561 - mean_pred: 0.9013 - mae_t1: 0.0304 - val_loss: 4.2142 - val_mae: 0.5268 - val_mean_pred: 0.6776 - val_mae_t1: 0.0351\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.7732 - mae: 0.4716 - mean_pred: 0.6091 - mae_t1: 0.0314 - val_loss: 4.3962 - val_mae: 0.5495 - val_mean_pred: 0.6085 - val_mae_t1: 0.0366\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7631 - mae: 0.4704 - mean_pred: 0.5869 - mae_t1: 0.0314 - val_loss: 4.1484 - val_mae: 0.5185 - val_mean_pred: 0.7500 - val_mae_t1: 0.0346\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.3690 - mae: 0.4211 - mean_pred: 0.7086 - mae_t1: 0.0281 - val_loss: 4.1729 - val_mae: 0.5216 - val_mean_pred: 0.9769 - val_mae_t1: 0.0348\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.5075 - mae: 0.4384 - mean_pred: 0.8914 - mae_t1: 0.0292 - val_loss: 4.1830 - val_mae: 0.5229 - val_mean_pred: 1.0085 - val_mae_t1: 0.0349\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3504 - mae: 0.4188 - mean_pred: 0.8627 - mae_t1: 0.0279 - val_loss: 3.9945 - val_mae: 0.4993 - val_mean_pred: 0.7940 - val_mae_t1: 0.0333\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 3.2595 - mae: 0.4074 - mean_pred: 0.7009 - mae_t1: 0.0272 - val_loss: 3.8714 - val_mae: 0.4839 - val_mean_pred: 0.7442 - val_mae_t1: 0.0323\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.2886 - mae: 0.4111 - mean_pred: 0.6723 - mae_t1: 0.0274 - val_loss: 3.7360 - val_mae: 0.4670 - val_mean_pred: 0.8874 - val_mae_t1: 0.0311\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2460 - mae: 0.4057 - mean_pred: 0.8344 - mae_t1: 0.0270 - val_loss: 4.4257 - val_mae: 0.5532 - val_mean_pred: 1.0905 - val_mae_t1: 0.0369\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 3.9797 - mae: 0.4975 - mean_pred: 0.9582 - mae_t1: 0.0332 - val_loss: 4.2974 - val_mae: 0.5372 - val_mean_pred: 0.9289 - val_mae_t1: 0.0358\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.5459 - mae: 0.4432 - mean_pred: 0.7708 - mae_t1: 0.0295 - val_loss: 4.1948 - val_mae: 0.5243 - val_mean_pred: 0.8063 - val_mae_t1: 0.0350\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 3.5695 - mae: 0.4462 - mean_pred: 0.6955 - mae_t1: 0.0297 - val_loss: 4.1678 - val_mae: 0.5210 - val_mean_pred: 0.8083 - val_mae_t1: 0.0347\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.4865 - mae: 0.4358 - mean_pred: 0.7226 - mae_t1: 0.0291 - val_loss: 3.8826 - val_mae: 0.4853 - val_mean_pred: 0.8537 - val_mae_t1: 0.0324\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0970 - mae: 0.3871 - mean_pred: 0.7568 - mae_t1: 0.0258 - val_loss: 3.9600 - val_mae: 0.4950 - val_mean_pred: 0.9252 - val_mae_t1: 0.0330\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0340 - mae: 0.3792 - mean_pred: 0.8226 - mae_t1: 0.0253 - val_loss: 4.0309 - val_mae: 0.5039 - val_mean_pred: 0.9342 - val_mae_t1: 0.0336\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.0809 - mae: 0.3851 - mean_pred: 0.8064 - mae_t1: 0.0257 - val_loss: 4.0908 - val_mae: 0.5113 - val_mean_pred: 0.9127 - val_mae_t1: 0.0341\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9741 - mae: 0.3718 - mean_pred: 0.7997 - mae_t1: 0.0248 - val_loss: 4.0082 - val_mae: 0.5010 - val_mean_pred: 1.0011 - val_mae_t1: 0.0334\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 2.9992 - mae: 0.3749 - mean_pred: 0.8763 - mae_t1: 0.0250 - val_loss: 3.6817 - val_mae: 0.4602 - val_mean_pred: 0.9657 - val_mae_t1: 0.0307\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 134us/sample - loss: 2.9632 - mae: 0.3704 - mean_pred: 0.8267 - mae_t1: 0.0247 - val_loss: 3.5888 - val_mae: 0.4486 - val_mean_pred: 0.8354 - val_mae_t1: 0.0299\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.3427 - mae: 0.4178 - mean_pred: 0.7378 - mae_t1: 0.0279 - val_loss: 3.9006 - val_mae: 0.4876 - val_mean_pred: 0.8987 - val_mae_t1: 0.0325\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 3.5561 - mae: 0.4445 - mean_pred: 0.8508 - mae_t1: 0.0296 - val_loss: 3.1434 - val_mae: 0.3929 - val_mean_pred: 0.9012 - val_mae_t1: 0.0262\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2141 - mae: 0.4018 - mean_pred: 0.8025 - mae_t1: 0.0268 - val_loss: 3.7698 - val_mae: 0.4712 - val_mean_pred: 0.7992 - val_mae_t1: 0.0314\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6774 - mae: 0.4597 - mean_pred: 0.7861 - mae_t1: 0.0306 - val_loss: 3.6584 - val_mae: 0.4573 - val_mean_pred: 0.9175 - val_mae_t1: 0.0305\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0298 - mae: 0.3787 - mean_pred: 0.8546 - mae_t1: 0.0252 - val_loss: 3.9441 - val_mae: 0.4930 - val_mean_pred: 0.8872 - val_mae_t1: 0.0329\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.4592 - mae: 0.4324 - mean_pred: 0.7591 - mae_t1: 0.0288 - val_loss: 3.9330 - val_mae: 0.4916 - val_mean_pred: 0.7276 - val_mae_t1: 0.0328\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.2598 - mae: 0.4075 - mean_pred: 0.6597 - mae_t1: 0.0272 - val_loss: 3.9472 - val_mae: 0.4934 - val_mean_pred: 0.8305 - val_mae_t1: 0.0329\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.3950 - mae: 0.4244 - mean_pred: 0.8107 - mae_t1: 0.0283 - val_loss: 3.3882 - val_mae: 0.4235 - val_mean_pred: 0.8464 - val_mae_t1: 0.0282\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.9396 - mae: 0.3674 - mean_pred: 0.7071 - mae_t1: 0.0245 - val_loss: 4.0081 - val_mae: 0.5010 - val_mean_pred: 0.7486 - val_mae_t1: 0.0334\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1841 - mae: 0.3980 - mean_pred: 0.6474 - mae_t1: 0.0265 - val_loss: 3.6586 - val_mae: 0.4573 - val_mean_pred: 0.9283 - val_mae_t1: 0.0305\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 3.1226 - mae: 0.3903 - mean_pred: 0.8432 - mae_t1: 0.0260 - val_loss: 3.8048 - val_mae: 0.4756 - val_mean_pred: 0.9079 - val_mae_t1: 0.0317\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.0066 - mae: 0.3758 - mean_pred: 0.7554 - mae_t1: 0.0251 - val_loss: 4.2980 - val_mae: 0.5373 - val_mean_pred: 0.6642 - val_mae_t1: 0.0358\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.6654 - mae: 0.4582 - mean_pred: 0.5405 - mae_t1: 0.0305 - val_loss: 4.4278 - val_mae: 0.5535 - val_mean_pred: 0.7179 - val_mae_t1: 0.0369\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 3.3680 - mae: 0.4210 - mean_pred: 0.6489 - mae_t1: 0.0281 - val_loss: 4.3774 - val_mae: 0.5472 - val_mean_pred: 0.9129 - val_mae_t1: 0.0365\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1499 - mae: 0.3937 - mean_pred: 0.7820 - mae_t1: 0.0262 - val_loss: 4.0886 - val_mae: 0.5111 - val_mean_pred: 0.9199 - val_mae_t1: 0.0341\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.2963 - mae: 0.4120 - mean_pred: 0.8082 - mae_t1: 0.0275 - val_loss: 4.5645 - val_mae: 0.5706 - val_mean_pred: 1.0207 - val_mae_t1: 0.0380\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.5422 - mae: 0.4428 - mean_pred: 0.9368 - mae_t1: 0.0295 - val_loss: 4.1404 - val_mae: 0.5176 - val_mean_pred: 0.9007 - val_mae_t1: 0.0345\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4358 - mae: 0.4295 - mean_pred: 0.8555 - mae_t1: 0.0286 - val_loss: 3.9247 - val_mae: 0.4906 - val_mean_pred: 0.7298 - val_mae_t1: 0.0327\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 3.2410 - mae: 0.4051 - mean_pred: 0.7183 - mae_t1: 0.0270 - val_loss: 3.9410 - val_mae: 0.4926 - val_mean_pred: 0.8854 - val_mae_t1: 0.0328\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 3.3766 - mae: 0.4221 - mean_pred: 0.8655 - mae_t1: 0.0281 - val_loss: 3.9804 - val_mae: 0.4975 - val_mean_pred: 1.0484 - val_mae_t1: 0.0332\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 3.3020 - mae: 0.4128 - mean_pred: 0.9646 - mae_t1: 0.0275 - val_loss: 3.4733 - val_mae: 0.4342 - val_mean_pred: 0.9330 - val_mae_t1: 0.0289\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9154 - mae: 0.3644 - mean_pred: 0.8354 - mae_t1: 0.0243 - val_loss: 3.6083 - val_mae: 0.4510 - val_mean_pred: 0.8967 - val_mae_t1: 0.0301\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9128 - mae: 0.3641 - mean_pred: 0.8158 - mae_t1: 0.0243 - val_loss: 3.7858 - val_mae: 0.4732 - val_mean_pred: 0.9787 - val_mae_t1: 0.0315\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.3817 - mae: 0.4227 - mean_pred: 0.9075 - mae_t1: 0.0282 - val_loss: 3.9042 - val_mae: 0.4880 - val_mean_pred: 1.0190 - val_mae_t1: 0.0325\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.3297 - mae: 0.4162 - mean_pred: 0.9447 - mae_t1: 0.0277 - val_loss: 3.8792 - val_mae: 0.4849 - val_mean_pred: 0.9531 - val_mae_t1: 0.0323\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9148 - mae: 0.3643 - mean_pred: 0.8360 - mae_t1: 0.0243 - val_loss: 4.3389 - val_mae: 0.5424 - val_mean_pred: 0.6986 - val_mae_t1: 0.0362\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.3289 - mae: 0.4161 - mean_pred: 0.6093 - mae_t1: 0.0277 - val_loss: 4.1756 - val_mae: 0.5219 - val_mean_pred: 0.6189 - val_mae_t1: 0.0348\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.4310 - mae: 0.4289 - mean_pred: 0.6361 - mae_t1: 0.0286 - val_loss: 4.0659 - val_mae: 0.5082 - val_mean_pred: 0.8307 - val_mae_t1: 0.0339\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.5517 - mae: 0.4440 - mean_pred: 0.8167 - mae_t1: 0.0296 - val_loss: 3.8296 - val_mae: 0.4787 - val_mean_pred: 1.0131 - val_mae_t1: 0.0319\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.0066 - mae: 0.3758 - mean_pred: 0.9412 - mae_t1: 0.0251 - val_loss: 4.3466 - val_mae: 0.5433 - val_mean_pred: 1.0336 - val_mae_t1: 0.0362\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.1868 - mae: 0.3984 - mean_pred: 0.8911 - mae_t1: 0.0266 - val_loss: 3.6268 - val_mae: 0.4534 - val_mean_pred: 0.8496 - val_mae_t1: 0.0302\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6163 - mae: 0.3270 - mean_pred: 0.7824 - mae_t1: 0.0218 - val_loss: 3.5981 - val_mae: 0.4498 - val_mean_pred: 0.7513 - val_mae_t1: 0.0300\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.8113 - mae: 0.3514 - mean_pred: 0.7136 - mae_t1: 0.0234 - val_loss: 4.2376 - val_mae: 0.5297 - val_mean_pred: 0.7428 - val_mae_t1: 0.0353\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 3.0878 - mae: 0.3860 - mean_pred: 0.7066 - mae_t1: 0.0257 - val_loss: 3.4008 - val_mae: 0.4251 - val_mean_pred: 0.8502 - val_mae_t1: 0.0283\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.7427 - mae: 0.3428 - mean_pred: 0.8314 - mae_t1: 0.0229 - val_loss: 3.4650 - val_mae: 0.4331 - val_mean_pred: 0.9545 - val_mae_t1: 0.0289\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9621 - mae: 0.3703 - mean_pred: 0.8839 - mae_t1: 0.0247 - val_loss: 4.0885 - val_mae: 0.5111 - val_mean_pred: 0.8059 - val_mae_t1: 0.0341\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.2349 - mae: 0.4044 - mean_pred: 0.6948 - mae_t1: 0.0270 - val_loss: 4.9685 - val_mae: 0.6211 - val_mean_pred: 0.6171 - val_mae_t1: 0.0414\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.9547 - mae: 0.4943 - mean_pred: 0.5287 - mae_t1: 0.0330 - val_loss: 4.1732 - val_mae: 0.5216 - val_mean_pred: 0.6447 - val_mae_t1: 0.0348\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.0541 - mae: 0.3818 - mean_pred: 0.6225 - mae_t1: 0.0255 - val_loss: 3.9680 - val_mae: 0.4960 - val_mean_pred: 1.0443 - val_mae_t1: 0.0331\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1482 - mae: 0.3935 - mean_pred: 0.9659 - mae_t1: 0.0262 - val_loss: 4.8882 - val_mae: 0.6110 - val_mean_pred: 1.2286 - val_mae_t1: 0.0407\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.5386 - mae: 0.4423 - mean_pred: 1.0325 - mae_t1: 0.0295 - val_loss: 3.8746 - val_mae: 0.4843 - val_mean_pred: 0.9371 - val_mae_t1: 0.0323\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.8532 - mae: 0.3567 - mean_pred: 0.7490 - mae_t1: 0.0238 - val_loss: 4.1007 - val_mae: 0.5126 - val_mean_pred: 0.6465 - val_mae_t1: 0.0342\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.2780 - mae: 0.4098 - mean_pred: 0.5776 - mae_t1: 0.0273 - val_loss: 3.9017 - val_mae: 0.4877 - val_mean_pred: 0.6813 - val_mae_t1: 0.0325\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.3898 - mae: 0.4237 - mean_pred: 0.6878 - mae_t1: 0.0282 - val_loss: 3.6464 - val_mae: 0.4558 - val_mean_pred: 0.8711 - val_mae_t1: 0.0304\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.6070 - mae: 0.3259 - mean_pred: 0.8046 - mae_t1: 0.0217 - val_loss: 4.0520 - val_mae: 0.5065 - val_mean_pred: 0.9919 - val_mae_t1: 0.0338\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0534 - mae: 0.3817 - mean_pred: 0.8607 - mae_t1: 0.0254 - val_loss: 3.6328 - val_mae: 0.4541 - val_mean_pred: 0.8665 - val_mae_t1: 0.0303\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.4897 - mae: 0.3112 - mean_pred: 0.7338 - mae_t1: 0.0207 - val_loss: 4.0543 - val_mae: 0.5068 - val_mean_pred: 0.7438 - val_mae_t1: 0.0338\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1922 - mae: 0.3990 - mean_pred: 0.6839 - mae_t1: 0.0266 - val_loss: 4.1310 - val_mae: 0.5164 - val_mean_pred: 0.7487 - val_mae_t1: 0.0344\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1222 - mae: 0.3903 - mean_pred: 0.6653 - mae_t1: 0.0260 - val_loss: 3.8296 - val_mae: 0.4787 - val_mean_pred: 0.8461 - val_mae_t1: 0.0319\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 2.8660 - mae: 0.3583 - mean_pred: 0.7446 - mae_t1: 0.0239 - val_loss: 4.0323 - val_mae: 0.5040 - val_mean_pred: 1.0294 - val_mae_t1: 0.0336\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 2.8872 - mae: 0.3609 - mean_pred: 0.8706 - mae_t1: 0.0241 - val_loss: 3.9714 - val_mae: 0.4964 - val_mean_pred: 1.0377 - val_mae_t1: 0.0331\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7422 - mae: 0.3428 - mean_pred: 0.8809 - mae_t1: 0.0229 - val_loss: 3.7166 - val_mae: 0.4646 - val_mean_pred: 0.9166 - val_mae_t1: 0.0310\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 2.6025 - mae: 0.3253 - mean_pred: 0.7725 - mae_t1: 0.0217 - val_loss: 3.4596 - val_mae: 0.4325 - val_mean_pred: 0.8466 - val_mae_t1: 0.0288\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5043 - mae: 0.3130 - mean_pred: 0.7932 - mae_t1: 0.0209 - val_loss: 3.3190 - val_mae: 0.4149 - val_mean_pred: 0.9461 - val_mae_t1: 0.0277\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 2.5385 - mae: 0.3173 - mean_pred: 0.8746 - mae_t1: 0.0212 - val_loss: 3.4554 - val_mae: 0.4319 - val_mean_pred: 0.9187 - val_mae_t1: 0.0288\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.5758 - mae: 0.3220 - mean_pred: 0.8078 - mae_t1: 0.0215 - val_loss: 3.8351 - val_mae: 0.4794 - val_mean_pred: 0.9234 - val_mae_t1: 0.0320\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.35802068676864934  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▃▃▃▃▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▃▂▂▁▂▁▁▁</td></tr><tr><td>mae</td><td>█▅▃▃▃▃▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▃▂▂▁▂▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▃▃▃▃▂▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▃▂▂▁▂▁▁▁</td></tr><tr><td>mean_pred</td><td>▁█▆▄▇▅▅▇▇▄▇▄▆▆▅▅▆▆▆▆▅▅▅▆▇▆▆▇▄▇▅▆▄▇▄▆▅▅▅▆</td></tr><tr><td>val_loss</td><td>▆▃▄▆▄▆▄▃▄▇▅▅▄▆▅▄▅▃▁▄▄▃▆▅▅▄▄▄▅▆▅▂▅█▄▅▅▅▂▄</td></tr><tr><td>val_mae</td><td>▆▃▄▆▄▆▄▃▄▇▅▅▄▆▅▄▅▃▁▄▄▃▆▅▅▄▄▄▅▆▅▂▅█▄▅▅▅▂▄</td></tr><tr><td>val_mae_t1</td><td>▆▃▄▆▄▆▄▃▄▇▅▅▄▆▅▄▅▃▁▄▄▃▆▅▅▄▄▄▅▆▅▂▅█▄▅▅▅▂▄</td></tr><tr><td>val_mean_pred</td><td>▁▃▅▁▄▄▆▄▅▂▃▃▄▇▄▅▅▄▅▅▄▅▂▅▅▆▆▅▄▆▃▅▂█▃▆▃▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.39921</td></tr><tr><td>AE_2</td><td>0.37317</td></tr><tr><td>AE_3</td><td>0.37055</td></tr><tr><td>MAE</td><td>0.35802</td></tr><tr><td>best_epoch</td><td>45</td></tr><tr><td>best_val_loss</td><td>3.14341</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.57578</td></tr><tr><td>mae</td><td>0.32197</td></tr><tr><td>mae_t1</td><td>0.02146</td></tr><tr><td>mean_pred</td><td>0.80778</td></tr><tr><td>val_loss</td><td>3.83509</td></tr><tr><td>val_mae</td><td>0.47939</td></tr><tr><td>val_mae_t1</td><td>0.03196</td></tr><tr><td>val_mean_pred</td><td>0.92336</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lilac-night-52</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35ig8b2z\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35ig8b2z</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154039-35ig8b2z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154101-qbsbragg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/qbsbragg\" target=\"_blank\">volcanic-snowflake-53</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 865us/sample - loss: 14.0022 - mae: 0.7779 - mean_pred: 0.5088 - mae_t1: 0.0519 - val_loss: 10.5037 - val_mae: 0.5835 - val_mean_pred: 1.1556 - val_mae_t1: 0.0389\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 332us/sample - loss: 12.5319 - mae: 0.6962 - mean_pred: 0.6615 - mae_t1: 0.0464 - val_loss: 9.1505 - val_mae: 0.5084 - val_mean_pred: 0.5850 - val_mae_t1: 0.0339\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 249us/sample - loss: 10.9955 - mae: 0.6109 - mean_pred: 0.8781 - mae_t1: 0.0407 - val_loss: 9.4436 - val_mae: 0.5246 - val_mean_pred: 0.6370 - val_mae_t1: 0.0350\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 9.9714 - mae: 0.5540 - mean_pred: 0.5155 - mae_t1: 0.0369 - val_loss: 8.9900 - val_mae: 0.4994 - val_mean_pred: 1.0127 - val_mae_t1: 0.0333\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 10.2467 - mae: 0.5693 - mean_pred: 0.9235 - mae_t1: 0.0380 - val_loss: 10.2816 - val_mae: 0.5712 - val_mean_pred: 0.8035 - val_mae_t1: 0.0381\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 9.0837 - mae: 0.5047 - mean_pred: 0.6714 - mae_t1: 0.0336 - val_loss: 9.3518 - val_mae: 0.5195 - val_mean_pred: 0.9546 - val_mae_t1: 0.0346\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 8.8130 - mae: 0.4896 - mean_pred: 0.8791 - mae_t1: 0.0326 - val_loss: 8.8512 - val_mae: 0.4917 - val_mean_pred: 0.8406 - val_mae_t1: 0.0328\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 8.3122 - mae: 0.4618 - mean_pred: 0.7770 - mae_t1: 0.0308 - val_loss: 11.6681 - val_mae: 0.6482 - val_mean_pred: 1.1666 - val_mae_t1: 0.0432\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 11.5975 - mae: 0.6443 - mean_pred: 1.0327 - mae_t1: 0.0430 - val_loss: 13.8783 - val_mae: 0.7710 - val_mean_pred: 0.5451 - val_mae_t1: 0.0514\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 10.5128 - mae: 0.5840 - mean_pred: 0.7054 - mae_t1: 0.0389 - val_loss: 11.4296 - val_mae: 0.6350 - val_mean_pred: 0.9329 - val_mae_t1: 0.0423\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 9.3074 - mae: 0.5171 - mean_pred: 0.7343 - mae_t1: 0.0345 - val_loss: 10.9455 - val_mae: 0.6081 - val_mean_pred: 0.6339 - val_mae_t1: 0.0405\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 8.5094 - mae: 0.4727 - mean_pred: 0.7216 - mae_t1: 0.0315 - val_loss: 9.2272 - val_mae: 0.5126 - val_mean_pred: 0.7626 - val_mae_t1: 0.0342\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 10.1121 - mae: 0.5618 - mean_pred: 0.5665 - mae_t1: 0.0375 - val_loss: 9.0171 - val_mae: 0.5010 - val_mean_pred: 0.7563 - val_mae_t1: 0.0334\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 8.5100 - mae: 0.4728 - mean_pred: 0.8022 - mae_t1: 0.0315 - val_loss: 8.9555 - val_mae: 0.4975 - val_mean_pred: 0.6941 - val_mae_t1: 0.0332\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.4725 - mae: 0.5263 - mean_pred: 0.5841 - mae_t1: 0.0351 - val_loss: 8.9134 - val_mae: 0.4952 - val_mean_pred: 0.7903 - val_mae_t1: 0.0330\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 8.5965 - mae: 0.4776 - mean_pred: 0.8569 - mae_t1: 0.0318 - val_loss: 8.4760 - val_mae: 0.4709 - val_mean_pred: 0.9436 - val_mae_t1: 0.0314\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.5077 - mae: 0.4171 - mean_pred: 0.7250 - mae_t1: 0.0278 - val_loss: 9.3750 - val_mae: 0.5208 - val_mean_pred: 0.8659 - val_mae_t1: 0.0347\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 8.3338 - mae: 0.4630 - mean_pred: 0.9178 - mae_t1: 0.0309 - val_loss: 8.7276 - val_mae: 0.4849 - val_mean_pred: 0.7698 - val_mae_t1: 0.0323\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.6177 - mae: 0.4232 - mean_pred: 0.7382 - mae_t1: 0.0282 - val_loss: 10.5920 - val_mae: 0.5884 - val_mean_pred: 1.1023 - val_mae_t1: 0.0392\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.5513 - mae: 0.4195 - mean_pred: 0.8621 - mae_t1: 0.0280 - val_loss: 8.8431 - val_mae: 0.4913 - val_mean_pred: 0.7779 - val_mae_t1: 0.0328\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.5141 - mae: 0.4175 - mean_pred: 0.6798 - mae_t1: 0.0278 - val_loss: 9.8311 - val_mae: 0.5462 - val_mean_pred: 1.1067 - val_mae_t1: 0.0364\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.1879 - mae: 0.4549 - mean_pred: 0.9139 - mae_t1: 0.0303 - val_loss: 10.0932 - val_mae: 0.5607 - val_mean_pred: 0.5563 - val_mae_t1: 0.0374\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 9.2272 - mae: 0.5126 - mean_pred: 0.4777 - mae_t1: 0.0342 - val_loss: 9.4259 - val_mae: 0.5237 - val_mean_pred: 0.9424 - val_mae_t1: 0.0349\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.1527 - mae: 0.4529 - mean_pred: 0.9239 - mae_t1: 0.0302 - val_loss: 8.8889 - val_mae: 0.4938 - val_mean_pred: 0.8651 - val_mae_t1: 0.0329\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 7.2635 - mae: 0.4035 - mean_pred: 0.7205 - mae_t1: 0.0269 - val_loss: 8.3293 - val_mae: 0.4627 - val_mean_pred: 0.7984 - val_mae_t1: 0.0308\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 7.1893 - mae: 0.3994 - mean_pred: 0.7626 - mae_t1: 0.0266 - val_loss: 12.1089 - val_mae: 0.6727 - val_mean_pred: 0.9607 - val_mae_t1: 0.0448\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.9081 - mae: 0.4949 - mean_pred: 0.7261 - mae_t1: 0.0330 - val_loss: 9.4618 - val_mae: 0.5257 - val_mean_pred: 0.9890 - val_mae_t1: 0.0350\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.9661 - mae: 0.4981 - mean_pred: 0.9513 - mae_t1: 0.0332 - val_loss: 10.8113 - val_mae: 0.6006 - val_mean_pred: 0.7780 - val_mae_t1: 0.0400\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 8.9613 - mae: 0.4979 - mean_pred: 0.6376 - mae_t1: 0.0332 - val_loss: 8.2792 - val_mae: 0.4600 - val_mean_pred: 0.8675 - val_mae_t1: 0.0307\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.5973 - mae: 0.4776 - mean_pred: 0.9944 - mae_t1: 0.0318 - val_loss: 8.8864 - val_mae: 0.4937 - val_mean_pred: 0.9715 - val_mae_t1: 0.0329\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 9.0141 - mae: 0.5008 - mean_pred: 0.7411 - mae_t1: 0.0334 - val_loss: 9.6113 - val_mae: 0.5340 - val_mean_pred: 0.6292 - val_mae_t1: 0.0356\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.5722 - mae: 0.4207 - mean_pred: 0.6738 - mae_t1: 0.0280 - val_loss: 9.4025 - val_mae: 0.5224 - val_mean_pred: 1.0437 - val_mae_t1: 0.0348\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 7.6105 - mae: 0.4228 - mean_pred: 0.8207 - mae_t1: 0.0282 - val_loss: 8.5199 - val_mae: 0.4733 - val_mean_pred: 0.7125 - val_mae_t1: 0.0316\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 7.6834 - mae: 0.4269 - mean_pred: 0.6511 - mae_t1: 0.0285 - val_loss: 8.8359 - val_mae: 0.4909 - val_mean_pred: 0.9297 - val_mae_t1: 0.0327\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.8109 - mae: 0.3784 - mean_pred: 0.8594 - mae_t1: 0.0252 - val_loss: 8.5459 - val_mae: 0.4748 - val_mean_pred: 0.9084 - val_mae_t1: 0.0317\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.2749 - mae: 0.3486 - mean_pred: 0.7777 - mae_t1: 0.0232 - val_loss: 8.5981 - val_mae: 0.4777 - val_mean_pred: 0.8690 - val_mae_t1: 0.0318\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.5092 - mae: 0.3616 - mean_pred: 0.7911 - mae_t1: 0.0241 - val_loss: 8.3514 - val_mae: 0.4640 - val_mean_pred: 0.8492 - val_mae_t1: 0.0309\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.3605 - mae: 0.4089 - mean_pred: 0.6613 - mae_t1: 0.0273 - val_loss: 9.3200 - val_mae: 0.5178 - val_mean_pred: 0.7079 - val_mae_t1: 0.0345\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.9346 - mae: 0.3853 - mean_pred: 0.6976 - mae_t1: 0.0257 - val_loss: 8.8494 - val_mae: 0.4916 - val_mean_pred: 0.9371 - val_mae_t1: 0.0328\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 6.4952 - mae: 0.3608 - mean_pred: 0.8090 - mae_t1: 0.0241 - val_loss: 9.7855 - val_mae: 0.5436 - val_mean_pred: 0.7762 - val_mae_t1: 0.0362\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.6202 - mae: 0.5345 - mean_pred: 0.6514 - mae_t1: 0.0356 - val_loss: 10.8996 - val_mae: 0.6055 - val_mean_pred: 0.6957 - val_mae_t1: 0.0404\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.3805 - mae: 0.4100 - mean_pred: 0.7649 - mae_t1: 0.0273 - val_loss: 8.3692 - val_mae: 0.4650 - val_mean_pred: 0.9983 - val_mae_t1: 0.0310\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.3624 - mae: 0.3535 - mean_pred: 0.7621 - mae_t1: 0.0236 - val_loss: 9.7813 - val_mae: 0.5434 - val_mean_pred: 0.6966 - val_mae_t1: 0.0362\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.8028 - mae: 0.3779 - mean_pred: 0.6765 - mae_t1: 0.0252 - val_loss: 9.3618 - val_mae: 0.5201 - val_mean_pred: 1.0302 - val_mae_t1: 0.0347\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 221us/sample - loss: 6.4500 - mae: 0.3583 - mean_pred: 0.8456 - mae_t1: 0.0239 - val_loss: 8.0175 - val_mae: 0.4454 - val_mean_pred: 0.8435 - val_mae_t1: 0.0297\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.7885 - mae: 0.3771 - mean_pred: 0.8009 - mae_t1: 0.0251 - val_loss: 9.4351 - val_mae: 0.5242 - val_mean_pred: 0.8704 - val_mae_t1: 0.0349\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.8382 - mae: 0.3799 - mean_pred: 0.7585 - mae_t1: 0.0253 - val_loss: 10.1429 - val_mae: 0.5635 - val_mean_pred: 0.9794 - val_mae_t1: 0.0376\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.6593 - mae: 0.4255 - mean_pred: 0.8804 - mae_t1: 0.0284 - val_loss: 9.9805 - val_mae: 0.5545 - val_mean_pred: 0.8460 - val_mae_t1: 0.0370\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 7.3600 - mae: 0.4089 - mean_pred: 0.6754 - mae_t1: 0.0273 - val_loss: 8.7176 - val_mae: 0.4843 - val_mean_pred: 0.7314 - val_mae_t1: 0.0323\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.0539 - mae: 0.3919 - mean_pred: 0.6863 - mae_t1: 0.0261 - val_loss: 8.2145 - val_mae: 0.4564 - val_mean_pred: 0.9204 - val_mae_t1: 0.0304\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.3910 - mae: 0.3551 - mean_pred: 0.7884 - mae_t1: 0.0237 - val_loss: 9.2655 - val_mae: 0.5148 - val_mean_pred: 0.9761 - val_mae_t1: 0.0343\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 5.8479 - mae: 0.3249 - mean_pred: 0.8047 - mae_t1: 0.0217 - val_loss: 8.4889 - val_mae: 0.4716 - val_mean_pred: 0.8948 - val_mae_t1: 0.0314\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.0308 - mae: 0.3350 - mean_pred: 0.8040 - mae_t1: 0.0223 - val_loss: 8.5431 - val_mae: 0.4746 - val_mean_pred: 0.9172 - val_mae_t1: 0.0316\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.0646 - mae: 0.3369 - mean_pred: 0.7562 - mae_t1: 0.0225 - val_loss: 8.0961 - val_mae: 0.4498 - val_mean_pred: 0.8249 - val_mae_t1: 0.0300\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.5446 - mae: 0.3636 - mean_pred: 0.7272 - mae_t1: 0.0242 - val_loss: 10.7340 - val_mae: 0.5963 - val_mean_pred: 0.8975 - val_mae_t1: 0.0398\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 7.7807 - mae: 0.4323 - mean_pred: 0.8103 - mae_t1: 0.0288 - val_loss: 10.9038 - val_mae: 0.6058 - val_mean_pred: 1.0788 - val_mae_t1: 0.0404\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 7.3038 - mae: 0.4058 - mean_pred: 0.9497 - mae_t1: 0.0271 - val_loss: 8.9015 - val_mae: 0.4945 - val_mean_pred: 0.7182 - val_mae_t1: 0.0330\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 7.9948 - mae: 0.4442 - mean_pred: 0.6063 - mae_t1: 0.0296 - val_loss: 7.5001 - val_mae: 0.4167 - val_mean_pred: 0.7080 - val_mae_t1: 0.0278\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.0560 - mae: 0.3920 - mean_pred: 0.8420 - mae_t1: 0.0261 - val_loss: 11.9359 - val_mae: 0.6631 - val_mean_pred: 1.2784 - val_mae_t1: 0.0442\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.9825 - mae: 0.3879 - mean_pred: 0.9804 - mae_t1: 0.0259 - val_loss: 9.0973 - val_mae: 0.5054 - val_mean_pred: 0.7519 - val_mae_t1: 0.0337\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.6505 - mae: 0.5917 - mean_pred: 0.5982 - mae_t1: 0.0394 - val_loss: 11.1289 - val_mae: 0.6183 - val_mean_pred: 0.4734 - val_mae_t1: 0.0412\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.9441 - mae: 0.4969 - mean_pred: 0.5360 - mae_t1: 0.0331 - val_loss: 12.0652 - val_mae: 0.6703 - val_mean_pred: 1.1641 - val_mae_t1: 0.0447\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 8.9806 - mae: 0.4989 - mean_pred: 1.0185 - mae_t1: 0.0333 - val_loss: 9.5658 - val_mae: 0.5314 - val_mean_pred: 0.9622 - val_mae_t1: 0.0354\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.2608 - mae: 0.4034 - mean_pred: 0.7732 - mae_t1: 0.0269 - val_loss: 9.9070 - val_mae: 0.5504 - val_mean_pred: 0.6691 - val_mae_t1: 0.0367\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.0222 - mae: 0.3901 - mean_pred: 0.7133 - mae_t1: 0.0260 - val_loss: 7.9715 - val_mae: 0.4429 - val_mean_pred: 0.9601 - val_mae_t1: 0.0295\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2029 - mae: 0.3446 - mean_pred: 0.8753 - mae_t1: 0.0230 - val_loss: 10.7406 - val_mae: 0.5967 - val_mean_pred: 0.8266 - val_mae_t1: 0.0398\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.0476 - mae: 0.3915 - mean_pred: 0.6518 - mae_t1: 0.0261 - val_loss: 8.2598 - val_mae: 0.4589 - val_mean_pred: 0.9402 - val_mae_t1: 0.0306\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 7.0245 - mae: 0.3903 - mean_pred: 0.9388 - mae_t1: 0.0260 - val_loss: 9.9456 - val_mae: 0.5525 - val_mean_pred: 1.1044 - val_mae_t1: 0.0368\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.7765 - mae: 0.3765 - mean_pred: 0.9195 - mae_t1: 0.0251 - val_loss: 12.7787 - val_mae: 0.7099 - val_mean_pred: 0.8003 - val_mae_t1: 0.0473\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 10.5400 - mae: 0.5856 - mean_pred: 0.6428 - mae_t1: 0.0390 - val_loss: 9.0549 - val_mae: 0.5031 - val_mean_pred: 0.5914 - val_mae_t1: 0.0335\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 9.1899 - mae: 0.5105 - mean_pred: 0.7137 - mae_t1: 0.0340 - val_loss: 9.9797 - val_mae: 0.5544 - val_mean_pred: 1.0128 - val_mae_t1: 0.0370\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.5123 - mae: 0.4174 - mean_pred: 0.9634 - mae_t1: 0.0278 - val_loss: 8.7304 - val_mae: 0.4850 - val_mean_pred: 1.0387 - val_mae_t1: 0.0323\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 5.7703 - mae: 0.3206 - mean_pred: 0.7800 - mae_t1: 0.0214 - val_loss: 9.4199 - val_mae: 0.5233 - val_mean_pred: 0.6586 - val_mae_t1: 0.0349\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.8624 - mae: 0.3812 - mean_pred: 0.6065 - mae_t1: 0.0254 - val_loss: 8.0924 - val_mae: 0.4496 - val_mean_pred: 0.8846 - val_mae_t1: 0.0300\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2935 - mae: 0.3496 - mean_pred: 0.8346 - mae_t1: 0.0233 - val_loss: 8.4450 - val_mae: 0.4692 - val_mean_pred: 0.8692 - val_mae_t1: 0.0313\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 6.3639 - mae: 0.3536 - mean_pred: 0.7203 - mae_t1: 0.0236 - val_loss: 8.3947 - val_mae: 0.4664 - val_mean_pred: 0.7918 - val_mae_t1: 0.0311\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.0775 - mae: 0.3376 - mean_pred: 0.7500 - mae_t1: 0.0225 - val_loss: 9.0472 - val_mae: 0.5026 - val_mean_pred: 1.0257 - val_mae_t1: 0.0335\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2284 - mae: 0.3460 - mean_pred: 0.8944 - mae_t1: 0.0231 - val_loss: 9.9790 - val_mae: 0.5544 - val_mean_pred: 0.8873 - val_mae_t1: 0.0370\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 6.6458 - mae: 0.3692 - mean_pred: 0.7103 - mae_t1: 0.0246 - val_loss: 7.2094 - val_mae: 0.4005 - val_mean_pred: 0.8125 - val_mae_t1: 0.0267\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 5.7097 - mae: 0.3172 - mean_pred: 0.8150 - mae_t1: 0.0211 - val_loss: 12.0741 - val_mae: 0.6708 - val_mean_pred: 1.0679 - val_mae_t1: 0.0447\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.8378 - mae: 0.3799 - mean_pred: 0.8653 - mae_t1: 0.0253 - val_loss: 8.4037 - val_mae: 0.4669 - val_mean_pred: 0.7740 - val_mae_t1: 0.0311\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.7225 - mae: 0.3735 - mean_pred: 0.7137 - mae_t1: 0.0249 - val_loss: 9.9504 - val_mae: 0.5528 - val_mean_pred: 0.8235 - val_mae_t1: 0.0369\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2425 - mae: 0.3468 - mean_pred: 0.7968 - mae_t1: 0.0231 - val_loss: 8.1798 - val_mae: 0.4544 - val_mean_pred: 0.9345 - val_mae_t1: 0.0303\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 6.6675 - mae: 0.3704 - mean_pred: 0.7955 - mae_t1: 0.0247 - val_loss: 8.6914 - val_mae: 0.4829 - val_mean_pred: 0.7784 - val_mae_t1: 0.0322\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.6065 - mae: 0.4226 - mean_pred: 0.7130 - mae_t1: 0.0282 - val_loss: 12.5627 - val_mae: 0.6979 - val_mean_pred: 0.9063 - val_mae_t1: 0.0465\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.5813 - mae: 0.5323 - mean_pred: 0.8429 - mae_t1: 0.0355 - val_loss: 8.6574 - val_mae: 0.4810 - val_mean_pred: 0.9458 - val_mae_t1: 0.0321\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 5.5245 - mae: 0.3069 - mean_pred: 0.7918 - mae_t1: 0.0205 - val_loss: 12.2099 - val_mae: 0.6783 - val_mean_pred: 0.8848 - val_mae_t1: 0.0452\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.2126 - mae: 0.4007 - mean_pred: 0.7786 - mae_t1: 0.0267 - val_loss: 7.8799 - val_mae: 0.4378 - val_mean_pred: 0.8855 - val_mae_t1: 0.0292\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 6.0060 - mae: 0.3337 - mean_pred: 0.7711 - mae_t1: 0.0222 - val_loss: 8.6809 - val_mae: 0.4823 - val_mean_pred: 0.8860 - val_mae_t1: 0.0322\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5796 - mae: 0.3100 - mean_pred: 0.8093 - mae_t1: 0.0207 - val_loss: 9.3601 - val_mae: 0.5200 - val_mean_pred: 0.9785 - val_mae_t1: 0.0347\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.6971 - mae: 0.3165 - mean_pred: 0.7982 - mae_t1: 0.0211 - val_loss: 9.1660 - val_mae: 0.5092 - val_mean_pred: 0.8251 - val_mae_t1: 0.0339\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 8.1593 - mae: 0.4533 - mean_pred: 0.8179 - mae_t1: 0.0302 - val_loss: 13.2938 - val_mae: 0.7385 - val_mean_pred: 0.9931 - val_mae_t1: 0.0492\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.0276 - mae: 0.5015 - mean_pred: 0.8837 - mae_t1: 0.0334 - val_loss: 9.4333 - val_mae: 0.5241 - val_mean_pred: 0.8876 - val_mae_t1: 0.0349\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.0790 - mae: 0.4488 - mean_pred: 0.8144 - mae_t1: 0.0299 - val_loss: 11.9406 - val_mae: 0.6634 - val_mean_pred: 0.8297 - val_mae_t1: 0.0442\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.1027 - mae: 0.3946 - mean_pred: 0.7524 - mae_t1: 0.0263 - val_loss: 8.7357 - val_mae: 0.4853 - val_mean_pred: 0.9958 - val_mae_t1: 0.0324\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.0892 - mae: 0.3383 - mean_pred: 0.9280 - mae_t1: 0.0226 - val_loss: 8.6234 - val_mae: 0.4791 - val_mean_pred: 0.9563 - val_mae_t1: 0.0319\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 5.9036 - mae: 0.3280 - mean_pred: 0.7384 - mae_t1: 0.0219 - val_loss: 9.3142 - val_mae: 0.5175 - val_mean_pred: 0.7605 - val_mae_t1: 0.0345\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 5.7531 - mae: 0.3196 - mean_pred: 0.7323 - mae_t1: 0.0213 - val_loss: 8.6626 - val_mae: 0.4813 - val_mean_pred: 0.9723 - val_mae_t1: 0.0321\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.8722 - mae: 0.2707 - mean_pred: 0.8106 - mae_t1: 0.0180 - val_loss: 9.3743 - val_mae: 0.5208 - val_mean_pred: 0.9340 - val_mae_t1: 0.0347\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5353 - mae: 0.3075 - mean_pred: 0.8065 - mae_t1: 0.0205 - val_loss: 7.6357 - val_mae: 0.4242 - val_mean_pred: 0.8298 - val_mae_t1: 0.0283\n",
      "Earliness...\n",
      "0.0014994144439697266\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3167605812414809  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▃▄▅▄▃▃▄▂▄▄▃▂▂▄▂▂▃▂▁▃▂▅▂▂▂▃▂▁▂▂▂▁▁▃▃▁▁</td></tr><tr><td>mae</td><td>█▆▄▃▄▅▄▃▃▄▂▄▄▃▂▂▄▂▂▃▂▁▃▂▅▂▂▂▃▂▁▂▂▂▁▁▃▃▁▁</td></tr><tr><td>mae_t1</td><td>█▆▄▃▄▅▄▃▃▄▂▄▄▃▂▂▄▂▂▃▂▁▃▂▅▂▂▂▃▂▁▂▂▂▁▁▃▃▁▁</td></tr><tr><td>mean_pred</td><td>▁▇▄▅▅▂▆▇▄▁▅█▅▃▅▄▄▄▆▄▅▅▆▆▃▅▄▇█▃▅▄▄▆▆▅▆▆▅▆</td></tr><tr><td>val_loss</td><td>▅▄▃▆▅▃▂▃▄▄▇▅▄▃▃▃▅▃▄▃▃▂▅▆▆▄▂▇▃▂▃▁▄▃▇▃█▆▃▁</td></tr><tr><td>val_mae</td><td>▅▄▃▆▅▃▂▃▄▄▇▅▄▃▃▃▅▃▄▃▃▂▅▆▆▄▂▇▃▂▃▁▄▃▇▃█▆▃▁</td></tr><tr><td>val_mae_t1</td><td>▅▄▃▆▅▃▂▃▄▄▇▅▄▃▃▃▅▃▄▃▃▂▅▆▆▄▂▇▃▂▃▁▄▃▇▃█▆▃▁</td></tr><tr><td>val_mean_pred</td><td>▇▂▅▇▂▃▅▄▇▅▅▄▂▅▄▅▃▆▄▃▅▄▆█▁▃▅▄▆▅▆▄▄▄▅▅▆▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.3087</td></tr><tr><td>AE_2</td><td>0.29301</td></tr><tr><td>AE_3</td><td>0.29222</td></tr><tr><td>MAE</td><td>0.31676</td></tr><tr><td>best_epoch</td><td>78</td></tr><tr><td>best_val_loss</td><td>7.20945</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.53528</td></tr><tr><td>mae</td><td>0.30752</td></tr><tr><td>mae_t1</td><td>0.0205</td></tr><tr><td>mean_pred</td><td>0.80646</td></tr><tr><td>val_loss</td><td>7.6357</td></tr><tr><td>val_mae</td><td>0.42421</td></tr><tr><td>val_mae_t1</td><td>0.02828</td></tr><tr><td>val_mean_pred</td><td>0.82976</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">volcanic-snowflake-53</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/qbsbragg\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/qbsbragg</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154101-qbsbragg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154130-25mhuuyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/25mhuuyj\" target=\"_blank\">polar-shape-54</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 682us/sample - loss: 13.8161 - mae: 0.7676 - mean_pred: 0.2424 - mae_t1: 0.0512 - val_loss: 9.3687 - val_mae: 0.5205 - val_mean_pred: 0.8209 - val_mae_t1: 0.0347\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 155us/sample - loss: 11.4651 - mae: 0.6370 - mean_pred: 0.6979 - mae_t1: 0.0425 - val_loss: 9.8899 - val_mae: 0.5494 - val_mean_pred: 0.6987 - val_mae_t1: 0.0366\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.5218 - mae: 0.5845 - mean_pred: 0.6682 - mae_t1: 0.0390 - val_loss: 9.1640 - val_mae: 0.5091 - val_mean_pred: 0.8644 - val_mae_t1: 0.0339\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 10.2894 - mae: 0.5716 - mean_pred: 0.7372 - mae_t1: 0.0381 - val_loss: 10.7477 - val_mae: 0.5971 - val_mean_pred: 0.6199 - val_mae_t1: 0.0398\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 11.5712 - mae: 0.6428 - mean_pred: 0.5209 - mae_t1: 0.0429 - val_loss: 9.3184 - val_mae: 0.5177 - val_mean_pred: 1.0478 - val_mae_t1: 0.0345\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 149us/sample - loss: 12.6423 - mae: 0.7023 - mean_pred: 1.1163 - mae_t1: 0.0468 - val_loss: 8.6848 - val_mae: 0.4825 - val_mean_pred: 0.8349 - val_mae_t1: 0.0322\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 10.9712 - mae: 0.6095 - mean_pred: 0.6622 - mae_t1: 0.0406 - val_loss: 14.9046 - val_mae: 0.8280 - val_mean_pred: 0.2597 - val_mae_t1: 0.0552\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 15.4025 - mae: 0.8557 - mean_pred: 0.4414 - mae_t1: 0.0570 - val_loss: 14.8727 - val_mae: 0.8263 - val_mean_pred: 0.6748 - val_mae_t1: 0.0551\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 12.9586 - mae: 0.7199 - mean_pred: 0.5602 - mae_t1: 0.0480 - val_loss: 11.8961 - val_mae: 0.6609 - val_mean_pred: 1.1319 - val_mae_t1: 0.0441\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 12.5427 - mae: 0.6968 - mean_pred: 1.0655 - mae_t1: 0.0465 - val_loss: 8.9735 - val_mae: 0.4985 - val_mean_pred: 0.8459 - val_mae_t1: 0.0332\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 9.9964 - mae: 0.5554 - mean_pred: 0.6119 - mae_t1: 0.0370 - val_loss: 11.0844 - val_mae: 0.6158 - val_mean_pred: 0.4934 - val_mae_t1: 0.0411\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 11.4689 - mae: 0.6372 - mean_pred: 0.4980 - mae_t1: 0.0425 - val_loss: 10.1952 - val_mae: 0.5664 - val_mean_pred: 0.6077 - val_mae_t1: 0.0378\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 9.5830 - mae: 0.5324 - mean_pred: 0.5386 - mae_t1: 0.0355 - val_loss: 11.3922 - val_mae: 0.6329 - val_mean_pred: 1.0344 - val_mae_t1: 0.0422\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 10.6141 - mae: 0.5897 - mean_pred: 1.0183 - mae_t1: 0.0393 - val_loss: 10.6780 - val_mae: 0.5932 - val_mean_pred: 0.8238 - val_mae_t1: 0.0395\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 10.5103 - mae: 0.5839 - mean_pred: 0.6504 - mae_t1: 0.0389 - val_loss: 12.7385 - val_mae: 0.7077 - val_mean_pred: 0.6613 - val_mae_t1: 0.0472\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 12.6709 - mae: 0.7039 - mean_pred: 0.7370 - mae_t1: 0.0469 - val_loss: 9.6164 - val_mae: 0.5342 - val_mean_pred: 0.8816 - val_mae_t1: 0.0356\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 10.4678 - mae: 0.5815 - mean_pred: 0.8604 - mae_t1: 0.0388 - val_loss: 12.8817 - val_mae: 0.7156 - val_mean_pred: 1.1142 - val_mae_t1: 0.0477\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 13.1589 - mae: 0.7310 - mean_pred: 1.1373 - mae_t1: 0.0487 - val_loss: 8.3841 - val_mae: 0.4658 - val_mean_pred: 0.7036 - val_mae_t1: 0.0311\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.7391 - mae: 0.5966 - mean_pred: 0.4931 - mae_t1: 0.0398 - val_loss: 14.3574 - val_mae: 0.7976 - val_mean_pred: 0.2540 - val_mae_t1: 0.0532\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 14.4511 - mae: 0.8028 - mean_pred: 0.2879 - mae_t1: 0.0535 - val_loss: 8.5208 - val_mae: 0.4734 - val_mean_pred: 0.7553 - val_mae_t1: 0.0316\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 9.5798 - mae: 0.5322 - mean_pred: 0.7534 - mae_t1: 0.0355 - val_loss: 9.0250 - val_mae: 0.5014 - val_mean_pred: 0.9298 - val_mae_t1: 0.0334\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 9.2371 - mae: 0.5132 - mean_pred: 0.7736 - mae_t1: 0.0342 - val_loss: 9.9816 - val_mae: 0.5545 - val_mean_pred: 0.8675 - val_mae_t1: 0.0370\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 10.1183 - mae: 0.5621 - mean_pred: 0.8345 - mae_t1: 0.0375 - val_loss: 10.0280 - val_mae: 0.5571 - val_mean_pred: 0.9145 - val_mae_t1: 0.0371\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.3672 - mae: 0.5204 - mean_pred: 0.8354 - mae_t1: 0.0347 - val_loss: 12.0573 - val_mae: 0.6698 - val_mean_pred: 0.8110 - val_mae_t1: 0.0447\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.4206 - mae: 0.5789 - mean_pred: 0.7054 - mae_t1: 0.0386 - val_loss: 11.5415 - val_mae: 0.6412 - val_mean_pred: 0.9782 - val_mae_t1: 0.0427\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.2946 - mae: 0.6275 - mean_pred: 0.9090 - mae_t1: 0.0418 - val_loss: 10.9656 - val_mae: 0.6092 - val_mean_pred: 1.0148 - val_mae_t1: 0.0406\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.5612 - mae: 0.5867 - mean_pred: 0.8201 - mae_t1: 0.0391 - val_loss: 11.5025 - val_mae: 0.6390 - val_mean_pred: 0.7640 - val_mae_t1: 0.0426\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.7355 - mae: 0.5964 - mean_pred: 0.6154 - mae_t1: 0.0398 - val_loss: 9.8183 - val_mae: 0.5455 - val_mean_pred: 0.5830 - val_mae_t1: 0.0364\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 134us/sample - loss: 10.0755 - mae: 0.5598 - mean_pred: 0.5173 - mae_t1: 0.0373 - val_loss: 8.2696 - val_mae: 0.4594 - val_mean_pred: 0.9537 - val_mae_t1: 0.0306\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 10.6562 - mae: 0.5920 - mean_pred: 0.9450 - mae_t1: 0.0395 - val_loss: 7.9324 - val_mae: 0.4407 - val_mean_pred: 0.9558 - val_mae_t1: 0.0294\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 9.1188 - mae: 0.5066 - mean_pred: 0.7473 - mae_t1: 0.0338 - val_loss: 10.3588 - val_mae: 0.5755 - val_mean_pred: 0.5559 - val_mae_t1: 0.0384\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.8829 - mae: 0.5491 - mean_pred: 0.4895 - mae_t1: 0.0366 - val_loss: 8.8216 - val_mae: 0.4901 - val_mean_pred: 0.9731 - val_mae_t1: 0.0327\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.6780 - mae: 0.4821 - mean_pred: 0.9051 - mae_t1: 0.0321 - val_loss: 10.0815 - val_mae: 0.5601 - val_mean_pred: 1.1263 - val_mae_t1: 0.0373\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.3425 - mae: 0.4635 - mean_pred: 0.8868 - mae_t1: 0.0309 - val_loss: 9.6889 - val_mae: 0.5383 - val_mean_pred: 0.7808 - val_mae_t1: 0.0359\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.0732 - mae: 0.4485 - mean_pred: 0.6882 - mae_t1: 0.0299 - val_loss: 9.5352 - val_mae: 0.5297 - val_mean_pred: 1.0346 - val_mae_t1: 0.0353\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.9377 - mae: 0.4410 - mean_pred: 0.9227 - mae_t1: 0.0294 - val_loss: 8.9393 - val_mae: 0.4966 - val_mean_pred: 0.9174 - val_mae_t1: 0.0331\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 7.5108 - mae: 0.4173 - mean_pred: 0.7177 - mae_t1: 0.0278 - val_loss: 8.8889 - val_mae: 0.4938 - val_mean_pred: 0.8093 - val_mae_t1: 0.0329\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 7.6673 - mae: 0.4260 - mean_pred: 0.6983 - mae_t1: 0.0284 - val_loss: 13.3523 - val_mae: 0.7418 - val_mean_pred: 1.3044 - val_mae_t1: 0.0495\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 11.2638 - mae: 0.6258 - mean_pred: 1.1434 - mae_t1: 0.0417 - val_loss: 12.0766 - val_mae: 0.6709 - val_mean_pred: 1.1746 - val_mae_t1: 0.0447\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.6628 - mae: 0.4813 - mean_pred: 0.8467 - mae_t1: 0.0321 - val_loss: 9.4102 - val_mae: 0.5228 - val_mean_pred: 0.6006 - val_mae_t1: 0.0349\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 11.0242 - mae: 0.6125 - mean_pred: 0.5519 - mae_t1: 0.0408 - val_loss: 9.0746 - val_mae: 0.5041 - val_mean_pred: 0.5993 - val_mae_t1: 0.0336\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 9.4043 - mae: 0.5225 - mean_pred: 0.5566 - mae_t1: 0.0348 - val_loss: 10.1632 - val_mae: 0.5646 - val_mean_pred: 1.0143 - val_mae_t1: 0.0376\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.4505 - mae: 0.5250 - mean_pred: 0.9698 - mae_t1: 0.0350 - val_loss: 12.2345 - val_mae: 0.6797 - val_mean_pred: 1.2301 - val_mae_t1: 0.0453\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 9.3441 - mae: 0.5191 - mean_pred: 1.0182 - mae_t1: 0.0346 - val_loss: 7.9031 - val_mae: 0.4391 - val_mean_pred: 0.7764 - val_mae_t1: 0.0293\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.3183 - mae: 0.4621 - mean_pred: 0.6508 - mae_t1: 0.0308 - val_loss: 10.0114 - val_mae: 0.5562 - val_mean_pred: 0.4937 - val_mae_t1: 0.0371\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 9.7844 - mae: 0.5436 - mean_pred: 0.4791 - mae_t1: 0.0362 - val_loss: 10.2269 - val_mae: 0.5682 - val_mean_pred: 0.6783 - val_mae_t1: 0.0379\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.7397 - mae: 0.4855 - mean_pred: 0.6655 - mae_t1: 0.0324 - val_loss: 9.6887 - val_mae: 0.5383 - val_mean_pred: 1.0738 - val_mae_t1: 0.0359\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.7974 - mae: 0.4887 - mean_pred: 1.0079 - mae_t1: 0.0326 - val_loss: 8.9944 - val_mae: 0.4997 - val_mean_pred: 1.0269 - val_mae_t1: 0.0333\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.4539 - mae: 0.4141 - mean_pred: 0.8389 - mae_t1: 0.0276 - val_loss: 10.8206 - val_mae: 0.6011 - val_mean_pred: 0.5788 - val_mae_t1: 0.0401\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 10.0531 - mae: 0.5585 - mean_pred: 0.4575 - mae_t1: 0.0372 - val_loss: 11.3564 - val_mae: 0.6309 - val_mean_pred: 0.4181 - val_mae_t1: 0.0421\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.0562 - mae: 0.5587 - mean_pred: 0.4021 - mae_t1: 0.0372 - val_loss: 9.5991 - val_mae: 0.5333 - val_mean_pred: 0.5938 - val_mae_t1: 0.0356\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.4906 - mae: 0.4717 - mean_pred: 0.5537 - mae_t1: 0.0314 - val_loss: 9.2626 - val_mae: 0.5146 - val_mean_pred: 0.9153 - val_mae_t1: 0.0343\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1639 - mae: 0.4536 - mean_pred: 0.8484 - mae_t1: 0.0302 - val_loss: 10.7967 - val_mae: 0.5998 - val_mean_pred: 1.1727 - val_mae_t1: 0.0400\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.2464 - mae: 0.5137 - mean_pred: 1.0554 - mae_t1: 0.0342 - val_loss: 8.9992 - val_mae: 0.5000 - val_mean_pred: 1.0024 - val_mae_t1: 0.0333\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.4460 - mae: 0.4137 - mean_pred: 0.8650 - mae_t1: 0.0276 - val_loss: 9.3285 - val_mae: 0.5182 - val_mean_pred: 0.7248 - val_mae_t1: 0.0345\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 8.1546 - mae: 0.4530 - mean_pred: 0.7213 - mae_t1: 0.0302 - val_loss: 9.3785 - val_mae: 0.5210 - val_mean_pred: 0.7315 - val_mae_t1: 0.0347\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.1111 - mae: 0.4506 - mean_pred: 0.7400 - mae_t1: 0.0300 - val_loss: 10.4771 - val_mae: 0.5821 - val_mean_pred: 0.6955 - val_mae_t1: 0.0388\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1542 - mae: 0.4530 - mean_pred: 0.6342 - mae_t1: 0.0302 - val_loss: 9.9674 - val_mae: 0.5537 - val_mean_pred: 0.6814 - val_mae_t1: 0.0369\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 8.6541 - mae: 0.4808 - mean_pred: 0.6821 - mae_t1: 0.0321 - val_loss: 10.8648 - val_mae: 0.6036 - val_mean_pred: 0.8288 - val_mae_t1: 0.0402\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.6496 - mae: 0.5361 - mean_pred: 0.7477 - mae_t1: 0.0357 - val_loss: 10.5881 - val_mae: 0.5882 - val_mean_pred: 0.8399 - val_mae_t1: 0.0392\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.9255 - mae: 0.4403 - mean_pred: 0.7323 - mae_t1: 0.0294 - val_loss: 12.0177 - val_mae: 0.6677 - val_mean_pred: 0.9608 - val_mae_t1: 0.0445\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.5623 - mae: 0.5868 - mean_pred: 0.8837 - mae_t1: 0.0391 - val_loss: 9.5719 - val_mae: 0.5318 - val_mean_pred: 1.0042 - val_mae_t1: 0.0355\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.2508 - mae: 0.4584 - mean_pred: 0.8786 - mae_t1: 0.0306 - val_loss: 10.2673 - val_mae: 0.5704 - val_mean_pred: 0.8761 - val_mae_t1: 0.0380\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.2671 - mae: 0.4593 - mean_pred: 0.7741 - mae_t1: 0.0306 - val_loss: 9.7123 - val_mae: 0.5396 - val_mean_pred: 0.7575 - val_mae_t1: 0.0360\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 8.6595 - mae: 0.4811 - mean_pred: 0.7534 - mae_t1: 0.0321 - val_loss: 9.1914 - val_mae: 0.5106 - val_mean_pred: 0.8388 - val_mae_t1: 0.0340\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.1097 - mae: 0.4505 - mean_pred: 0.8402 - mae_t1: 0.0300 - val_loss: 9.2475 - val_mae: 0.5137 - val_mean_pred: 0.9439 - val_mae_t1: 0.0342\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 8.2874 - mae: 0.4604 - mean_pred: 0.8683 - mae_t1: 0.0307 - val_loss: 8.7923 - val_mae: 0.4885 - val_mean_pred: 0.8893 - val_mae_t1: 0.0326\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 7.1886 - mae: 0.3994 - mean_pred: 0.8038 - mae_t1: 0.0266 - val_loss: 10.2010 - val_mae: 0.5667 - val_mean_pred: 0.8587 - val_mae_t1: 0.0378\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.2329 - mae: 0.5129 - mean_pred: 0.7945 - mae_t1: 0.0342 - val_loss: 9.3322 - val_mae: 0.5185 - val_mean_pred: 0.8322 - val_mae_t1: 0.0346\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 7.5976 - mae: 0.4221 - mean_pred: 0.7340 - mae_t1: 0.0281 - val_loss: 8.8907 - val_mae: 0.4939 - val_mean_pred: 0.8977 - val_mae_t1: 0.0329\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.2331 - mae: 0.4018 - mean_pred: 0.7599 - mae_t1: 0.0268 - val_loss: 8.8316 - val_mae: 0.4906 - val_mean_pred: 0.9260 - val_mae_t1: 0.0327\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 7.1463 - mae: 0.3970 - mean_pred: 0.8271 - mae_t1: 0.0265 - val_loss: 9.4879 - val_mae: 0.5271 - val_mean_pred: 0.8532 - val_mae_t1: 0.0351\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.7492 - mae: 0.4305 - mean_pred: 0.6730 - mae_t1: 0.0287 - val_loss: 9.4415 - val_mae: 0.5245 - val_mean_pred: 0.7235 - val_mae_t1: 0.0350\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.2343 - mae: 0.5130 - mean_pred: 0.7294 - mae_t1: 0.0342 - val_loss: 8.6400 - val_mae: 0.4800 - val_mean_pred: 0.8761 - val_mae_t1: 0.0320\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.8555 - mae: 0.3809 - mean_pred: 0.7753 - mae_t1: 0.0254 - val_loss: 11.1382 - val_mae: 0.6188 - val_mean_pred: 1.0055 - val_mae_t1: 0.0413\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1654 - mae: 0.4536 - mean_pred: 0.8555 - mae_t1: 0.0302 - val_loss: 10.8127 - val_mae: 0.6007 - val_mean_pred: 0.9664 - val_mae_t1: 0.0400\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.8694 - mae: 0.6039 - mean_pred: 0.8690 - mae_t1: 0.0403 - val_loss: 8.5219 - val_mae: 0.4734 - val_mean_pred: 0.8445 - val_mae_t1: 0.0316\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 7.3984 - mae: 0.4110 - mean_pred: 0.7550 - mae_t1: 0.0274 - val_loss: 11.3773 - val_mae: 0.6321 - val_mean_pred: 0.9045 - val_mae_t1: 0.0421\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 8.3738 - mae: 0.4652 - mean_pred: 0.7305 - mae_t1: 0.0310 - val_loss: 10.2698 - val_mae: 0.5705 - val_mean_pred: 0.8167 - val_mae_t1: 0.0380\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 10.8690 - mae: 0.6038 - mean_pred: 0.7954 - mae_t1: 0.0403 - val_loss: 8.8848 - val_mae: 0.4936 - val_mean_pred: 0.6619 - val_mae_t1: 0.0329\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1888 - mae: 0.4549 - mean_pred: 0.5634 - mae_t1: 0.0303 - val_loss: 12.2351 - val_mae: 0.6797 - val_mean_pred: 0.8177 - val_mae_t1: 0.0453\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.9290 - mae: 0.5516 - mean_pred: 0.6783 - mae_t1: 0.0368 - val_loss: 9.8143 - val_mae: 0.5452 - val_mean_pred: 1.0284 - val_mae_t1: 0.0363\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.6421 - mae: 0.4801 - mean_pred: 0.8971 - mae_t1: 0.0320 - val_loss: 10.8306 - val_mae: 0.6017 - val_mean_pred: 1.1333 - val_mae_t1: 0.0401\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.6053 - mae: 0.4225 - mean_pred: 0.9178 - mae_t1: 0.0282 - val_loss: 10.6912 - val_mae: 0.5940 - val_mean_pred: 0.9489 - val_mae_t1: 0.0396\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.7631 - mae: 0.4313 - mean_pred: 0.7142 - mae_t1: 0.0288 - val_loss: 10.1695 - val_mae: 0.5650 - val_mean_pred: 0.6723 - val_mae_t1: 0.0377\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.2206 - mae: 0.5123 - mean_pred: 0.6059 - mae_t1: 0.0342 - val_loss: 9.9236 - val_mae: 0.5513 - val_mean_pred: 0.5470 - val_mae_t1: 0.0368\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 9.0001 - mae: 0.5000 - mean_pred: 0.5264 - mae_t1: 0.0333 - val_loss: 12.0909 - val_mae: 0.6717 - val_mean_pred: 0.9289 - val_mae_t1: 0.0448\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.9854 - mae: 0.6103 - mean_pred: 0.8642 - mae_t1: 0.0407 - val_loss: 10.0038 - val_mae: 0.5558 - val_mean_pred: 1.1461 - val_mae_t1: 0.0371\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.3943 - mae: 0.4664 - mean_pred: 0.9995 - mae_t1: 0.0311 - val_loss: 11.0893 - val_mae: 0.6161 - val_mean_pred: 0.9931 - val_mae_t1: 0.0411\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.4532 - mae: 0.5807 - mean_pred: 0.8689 - mae_t1: 0.0387 - val_loss: 11.1124 - val_mae: 0.6174 - val_mean_pred: 0.5735 - val_mae_t1: 0.0412\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.4725 - mae: 0.5818 - mean_pred: 0.5131 - mae_t1: 0.0388 - val_loss: 15.9390 - val_mae: 0.8855 - val_mean_pred: 0.6639 - val_mae_t1: 0.0590\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 12.7395 - mae: 0.7078 - mean_pred: 0.5667 - mae_t1: 0.0472 - val_loss: 11.7641 - val_mae: 0.6536 - val_mean_pred: 0.8074 - val_mae_t1: 0.0436\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.6805 - mae: 0.4822 - mean_pred: 0.7667 - mae_t1: 0.0321 - val_loss: 11.9178 - val_mae: 0.6621 - val_mean_pred: 1.2366 - val_mae_t1: 0.0441\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 9.6605 - mae: 0.5367 - mean_pred: 1.1292 - mae_t1: 0.0358 - val_loss: 11.9121 - val_mae: 0.6618 - val_mean_pred: 1.1442 - val_mae_t1: 0.0441\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.6736 - mae: 0.5930 - mean_pred: 0.9819 - mae_t1: 0.0395 - val_loss: 10.5885 - val_mae: 0.5883 - val_mean_pred: 0.6418 - val_mae_t1: 0.0392\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.2546 - mae: 0.5141 - mean_pred: 0.5351 - mae_t1: 0.0343 - val_loss: 14.3694 - val_mae: 0.7983 - val_mean_pred: 0.4818 - val_mae_t1: 0.0532\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 11.7149 - mae: 0.6508 - mean_pred: 0.4072 - mae_t1: 0.0434 - val_loss: 11.9801 - val_mae: 0.6656 - val_mean_pred: 0.4983 - val_mae_t1: 0.0444\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.6817 - mae: 0.5934 - mean_pred: 0.5753 - mae_t1: 0.0396 - val_loss: 10.4313 - val_mae: 0.5795 - val_mean_pred: 0.8261 - val_mae_t1: 0.0386\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.2512 - mae: 0.4584 - mean_pred: 0.8086 - mae_t1: 0.0306 - val_loss: 11.5432 - val_mae: 0.6413 - val_mean_pred: 1.1610 - val_mae_t1: 0.0428\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.9233 - mae: 0.4957 - mean_pred: 1.0134 - mae_t1: 0.0330 - val_loss: 9.6919 - val_mae: 0.5384 - val_mean_pred: 1.0730 - val_mae_t1: 0.0359\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.35638360374852185  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▇▄▆█▃▃▆▆▃▄▅▄▃▂▂▄▄▃▃▁▃▃▂▂▂▂▂▃▁▃▄▂▃▁▃▂▆▃▅▃</td></tr><tr><td>mae</td><td>▇▄▆█▃▃▆▆▃▄▅▄▃▂▂▄▄▃▃▁▃▃▂▂▂▂▂▃▁▃▄▂▃▁▃▂▆▃▅▃</td></tr><tr><td>mae_t1</td><td>▇▄▆█▃▃▆▆▃▄▅▄▃▂▂▄▄▃▃▁▃▃▂▂▂▂▂▃▁▃▄▂▃▁▃▂▆▃▅▃</td></tr><tr><td>mean_pred</td><td>▁▄█▃▄▃▅█▅▆▆▄▅▆▆█▃▇▃▆▂▇▅▄▅▅▆▅▆▅▆▅▄▆▃▇▄█▂▇</td></tr><tr><td>val_loss</td><td>▂▂▂█▄▅▃▁▂▃▄▃▃▃▂▅▂▁▃▄▃▂▂▄▅▃▂▂▃▂▂▃▃▄▅▄▅▅▅▃</td></tr><tr><td>val_mae</td><td>▂▂▂█▄▅▃▁▂▃▄▃▃▃▂▅▂▁▃▄▃▂▂▄▅▃▂▂▃▂▂▃▃▄▅▄▅▅▅▃</td></tr><tr><td>val_mae_t1</td><td>▂▂▂█▄▅▃▁▂▃▄▃▃▃▂▅▂▁▃▄▃▂▂▄▅▃▂▂▃▂▂▃▃▄▅▄▅▅▅▃</td></tr><tr><td>val_mean_pred</td><td>▄▅▅▃▁▇▅▃▅▅▆▂▂▄▅█▂▄▃▂▂▆▃▄▆▄▅▄▅▅▅▄▆▆▅▆▄█▁▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.3616</td></tr><tr><td>AE_2</td><td>0.29078</td></tr><tr><td>AE_3</td><td>0.33456</td></tr><tr><td>MAE</td><td>0.35638</td></tr><tr><td>best_epoch</td><td>43</td></tr><tr><td>best_val_loss</td><td>7.90306</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>8.92329</td></tr><tr><td>mae</td><td>0.49574</td></tr><tr><td>mae_t1</td><td>0.03305</td></tr><tr><td>mean_pred</td><td>1.01336</td></tr><tr><td>val_loss</td><td>9.6919</td></tr><tr><td>val_mae</td><td>0.53844</td></tr><tr><td>val_mae_t1</td><td>0.0359</td></tr><tr><td>val_mean_pred</td><td>1.07296</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polar-shape-54</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/25mhuuyj\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/25mhuuyj</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154130-25mhuuyj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154152-1fjlvpnh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fjlvpnh\" target=\"_blank\">young-lake-55</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 863us/sample - loss: 19.4112 - mae: 0.6933 - mean_pred: 0.6474 - mae_t1: 0.0462 - val_loss: 18.2858 - val_mae: 0.6531 - val_mean_pred: 1.2095 - val_mae_t1: 0.0435\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 325us/sample - loss: 17.6342 - mae: 0.6298 - mean_pred: 0.8905 - mae_t1: 0.0420 - val_loss: 16.6906 - val_mae: 0.5961 - val_mean_pred: 0.6240 - val_mae_t1: 0.0397\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 258us/sample - loss: 17.0313 - mae: 0.6083 - mean_pred: 0.8067 - mae_t1: 0.0406 - val_loss: 24.4021 - val_mae: 0.8715 - val_mean_pred: 0.7113 - val_mae_t1: 0.0581\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 23.7025 - mae: 0.8465 - mean_pred: 0.6844 - mae_t1: 0.0564 - val_loss: 33.1219 - val_mae: 1.1829 - val_mean_pred: 1.1908 - val_mae_t1: 0.0789\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 27.3168 - mae: 0.9756 - mean_pred: 1.1330 - mae_t1: 0.0650 - val_loss: 22.7539 - val_mae: 0.8126 - val_mean_pred: 0.5362 - val_mae_t1: 0.0542\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 214us/sample - loss: 19.6168 - mae: 0.7006 - mean_pred: 0.5925 - mae_t1: 0.0467 - val_loss: 13.6875 - val_mae: 0.4888 - val_mean_pred: 0.9775 - val_mae_t1: 0.0326\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 18.8294 - mae: 0.6725 - mean_pred: 0.4363 - mae_t1: 0.0448 - val_loss: 14.7110 - val_mae: 0.5254 - val_mean_pred: 0.7037 - val_mae_t1: 0.0350\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 15.9303 - mae: 0.5689 - mean_pred: 0.7962 - mae_t1: 0.0379 - val_loss: 15.6726 - val_mae: 0.5597 - val_mean_pred: 0.7487 - val_mae_t1: 0.0373\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 16.8047 - mae: 0.6002 - mean_pred: 0.6402 - mae_t1: 0.0400 - val_loss: 17.9460 - val_mae: 0.6409 - val_mean_pred: 0.6027 - val_mae_t1: 0.0427\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 16.5018 - mae: 0.5894 - mean_pred: 0.6395 - mae_t1: 0.0393 - val_loss: 17.0107 - val_mae: 0.6075 - val_mean_pred: 1.0389 - val_mae_t1: 0.0405\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 16.2474 - mae: 0.5803 - mean_pred: 0.7962 - mae_t1: 0.0387 - val_loss: 21.6707 - val_mae: 0.7740 - val_mean_pred: 0.9961 - val_mae_t1: 0.0516\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 22.0726 - mae: 0.7883 - mean_pred: 0.7335 - mae_t1: 0.0526 - val_loss: 19.9822 - val_mae: 0.7137 - val_mean_pred: 0.5773 - val_mae_t1: 0.0476\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 18.4929 - mae: 0.6605 - mean_pred: 0.7575 - mae_t1: 0.0440 - val_loss: 27.3053 - val_mae: 0.9752 - val_mean_pred: 1.3554 - val_mae_t1: 0.0650\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 22.0325 - mae: 0.7869 - mean_pred: 0.9396 - mae_t1: 0.0525 - val_loss: 23.2323 - val_mae: 0.8297 - val_mean_pred: 0.7127 - val_mae_t1: 0.0553\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 17.2519 - mae: 0.6161 - mean_pred: 0.6778 - mae_t1: 0.0411 - val_loss: 14.3827 - val_mae: 0.5137 - val_mean_pred: 0.9056 - val_mae_t1: 0.0342\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 15.2529 - mae: 0.5447 - mean_pred: 0.6226 - mae_t1: 0.0363 - val_loss: 17.2002 - val_mae: 0.6143 - val_mean_pred: 0.5519 - val_mae_t1: 0.0410\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 14.2383 - mae: 0.5085 - mean_pred: 0.6364 - mae_t1: 0.0339 - val_loss: 18.9332 - val_mae: 0.6762 - val_mean_pred: 1.1948 - val_mae_t1: 0.0451\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 14.3127 - mae: 0.5112 - mean_pred: 0.8255 - mae_t1: 0.0341 - val_loss: 17.9798 - val_mae: 0.6421 - val_mean_pred: 0.4910 - val_mae_t1: 0.0428\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 15.7294 - mae: 0.5618 - mean_pred: 0.4867 - mae_t1: 0.0375 - val_loss: 19.9684 - val_mae: 0.7132 - val_mean_pred: 1.2148 - val_mae_t1: 0.0475\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 14.4100 - mae: 0.5146 - mean_pred: 0.8138 - mae_t1: 0.0343 - val_loss: 17.3523 - val_mae: 0.6197 - val_mean_pred: 0.9063 - val_mae_t1: 0.0413\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 14.8627 - mae: 0.5308 - mean_pred: 0.9540 - mae_t1: 0.0354 - val_loss: 17.7000 - val_mae: 0.6321 - val_mean_pred: 0.4807 - val_mae_t1: 0.0421\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 19.5410 - mae: 0.6979 - mean_pred: 0.5918 - mae_t1: 0.0465 - val_loss: 16.7108 - val_mae: 0.5968 - val_mean_pred: 0.6690 - val_mae_t1: 0.0398\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 15.0700 - mae: 0.5382 - mean_pred: 0.7949 - mae_t1: 0.0359 - val_loss: 14.7339 - val_mae: 0.5262 - val_mean_pred: 1.0494 - val_mae_t1: 0.0351\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 13.3357 - mae: 0.4763 - mean_pred: 0.7995 - mae_t1: 0.0318 - val_loss: 18.6833 - val_mae: 0.6673 - val_mean_pred: 0.6002 - val_mae_t1: 0.0445\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 14.3493 - mae: 0.5125 - mean_pred: 0.6261 - mae_t1: 0.0342 - val_loss: 14.3183 - val_mae: 0.5114 - val_mean_pred: 1.0212 - val_mae_t1: 0.0341\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 13.1413 - mae: 0.4693 - mean_pred: 0.9418 - mae_t1: 0.0313 - val_loss: 20.0262 - val_mae: 0.7152 - val_mean_pred: 0.7681 - val_mae_t1: 0.0477\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 24.1797 - mae: 0.8636 - mean_pred: 0.8356 - mae_t1: 0.0576 - val_loss: 20.6238 - val_mae: 0.7366 - val_mean_pred: 0.4215 - val_mae_t1: 0.0491\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 15.6161 - mae: 0.5577 - mean_pred: 0.5821 - mae_t1: 0.0372 - val_loss: 14.4952 - val_mae: 0.5177 - val_mean_pred: 1.0291 - val_mae_t1: 0.0345\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 12.0300 - mae: 0.4296 - mean_pred: 0.8237 - mae_t1: 0.0286 - val_loss: 13.4155 - val_mae: 0.4791 - val_mean_pred: 0.8098 - val_mae_t1: 0.0319\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 11.6834 - mae: 0.4173 - mean_pred: 0.8528 - mae_t1: 0.0278 - val_loss: 14.3590 - val_mae: 0.5128 - val_mean_pred: 1.0448 - val_mae_t1: 0.0342\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 11.7135 - mae: 0.4183 - mean_pred: 0.9024 - mae_t1: 0.0279 - val_loss: 14.2203 - val_mae: 0.5079 - val_mean_pred: 0.7595 - val_mae_t1: 0.0339\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 11.7321 - mae: 0.4190 - mean_pred: 0.7018 - mae_t1: 0.0279 - val_loss: 14.6762 - val_mae: 0.5242 - val_mean_pred: 0.9476 - val_mae_t1: 0.0349\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 10.6505 - mae: 0.3804 - mean_pred: 0.8426 - mae_t1: 0.0254 - val_loss: 14.4678 - val_mae: 0.5167 - val_mean_pred: 0.8896 - val_mae_t1: 0.0344\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.9176 - mae: 0.3899 - mean_pred: 0.7577 - mae_t1: 0.0260 - val_loss: 14.0320 - val_mae: 0.5011 - val_mean_pred: 0.9158 - val_mae_t1: 0.0334\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 10.0803 - mae: 0.3600 - mean_pred: 0.7411 - mae_t1: 0.0240 - val_loss: 17.5887 - val_mae: 0.6282 - val_mean_pred: 0.9959 - val_mae_t1: 0.0419\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 13.0199 - mae: 0.4650 - mean_pred: 0.9475 - mae_t1: 0.0310 - val_loss: 13.6830 - val_mae: 0.4887 - val_mean_pred: 0.7523 - val_mae_t1: 0.0326\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 12.5607 - mae: 0.4486 - mean_pred: 0.5756 - mae_t1: 0.0299 - val_loss: 13.3219 - val_mae: 0.4758 - val_mean_pred: 0.7156 - val_mae_t1: 0.0317\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 11.2026 - mae: 0.4001 - mean_pred: 0.8218 - mae_t1: 0.0267 - val_loss: 12.7893 - val_mae: 0.4568 - val_mean_pred: 0.9532 - val_mae_t1: 0.0305\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 11.3784 - mae: 0.4064 - mean_pred: 0.7062 - mae_t1: 0.0271 - val_loss: 15.0055 - val_mae: 0.5359 - val_mean_pred: 0.7948 - val_mae_t1: 0.0357\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.9032 - mae: 0.4251 - mean_pred: 0.7696 - mae_t1: 0.0283 - val_loss: 15.4547 - val_mae: 0.5520 - val_mean_pred: 1.1008 - val_mae_t1: 0.0368\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 12.2389 - mae: 0.4371 - mean_pred: 0.9422 - mae_t1: 0.0291 - val_loss: 13.5211 - val_mae: 0.4829 - val_mean_pred: 0.8550 - val_mae_t1: 0.0322\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.1727 - mae: 0.3990 - mean_pred: 0.7127 - mae_t1: 0.0266 - val_loss: 13.6022 - val_mae: 0.4858 - val_mean_pred: 0.9168 - val_mae_t1: 0.0324\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 14.0820 - mae: 0.5029 - mean_pred: 0.9459 - mae_t1: 0.0335 - val_loss: 16.3594 - val_mae: 0.5843 - val_mean_pred: 1.1194 - val_mae_t1: 0.0390\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.8019 - mae: 0.4215 - mean_pred: 0.8780 - mae_t1: 0.0281 - val_loss: 14.2726 - val_mae: 0.5097 - val_mean_pred: 0.8460 - val_mae_t1: 0.0340\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.2416 - mae: 0.3658 - mean_pred: 0.7297 - mae_t1: 0.0244 - val_loss: 14.3565 - val_mae: 0.5127 - val_mean_pred: 0.9307 - val_mae_t1: 0.0342\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.6971 - mae: 0.3820 - mean_pred: 0.8407 - mae_t1: 0.0255 - val_loss: 16.2456 - val_mae: 0.5802 - val_mean_pred: 0.7624 - val_mae_t1: 0.0387\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 12.0278 - mae: 0.4296 - mean_pred: 0.6707 - mae_t1: 0.0286 - val_loss: 17.2625 - val_mae: 0.6165 - val_mean_pred: 0.7569 - val_mae_t1: 0.0411\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 10.6397 - mae: 0.3800 - mean_pred: 0.7304 - mae_t1: 0.0253 - val_loss: 14.6443 - val_mae: 0.5230 - val_mean_pred: 0.9449 - val_mae_t1: 0.0349\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 9.6050 - mae: 0.3430 - mean_pred: 0.8368 - mae_t1: 0.0229 - val_loss: 13.0249 - val_mae: 0.4652 - val_mean_pred: 0.8656 - val_mae_t1: 0.0310\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.8377 - mae: 0.3513 - mean_pred: 0.7764 - mae_t1: 0.0234 - val_loss: 14.0793 - val_mae: 0.5028 - val_mean_pred: 0.9442 - val_mae_t1: 0.0335\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 10.5629 - mae: 0.3772 - mean_pred: 0.8927 - mae_t1: 0.0251 - val_loss: 12.8025 - val_mae: 0.4572 - val_mean_pred: 0.8965 - val_mae_t1: 0.0305\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.3073 - mae: 0.3324 - mean_pred: 0.7743 - mae_t1: 0.0222 - val_loss: 18.4629 - val_mae: 0.6594 - val_mean_pred: 0.9258 - val_mae_t1: 0.0440\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 11.2171 - mae: 0.4006 - mean_pred: 0.7641 - mae_t1: 0.0267 - val_loss: 12.8190 - val_mae: 0.4578 - val_mean_pred: 0.8891 - val_mae_t1: 0.0305\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 10.5404 - mae: 0.3764 - mean_pred: 0.8028 - mae_t1: 0.0251 - val_loss: 16.0703 - val_mae: 0.5739 - val_mean_pred: 0.8586 - val_mae_t1: 0.0383\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 11.7206 - mae: 0.4186 - mean_pred: 0.7909 - mae_t1: 0.0279 - val_loss: 16.8221 - val_mae: 0.6008 - val_mean_pred: 1.0213 - val_mae_t1: 0.0401\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 10.4566 - mae: 0.3735 - mean_pred: 0.8552 - mae_t1: 0.0249 - val_loss: 15.4218 - val_mae: 0.5508 - val_mean_pred: 0.9022 - val_mae_t1: 0.0367\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 9.8481 - mae: 0.3517 - mean_pred: 0.7050 - mae_t1: 0.0234 - val_loss: 15.0744 - val_mae: 0.5384 - val_mean_pred: 0.9047 - val_mae_t1: 0.0359\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.2147 - mae: 0.4005 - mean_pred: 0.8671 - mae_t1: 0.0267 - val_loss: 15.8520 - val_mae: 0.5661 - val_mean_pred: 0.9875 - val_mae_t1: 0.0377\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.1051 - mae: 0.3966 - mean_pred: 0.8557 - mae_t1: 0.0264 - val_loss: 13.0962 - val_mae: 0.4677 - val_mean_pred: 0.8460 - val_mae_t1: 0.0312\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 13.7548 - mae: 0.4912 - mean_pred: 0.8536 - mae_t1: 0.0327 - val_loss: 19.1827 - val_mae: 0.6851 - val_mean_pred: 0.9676 - val_mae_t1: 0.0457\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.9331 - mae: 0.4262 - mean_pred: 0.8168 - mae_t1: 0.0284 - val_loss: 15.4413 - val_mae: 0.5515 - val_mean_pred: 0.8958 - val_mae_t1: 0.0368\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.6859 - mae: 0.4174 - mean_pred: 0.8201 - mae_t1: 0.0278 - val_loss: 15.0816 - val_mae: 0.5386 - val_mean_pred: 0.7750 - val_mae_t1: 0.0359\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 12.3474 - mae: 0.4410 - mean_pred: 0.6677 - mae_t1: 0.0294 - val_loss: 15.3182 - val_mae: 0.5471 - val_mean_pred: 0.7953 - val_mae_t1: 0.0365\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.0500 - mae: 0.3946 - mean_pred: 0.8352 - mae_t1: 0.0263 - val_loss: 19.3342 - val_mae: 0.6905 - val_mean_pred: 1.0081 - val_mae_t1: 0.0460\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 13.1325 - mae: 0.4690 - mean_pred: 0.7821 - mae_t1: 0.0313 - val_loss: 14.5698 - val_mae: 0.5203 - val_mean_pred: 0.7661 - val_mae_t1: 0.0347\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 10.9318 - mae: 0.3904 - mean_pred: 0.7756 - mae_t1: 0.0260 - val_loss: 12.4343 - val_mae: 0.4441 - val_mean_pred: 0.9689 - val_mae_t1: 0.0296\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 12.0934 - mae: 0.4319 - mean_pred: 0.7983 - mae_t1: 0.0288 - val_loss: 17.3419 - val_mae: 0.6194 - val_mean_pred: 0.9907 - val_mae_t1: 0.0413\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 11.1542 - mae: 0.3984 - mean_pred: 0.8068 - mae_t1: 0.0266 - val_loss: 13.2830 - val_mae: 0.4744 - val_mean_pred: 1.0222 - val_mae_t1: 0.0316\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 10.6065 - mae: 0.3788 - mean_pred: 0.8678 - mae_t1: 0.0253 - val_loss: 14.7297 - val_mae: 0.5261 - val_mean_pred: 0.8681 - val_mae_t1: 0.0351\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 14.6393 - mae: 0.5228 - mean_pred: 0.8775 - mae_t1: 0.0349 - val_loss: 22.4430 - val_mae: 0.8015 - val_mean_pred: 1.0247 - val_mae_t1: 0.0534\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 12.5122 - mae: 0.4469 - mean_pred: 0.8083 - mae_t1: 0.0298 - val_loss: 13.1065 - val_mae: 0.4681 - val_mean_pred: 0.8407 - val_mae_t1: 0.0312\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 12.2202 - mae: 0.4364 - mean_pred: 0.7139 - mae_t1: 0.0291 - val_loss: 15.3654 - val_mae: 0.5488 - val_mean_pred: 0.8608 - val_mae_t1: 0.0366\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 16.4844 - mae: 0.5887 - mean_pred: 0.8655 - mae_t1: 0.0392 - val_loss: 20.1439 - val_mae: 0.7194 - val_mean_pred: 0.9661 - val_mae_t1: 0.0480\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 14.0022 - mae: 0.5001 - mean_pred: 0.6722 - mae_t1: 0.0333 - val_loss: 13.6676 - val_mae: 0.4881 - val_mean_pred: 0.7305 - val_mae_t1: 0.0325\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.8235 - mae: 0.3866 - mean_pred: 0.6906 - mae_t1: 0.0258 - val_loss: 13.5329 - val_mae: 0.4833 - val_mean_pred: 0.9478 - val_mae_t1: 0.0322\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 10.3139 - mae: 0.3684 - mean_pred: 0.8233 - mae_t1: 0.0246 - val_loss: 13.2081 - val_mae: 0.4717 - val_mean_pred: 0.7658 - val_mae_t1: 0.0314\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.8466 - mae: 0.4231 - mean_pred: 0.6159 - mae_t1: 0.0282 - val_loss: 14.0776 - val_mae: 0.5028 - val_mean_pred: 0.7032 - val_mae_t1: 0.0335\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 10.2782 - mae: 0.3671 - mean_pred: 0.7816 - mae_t1: 0.0245 - val_loss: 15.1620 - val_mae: 0.5415 - val_mean_pred: 1.0542 - val_mae_t1: 0.0361\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 9.6138 - mae: 0.3434 - mean_pred: 0.8923 - mae_t1: 0.0229 - val_loss: 12.1381 - val_mae: 0.4335 - val_mean_pred: 0.7791 - val_mae_t1: 0.0289\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 8.7561 - mae: 0.3127 - mean_pred: 0.6999 - mae_t1: 0.0208 - val_loss: 16.6281 - val_mae: 0.5939 - val_mean_pred: 0.9289 - val_mae_t1: 0.0396\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.0424 - mae: 0.3944 - mean_pred: 0.8053 - mae_t1: 0.0263 - val_loss: 13.2879 - val_mae: 0.4746 - val_mean_pred: 0.8620 - val_mae_t1: 0.0316\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 9.5021 - mae: 0.3394 - mean_pred: 0.8104 - mae_t1: 0.0226 - val_loss: 14.4780 - val_mae: 0.5171 - val_mean_pred: 0.9642 - val_mae_t1: 0.0345\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 9.2154 - mae: 0.3291 - mean_pred: 0.8942 - mae_t1: 0.0219 - val_loss: 14.0615 - val_mae: 0.5022 - val_mean_pred: 0.9069 - val_mae_t1: 0.0335\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.7339 - mae: 0.3476 - mean_pred: 0.8192 - mae_t1: 0.0232 - val_loss: 13.5383 - val_mae: 0.4835 - val_mean_pred: 0.7922 - val_mae_t1: 0.0322\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.1415 - mae: 0.3265 - mean_pred: 0.7508 - mae_t1: 0.0218 - val_loss: 13.8637 - val_mae: 0.4951 - val_mean_pred: 0.9256 - val_mae_t1: 0.0330\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.3141 - mae: 0.2969 - mean_pred: 0.8496 - mae_t1: 0.0198 - val_loss: 13.8373 - val_mae: 0.4942 - val_mean_pred: 0.8908 - val_mae_t1: 0.0329\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.1523 - mae: 0.3269 - mean_pred: 0.7392 - mae_t1: 0.0218 - val_loss: 13.3929 - val_mae: 0.4783 - val_mean_pred: 0.9482 - val_mae_t1: 0.0319\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 10.4690 - mae: 0.3739 - mean_pred: 0.9333 - mae_t1: 0.0249 - val_loss: 14.3079 - val_mae: 0.5110 - val_mean_pred: 1.0192 - val_mae_t1: 0.0341\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 10.2181 - mae: 0.3649 - mean_pred: 0.8414 - mae_t1: 0.0243 - val_loss: 13.2167 - val_mae: 0.4720 - val_mean_pred: 0.7295 - val_mae_t1: 0.0315\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.2204 - mae: 0.3650 - mean_pred: 0.6648 - mae_t1: 0.0243 - val_loss: 13.1189 - val_mae: 0.4685 - val_mean_pred: 0.8055 - val_mae_t1: 0.0312\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.6750 - mae: 0.3455 - mean_pred: 0.7634 - mae_t1: 0.0230 - val_loss: 12.9947 - val_mae: 0.4641 - val_mean_pred: 0.9261 - val_mae_t1: 0.0309\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.8481 - mae: 0.3160 - mean_pred: 0.8074 - mae_t1: 0.0211 - val_loss: 12.1948 - val_mae: 0.4355 - val_mean_pred: 0.8483 - val_mae_t1: 0.0290\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 8.7496 - mae: 0.3125 - mean_pred: 0.7546 - mae_t1: 0.0208 - val_loss: 11.7164 - val_mae: 0.4184 - val_mean_pred: 0.8042 - val_mae_t1: 0.0279\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.9049 - mae: 0.3180 - mean_pred: 0.7750 - mae_t1: 0.0212 - val_loss: 13.8816 - val_mae: 0.4958 - val_mean_pred: 0.9826 - val_mae_t1: 0.0331\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.3817 - mae: 0.2993 - mean_pred: 0.8706 - mae_t1: 0.0200 - val_loss: 15.4056 - val_mae: 0.5502 - val_mean_pred: 0.9864 - val_mae_t1: 0.0367\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 10.0654 - mae: 0.3595 - mean_pred: 0.8337 - mae_t1: 0.0240 - val_loss: 13.1471 - val_mae: 0.4695 - val_mean_pred: 0.9414 - val_mae_t1: 0.0313\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 9.4114 - mae: 0.3361 - mean_pred: 0.8743 - mae_t1: 0.0224 - val_loss: 13.7714 - val_mae: 0.4918 - val_mean_pred: 0.9455 - val_mae_t1: 0.0328\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 9.3669 - mae: 0.3345 - mean_pred: 0.8260 - mae_t1: 0.0223 - val_loss: 13.1154 - val_mae: 0.4684 - val_mean_pred: 0.9322 - val_mae_t1: 0.0312\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.4940 - mae: 0.3034 - mean_pred: 0.8751 - mae_t1: 0.0202 - val_loss: 15.1964 - val_mae: 0.5427 - val_mean_pred: 1.0046 - val_mae_t1: 0.0362\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.6636 - mae: 0.3094 - mean_pred: 0.8780 - mae_t1: 0.0206 - val_loss: 14.2668 - val_mae: 0.5095 - val_mean_pred: 0.9096 - val_mae_t1: 0.0340\n",
      "Earliness...\n",
      "0.0019996166229248047\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3681418822662176  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆█▆▆▇▅▅▅▅▄▅▃▂▄▃▃▃▂▂▂▂▂▃▃▃▃▂▃▄▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>mae</td><td>█▆█▆▆▇▅▅▅▅▄▅▃▂▄▃▃▃▂▂▂▂▂▃▃▃▃▂▃▄▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆█▆▆▇▅▅▅▅▄▅▃▂▄▃▃▃▂▂▂▂▂▃▃▃▃▂▃▄▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▂▅▁▅▅▄▂▆█▅█▁▇▄█▃█▇▆▆▇▅▆▆▅▆▅▆▃▃▂▇▅▅▄▆▅▅▆▇</td></tr><tr><td>val_loss</td><td>▄▇▂▃▅█▃▄▄▂▅▂▂▂▂▂▂▂▃▁▁▃▃▁▃▄▃▂▂▂▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>val_mae</td><td>▄▇▂▃▅█▃▄▄▂▅▂▂▂▂▂▂▂▃▁▁▃▃▁▃▄▃▂▂▂▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>val_mae_t1</td><td>▄▇▂▃▅█▃▄▄▂▅▂▂▂▂▂▂▂▃▁▁▃▃▁▃▄▃▂▂▂▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>val_mean_pred</td><td>▇▃▅▃▅█▂▁▁▆▃▅▃▄▃▄▄▄▃▄▄▄▄▄▄▅▅▄▄▃▃▃▅▃▅▃▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.41912</td></tr><tr><td>AE_2</td><td>0.40977</td></tr><tr><td>AE_3</td><td>0.31666</td></tr><tr><td>MAE</td><td>0.36814</td></tr><tr><td>best_epoch</td><td>92</td></tr><tr><td>best_val_loss</td><td>11.71636</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>8.66358</td></tr><tr><td>mae</td><td>0.30941</td></tr><tr><td>mae_t1</td><td>0.02063</td></tr><tr><td>mean_pred</td><td>0.87798</td></tr><tr><td>val_loss</td><td>14.26682</td></tr><tr><td>val_mae</td><td>0.50953</td></tr><tr><td>val_mae_t1</td><td>0.03397</td></tr><tr><td>val_mean_pred</td><td>0.90956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">young-lake-55</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fjlvpnh\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fjlvpnh</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154152-1fjlvpnh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154221-1lvstoow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lvstoow\" target=\"_blank\">young-dust-56</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 703us/sample - loss: 21.9218 - mae: 0.7829 - mean_pred: 0.3603 - mae_t1: 0.0522 - val_loss: 16.5304 - val_mae: 0.5904 - val_mean_pred: 0.7710 - val_mae_t1: 0.0394\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 162us/sample - loss: 18.0299 - mae: 0.6439 - mean_pred: 0.5416 - mae_t1: 0.0429 - val_loss: 16.9062 - val_mae: 0.6038 - val_mean_pred: 0.4663 - val_mae_t1: 0.0403\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 16.6833 - mae: 0.5958 - mean_pred: 0.4407 - mae_t1: 0.0397 - val_loss: 18.6308 - val_mae: 0.6654 - val_mean_pred: 1.1418 - val_mae_t1: 0.0444\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 129us/sample - loss: 19.5201 - mae: 0.6971 - mean_pred: 1.0137 - mae_t1: 0.0465 - val_loss: 17.6791 - val_mae: 0.6314 - val_mean_pred: 0.5190 - val_mae_t1: 0.0421\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 144us/sample - loss: 19.4987 - mae: 0.6964 - mean_pred: 0.5300 - mae_t1: 0.0464 - val_loss: 14.5762 - val_mae: 0.5206 - val_mean_pred: 0.9892 - val_mae_t1: 0.0347\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 18.0573 - mae: 0.6449 - mean_pred: 0.9536 - mae_t1: 0.0430 - val_loss: 15.7351 - val_mae: 0.5620 - val_mean_pred: 0.8897 - val_mae_t1: 0.0375\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 16.0239 - mae: 0.5723 - mean_pred: 0.6794 - mae_t1: 0.0382 - val_loss: 18.5289 - val_mae: 0.6617 - val_mean_pred: 0.6799 - val_mae_t1: 0.0441\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 16.4463 - mae: 0.5874 - mean_pred: 0.7307 - mae_t1: 0.0392 - val_loss: 23.9671 - val_mae: 0.8560 - val_mean_pred: 1.3465 - val_mae_t1: 0.0571\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 23.0033 - mae: 0.8215 - mean_pred: 1.2708 - mae_t1: 0.0548 - val_loss: 25.9214 - val_mae: 0.9258 - val_mean_pred: 0.2677 - val_mae_t1: 0.0617\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 25.2443 - mae: 0.9016 - mean_pred: 0.2755 - mae_t1: 0.0601 - val_loss: 20.5269 - val_mae: 0.7331 - val_mean_pred: 0.3246 - val_mae_t1: 0.0489\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 18.8340 - mae: 0.6726 - mean_pred: 0.4626 - mae_t1: 0.0448 - val_loss: 20.7197 - val_mae: 0.7400 - val_mean_pred: 1.0657 - val_mae_t1: 0.0493\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 19.0856 - mae: 0.6816 - mean_pred: 0.8413 - mae_t1: 0.0454 - val_loss: 23.6359 - val_mae: 0.8441 - val_mean_pred: 0.5631 - val_mae_t1: 0.0563\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 23.6608 - mae: 0.8450 - mean_pred: 0.5738 - mae_t1: 0.0563 - val_loss: 18.1608 - val_mae: 0.6486 - val_mean_pred: 0.9922 - val_mae_t1: 0.0432\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 20.0961 - mae: 0.7177 - mean_pred: 1.0567 - mae_t1: 0.0478 - val_loss: 16.8691 - val_mae: 0.6025 - val_mean_pred: 1.0320 - val_mae_t1: 0.0402\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 15.1495 - mae: 0.5411 - mean_pred: 0.7828 - mae_t1: 0.0361 - val_loss: 18.6041 - val_mae: 0.6644 - val_mean_pred: 0.5761 - val_mae_t1: 0.0443\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 16.5283 - mae: 0.5903 - mean_pred: 0.6201 - mae_t1: 0.0394 - val_loss: 16.5620 - val_mae: 0.5915 - val_mean_pred: 1.0488 - val_mae_t1: 0.0394\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 16.6383 - mae: 0.5942 - mean_pred: 0.9423 - mae_t1: 0.0396 - val_loss: 14.8035 - val_mae: 0.5287 - val_mean_pred: 0.8998 - val_mae_t1: 0.0352\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 15.6794 - mae: 0.5600 - mean_pred: 0.8159 - mae_t1: 0.0373 - val_loss: 16.9175 - val_mae: 0.6042 - val_mean_pred: 1.0474 - val_mae_t1: 0.0403\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 17.4753 - mae: 0.6241 - mean_pred: 0.9361 - mae_t1: 0.0416 - val_loss: 15.1529 - val_mae: 0.5412 - val_mean_pred: 0.8690 - val_mae_t1: 0.0361\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 14.7840 - mae: 0.5280 - mean_pred: 0.7431 - mae_t1: 0.0352 - val_loss: 18.5084 - val_mae: 0.6610 - val_mean_pred: 1.1347 - val_mae_t1: 0.0441\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.5020 - mae: 0.5894 - mean_pred: 0.9708 - mae_t1: 0.0393 - val_loss: 20.6961 - val_mae: 0.7391 - val_mean_pred: 0.6614 - val_mae_t1: 0.0493\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 17.6797 - mae: 0.6314 - mean_pred: 0.5692 - mae_t1: 0.0421 - val_loss: 22.4073 - val_mae: 0.8003 - val_mean_pred: 0.5081 - val_mae_t1: 0.0534\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 17.5547 - mae: 0.6270 - mean_pred: 0.5483 - mae_t1: 0.0418 - val_loss: 15.4952 - val_mae: 0.5534 - val_mean_pred: 1.0054 - val_mae_t1: 0.0369\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 15.2523 - mae: 0.5447 - mean_pred: 0.9432 - mae_t1: 0.0363 - val_loss: 16.3211 - val_mae: 0.5829 - val_mean_pred: 0.9041 - val_mae_t1: 0.0389\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 15.2983 - mae: 0.5464 - mean_pred: 0.6734 - mae_t1: 0.0364 - val_loss: 15.8967 - val_mae: 0.5677 - val_mean_pred: 0.6338 - val_mae_t1: 0.0378\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 14.6350 - mae: 0.5227 - mean_pred: 0.6267 - mae_t1: 0.0348 - val_loss: 19.0913 - val_mae: 0.6818 - val_mean_pred: 1.0948 - val_mae_t1: 0.0455\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 18.6171 - mae: 0.6649 - mean_pred: 0.9689 - mae_t1: 0.0443 - val_loss: 15.3061 - val_mae: 0.5466 - val_mean_pred: 0.7524 - val_mae_t1: 0.0364\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 15.9080 - mae: 0.5681 - mean_pred: 0.6163 - mae_t1: 0.0379 - val_loss: 18.1152 - val_mae: 0.6470 - val_mean_pred: 0.6108 - val_mae_t1: 0.0431\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 17.0008 - mae: 0.6072 - mean_pred: 0.5542 - mae_t1: 0.0405 - val_loss: 19.1750 - val_mae: 0.6848 - val_mean_pred: 0.8900 - val_mae_t1: 0.0457\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 18.8736 - mae: 0.6741 - mean_pred: 0.8805 - mae_t1: 0.0449 - val_loss: 18.7221 - val_mae: 0.6686 - val_mean_pred: 0.9966 - val_mae_t1: 0.0446\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.4048 - mae: 0.5859 - mean_pred: 0.9245 - mae_t1: 0.0391 - val_loss: 21.2728 - val_mae: 0.7597 - val_mean_pred: 0.9234 - val_mae_t1: 0.0506\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 16.9062 - mae: 0.6038 - mean_pred: 0.8138 - mae_t1: 0.0403 - val_loss: 20.9648 - val_mae: 0.7487 - val_mean_pred: 0.9267 - val_mae_t1: 0.0499\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.1163 - mae: 0.5756 - mean_pred: 0.8323 - mae_t1: 0.0384 - val_loss: 16.0846 - val_mae: 0.5744 - val_mean_pred: 1.0054 - val_mae_t1: 0.0383\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 13.9098 - mae: 0.4968 - mean_pred: 0.8918 - mae_t1: 0.0331 - val_loss: 16.3728 - val_mae: 0.5847 - val_mean_pred: 0.9422 - val_mae_t1: 0.0390\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 14.7854 - mae: 0.5281 - mean_pred: 0.7979 - mae_t1: 0.0352 - val_loss: 16.8009 - val_mae: 0.6000 - val_mean_pred: 0.5417 - val_mae_t1: 0.0400\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 17.5761 - mae: 0.6277 - mean_pred: 0.4546 - mae_t1: 0.0418 - val_loss: 16.3518 - val_mae: 0.5840 - val_mean_pred: 0.5268 - val_mae_t1: 0.0389\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 16.5809 - mae: 0.5922 - mean_pred: 0.5663 - mae_t1: 0.0395 - val_loss: 13.3272 - val_mae: 0.4760 - val_mean_pred: 0.9387 - val_mae_t1: 0.0317\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 14.1088 - mae: 0.5039 - mean_pred: 0.7835 - mae_t1: 0.0336 - val_loss: 14.9007 - val_mae: 0.5322 - val_mean_pred: 0.8527 - val_mae_t1: 0.0355\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 14.0241 - mae: 0.5009 - mean_pred: 0.7190 - mae_t1: 0.0334 - val_loss: 17.3635 - val_mae: 0.6201 - val_mean_pred: 0.9316 - val_mae_t1: 0.0413\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 14.9046 - mae: 0.5323 - mean_pred: 0.7859 - mae_t1: 0.0355 - val_loss: 15.9255 - val_mae: 0.5688 - val_mean_pred: 1.0094 - val_mae_t1: 0.0379\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 12.9431 - mae: 0.4623 - mean_pred: 0.8831 - mae_t1: 0.0308 - val_loss: 16.1220 - val_mae: 0.5758 - val_mean_pred: 1.0698 - val_mae_t1: 0.0384\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 13.9448 - mae: 0.4980 - mean_pred: 0.9665 - mae_t1: 0.0332 - val_loss: 15.1397 - val_mae: 0.5407 - val_mean_pred: 0.8346 - val_mae_t1: 0.0360\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 14.7663 - mae: 0.5274 - mean_pred: 0.7402 - mae_t1: 0.0352 - val_loss: 16.6855 - val_mae: 0.5959 - val_mean_pred: 0.7106 - val_mae_t1: 0.0397\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 16.5939 - mae: 0.5926 - mean_pred: 0.6866 - mae_t1: 0.0395 - val_loss: 16.4816 - val_mae: 0.5886 - val_mean_pred: 0.5125 - val_mae_t1: 0.0392\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 20.4798 - mae: 0.7314 - mean_pred: 0.6227 - mae_t1: 0.0488 - val_loss: 17.2434 - val_mae: 0.6158 - val_mean_pred: 0.6135 - val_mae_t1: 0.0411\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 16.9070 - mae: 0.6038 - mean_pred: 0.6061 - mae_t1: 0.0403 - val_loss: 19.4164 - val_mae: 0.6934 - val_mean_pred: 1.0441 - val_mae_t1: 0.0462\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.9809 - mae: 0.6065 - mean_pred: 0.9942 - mae_t1: 0.0404 - val_loss: 15.5065 - val_mae: 0.5538 - val_mean_pred: 1.0049 - val_mae_t1: 0.0369\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 15.3295 - mae: 0.5475 - mean_pred: 0.8375 - mae_t1: 0.0365 - val_loss: 19.0751 - val_mae: 0.6813 - val_mean_pred: 0.7567 - val_mae_t1: 0.0454\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 19.5734 - mae: 0.6990 - mean_pred: 0.6755 - mae_t1: 0.0466 - val_loss: 17.1848 - val_mae: 0.6137 - val_mean_pred: 0.6368 - val_mae_t1: 0.0409\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 15.6791 - mae: 0.5600 - mean_pred: 0.6072 - mae_t1: 0.0373 - val_loss: 20.5697 - val_mae: 0.7346 - val_mean_pred: 0.9416 - val_mae_t1: 0.0490\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 15.6461 - mae: 0.5588 - mean_pred: 0.8575 - mae_t1: 0.0373 - val_loss: 21.9399 - val_mae: 0.7836 - val_mean_pred: 1.1494 - val_mae_t1: 0.0522\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 18.2119 - mae: 0.6504 - mean_pred: 0.9747 - mae_t1: 0.0434 - val_loss: 17.5841 - val_mae: 0.6280 - val_mean_pred: 1.0276 - val_mae_t1: 0.0419\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 15.1791 - mae: 0.5421 - mean_pred: 0.8894 - mae_t1: 0.0361 - val_loss: 16.7284 - val_mae: 0.5974 - val_mean_pred: 0.7889 - val_mae_t1: 0.0398\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 15.5328 - mae: 0.5547 - mean_pred: 0.6269 - mae_t1: 0.0370 - val_loss: 17.7013 - val_mae: 0.6322 - val_mean_pred: 0.5665 - val_mae_t1: 0.0421\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 17.7729 - mae: 0.6347 - mean_pred: 0.5867 - mae_t1: 0.0423 - val_loss: 16.4168 - val_mae: 0.5863 - val_mean_pred: 0.6383 - val_mae_t1: 0.0391\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 14.0619 - mae: 0.5022 - mean_pred: 0.5978 - mae_t1: 0.0335 - val_loss: 16.7917 - val_mae: 0.5997 - val_mean_pred: 0.8941 - val_mae_t1: 0.0400\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 13.8777 - mae: 0.4956 - mean_pred: 0.7936 - mae_t1: 0.0330 - val_loss: 18.2480 - val_mae: 0.6517 - val_mean_pred: 1.0009 - val_mae_t1: 0.0434\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 19.2859 - mae: 0.6888 - mean_pred: 0.9385 - mae_t1: 0.0459 - val_loss: 14.6275 - val_mae: 0.5224 - val_mean_pred: 0.8592 - val_mae_t1: 0.0348\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 14.1258 - mae: 0.5045 - mean_pred: 0.7187 - mae_t1: 0.0336 - val_loss: 20.5714 - val_mae: 0.7347 - val_mean_pred: 0.8926 - val_mae_t1: 0.0490\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 16.4912 - mae: 0.5890 - mean_pred: 0.7648 - mae_t1: 0.0393 - val_loss: 16.4910 - val_mae: 0.5890 - val_mean_pred: 0.9630 - val_mae_t1: 0.0393\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 16.8241 - mae: 0.6009 - mean_pred: 0.9200 - mae_t1: 0.0401 - val_loss: 16.2119 - val_mae: 0.5790 - val_mean_pred: 1.1028 - val_mae_t1: 0.0386\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 15.6867 - mae: 0.5602 - mean_pred: 0.9969 - mae_t1: 0.0373 - val_loss: 17.4065 - val_mae: 0.6217 - val_mean_pred: 0.9906 - val_mae_t1: 0.0414\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 14.5996 - mae: 0.5214 - mean_pred: 0.8116 - mae_t1: 0.0348 - val_loss: 22.6565 - val_mae: 0.8092 - val_mean_pred: 0.8809 - val_mae_t1: 0.0539\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 24.6581 - mae: 0.8806 - mean_pred: 0.9212 - mae_t1: 0.0587 - val_loss: 20.8044 - val_mae: 0.7430 - val_mean_pred: 0.4359 - val_mae_t1: 0.0495\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 21.2378 - mae: 0.7585 - mean_pred: 0.5083 - mae_t1: 0.0506 - val_loss: 25.3419 - val_mae: 0.9051 - val_mean_pred: 0.7414 - val_mae_t1: 0.0603\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 20.9485 - mae: 0.7482 - mean_pred: 0.6090 - mae_t1: 0.0499 - val_loss: 19.7851 - val_mae: 0.7066 - val_mean_pred: 0.8781 - val_mae_t1: 0.0471\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 23.9716 - mae: 0.8561 - mean_pred: 0.9659 - mae_t1: 0.0571 - val_loss: 22.1156 - val_mae: 0.7898 - val_mean_pred: 1.2194 - val_mae_t1: 0.0527\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 20.4169 - mae: 0.7292 - mean_pred: 1.1547 - mae_t1: 0.0486 - val_loss: 31.5657 - val_mae: 1.1273 - val_mean_pred: 1.4045 - val_mae_t1: 0.0752\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 32.7064 - mae: 1.1681 - mean_pred: 1.3308 - mae_t1: 0.0779 - val_loss: 14.7257 - val_mae: 0.5259 - val_mean_pred: 0.7829 - val_mae_t1: 0.0351\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 16.2780 - mae: 0.5814 - mean_pred: 0.7021 - mae_t1: 0.0388 - val_loss: 22.6321 - val_mae: 0.8083 - val_mean_pred: 0.7497 - val_mae_t1: 0.0539\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 22.0455 - mae: 0.7873 - mean_pred: 0.6243 - mae_t1: 0.0525 - val_loss: 21.4292 - val_mae: 0.7653 - val_mean_pred: 0.4517 - val_mae_t1: 0.0510\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 21.3034 - mae: 0.7608 - mean_pred: 0.5028 - mae_t1: 0.0507 - val_loss: 17.4346 - val_mae: 0.6227 - val_mean_pred: 0.6328 - val_mae_t1: 0.0415\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 15.1783 - mae: 0.5421 - mean_pred: 0.5865 - mae_t1: 0.0361 - val_loss: 17.0937 - val_mae: 0.6105 - val_mean_pred: 0.8557 - val_mae_t1: 0.0407\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 14.7088 - mae: 0.5253 - mean_pred: 0.7868 - mae_t1: 0.0350 - val_loss: 24.1813 - val_mae: 0.8636 - val_mean_pred: 1.0925 - val_mae_t1: 0.0576\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 23.1675 - mae: 0.8274 - mean_pred: 1.0065 - mae_t1: 0.0552 - val_loss: 17.3730 - val_mae: 0.6205 - val_mean_pred: 1.0154 - val_mae_t1: 0.0414\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 14.8408 - mae: 0.5300 - mean_pred: 0.9096 - mae_t1: 0.0353 - val_loss: 19.1098 - val_mae: 0.6825 - val_mean_pred: 0.9775 - val_mae_t1: 0.0455\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.3336 - mae: 0.5833 - mean_pred: 0.9283 - mae_t1: 0.0389 - val_loss: 18.7449 - val_mae: 0.6695 - val_mean_pred: 0.9631 - val_mae_t1: 0.0446\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.6029 - mae: 0.5930 - mean_pred: 0.8567 - mae_t1: 0.0395 - val_loss: 16.1664 - val_mae: 0.5774 - val_mean_pred: 0.9440 - val_mae_t1: 0.0385\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 13.1452 - mae: 0.4695 - mean_pred: 0.8581 - mae_t1: 0.0313 - val_loss: 16.0409 - val_mae: 0.5729 - val_mean_pred: 0.9742 - val_mae_t1: 0.0382\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 13.1787 - mae: 0.4707 - mean_pred: 0.8493 - mae_t1: 0.0314 - val_loss: 21.7961 - val_mae: 0.7784 - val_mean_pred: 1.1065 - val_mae_t1: 0.0519\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 22.0439 - mae: 0.7873 - mean_pred: 1.0062 - mae_t1: 0.0525 - val_loss: 16.5071 - val_mae: 0.5895 - val_mean_pred: 1.0775 - val_mae_t1: 0.0393\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 14.2060 - mae: 0.5074 - mean_pred: 0.9055 - mae_t1: 0.0338 - val_loss: 26.3253 - val_mae: 0.9402 - val_mean_pred: 1.0818 - val_mae_t1: 0.0627\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 24.5097 - mae: 0.8753 - mean_pred: 0.9925 - mae_t1: 0.0584 - val_loss: 16.6830 - val_mae: 0.5958 - val_mean_pred: 0.8905 - val_mae_t1: 0.0397\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 17.7257 - mae: 0.6331 - mean_pred: 0.8283 - mae_t1: 0.0422 - val_loss: 23.8562 - val_mae: 0.8520 - val_mean_pred: 0.9461 - val_mae_t1: 0.0568\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 16.9143 - mae: 0.6041 - mean_pred: 0.7310 - mae_t1: 0.0403 - val_loss: 19.8421 - val_mae: 0.7086 - val_mean_pred: 0.8959 - val_mae_t1: 0.0472\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 18.7292 - mae: 0.6689 - mean_pred: 0.9052 - mae_t1: 0.0446 - val_loss: 18.6280 - val_mae: 0.6653 - val_mean_pred: 1.1140 - val_mae_t1: 0.0444\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 15.5671 - mae: 0.5560 - mean_pred: 1.0198 - mae_t1: 0.0371 - val_loss: 17.8516 - val_mae: 0.6376 - val_mean_pred: 1.0132 - val_mae_t1: 0.0425\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 13.5811 - mae: 0.4850 - mean_pred: 0.8651 - mae_t1: 0.0323 - val_loss: 15.6353 - val_mae: 0.5584 - val_mean_pred: 0.8047 - val_mae_t1: 0.0372\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 12.9708 - mae: 0.4632 - mean_pred: 0.6895 - mae_t1: 0.0309 - val_loss: 24.4813 - val_mae: 0.8743 - val_mean_pred: 0.9768 - val_mae_t1: 0.0583\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 23.5893 - mae: 0.8425 - mean_pred: 0.9114 - mae_t1: 0.0562 - val_loss: 15.3936 - val_mae: 0.5498 - val_mean_pred: 0.7658 - val_mae_t1: 0.0367\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 13.8136 - mae: 0.4933 - mean_pred: 0.6475 - mae_t1: 0.0329 - val_loss: 18.0283 - val_mae: 0.6439 - val_mean_pred: 0.7414 - val_mae_t1: 0.0429\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 17.9505 - mae: 0.6411 - mean_pred: 0.6568 - mae_t1: 0.0427 - val_loss: 14.2881 - val_mae: 0.5103 - val_mean_pred: 0.6313 - val_mae_t1: 0.0340\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 13.4751 - mae: 0.4813 - mean_pred: 0.5688 - mae_t1: 0.0321 - val_loss: 14.1770 - val_mae: 0.5063 - val_mean_pred: 0.8200 - val_mae_t1: 0.0338\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 12.1524 - mae: 0.4340 - mean_pred: 0.7358 - mae_t1: 0.0289 - val_loss: 14.5498 - val_mae: 0.5196 - val_mean_pred: 0.9245 - val_mae_t1: 0.0346\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 12.4292 - mae: 0.4439 - mean_pred: 0.7950 - mae_t1: 0.0296 - val_loss: 14.0493 - val_mae: 0.5018 - val_mean_pred: 0.8428 - val_mae_t1: 0.0335\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 11.8340 - mae: 0.4226 - mean_pred: 0.7092 - mae_t1: 0.0282 - val_loss: 14.7840 - val_mae: 0.5280 - val_mean_pred: 0.7604 - val_mae_t1: 0.0352\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 12.2768 - mae: 0.4385 - mean_pred: 0.6380 - mae_t1: 0.0292 - val_loss: 15.2593 - val_mae: 0.5450 - val_mean_pred: 0.7038 - val_mae_t1: 0.0363\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 12.9686 - mae: 0.4632 - mean_pred: 0.6344 - mae_t1: 0.0309 - val_loss: 18.4846 - val_mae: 0.6602 - val_mean_pred: 0.8759 - val_mae_t1: 0.0440\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 13.8045 - mae: 0.4930 - mean_pred: 0.7230 - mae_t1: 0.0329 - val_loss: 15.3827 - val_mae: 0.5494 - val_mean_pred: 0.9165 - val_mae_t1: 0.0366\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.9922 - mae: 0.3926 - mean_pred: 0.7846 - mae_t1: 0.0262 - val_loss: 16.7478 - val_mae: 0.5981 - val_mean_pred: 0.9335 - val_mae_t1: 0.0399\n",
      "Earliness...\n",
      "0.002028942108154297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.33188644996395816  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▅▃▃▃▄▅▃▃▃▃▂▃▃▂▃▂▂▃▃▄▃▂▂▂▃▅▅█▄▂▃▂▂▃▂▂▃▁▁▁</td></tr><tr><td>mae</td><td>▅▃▃▃▄▅▃▃▃▃▂▃▃▂▃▂▂▃▃▄▃▂▂▂▃▅▅█▄▂▃▂▂▃▂▂▃▁▁▁</td></tr><tr><td>mae_t1</td><td>▅▃▃▃▄▅▃▃▃▃▂▃▃▂▃▂▂▃▃▄▃▂▂▂▃▅▅█▄▂▃▂▂▃▂▂▃▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▂▅▄▂▃▃▄▅▂▃▃▅▅▂▄▅▃▃▃▅▃▃▄▅▅▅█▂▄▅▅▅▄▆▃▃▄▃▄</td></tr><tr><td>val_loss</td><td>▂▄▂▇▅▃▂▃▅▂▄▃▅▂▂▃▂▂▄▃▅▃▂▅▂▅▆▁▃▇▄▂█▇▃▇▁▁▂▂</td></tr><tr><td>val_mae</td><td>▂▄▂▇▅▃▂▃▅▂▄▃▅▂▂▃▂▂▄▃▅▃▂▅▂▅▆▁▃▇▄▂█▇▃▇▁▁▂▂</td></tr><tr><td>val_mae_t1</td><td>▂▄▂▇▅▃▂▃▅▂▄▃▅▂▂▃▂▂▄▃▅▃▂▅▂▅▆▁▃▇▄▂█▇▃▇▁▁▂▂</td></tr><tr><td>val_mean_pred</td><td>▄▆▄█▆▅▆▆▃▅▆▂▅▅▂▅▆▂▆▃▆▂▅▅▆▁▇▄▃▆▅▅▆▅▅▅▃▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.32335</td></tr><tr><td>AE_2</td><td>0.28152</td></tr><tr><td>AE_3</td><td>0.33402</td></tr><tr><td>MAE</td><td>0.33189</td></tr><tr><td>best_epoch</td><td>36</td></tr><tr><td>best_val_loss</td><td>13.32719</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>10.99222</td></tr><tr><td>mae</td><td>0.39258</td></tr><tr><td>mae_t1</td><td>0.02617</td></tr><tr><td>mean_pred</td><td>0.78463</td></tr><tr><td>val_loss</td><td>16.74784</td></tr><tr><td>val_mae</td><td>0.59814</td></tr><tr><td>val_mae_t1</td><td>0.03988</td></tr><tr><td>val_mean_pred</td><td>0.93353</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">young-dust-56</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lvstoow\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lvstoow</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154221-1lvstoow\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154244-23v5g76p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/23v5g76p\" target=\"_blank\">desert-puddle-57</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 881us/sample - loss: 29.1761 - mae: 0.7678 - mean_pred: 0.7086 - mae_t1: 0.0512 - val_loss: 31.1832 - val_mae: 0.8206 - val_mean_pred: 0.3537 - val_mae_t1: 0.0547\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 327us/sample - loss: 30.8953 - mae: 0.8130 - mean_pred: 0.5543 - mae_t1: 0.0542 - val_loss: 28.9829 - val_mae: 0.7627 - val_mean_pred: 1.3941 - val_mae_t1: 0.0508\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 255us/sample - loss: 26.4852 - mae: 0.6970 - mean_pred: 0.7233 - mae_t1: 0.0465 - val_loss: 35.6725 - val_mae: 0.9387 - val_mean_pred: 1.5869 - val_mae_t1: 0.0626\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 29.8442 - mae: 0.7854 - mean_pred: 1.0673 - mae_t1: 0.0524 - val_loss: 36.8570 - val_mae: 0.9699 - val_mean_pred: 0.7835 - val_mae_t1: 0.0647\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 242us/sample - loss: 30.3587 - mae: 0.7989 - mean_pred: 0.9009 - mae_t1: 0.0533 - val_loss: 26.9889 - val_mae: 0.7102 - val_mean_pred: 0.3726 - val_mae_t1: 0.0473\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 24.6065 - mae: 0.6475 - mean_pred: 0.6043 - mae_t1: 0.0432 - val_loss: 20.0147 - val_mae: 0.5267 - val_mean_pred: 0.8700 - val_mae_t1: 0.0351\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 221us/sample - loss: 22.2669 - mae: 0.5860 - mean_pred: 0.7070 - mae_t1: 0.0391 - val_loss: 19.4203 - val_mae: 0.5111 - val_mean_pred: 0.8328 - val_mae_t1: 0.0341\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 19.4874 - mae: 0.5128 - mean_pred: 0.7872 - mae_t1: 0.0342 - val_loss: 20.2387 - val_mae: 0.5326 - val_mean_pred: 0.9572 - val_mae_t1: 0.0355\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 19.4723 - mae: 0.5124 - mean_pred: 0.7353 - mae_t1: 0.0342 - val_loss: 48.7790 - val_mae: 1.2837 - val_mean_pred: 1.6553 - val_mae_t1: 0.0856\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 40.7477 - mae: 1.0723 - mean_pred: 1.4832 - mae_t1: 0.0715 - val_loss: 35.9939 - val_mae: 0.9472 - val_mean_pred: 0.3660 - val_mae_t1: 0.0631\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 230us/sample - loss: 36.5191 - mae: 0.9610 - mean_pred: 0.4373 - mae_t1: 0.0641 - val_loss: 18.5777 - val_mae: 0.4889 - val_mean_pred: 0.8913 - val_mae_t1: 0.0326\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 24.4880 - mae: 0.6444 - mean_pred: 1.0132 - mae_t1: 0.0430 - val_loss: 25.4029 - val_mae: 0.6685 - val_mean_pred: 0.8576 - val_mae_t1: 0.0446\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 22.1715 - mae: 0.5835 - mean_pred: 0.7019 - mae_t1: 0.0389 - val_loss: 22.0408 - val_mae: 0.5800 - val_mean_pred: 0.7330 - val_mae_t1: 0.0387\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 18.9002 - mae: 0.4974 - mean_pred: 0.8145 - mae_t1: 0.0332 - val_loss: 22.3370 - val_mae: 0.5878 - val_mean_pred: 0.7596 - val_mae_t1: 0.0392\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 21.0720 - mae: 0.5545 - mean_pred: 0.5684 - mae_t1: 0.0370 - val_loss: 23.3297 - val_mae: 0.6139 - val_mean_pred: 0.6342 - val_mae_t1: 0.0409\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 20.3738 - mae: 0.5362 - mean_pred: 0.7367 - mae_t1: 0.0357 - val_loss: 22.4308 - val_mae: 0.5903 - val_mean_pred: 0.9879 - val_mae_t1: 0.0394\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 19.6670 - mae: 0.5176 - mean_pred: 0.7918 - mae_t1: 0.0345 - val_loss: 22.4682 - val_mae: 0.5913 - val_mean_pred: 0.6683 - val_mae_t1: 0.0394\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 22.6385 - mae: 0.5957 - mean_pred: 0.6655 - mae_t1: 0.0397 - val_loss: 26.4834 - val_mae: 0.6969 - val_mean_pred: 0.7397 - val_mae_t1: 0.0465\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 20.2369 - mae: 0.5326 - mean_pred: 0.7160 - mae_t1: 0.0355 - val_loss: 27.9511 - val_mae: 0.7356 - val_mean_pred: 1.1247 - val_mae_t1: 0.0490\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 26.6942 - mae: 0.7025 - mean_pred: 1.1067 - mae_t1: 0.0468 - val_loss: 25.8150 - val_mae: 0.6793 - val_mean_pred: 0.8729 - val_mae_t1: 0.0453\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 21.8746 - mae: 0.5756 - mean_pred: 0.6512 - mae_t1: 0.0384 - val_loss: 23.0124 - val_mae: 0.6056 - val_mean_pred: 0.8825 - val_mae_t1: 0.0404\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 23.2222 - mae: 0.6111 - mean_pred: 0.9326 - mae_t1: 0.0407 - val_loss: 20.8224 - val_mae: 0.5480 - val_mean_pred: 0.8474 - val_mae_t1: 0.0365\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 19.3555 - mae: 0.5094 - mean_pred: 0.6666 - mae_t1: 0.0340 - val_loss: 19.7444 - val_mae: 0.5196 - val_mean_pred: 0.7563 - val_mae_t1: 0.0346\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 18.5994 - mae: 0.4895 - mean_pred: 0.8692 - mae_t1: 0.0326 - val_loss: 23.3971 - val_mae: 0.6157 - val_mean_pred: 1.1192 - val_mae_t1: 0.0410\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 20.0496 - mae: 0.5276 - mean_pred: 0.9573 - mae_t1: 0.0352 - val_loss: 21.4396 - val_mae: 0.5642 - val_mean_pred: 0.6398 - val_mae_t1: 0.0376\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 21.9018 - mae: 0.5764 - mean_pred: 0.5154 - mae_t1: 0.0384 - val_loss: 28.9480 - val_mae: 0.7618 - val_mean_pred: 0.7582 - val_mae_t1: 0.0508\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 27.2366 - mae: 0.7168 - mean_pred: 0.7967 - mae_t1: 0.0478 - val_loss: 42.6562 - val_mae: 1.1225 - val_mean_pred: 1.2131 - val_mae_t1: 0.0748\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 30.9966 - mae: 0.8157 - mean_pred: 1.0012 - mae_t1: 0.0544 - val_loss: 29.4597 - val_mae: 0.7753 - val_mean_pred: 0.7920 - val_mae_t1: 0.0517\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 25.2048 - mae: 0.6633 - mean_pred: 0.7176 - mae_t1: 0.0442 - val_loss: 25.7841 - val_mae: 0.6785 - val_mean_pred: 0.8019 - val_mae_t1: 0.0452\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 22.6526 - mae: 0.5961 - mean_pred: 0.8771 - mae_t1: 0.0397 - val_loss: 24.7555 - val_mae: 0.6515 - val_mean_pred: 1.0354 - val_mae_t1: 0.0434\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 22.4774 - mae: 0.5915 - mean_pred: 0.8887 - mae_t1: 0.0394 - val_loss: 24.4833 - val_mae: 0.6443 - val_mean_pred: 0.7577 - val_mae_t1: 0.0430\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 22.5460 - mae: 0.5933 - mean_pred: 0.6925 - mae_t1: 0.0396 - val_loss: 20.6978 - val_mae: 0.5447 - val_mean_pred: 0.9157 - val_mae_t1: 0.0363\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 18.1248 - mae: 0.4770 - mean_pred: 0.8602 - mae_t1: 0.0318 - val_loss: 22.5356 - val_mae: 0.5930 - val_mean_pred: 1.0507 - val_mae_t1: 0.0395\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 18.3794 - mae: 0.4837 - mean_pred: 0.8317 - mae_t1: 0.0322 - val_loss: 31.1960 - val_mae: 0.8209 - val_mean_pred: 1.0219 - val_mae_t1: 0.0547\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 24.4220 - mae: 0.6427 - mean_pred: 0.7878 - mae_t1: 0.0428 - val_loss: 23.9867 - val_mae: 0.6312 - val_mean_pred: 0.8668 - val_mae_t1: 0.0421\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 22.9023 - mae: 0.6027 - mean_pred: 0.7988 - mae_t1: 0.0402 - val_loss: 19.2916 - val_mae: 0.5077 - val_mean_pred: 0.7784 - val_mae_t1: 0.0338\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 23.7216 - mae: 0.6243 - mean_pred: 0.7065 - mae_t1: 0.0416 - val_loss: 24.4295 - val_mae: 0.6429 - val_mean_pred: 0.7424 - val_mae_t1: 0.0429\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 18.8994 - mae: 0.4974 - mean_pred: 0.7249 - mae_t1: 0.0332 - val_loss: 18.5792 - val_mae: 0.4889 - val_mean_pred: 1.0083 - val_mae_t1: 0.0326\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 19.8469 - mae: 0.5223 - mean_pred: 0.9338 - mae_t1: 0.0348 - val_loss: 22.6254 - val_mae: 0.5954 - val_mean_pred: 0.8250 - val_mae_t1: 0.0397\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 22.8504 - mae: 0.6013 - mean_pred: 0.6381 - mae_t1: 0.0401 - val_loss: 28.8120 - val_mae: 0.7582 - val_mean_pred: 0.8410 - val_mae_t1: 0.0505\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 19.2424 - mae: 0.5064 - mean_pred: 0.7759 - mae_t1: 0.0338 - val_loss: 21.6643 - val_mae: 0.5701 - val_mean_pred: 0.9519 - val_mae_t1: 0.0380\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 18.7000 - mae: 0.4921 - mean_pred: 0.7608 - mae_t1: 0.0328 - val_loss: 21.1405 - val_mae: 0.5563 - val_mean_pred: 0.7845 - val_mae_t1: 0.0371\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 18.9522 - mae: 0.4987 - mean_pred: 0.7769 - mae_t1: 0.0332 - val_loss: 26.0278 - val_mae: 0.6849 - val_mean_pred: 1.0400 - val_mae_t1: 0.0457\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 19.8194 - mae: 0.5216 - mean_pred: 0.8039 - mae_t1: 0.0348 - val_loss: 19.6730 - val_mae: 0.5177 - val_mean_pred: 0.7526 - val_mae_t1: 0.0345\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 18.2511 - mae: 0.4803 - mean_pred: 0.7530 - mae_t1: 0.0320 - val_loss: 24.9925 - val_mae: 0.6577 - val_mean_pred: 0.8497 - val_mae_t1: 0.0438\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 21.2203 - mae: 0.5584 - mean_pred: 0.7786 - mae_t1: 0.0372 - val_loss: 22.6279 - val_mae: 0.5955 - val_mean_pred: 0.8455 - val_mae_t1: 0.0397\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 20.2111 - mae: 0.5319 - mean_pred: 0.8090 - mae_t1: 0.0355 - val_loss: 38.2407 - val_mae: 1.0063 - val_mean_pred: 1.0612 - val_mae_t1: 0.0671\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 31.8040 - mae: 0.8369 - mean_pred: 0.9812 - mae_t1: 0.0558 - val_loss: 33.8590 - val_mae: 0.8910 - val_mean_pred: 1.0573 - val_mae_t1: 0.0594\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 36.2677 - mae: 0.9544 - mean_pred: 1.0945 - mae_t1: 0.0636 - val_loss: 36.7722 - val_mae: 0.9677 - val_mean_pred: 1.0651 - val_mae_t1: 0.0645\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 22.8294 - mae: 0.6008 - mean_pred: 0.7770 - mae_t1: 0.0401 - val_loss: 18.7216 - val_mae: 0.4927 - val_mean_pred: 0.8679 - val_mae_t1: 0.0328\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 20.1907 - mae: 0.5313 - mean_pred: 0.9388 - mae_t1: 0.0354 - val_loss: 25.0691 - val_mae: 0.6597 - val_mean_pred: 1.1490 - val_mae_t1: 0.0440\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 18.0137 - mae: 0.4740 - mean_pred: 0.9232 - mae_t1: 0.0316 - val_loss: 26.6877 - val_mae: 0.7023 - val_mean_pred: 0.8512 - val_mae_t1: 0.0468\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 18.7869 - mae: 0.4944 - mean_pred: 0.6917 - mae_t1: 0.0330 - val_loss: 20.5760 - val_mae: 0.5415 - val_mean_pred: 0.8464 - val_mae_t1: 0.0361\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 17.4167 - mae: 0.4583 - mean_pred: 0.7886 - mae_t1: 0.0306 - val_loss: 20.3577 - val_mae: 0.5357 - val_mean_pred: 0.9889 - val_mae_t1: 0.0357\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 17.4864 - mae: 0.4602 - mean_pred: 0.8437 - mae_t1: 0.0307 - val_loss: 28.2601 - val_mae: 0.7437 - val_mean_pred: 1.0594 - val_mae_t1: 0.0496\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 21.0063 - mae: 0.5528 - mean_pred: 0.8957 - mae_t1: 0.0369 - val_loss: 24.1284 - val_mae: 0.6350 - val_mean_pred: 0.9909 - val_mae_t1: 0.0423\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 244us/sample - loss: 21.4826 - mae: 0.5653 - mean_pred: 0.8523 - mae_t1: 0.0377 - val_loss: 18.2872 - val_mae: 0.4812 - val_mean_pred: 0.7860 - val_mae_t1: 0.0321\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 22.5692 - mae: 0.5939 - mean_pred: 0.7071 - mae_t1: 0.0396 - val_loss: 20.1141 - val_mae: 0.5293 - val_mean_pred: 0.6708 - val_mae_t1: 0.0353\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 18.1130 - mae: 0.4767 - mean_pred: 0.6539 - mae_t1: 0.0318 - val_loss: 20.1769 - val_mae: 0.5310 - val_mean_pred: 0.9429 - val_mae_t1: 0.0354\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 229us/sample - loss: 16.8633 - mae: 0.4438 - mean_pred: 0.8190 - mae_t1: 0.0296 - val_loss: 17.3739 - val_mae: 0.4572 - val_mean_pred: 0.9124 - val_mae_t1: 0.0305\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 15.2588 - mae: 0.4015 - mean_pred: 0.7135 - mae_t1: 0.0268 - val_loss: 25.0712 - val_mae: 0.6598 - val_mean_pred: 0.9234 - val_mae_t1: 0.0440\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 19.1097 - mae: 0.5029 - mean_pred: 0.8620 - mae_t1: 0.0335 - val_loss: 19.7491 - val_mae: 0.5197 - val_mean_pred: 0.9678 - val_mae_t1: 0.0346\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 17.8827 - mae: 0.4706 - mean_pred: 0.9012 - mae_t1: 0.0314 - val_loss: 20.9296 - val_mae: 0.5508 - val_mean_pred: 0.8017 - val_mae_t1: 0.0367\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 17.3196 - mae: 0.4558 - mean_pred: 0.7684 - mae_t1: 0.0304 - val_loss: 22.9466 - val_mae: 0.6039 - val_mean_pred: 0.8279 - val_mae_t1: 0.0403\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 16.6183 - mae: 0.4373 - mean_pred: 0.8493 - mae_t1: 0.0292 - val_loss: 20.1635 - val_mae: 0.5306 - val_mean_pred: 0.9739 - val_mae_t1: 0.0354\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 14.4378 - mae: 0.3799 - mean_pred: 0.8390 - mae_t1: 0.0253 - val_loss: 18.8957 - val_mae: 0.4973 - val_mean_pred: 0.7974 - val_mae_t1: 0.0332\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 14.3650 - mae: 0.3780 - mean_pred: 0.6789 - mae_t1: 0.0252 - val_loss: 22.6023 - val_mae: 0.5948 - val_mean_pred: 0.9304 - val_mae_t1: 0.0397\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 18.0838 - mae: 0.4759 - mean_pred: 0.8526 - mae_t1: 0.0317 - val_loss: 21.5636 - val_mae: 0.5675 - val_mean_pred: 1.0211 - val_mae_t1: 0.0378\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 15.8243 - mae: 0.4164 - mean_pred: 0.8966 - mae_t1: 0.0278 - val_loss: 20.4447 - val_mae: 0.5380 - val_mean_pred: 0.8768 - val_mae_t1: 0.0359\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 16.7985 - mae: 0.4421 - mean_pred: 0.7112 - mae_t1: 0.0295 - val_loss: 20.5772 - val_mae: 0.5415 - val_mean_pred: 0.7084 - val_mae_t1: 0.0361\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 18.3913 - mae: 0.4840 - mean_pred: 0.7500 - mae_t1: 0.0323 - val_loss: 24.0108 - val_mae: 0.6319 - val_mean_pred: 0.8608 - val_mae_t1: 0.0421\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 16.5740 - mae: 0.4362 - mean_pred: 0.6763 - mae_t1: 0.0291 - val_loss: 24.8047 - val_mae: 0.6528 - val_mean_pred: 0.9292 - val_mae_t1: 0.0435\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 19.4124 - mae: 0.5109 - mean_pred: 0.7785 - mae_t1: 0.0341 - val_loss: 20.5955 - val_mae: 0.5420 - val_mean_pred: 0.9390 - val_mae_t1: 0.0361\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 16.0539 - mae: 0.4225 - mean_pred: 0.8081 - mae_t1: 0.0282 - val_loss: 20.5086 - val_mae: 0.5397 - val_mean_pred: 0.9450 - val_mae_t1: 0.0360\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 16.5249 - mae: 0.4349 - mean_pred: 0.8765 - mae_t1: 0.0290 - val_loss: 20.6436 - val_mae: 0.5433 - val_mean_pred: 0.9889 - val_mae_t1: 0.0362\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 14.3003 - mae: 0.3763 - mean_pred: 0.8031 - mae_t1: 0.0251 - val_loss: 21.9636 - val_mae: 0.5780 - val_mean_pred: 0.7972 - val_mae_t1: 0.0385\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 15.1036 - mae: 0.3975 - mean_pred: 0.6571 - mae_t1: 0.0265 - val_loss: 22.1510 - val_mae: 0.5829 - val_mean_pred: 0.8617 - val_mae_t1: 0.0389\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 16.8709 - mae: 0.4440 - mean_pred: 0.8602 - mae_t1: 0.0296 - val_loss: 23.9371 - val_mae: 0.6299 - val_mean_pred: 1.0561 - val_mae_t1: 0.0420\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 16.3230 - mae: 0.4296 - mean_pred: 0.9213 - mae_t1: 0.0286 - val_loss: 21.6567 - val_mae: 0.5699 - val_mean_pred: 0.9330 - val_mae_t1: 0.0380\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 14.2046 - mae: 0.3738 - mean_pred: 0.7008 - mae_t1: 0.0249 - val_loss: 23.5684 - val_mae: 0.6202 - val_mean_pred: 0.8247 - val_mae_t1: 0.0413\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 17.8027 - mae: 0.4685 - mean_pred: 0.6515 - mae_t1: 0.0312 - val_loss: 21.3651 - val_mae: 0.5622 - val_mean_pred: 0.8221 - val_mae_t1: 0.0375\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 16.8948 - mae: 0.4446 - mean_pred: 0.7253 - mae_t1: 0.0296 - val_loss: 19.3046 - val_mae: 0.5080 - val_mean_pred: 0.9184 - val_mae_t1: 0.0339\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 13.4162 - mae: 0.3531 - mean_pred: 0.7703 - mae_t1: 0.0235 - val_loss: 18.4757 - val_mae: 0.4862 - val_mean_pred: 0.9029 - val_mae_t1: 0.0324\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 13.0970 - mae: 0.3447 - mean_pred: 0.7889 - mae_t1: 0.0230 - val_loss: 21.8708 - val_mae: 0.5755 - val_mean_pred: 0.8661 - val_mae_t1: 0.0384\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 18.8689 - mae: 0.4966 - mean_pred: 0.8178 - mae_t1: 0.0331 - val_loss: 18.8411 - val_mae: 0.4958 - val_mean_pred: 0.8718 - val_mae_t1: 0.0331\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 16.2160 - mae: 0.4267 - mean_pred: 0.8117 - mae_t1: 0.0284 - val_loss: 29.1911 - val_mae: 0.7682 - val_mean_pred: 1.0792 - val_mae_t1: 0.0512\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 19.2778 - mae: 0.5073 - mean_pred: 0.9103 - mae_t1: 0.0338 - val_loss: 16.7437 - val_mae: 0.4406 - val_mean_pred: 0.9138 - val_mae_t1: 0.0294\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 222us/sample - loss: 14.7564 - mae: 0.3883 - mean_pred: 0.7348 - mae_t1: 0.0259 - val_loss: 16.6842 - val_mae: 0.4391 - val_mean_pred: 0.7707 - val_mae_t1: 0.0293\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 12.8240 - mae: 0.3375 - mean_pred: 0.6923 - mae_t1: 0.0225 - val_loss: 18.9507 - val_mae: 0.4987 - val_mean_pred: 0.9488 - val_mae_t1: 0.0332\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 13.4856 - mae: 0.3549 - mean_pred: 0.9010 - mae_t1: 0.0237 - val_loss: 18.7657 - val_mae: 0.4938 - val_mean_pred: 0.9360 - val_mae_t1: 0.0329\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 12.1284 - mae: 0.3192 - mean_pred: 0.7503 - mae_t1: 0.0213 - val_loss: 21.4500 - val_mae: 0.5645 - val_mean_pred: 0.8182 - val_mae_t1: 0.0376\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 15.5314 - mae: 0.4087 - mean_pred: 0.7015 - mae_t1: 0.0272 - val_loss: 19.1676 - val_mae: 0.5044 - val_mean_pred: 0.8418 - val_mae_t1: 0.0336\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 14.8259 - mae: 0.3902 - mean_pred: 0.7706 - mae_t1: 0.0260 - val_loss: 25.0854 - val_mae: 0.6601 - val_mean_pred: 0.9286 - val_mae_t1: 0.0440\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 17.8642 - mae: 0.4701 - mean_pred: 0.8444 - mae_t1: 0.0313 - val_loss: 20.1924 - val_mae: 0.5314 - val_mean_pred: 0.8027 - val_mae_t1: 0.0354\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 15.5002 - mae: 0.4079 - mean_pred: 0.6861 - mae_t1: 0.0272 - val_loss: 21.5723 - val_mae: 0.5677 - val_mean_pred: 0.7263 - val_mae_t1: 0.0378\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 14.9670 - mae: 0.3939 - mean_pred: 0.7109 - mae_t1: 0.0263 - val_loss: 24.5674 - val_mae: 0.6465 - val_mean_pred: 1.0358 - val_mae_t1: 0.0431\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 19.3424 - mae: 0.5090 - mean_pred: 0.9269 - mae_t1: 0.0339 - val_loss: 26.5413 - val_mae: 0.6985 - val_mean_pred: 0.9980 - val_mae_t1: 0.0466\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 17.1720 - mae: 0.4519 - mean_pred: 0.7803 - mae_t1: 0.0301 - val_loss: 20.4743 - val_mae: 0.5388 - val_mean_pred: 0.7764 - val_mae_t1: 0.0359\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 14.3718 - mae: 0.3782 - mean_pred: 0.6981 - mae_t1: 0.0252 - val_loss: 27.4045 - val_mae: 0.7212 - val_mean_pred: 1.0795 - val_mae_t1: 0.0481\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 20.1357 - mae: 0.5299 - mean_pred: 0.9156 - mae_t1: 0.0353 - val_loss: 25.1279 - val_mae: 0.6613 - val_mean_pred: 0.9776 - val_mae_t1: 0.0441\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.485406498340254  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆▅▄▃█▄▃▄▄▃▄▆▄▃▄▃▃▃▃█▃▂▃▃▂▂▁▂▂▂▂▂▂▁▃▁▂▂▃▃</td></tr><tr><td>mae</td><td>▆▅▄▃█▄▃▄▄▃▄▆▄▃▄▃▃▃▃█▃▂▃▃▂▂▁▂▂▂▂▂▂▁▃▁▂▂▃▃</td></tr><tr><td>mae_t1</td><td>▆▅▄▃█▄▃▄▄▃▄▆▄▃▄▃▃▃▃█▃▂▃▃▂▂▁▂▂▂▂▂▂▁▃▁▂▂▃▃</td></tr><tr><td>mean_pred</td><td>▄▄▃▅▁▄▄▃▃▃▂▇▆▅▅▆▅▅▅█▆▅▆▃▄▅▄▆▄▅▃▆▄▅▆▄▄▅▆▆</td></tr><tr><td>val_loss</td><td>▆█▂▂▂▃▃▄▃▂▅▅▄▆▂▃▃▂▃█▄▂▄▂▄▃▃▂▄▂▃▃▂▃▁▂▂▂▄▄</td></tr><tr><td>val_mae</td><td>▆█▂▂▂▃▃▄▃▂▅▅▄▆▂▃▃▂▃█▄▂▄▂▄▃▃▂▄▂▃▃▂▃▁▂▂▂▄▄</td></tr><tr><td>val_mae_t1</td><td>▆█▂▂▂▃▃▄▃▂▅▅▄▆▂▃▃▂▃█▄▂▄▂▄▃▃▂▄▂▃▃▂▃▁▂▂▂▄▄</td></tr><tr><td>val_mean_pred</td><td>▁█▄▄▄▃▅▃▄▃▃▃▃▅▃▄▄▃▄▅▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.45316</td></tr><tr><td>AE_2</td><td>0.36626</td></tr><tr><td>AE_3</td><td>0.4219</td></tr><tr><td>MAE</td><td>0.48541</td></tr><tr><td>best_epoch</td><td>87</td></tr><tr><td>best_val_loss</td><td>16.68419</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>20.13568</td></tr><tr><td>mae</td><td>0.52989</td></tr><tr><td>mae_t1</td><td>0.03533</td></tr><tr><td>mean_pred</td><td>0.91558</td></tr><tr><td>val_loss</td><td>25.12788</td></tr><tr><td>val_mae</td><td>0.66126</td></tr><tr><td>val_mae_t1</td><td>0.04408</td></tr><tr><td>val_mean_pred</td><td>0.97764</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">desert-puddle-57</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/23v5g76p\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/23v5g76p</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154244-23v5g76p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154314-16cnsejg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/16cnsejg\" target=\"_blank\">lunar-lion-58</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 693us/sample - loss: 28.9226 - mae: 0.7611 - mean_pred: 0.3598 - mae_t1: 0.0507 - val_loss: 27.2215 - val_mae: 0.7164 - val_mean_pred: 0.5039 - val_mae_t1: 0.0478\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 29.3086 - mae: 0.7713 - mean_pred: 0.5066 - mae_t1: 0.0514 - val_loss: 24.5991 - val_mae: 0.6473 - val_mean_pred: 0.4789 - val_mae_t1: 0.0432\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 161us/sample - loss: 23.3685 - mae: 0.6150 - mean_pred: 0.6963 - mae_t1: 0.0410 - val_loss: 24.3744 - val_mae: 0.6414 - val_mean_pred: 0.9057 - val_mae_t1: 0.0428\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 137us/sample - loss: 22.4043 - mae: 0.5896 - mean_pred: 0.7184 - mae_t1: 0.0393 - val_loss: 32.0960 - val_mae: 0.8446 - val_mean_pred: 1.3608 - val_mae_t1: 0.0563\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 29.7457 - mae: 0.7828 - mean_pred: 1.2106 - mae_t1: 0.0522 - val_loss: 25.1134 - val_mae: 0.6609 - val_mean_pred: 0.5836 - val_mae_t1: 0.0441\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 22.1883 - mae: 0.5839 - mean_pred: 0.5239 - mae_t1: 0.0389 - val_loss: 23.2712 - val_mae: 0.6124 - val_mean_pred: 1.0477 - val_mae_t1: 0.0408\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 141us/sample - loss: 22.6583 - mae: 0.5963 - mean_pred: 0.8534 - mae_t1: 0.0398 - val_loss: 21.8756 - val_mae: 0.5757 - val_mean_pred: 0.7536 - val_mae_t1: 0.0384\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 21.3141 - mae: 0.5609 - mean_pred: 0.6507 - mae_t1: 0.0374 - val_loss: 25.9537 - val_mae: 0.6830 - val_mean_pred: 0.7326 - val_mae_t1: 0.0455\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 26.1608 - mae: 0.6884 - mean_pred: 0.6199 - mae_t1: 0.0459 - val_loss: 42.6704 - val_mae: 1.1229 - val_mean_pred: 1.1762 - val_mae_t1: 0.0749\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 47.0379 - mae: 1.2378 - mean_pred: 1.1949 - mae_t1: 0.0825 - val_loss: 34.2908 - val_mae: 0.9024 - val_mean_pred: 0.5756 - val_mae_t1: 0.0602\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 38.9589 - mae: 1.0252 - mean_pred: 0.8548 - mae_t1: 0.0683 - val_loss: 38.0648 - val_mae: 1.0017 - val_mean_pred: 0.6165 - val_mae_t1: 0.0668\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 33.6800 - mae: 0.8863 - mean_pred: 0.6265 - mae_t1: 0.0591 - val_loss: 37.6641 - val_mae: 0.9912 - val_mean_pred: 0.8480 - val_mae_t1: 0.0661\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 30.3970 - mae: 0.7999 - mean_pred: 0.6203 - mae_t1: 0.0533 - val_loss: 42.1836 - val_mae: 1.1101 - val_mean_pred: 1.5689 - val_mae_t1: 0.0740\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 45.1043 - mae: 1.1870 - mean_pred: 1.5622 - mae_t1: 0.0791 - val_loss: 59.6247 - val_mae: 1.5691 - val_mean_pred: 1.4067 - val_mae_t1: 0.1046\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 72.4469 - mae: 1.9065 - mean_pred: 1.7576 - mae_t1: 0.1271 - val_loss: 63.3262 - val_mae: 1.6665 - val_mean_pred: 1.0507 - val_mae_t1: 0.1111\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 52.1022 - mae: 1.3711 - mean_pred: 0.9069 - mae_t1: 0.0914 - val_loss: 24.5326 - val_mae: 0.6456 - val_mean_pred: 1.0359 - val_mae_t1: 0.0430\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 33.7281 - mae: 0.8876 - mean_pred: 1.0977 - mae_t1: 0.0592 - val_loss: 35.7166 - val_mae: 0.9399 - val_mean_pred: 1.1438 - val_mae_t1: 0.0627\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 42.6783 - mae: 1.1231 - mean_pred: 1.2247 - mae_t1: 0.0749 - val_loss: 39.4276 - val_mae: 1.0376 - val_mean_pred: 1.0782 - val_mae_t1: 0.0692\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 51.0754 - mae: 1.3441 - mean_pred: 1.3174 - mae_t1: 0.0896 - val_loss: 33.7698 - val_mae: 0.8887 - val_mean_pred: 0.8038 - val_mae_t1: 0.0592\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 33.0157 - mae: 0.8688 - mean_pred: 0.9149 - mae_t1: 0.0579 - val_loss: 38.3980 - val_mae: 1.0105 - val_mean_pred: 1.0185 - val_mae_t1: 0.0674\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 33.9919 - mae: 0.8945 - mean_pred: 0.8796 - mae_t1: 0.0596 - val_loss: 27.5834 - val_mae: 0.7259 - val_mean_pred: 0.5951 - val_mae_t1: 0.0484\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 25.7006 - mae: 0.6763 - mean_pred: 0.5300 - mae_t1: 0.0451 - val_loss: 30.6518 - val_mae: 0.8066 - val_mean_pred: 0.4581 - val_mae_t1: 0.0538\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 29.5154 - mae: 0.7767 - mean_pred: 0.4315 - mae_t1: 0.0518 - val_loss: 32.7598 - val_mae: 0.8621 - val_mean_pred: 0.7662 - val_mae_t1: 0.0575\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 31.3493 - mae: 0.8250 - mean_pred: 0.7655 - mae_t1: 0.0550 - val_loss: 24.5884 - val_mae: 0.6471 - val_mean_pred: 0.9098 - val_mae_t1: 0.0431\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 23.3100 - mae: 0.6134 - mean_pred: 0.8590 - mae_t1: 0.0409 - val_loss: 27.7890 - val_mae: 0.7313 - val_mean_pred: 0.7790 - val_mae_t1: 0.0488\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 25.5777 - mae: 0.6731 - mean_pred: 0.7829 - mae_t1: 0.0449 - val_loss: 23.8015 - val_mae: 0.6264 - val_mean_pred: 0.7275 - val_mae_t1: 0.0418\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 138us/sample - loss: 25.8297 - mae: 0.6797 - mean_pred: 0.7998 - mae_t1: 0.0453 - val_loss: 19.7760 - val_mae: 0.5204 - val_mean_pred: 0.8508 - val_mae_t1: 0.0347\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 22.9868 - mae: 0.6049 - mean_pred: 0.8137 - mae_t1: 0.0403 - val_loss: 23.2651 - val_mae: 0.6122 - val_mean_pred: 0.8517 - val_mae_t1: 0.0408\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 23.2145 - mae: 0.6109 - mean_pred: 0.7302 - mae_t1: 0.0407 - val_loss: 32.4950 - val_mae: 0.8551 - val_mean_pred: 0.8353 - val_mae_t1: 0.0570\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 31.4096 - mae: 0.8266 - mean_pred: 0.8064 - mae_t1: 0.0551 - val_loss: 25.9295 - val_mae: 0.6824 - val_mean_pred: 0.9125 - val_mae_t1: 0.0455\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 24.0963 - mae: 0.6341 - mean_pred: 0.8273 - mae_t1: 0.0423 - val_loss: 23.3786 - val_mae: 0.6152 - val_mean_pred: 0.9646 - val_mae_t1: 0.0410\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 24.0618 - mae: 0.6332 - mean_pred: 0.8332 - mae_t1: 0.0422 - val_loss: 23.1785 - val_mae: 0.6100 - val_mean_pred: 0.6724 - val_mae_t1: 0.0407\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 25.8780 - mae: 0.6810 - mean_pred: 0.6612 - mae_t1: 0.0454 - val_loss: 22.4909 - val_mae: 0.5919 - val_mean_pred: 0.7045 - val_mae_t1: 0.0395\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 24.2450 - mae: 0.6380 - mean_pred: 0.7192 - mae_t1: 0.0425 - val_loss: 25.0230 - val_mae: 0.6585 - val_mean_pred: 0.8587 - val_mae_t1: 0.0439\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 23.3127 - mae: 0.6135 - mean_pred: 0.8025 - mae_t1: 0.0409 - val_loss: 28.9249 - val_mae: 0.7612 - val_mean_pred: 0.8790 - val_mae_t1: 0.0507\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 26.3523 - mae: 0.6935 - mean_pred: 0.8286 - mae_t1: 0.0462 - val_loss: 28.6059 - val_mae: 0.7528 - val_mean_pred: 0.9043 - val_mae_t1: 0.0502\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 25.1413 - mae: 0.6616 - mean_pred: 0.8384 - mae_t1: 0.0441 - val_loss: 27.7450 - val_mae: 0.7301 - val_mean_pred: 0.9336 - val_mae_t1: 0.0487\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 24.3008 - mae: 0.6395 - mean_pred: 0.8122 - mae_t1: 0.0426 - val_loss: 23.3689 - val_mae: 0.6150 - val_mean_pred: 0.6529 - val_mae_t1: 0.0410\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 22.7160 - mae: 0.5978 - mean_pred: 0.6113 - mae_t1: 0.0399 - val_loss: 22.8719 - val_mae: 0.6019 - val_mean_pred: 0.6493 - val_mae_t1: 0.0401\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 21.8904 - mae: 0.5761 - mean_pred: 0.6929 - mae_t1: 0.0384 - val_loss: 22.6816 - val_mae: 0.5969 - val_mean_pred: 0.7966 - val_mae_t1: 0.0398\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 21.2931 - mae: 0.5603 - mean_pred: 0.7817 - mae_t1: 0.0374 - val_loss: 26.4146 - val_mae: 0.6951 - val_mean_pred: 0.6931 - val_mae_t1: 0.0463\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 24.6991 - mae: 0.6500 - mean_pred: 0.7347 - mae_t1: 0.0433 - val_loss: 25.5500 - val_mae: 0.6724 - val_mean_pred: 0.5276 - val_mae_t1: 0.0448\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 22.6457 - mae: 0.5959 - mean_pred: 0.5948 - mae_t1: 0.0397 - val_loss: 21.2083 - val_mae: 0.5581 - val_mean_pred: 0.8162 - val_mae_t1: 0.0372\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 22.0916 - mae: 0.5814 - mean_pred: 0.8859 - mae_t1: 0.0388 - val_loss: 26.6818 - val_mae: 0.7022 - val_mean_pred: 1.1786 - val_mae_t1: 0.0468\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 25.3517 - mae: 0.6672 - mean_pred: 1.0743 - mae_t1: 0.0445 - val_loss: 22.0175 - val_mae: 0.5794 - val_mean_pred: 0.9094 - val_mae_t1: 0.0386\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 20.8416 - mae: 0.5485 - mean_pred: 0.7394 - mae_t1: 0.0366 - val_loss: 29.0101 - val_mae: 0.7634 - val_mean_pred: 0.6130 - val_mae_t1: 0.0509\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 27.4617 - mae: 0.7227 - mean_pred: 0.5542 - mae_t1: 0.0482 - val_loss: 31.0427 - val_mae: 0.8169 - val_mean_pred: 0.4630 - val_mae_t1: 0.0545\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 26.1770 - mae: 0.6889 - mean_pred: 0.3948 - mae_t1: 0.0459 - val_loss: 25.5561 - val_mae: 0.6725 - val_mean_pred: 0.5373 - val_mae_t1: 0.0448\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 22.8054 - mae: 0.6001 - mean_pred: 0.5260 - mae_t1: 0.0400 - val_loss: 24.7625 - val_mae: 0.6516 - val_mean_pred: 0.8220 - val_mae_t1: 0.0434\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 23.2081 - mae: 0.6107 - mean_pred: 0.8055 - mae_t1: 0.0407 - val_loss: 28.3513 - val_mae: 0.7461 - val_mean_pred: 1.0948 - val_mae_t1: 0.0497\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 24.8734 - mae: 0.6546 - mean_pred: 0.9780 - mae_t1: 0.0436 - val_loss: 25.5387 - val_mae: 0.6721 - val_mean_pred: 1.0600 - val_mae_t1: 0.0448\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 23.3854 - mae: 0.6154 - mean_pred: 0.9142 - mae_t1: 0.0410 - val_loss: 20.4852 - val_mae: 0.5391 - val_mean_pred: 0.8165 - val_mae_t1: 0.0359\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 19.9383 - mae: 0.5247 - mean_pred: 0.7254 - mae_t1: 0.0350 - val_loss: 22.5257 - val_mae: 0.5928 - val_mean_pred: 0.7343 - val_mae_t1: 0.0395\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 26.1177 - mae: 0.6873 - mean_pred: 0.6812 - mae_t1: 0.0458 - val_loss: 22.1545 - val_mae: 0.5830 - val_mean_pred: 0.4822 - val_mae_t1: 0.0389\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 22.6030 - mae: 0.5948 - mean_pred: 0.4706 - mae_t1: 0.0397 - val_loss: 26.1886 - val_mae: 0.6892 - val_mean_pred: 0.6087 - val_mae_t1: 0.0459\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 22.3502 - mae: 0.5882 - mean_pred: 0.5740 - mae_t1: 0.0392 - val_loss: 28.9377 - val_mae: 0.7615 - val_mean_pred: 0.8871 - val_mae_t1: 0.0508\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 27.3029 - mae: 0.7185 - mean_pred: 0.8313 - mae_t1: 0.0479 - val_loss: 26.2907 - val_mae: 0.6919 - val_mean_pred: 0.9112 - val_mae_t1: 0.0461\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 29.6624 - mae: 0.7806 - mean_pred: 0.8974 - mae_t1: 0.0520 - val_loss: 26.9431 - val_mae: 0.7090 - val_mean_pred: 0.8978 - val_mae_t1: 0.0473\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 24.5729 - mae: 0.6467 - mean_pred: 0.7535 - mae_t1: 0.0431 - val_loss: 29.0275 - val_mae: 0.7639 - val_mean_pred: 0.8646 - val_mae_t1: 0.0509\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 26.6659 - mae: 0.7017 - mean_pred: 0.7845 - mae_t1: 0.0468 - val_loss: 21.3215 - val_mae: 0.5611 - val_mean_pred: 0.7956 - val_mae_t1: 0.0374\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 20.0203 - mae: 0.5269 - mean_pred: 0.7190 - mae_t1: 0.0351 - val_loss: 21.6986 - val_mae: 0.5710 - val_mean_pred: 0.8715 - val_mae_t1: 0.0381\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 20.8134 - mae: 0.5477 - mean_pred: 0.8131 - mae_t1: 0.0365 - val_loss: 26.0540 - val_mae: 0.6856 - val_mean_pred: 0.8922 - val_mae_t1: 0.0457\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 26.8472 - mae: 0.7065 - mean_pred: 0.8386 - mae_t1: 0.0471 - val_loss: 24.4948 - val_mae: 0.6446 - val_mean_pred: 0.6857 - val_mae_t1: 0.0430\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 22.7772 - mae: 0.5994 - mean_pred: 0.7405 - mae_t1: 0.0400 - val_loss: 22.5150 - val_mae: 0.5925 - val_mean_pred: 0.7967 - val_mae_t1: 0.0395\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 21.9529 - mae: 0.5777 - mean_pred: 0.8511 - mae_t1: 0.0385 - val_loss: 41.8128 - val_mae: 1.1003 - val_mean_pred: 1.1626 - val_mae_t1: 0.0734\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 31.7602 - mae: 0.8358 - mean_pred: 1.0272 - mae_t1: 0.0557 - val_loss: 25.2870 - val_mae: 0.6654 - val_mean_pred: 1.0922 - val_mae_t1: 0.0444\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 30.4423 - mae: 0.8011 - mean_pred: 1.0278 - mae_t1: 0.0534 - val_loss: 20.6995 - val_mae: 0.5447 - val_mean_pred: 0.8614 - val_mae_t1: 0.0363\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 23.2837 - mae: 0.6127 - mean_pred: 0.7548 - mae_t1: 0.0408 - val_loss: 29.2205 - val_mae: 0.7690 - val_mean_pred: 0.7893 - val_mae_t1: 0.0513\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 23.0121 - mae: 0.6056 - mean_pred: 0.6309 - mae_t1: 0.0404 - val_loss: 20.7871 - val_mae: 0.5470 - val_mean_pred: 0.7462 - val_mae_t1: 0.0365\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 21.4102 - mae: 0.5634 - mean_pred: 0.6355 - mae_t1: 0.0376 - val_loss: 24.3146 - val_mae: 0.6399 - val_mean_pred: 0.8956 - val_mae_t1: 0.0427\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 23.0416 - mae: 0.6064 - mean_pred: 0.8294 - mae_t1: 0.0404 - val_loss: 23.2618 - val_mae: 0.6122 - val_mean_pred: 1.1260 - val_mae_t1: 0.0408\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 21.2683 - mae: 0.5597 - mean_pred: 0.9763 - mae_t1: 0.0373 - val_loss: 22.3544 - val_mae: 0.5883 - val_mean_pred: 1.0654 - val_mae_t1: 0.0392\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 19.1660 - mae: 0.5044 - mean_pred: 0.8930 - mae_t1: 0.0336 - val_loss: 19.4741 - val_mae: 0.5125 - val_mean_pred: 0.8435 - val_mae_t1: 0.0342\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 16.7022 - mae: 0.4395 - mean_pred: 0.7333 - mae_t1: 0.0293 - val_loss: 20.0142 - val_mae: 0.5267 - val_mean_pred: 0.8042 - val_mae_t1: 0.0351\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 19.5882 - mae: 0.5155 - mean_pred: 0.7817 - mae_t1: 0.0344 - val_loss: 21.3040 - val_mae: 0.5606 - val_mean_pred: 0.7413 - val_mae_t1: 0.0374\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 18.5498 - mae: 0.4882 - mean_pred: 0.7119 - mae_t1: 0.0325 - val_loss: 21.1484 - val_mae: 0.5565 - val_mean_pred: 0.6811 - val_mae_t1: 0.0371\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 17.5928 - mae: 0.4630 - mean_pred: 0.6860 - mae_t1: 0.0309 - val_loss: 21.6020 - val_mae: 0.5685 - val_mean_pred: 0.6630 - val_mae_t1: 0.0379\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 19.1356 - mae: 0.5036 - mean_pred: 0.6264 - mae_t1: 0.0336 - val_loss: 21.4513 - val_mae: 0.5645 - val_mean_pred: 0.6462 - val_mae_t1: 0.0376\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 22.8077 - mae: 0.6002 - mean_pred: 0.7104 - mae_t1: 0.0400 - val_loss: 19.5101 - val_mae: 0.5134 - val_mean_pred: 0.6706 - val_mae_t1: 0.0342\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 17.4852 - mae: 0.4601 - mean_pred: 0.6680 - mae_t1: 0.0307 - val_loss: 30.1924 - val_mae: 0.7945 - val_mean_pred: 1.0280 - val_mae_t1: 0.0530\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 21.8268 - mae: 0.5744 - mean_pred: 0.8770 - mae_t1: 0.0383 - val_loss: 21.5451 - val_mae: 0.5670 - val_mean_pred: 1.0656 - val_mae_t1: 0.0378\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 19.3188 - mae: 0.5084 - mean_pred: 0.9347 - mae_t1: 0.0339 - val_loss: 19.1808 - val_mae: 0.5048 - val_mean_pred: 0.9403 - val_mae_t1: 0.0337\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 16.5861 - mae: 0.4365 - mean_pred: 0.8002 - mae_t1: 0.0291 - val_loss: 18.4508 - val_mae: 0.4855 - val_mean_pred: 0.7467 - val_mae_t1: 0.0324\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 17.8475 - mae: 0.4697 - mean_pred: 0.6880 - mae_t1: 0.0313 - val_loss: 23.5505 - val_mae: 0.6197 - val_mean_pred: 0.6983 - val_mae_t1: 0.0413\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 21.3782 - mae: 0.5626 - mean_pred: 0.6709 - mae_t1: 0.0375 - val_loss: 20.6657 - val_mae: 0.5438 - val_mean_pred: 0.6934 - val_mae_t1: 0.0363\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 22.1547 - mae: 0.5830 - mean_pred: 0.7579 - mae_t1: 0.0389 - val_loss: 20.5004 - val_mae: 0.5395 - val_mean_pred: 0.7143 - val_mae_t1: 0.0360\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 20.6699 - mae: 0.5439 - mean_pred: 0.7378 - mae_t1: 0.0363 - val_loss: 35.4990 - val_mae: 0.9342 - val_mean_pred: 0.9495 - val_mae_t1: 0.0623\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 28.0172 - mae: 0.7373 - mean_pred: 0.8201 - mae_t1: 0.0492 - val_loss: 22.3961 - val_mae: 0.5894 - val_mean_pred: 0.7142 - val_mae_t1: 0.0393\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 23.4806 - mae: 0.6179 - mean_pred: 0.7897 - mae_t1: 0.0412 - val_loss: 24.7328 - val_mae: 0.6509 - val_mean_pred: 0.7410 - val_mae_t1: 0.0434\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 23.6060 - mae: 0.6212 - mean_pred: 0.7297 - mae_t1: 0.0414 - val_loss: 26.5691 - val_mae: 0.6992 - val_mean_pred: 0.7649 - val_mae_t1: 0.0466\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 20.4747 - mae: 0.5388 - mean_pred: 0.7042 - mae_t1: 0.0359 - val_loss: 24.5707 - val_mae: 0.6466 - val_mean_pred: 0.7508 - val_mae_t1: 0.0431\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 22.3335 - mae: 0.5877 - mean_pred: 0.7230 - mae_t1: 0.0392 - val_loss: 26.9666 - val_mae: 0.7096 - val_mean_pred: 0.8697 - val_mae_t1: 0.0473\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 21.8515 - mae: 0.5750 - mean_pred: 0.7413 - mae_t1: 0.0383 - val_loss: 22.4561 - val_mae: 0.5910 - val_mean_pred: 0.8658 - val_mae_t1: 0.0394\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 20.0851 - mae: 0.5286 - mean_pred: 0.8079 - mae_t1: 0.0352 - val_loss: 22.3058 - val_mae: 0.5870 - val_mean_pred: 0.9351 - val_mae_t1: 0.0391\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 20.1242 - mae: 0.5296 - mean_pred: 0.8586 - mae_t1: 0.0353 - val_loss: 23.0044 - val_mae: 0.6054 - val_mean_pred: 0.9504 - val_mae_t1: 0.0404\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 133us/sample - loss: 22.1810 - mae: 0.5837 - mean_pred: 0.8452 - mae_t1: 0.0389 - val_loss: 17.7558 - val_mae: 0.4673 - val_mean_pred: 0.7152 - val_mae_t1: 0.0312\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 18.6090 - mae: 0.4897 - mean_pred: 0.6855 - mae_t1: 0.0326 - val_loss: 19.6466 - val_mae: 0.5170 - val_mean_pred: 0.6329 - val_mae_t1: 0.0345\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 17.9852 - mae: 0.4733 - mean_pred: 0.6170 - mae_t1: 0.0316 - val_loss: 23.4377 - val_mae: 0.6168 - val_mean_pred: 0.6965 - val_mae_t1: 0.0411\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 18.5698 - mae: 0.4887 - mean_pred: 0.7200 - mae_t1: 0.0326 - val_loss: 24.1935 - val_mae: 0.6367 - val_mean_pred: 0.9495 - val_mae_t1: 0.0424\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 22.7639 - mae: 0.5990 - mean_pred: 0.9357 - mae_t1: 0.0399 - val_loss: 24.3200 - val_mae: 0.6400 - val_mean_pred: 0.9061 - val_mae_t1: 0.0427\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.39277293545460795  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▃▂▂▂▅▄█▆▄▄▃▂▂▂▃▂▂▂▂▂▃▃▂▃▂▂▄▂▂▁▁▂▂▁▂▂▂▂▁▂</td></tr><tr><td>mae</td><td>▃▂▂▂▅▄█▆▄▄▃▂▂▂▃▂▂▂▂▂▃▃▂▃▂▂▄▂▂▁▁▂▂▁▂▂▂▂▁▂</td></tr><tr><td>mae_t1</td><td>▃▂▂▂▅▄█▆▄▄▃▂▂▂▃▂▂▂▂▂▃▃▂▃▂▂▄▂▂▁▁▂▂▁▂▂▂▂▁▂</td></tr><tr><td>mean_pred</td><td>▁▄▂▃▅▃▅█▅▂▄▅▅▄▅▃▄▅▄▂▆▄▃▄▄▄▆▃▆▄▄▄▆▄▄▄▄▅▄▆</td></tr><tr><td>val_loss</td><td>▃▃▂▃▇█▃▇▄▅▂▂▂▃▄▂▃▃▄▃▃▂▄▄▂▂▁▁▂▁▂▁▁▂▆▃▃▂▁▃</td></tr><tr><td>val_mae</td><td>▃▃▂▃▇█▃▇▄▅▂▂▂▃▄▂▃▃▄▃▃▂▄▄▂▂▁▁▂▁▂▁▁▂▆▃▃▂▁▃</td></tr><tr><td>val_mae_t1</td><td>▃▃▂▃▇█▃▇▄▅▂▂▂▃▄▂▃▃▄▃▃▂▄▄▂▂▁▁▂▁▂▁▁▂▆▃▃▂▁▃</td></tr><tr><td>val_mean_pred</td><td>▁▄▅▃▂█▅▅▂▃▃▃▄▃▄▂▂▅▂▃▅▁▄▃▄▃▃▃▅▃▂▂▄▂▄▃▃▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.39323</td></tr><tr><td>AE_2</td><td>0.3878</td></tr><tr><td>AE_3</td><td>0.36337</td></tr><tr><td>MAE</td><td>0.39277</td></tr><tr><td>best_epoch</td><td>95</td></tr><tr><td>best_val_loss</td><td>17.75578</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>22.76387</td></tr><tr><td>mae</td><td>0.59905</td></tr><tr><td>mae_t1</td><td>0.03994</td></tr><tr><td>mean_pred</td><td>0.93568</td></tr><tr><td>val_loss</td><td>24.32002</td></tr><tr><td>val_mae</td><td>0.64</td></tr><tr><td>val_mae_t1</td><td>0.04267</td></tr><tr><td>val_mean_pred</td><td>0.90611</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lunar-lion-58</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/16cnsejg\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/16cnsejg</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154314-16cnsejg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154349-35qx6oo3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35qx6oo3\" target=\"_blank\">mild-morning-59</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 850us/sample - loss: 33.4235 - mae: 0.6963 - mean_pred: 0.7097 - mae_t1: 0.0464 - val_loss: 38.9407 - val_mae: 0.8113 - val_mean_pred: 0.2041 - val_mae_t1: 0.0541\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 32.9668 - mae: 0.6868 - mean_pred: 0.6364 - mae_t1: 0.0458 - val_loss: 60.1494 - val_mae: 1.2531 - val_mean_pred: 0.4776 - val_mae_t1: 0.0835\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 54.4705 - mae: 1.1348 - mean_pred: 0.7312 - mae_t1: 0.0757 - val_loss: 26.9556 - val_mae: 0.5616 - val_mean_pred: 0.7932 - val_mae_t1: 0.0374\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 32.2455 - mae: 0.6718 - mean_pred: 0.7167 - mae_t1: 0.0448 - val_loss: 43.1706 - val_mae: 0.8994 - val_mean_pred: 1.4203 - val_mae_t1: 0.0600\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 35.0379 - mae: 0.7300 - mean_pred: 0.7504 - mae_t1: 0.0487 - val_loss: 84.6566 - val_mae: 1.7637 - val_mean_pred: 2.3533 - val_mae_t1: 0.1176\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 55.8967 - mae: 1.1645 - mean_pred: 1.4493 - mae_t1: 0.0776 - val_loss: 51.3011 - val_mae: 1.0688 - val_mean_pred: 0.6479 - val_mae_t1: 0.0713\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 38.0139 - mae: 0.7920 - mean_pred: 0.6335 - mae_t1: 0.0528 - val_loss: 27.3250 - val_mae: 0.5693 - val_mean_pred: 0.9563 - val_mae_t1: 0.0380\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 30.8530 - mae: 0.6428 - mean_pred: 0.5029 - mae_t1: 0.0429 - val_loss: 26.0397 - val_mae: 0.5425 - val_mean_pred: 0.6550 - val_mae_t1: 0.0362\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 26.9152 - mae: 0.5607 - mean_pred: 0.7886 - mae_t1: 0.0374 - val_loss: 30.9884 - val_mae: 0.6456 - val_mean_pred: 0.8667 - val_mae_t1: 0.0430\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 30.6777 - mae: 0.6391 - mean_pred: 0.6968 - mae_t1: 0.0426 - val_loss: 29.4633 - val_mae: 0.6138 - val_mean_pred: 0.9519 - val_mae_t1: 0.0409\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 30.0453 - mae: 0.6259 - mean_pred: 0.9632 - mae_t1: 0.0417 - val_loss: 33.3659 - val_mae: 0.6951 - val_mean_pred: 0.8177 - val_mae_t1: 0.0463\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 33.9381 - mae: 0.7070 - mean_pred: 0.4772 - mae_t1: 0.0471 - val_loss: 40.4622 - val_mae: 0.8430 - val_mean_pred: 0.5439 - val_mae_t1: 0.0562\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 31.3498 - mae: 0.6531 - mean_pred: 0.6214 - mae_t1: 0.0435 - val_loss: 31.9383 - val_mae: 0.6654 - val_mean_pred: 1.1866 - val_mae_t1: 0.0444\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 28.7864 - mae: 0.5997 - mean_pred: 0.9353 - mae_t1: 0.0400 - val_loss: 27.5688 - val_mae: 0.5744 - val_mean_pred: 0.6869 - val_mae_t1: 0.0383\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 25.7464 - mae: 0.5364 - mean_pred: 0.7514 - mae_t1: 0.0358 - val_loss: 26.2488 - val_mae: 0.5469 - val_mean_pred: 0.9430 - val_mae_t1: 0.0365\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 25.1506 - mae: 0.5240 - mean_pred: 0.8315 - mae_t1: 0.0349 - val_loss: 25.9005 - val_mae: 0.5396 - val_mean_pred: 0.8098 - val_mae_t1: 0.0360\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 24.3288 - mae: 0.5068 - mean_pred: 0.7829 - mae_t1: 0.0338 - val_loss: 23.4727 - val_mae: 0.4890 - val_mean_pred: 0.7288 - val_mae_t1: 0.0326\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 24.8671 - mae: 0.5181 - mean_pred: 0.5168 - mae_t1: 0.0345 - val_loss: 24.9545 - val_mae: 0.5199 - val_mean_pred: 0.7994 - val_mae_t1: 0.0347\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 27.1440 - mae: 0.5655 - mean_pred: 0.8263 - mae_t1: 0.0377 - val_loss: 36.8586 - val_mae: 0.7679 - val_mean_pred: 1.1560 - val_mae_t1: 0.0512\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 36.7963 - mae: 0.7666 - mean_pred: 0.9326 - mae_t1: 0.0511 - val_loss: 31.0273 - val_mae: 0.6464 - val_mean_pred: 0.9033 - val_mae_t1: 0.0431\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 27.4228 - mae: 0.5713 - mean_pred: 0.7567 - mae_t1: 0.0381 - val_loss: 31.8423 - val_mae: 0.6634 - val_mean_pred: 1.0287 - val_mae_t1: 0.0442\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 24.9924 - mae: 0.5207 - mean_pred: 0.8601 - mae_t1: 0.0347 - val_loss: 32.1465 - val_mae: 0.6697 - val_mean_pred: 0.9268 - val_mae_t1: 0.0446\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 28.1532 - mae: 0.5865 - mean_pred: 0.7365 - mae_t1: 0.0391 - val_loss: 26.6189 - val_mae: 0.5546 - val_mean_pred: 0.8001 - val_mae_t1: 0.0370\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 24.3409 - mae: 0.5071 - mean_pred: 0.7609 - mae_t1: 0.0338 - val_loss: 31.8561 - val_mae: 0.6637 - val_mean_pred: 0.9221 - val_mae_t1: 0.0442\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 24.5655 - mae: 0.5118 - mean_pred: 0.8270 - mae_t1: 0.0341 - val_loss: 27.1598 - val_mae: 0.5658 - val_mean_pred: 0.8055 - val_mae_t1: 0.0377\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 23.8576 - mae: 0.4970 - mean_pred: 0.7524 - mae_t1: 0.0331 - val_loss: 30.3672 - val_mae: 0.6327 - val_mean_pred: 0.9409 - val_mae_t1: 0.0422\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 27.7566 - mae: 0.5783 - mean_pred: 0.8322 - mae_t1: 0.0386 - val_loss: 25.5699 - val_mae: 0.5327 - val_mean_pred: 0.9686 - val_mae_t1: 0.0355\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 23.4913 - mae: 0.4894 - mean_pred: 0.8720 - mae_t1: 0.0326 - val_loss: 26.7257 - val_mae: 0.5568 - val_mean_pred: 1.1004 - val_mae_t1: 0.0371\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 25.0192 - mae: 0.5212 - mean_pred: 0.9978 - mae_t1: 0.0347 - val_loss: 24.8748 - val_mae: 0.5182 - val_mean_pred: 0.9622 - val_mae_t1: 0.0345\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 23.6813 - mae: 0.4934 - mean_pred: 0.6681 - mae_t1: 0.0329 - val_loss: 25.3106 - val_mae: 0.5273 - val_mean_pred: 0.7960 - val_mae_t1: 0.0352\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 21.7813 - mae: 0.4538 - mean_pred: 0.7361 - mae_t1: 0.0303 - val_loss: 27.9203 - val_mae: 0.5817 - val_mean_pred: 1.0325 - val_mae_t1: 0.0388\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 22.3990 - mae: 0.4666 - mean_pred: 0.8638 - mae_t1: 0.0311 - val_loss: 29.3929 - val_mae: 0.6124 - val_mean_pred: 0.7016 - val_mae_t1: 0.0408\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 27.1314 - mae: 0.5652 - mean_pred: 0.5576 - mae_t1: 0.0377 - val_loss: 30.1699 - val_mae: 0.6285 - val_mean_pred: 0.6750 - val_mae_t1: 0.0419\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 23.3266 - mae: 0.4860 - mean_pred: 0.6778 - mae_t1: 0.0324 - val_loss: 36.5824 - val_mae: 0.7621 - val_mean_pred: 1.0855 - val_mae_t1: 0.0508\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 28.9571 - mae: 0.6033 - mean_pred: 0.9709 - mae_t1: 0.0402 - val_loss: 24.6448 - val_mae: 0.5134 - val_mean_pred: 0.8533 - val_mae_t1: 0.0342\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 26.3785 - mae: 0.5496 - mean_pred: 0.7265 - mae_t1: 0.0366 - val_loss: 37.8867 - val_mae: 0.7893 - val_mean_pred: 0.8055 - val_mae_t1: 0.0526\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 30.5871 - mae: 0.6372 - mean_pred: 0.7029 - mae_t1: 0.0425 - val_loss: 27.6323 - val_mae: 0.5757 - val_mean_pred: 0.9558 - val_mae_t1: 0.0384\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 29.6415 - mae: 0.6175 - mean_pred: 0.9184 - mae_t1: 0.0412 - val_loss: 45.8696 - val_mae: 0.9556 - val_mean_pred: 1.3642 - val_mae_t1: 0.0637\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 34.2659 - mae: 0.7139 - mean_pred: 1.0230 - mae_t1: 0.0476 - val_loss: 25.2715 - val_mae: 0.5265 - val_mean_pred: 0.6429 - val_mae_t1: 0.0351\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 30.4437 - mae: 0.6342 - mean_pred: 0.6749 - mae_t1: 0.0423 - val_loss: 31.7374 - val_mae: 0.6612 - val_mean_pred: 0.6073 - val_mae_t1: 0.0441\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 38.0799 - mae: 0.7933 - mean_pred: 0.7652 - mae_t1: 0.0529 - val_loss: 57.8083 - val_mae: 1.2043 - val_mean_pred: 1.1722 - val_mae_t1: 0.0803\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 39.2312 - mae: 0.8173 - mean_pred: 0.9127 - mae_t1: 0.0545 - val_loss: 37.8658 - val_mae: 0.7889 - val_mean_pred: 1.0609 - val_mae_t1: 0.0526\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 33.9476 - mae: 0.7072 - mean_pred: 0.9511 - mae_t1: 0.0471 - val_loss: 28.4442 - val_mae: 0.5926 - val_mean_pred: 0.8843 - val_mae_t1: 0.0395\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 30.6283 - mae: 0.6381 - mean_pred: 0.8049 - mae_t1: 0.0425 - val_loss: 23.9220 - val_mae: 0.4984 - val_mean_pred: 0.8484 - val_mae_t1: 0.0332\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 22.2405 - mae: 0.4633 - mean_pred: 0.7605 - mae_t1: 0.0309 - val_loss: 26.8652 - val_mae: 0.5597 - val_mean_pred: 1.0046 - val_mae_t1: 0.0373\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 21.7570 - mae: 0.4533 - mean_pred: 0.8007 - mae_t1: 0.0302 - val_loss: 25.3927 - val_mae: 0.5290 - val_mean_pred: 0.9310 - val_mae_t1: 0.0353\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 21.6769 - mae: 0.4516 - mean_pred: 0.8457 - mae_t1: 0.0301 - val_loss: 22.6338 - val_mae: 0.4715 - val_mean_pred: 0.8430 - val_mae_t1: 0.0314\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 400us/sample - loss: 22.8604 - mae: 0.4763 - mean_pred: 0.7012 - mae_t1: 0.0318 - val_loss: 24.4278 - val_mae: 0.5089 - val_mean_pred: 0.7648 - val_mae_t1: 0.0339\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 20.3960 - mae: 0.4249 - mean_pred: 0.7372 - mae_t1: 0.0283 - val_loss: 26.4597 - val_mae: 0.5512 - val_mean_pred: 0.9847 - val_mae_t1: 0.0367\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 20.8798 - mae: 0.4350 - mean_pred: 0.9012 - mae_t1: 0.0290 - val_loss: 27.2415 - val_mae: 0.5675 - val_mean_pred: 1.0237 - val_mae_t1: 0.0378\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 19.2254 - mae: 0.4005 - mean_pred: 0.9222 - mae_t1: 0.0267 - val_loss: 26.9822 - val_mae: 0.5621 - val_mean_pred: 0.7787 - val_mae_t1: 0.0375\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 23.9928 - mae: 0.4998 - mean_pred: 0.5649 - mae_t1: 0.0333 - val_loss: 31.6112 - val_mae: 0.6586 - val_mean_pred: 0.4922 - val_mae_t1: 0.0439\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 25.3418 - mae: 0.5280 - mean_pred: 0.5866 - mae_t1: 0.0352 - val_loss: 37.2785 - val_mae: 0.7766 - val_mean_pred: 1.0641 - val_mae_t1: 0.0518\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 25.9099 - mae: 0.5398 - mean_pred: 1.0219 - mae_t1: 0.0360 - val_loss: 30.1255 - val_mae: 0.6276 - val_mean_pred: 1.0064 - val_mae_t1: 0.0418\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 23.1389 - mae: 0.4821 - mean_pred: 0.8701 - mae_t1: 0.0321 - val_loss: 26.3105 - val_mae: 0.5481 - val_mean_pred: 0.7080 - val_mae_t1: 0.0365\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 23.9587 - mae: 0.4991 - mean_pred: 0.6136 - mae_t1: 0.0333 - val_loss: 23.9730 - val_mae: 0.4994 - val_mean_pred: 0.6607 - val_mae_t1: 0.0333\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 22.0240 - mae: 0.4588 - mean_pred: 0.7299 - mae_t1: 0.0306 - val_loss: 32.7772 - val_mae: 0.6829 - val_mean_pred: 0.9767 - val_mae_t1: 0.0455\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 25.5855 - mae: 0.5330 - mean_pred: 0.8859 - mae_t1: 0.0355 - val_loss: 26.1923 - val_mae: 0.5457 - val_mean_pred: 0.8019 - val_mae_t1: 0.0364\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 21.7137 - mae: 0.4524 - mean_pred: 0.7882 - mae_t1: 0.0302 - val_loss: 27.3501 - val_mae: 0.5698 - val_mean_pred: 0.8446 - val_mae_t1: 0.0380\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 25.3176 - mae: 0.5275 - mean_pred: 0.9010 - mae_t1: 0.0352 - val_loss: 34.3271 - val_mae: 0.7151 - val_mean_pred: 1.0272 - val_mae_t1: 0.0477\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 26.4304 - mae: 0.5506 - mean_pred: 0.9987 - mae_t1: 0.0367 - val_loss: 23.6164 - val_mae: 0.4920 - val_mean_pred: 0.8924 - val_mae_t1: 0.0328\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 22.2158 - mae: 0.4628 - mean_pred: 0.7362 - mae_t1: 0.0309 - val_loss: 27.6830 - val_mae: 0.5767 - val_mean_pred: 0.7128 - val_mae_t1: 0.0384\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 21.5671 - mae: 0.4493 - mean_pred: 0.7416 - mae_t1: 0.0300 - val_loss: 24.7371 - val_mae: 0.5154 - val_mean_pred: 0.8565 - val_mae_t1: 0.0344\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 18.7196 - mae: 0.3900 - mean_pred: 0.7221 - mae_t1: 0.0260 - val_loss: 25.7628 - val_mae: 0.5367 - val_mean_pred: 0.7694 - val_mae_t1: 0.0358\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 21.5621 - mae: 0.4492 - mean_pred: 0.6917 - mae_t1: 0.0299 - val_loss: 30.8283 - val_mae: 0.6423 - val_mean_pred: 0.9723 - val_mae_t1: 0.0428\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 22.6461 - mae: 0.4718 - mean_pred: 0.8636 - mae_t1: 0.0315 - val_loss: 26.5843 - val_mae: 0.5538 - val_mean_pred: 0.9309 - val_mae_t1: 0.0369\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 19.9409 - mae: 0.4154 - mean_pred: 0.7291 - mae_t1: 0.0277 - val_loss: 27.4666 - val_mae: 0.5722 - val_mean_pred: 0.8220 - val_mae_t1: 0.0381\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 22.5704 - mae: 0.4702 - mean_pred: 0.7480 - mae_t1: 0.0313 - val_loss: 24.4874 - val_mae: 0.5102 - val_mean_pred: 0.9053 - val_mae_t1: 0.0340\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 22.5809 - mae: 0.4704 - mean_pred: 0.8629 - mae_t1: 0.0314 - val_loss: 32.7262 - val_mae: 0.6818 - val_mean_pred: 0.9420 - val_mae_t1: 0.0455\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 28.7575 - mae: 0.5991 - mean_pred: 0.8618 - mae_t1: 0.0399 - val_loss: 29.7648 - val_mae: 0.6201 - val_mean_pred: 0.7955 - val_mae_t1: 0.0413\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 25.6085 - mae: 0.5335 - mean_pred: 0.7022 - mae_t1: 0.0356 - val_loss: 48.0326 - val_mae: 1.0007 - val_mean_pred: 1.0266 - val_mae_t1: 0.0667\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 36.8975 - mae: 0.7687 - mean_pred: 0.9282 - mae_t1: 0.0512 - val_loss: 31.1425 - val_mae: 0.6488 - val_mean_pred: 1.0132 - val_mae_t1: 0.0433\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 26.7501 - mae: 0.5573 - mean_pred: 0.9420 - mae_t1: 0.0372 - val_loss: 27.6802 - val_mae: 0.5767 - val_mean_pred: 1.0524 - val_mae_t1: 0.0384\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 26.9607 - mae: 0.5617 - mean_pred: 1.0579 - mae_t1: 0.0374 - val_loss: 27.1959 - val_mae: 0.5666 - val_mean_pred: 0.8798 - val_mae_t1: 0.0378\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 20.5443 - mae: 0.4280 - mean_pred: 0.7345 - mae_t1: 0.0285 - val_loss: 28.9607 - val_mae: 0.6033 - val_mean_pred: 0.6880 - val_mae_t1: 0.0402\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 20.6969 - mae: 0.4312 - mean_pred: 0.7180 - mae_t1: 0.0287 - val_loss: 29.8162 - val_mae: 0.6212 - val_mean_pred: 0.9556 - val_mae_t1: 0.0414\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 22.4696 - mae: 0.4681 - mean_pred: 0.9485 - mae_t1: 0.0312 - val_loss: 31.4376 - val_mae: 0.6550 - val_mean_pred: 0.8564 - val_mae_t1: 0.0437\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 21.1810 - mae: 0.4413 - mean_pred: 0.7499 - mae_t1: 0.0294 - val_loss: 31.5286 - val_mae: 0.6568 - val_mean_pred: 0.7148 - val_mae_t1: 0.0438\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 20.9066 - mae: 0.4356 - mean_pred: 0.7357 - mae_t1: 0.0290 - val_loss: 30.0084 - val_mae: 0.6252 - val_mean_pred: 0.9712 - val_mae_t1: 0.0417\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 22.1435 - mae: 0.4613 - mean_pred: 0.8825 - mae_t1: 0.0308 - val_loss: 30.4797 - val_mae: 0.6350 - val_mean_pred: 0.8529 - val_mae_t1: 0.0423\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 22.4239 - mae: 0.4672 - mean_pred: 0.7390 - mae_t1: 0.0311 - val_loss: 29.5529 - val_mae: 0.6157 - val_mean_pred: 0.7666 - val_mae_t1: 0.0410\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 21.5300 - mae: 0.4485 - mean_pred: 0.7034 - mae_t1: 0.0299 - val_loss: 26.2896 - val_mae: 0.5477 - val_mean_pred: 0.7896 - val_mae_t1: 0.0365\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 21.3380 - mae: 0.4445 - mean_pred: 0.7621 - mae_t1: 0.0296 - val_loss: 33.8377 - val_mae: 0.7050 - val_mean_pred: 0.9374 - val_mae_t1: 0.0470\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 25.6281 - mae: 0.5339 - mean_pred: 0.8021 - mae_t1: 0.0356 - val_loss: 23.4603 - val_mae: 0.4888 - val_mean_pred: 0.7856 - val_mae_t1: 0.0326\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 18.7416 - mae: 0.3904 - mean_pred: 0.7396 - mae_t1: 0.0260 - val_loss: 25.8296 - val_mae: 0.5381 - val_mean_pred: 0.8119 - val_mae_t1: 0.0359\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 18.5773 - mae: 0.3870 - mean_pred: 0.7220 - mae_t1: 0.0258 - val_loss: 24.6062 - val_mae: 0.5126 - val_mean_pred: 0.8068 - val_mae_t1: 0.0342\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 18.8728 - mae: 0.3932 - mean_pred: 0.7325 - mae_t1: 0.0262 - val_loss: 25.3629 - val_mae: 0.5284 - val_mean_pred: 0.9159 - val_mae_t1: 0.0352\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 18.8316 - mae: 0.3923 - mean_pred: 0.8445 - mae_t1: 0.0262 - val_loss: 34.5905 - val_mae: 0.7206 - val_mean_pred: 0.9326 - val_mae_t1: 0.0480\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 26.5028 - mae: 0.5521 - mean_pred: 0.8200 - mae_t1: 0.0368 - val_loss: 35.1238 - val_mae: 0.7317 - val_mean_pred: 0.8186 - val_mae_t1: 0.0488\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 23.7091 - mae: 0.4939 - mean_pred: 0.8182 - mae_t1: 0.0329 - val_loss: 30.3241 - val_mae: 0.6318 - val_mean_pred: 0.9621 - val_mae_t1: 0.0421\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 20.6883 - mae: 0.4310 - mean_pred: 0.9149 - mae_t1: 0.0287 - val_loss: 31.3621 - val_mae: 0.6534 - val_mean_pred: 0.9708 - val_mae_t1: 0.0436\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 19.9726 - mae: 0.4161 - mean_pred: 0.8158 - mae_t1: 0.0277 - val_loss: 30.0712 - val_mae: 0.6265 - val_mean_pred: 0.9511 - val_mae_t1: 0.0418\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 22.1311 - mae: 0.4611 - mean_pred: 0.8487 - mae_t1: 0.0307 - val_loss: 25.5957 - val_mae: 0.5332 - val_mean_pred: 0.9113 - val_mae_t1: 0.0355\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 21.3127 - mae: 0.4440 - mean_pred: 0.7945 - mae_t1: 0.0296 - val_loss: 22.6079 - val_mae: 0.4710 - val_mean_pred: 0.7723 - val_mae_t1: 0.0314\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 19.2038 - mae: 0.4001 - mean_pred: 0.7465 - mae_t1: 0.0267 - val_loss: 22.6668 - val_mae: 0.4722 - val_mean_pred: 0.7549 - val_mae_t1: 0.0315\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 19.1844 - mae: 0.3997 - mean_pred: 0.7691 - mae_t1: 0.0266 - val_loss: 24.0421 - val_mae: 0.5009 - val_mean_pred: 0.8592 - val_mae_t1: 0.0334\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 18.5652 - mae: 0.3868 - mean_pred: 0.8705 - mae_t1: 0.0258 - val_loss: 27.5013 - val_mae: 0.5729 - val_mean_pred: 1.0074 - val_mae_t1: 0.0382\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 16.7226 - mae: 0.3484 - mean_pred: 0.8809 - mae_t1: 0.0232 - val_loss: 28.9628 - val_mae: 0.6034 - val_mean_pred: 0.8418 - val_mae_t1: 0.0402\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 18.6062 - mae: 0.3876 - mean_pred: 0.7407 - mae_t1: 0.0258 - val_loss: 44.6247 - val_mae: 0.9297 - val_mean_pred: 1.0248 - val_mae_t1: 0.0620\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 28.5697 - mae: 0.5952 - mean_pred: 0.8284 - mae_t1: 0.0397 - val_loss: 31.5774 - val_mae: 0.6579 - val_mean_pred: 0.9907 - val_mae_t1: 0.0439\n",
      "Earliness...\n",
      "0.0019991397857666016\n",
      "____________________________________________________________\n",
      "Test MAE:      0.4725152550710337  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▄██▃▃▃▂▂▃▃▂▂▂▂▂▄▅▃▂▁▁▂▂▂▂▁▁▂▄▃▂▁▂▂▁▂▁▂▁▃</td></tr><tr><td>mae</td><td>▄██▃▃▃▂▂▃▃▂▂▂▂▂▄▅▃▂▁▁▂▂▂▂▁▁▂▄▃▂▁▂▂▁▂▁▂▁▃</td></tr><tr><td>mae_t1</td><td>▄██▃▃▃▂▂▃▃▂▂▂▂▂▄▅▃▂▁▁▂▂▂▂▁▁▂▄▃▂▁▂▂▁▂▁▂▁▃</td></tr><tr><td>mean_pred</td><td>▃▃█▁▄▂▃▁▃▃▃▄▃▂▃▅▃▃▃▃▄▅▂▃▅▃▃▄▄▅▄▃▂▃▃▃▃▃▄▃</td></tr><tr><td>val_loss</td><td>▄▂▇▂▃▃▂▁▃▂▃▂▂▄▄▂█▁▂▂▂▂▁▂▁▂▂▃▃▂▃▂▂▁▂▃▂▁▂▃</td></tr><tr><td>val_mae</td><td>▄▂▇▂▃▃▂▁▃▂▃▂▂▄▄▂█▁▂▂▂▂▁▂▁▂▂▃▃▂▃▂▂▁▂▃▂▁▂▃</td></tr><tr><td>val_mae_t1</td><td>▄▂▇▂▃▃▂▁▃▂▃▂▂▄▄▂█▁▂▂▂▂▁▂▁▂▂▃▃▂▃▂▂▁▂▃▂▁▂▃</td></tr><tr><td>val_mean_pred</td><td>▁▅▄▄▅█▅▅▇▅▆▇▇▇▅▄█▆▆▇▅▇▄▆▆▅▅▆▇▆▆▆▅▅▆▅▆▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.48518</td></tr><tr><td>AE_2</td><td>0.4219</td></tr><tr><td>AE_3</td><td>0.48466</td></tr><tr><td>MAE</td><td>0.47252</td></tr><tr><td>best_epoch</td><td>93</td></tr><tr><td>best_val_loss</td><td>22.60791</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>28.56974</td></tr><tr><td>mae</td><td>0.5952</td></tr><tr><td>mae_t1</td><td>0.03968</td></tr><tr><td>mean_pred</td><td>0.82841</td></tr><tr><td>val_loss</td><td>31.5774</td></tr><tr><td>val_mae</td><td>0.65786</td></tr><tr><td>val_mae_t1</td><td>0.04386</td></tr><tr><td>val_mean_pred</td><td>0.9907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">mild-morning-59</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35qx6oo3\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/35qx6oo3</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154349-35qx6oo3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154415-1xga97en</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xga97en\" target=\"_blank\">crisp-oath-60</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 705us/sample - loss: 39.9163 - mae: 0.8316 - mean_pred: 0.4841 - mae_t1: 0.0554 - val_loss: 30.7643 - val_mae: 0.6409 - val_mean_pred: 0.6038 - val_mae_t1: 0.0427\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 37.6986 - mae: 0.7854 - mean_pred: 0.4331 - mae_t1: 0.0524 - val_loss: 29.0180 - val_mae: 0.6045 - val_mean_pred: 0.7384 - val_mae_t1: 0.0403\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 139us/sample - loss: 28.5587 - mae: 0.5950 - mean_pred: 0.7822 - mae_t1: 0.0397 - val_loss: 62.5030 - val_mae: 1.3021 - val_mean_pred: 0.5083 - val_mae_t1: 0.0868\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 129us/sample - loss: 60.4663 - mae: 1.2597 - mean_pred: 0.5764 - mae_t1: 0.0840 - val_loss: 45.6814 - val_mae: 0.9517 - val_mean_pred: 0.3292 - val_mae_t1: 0.0634\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 37.7072 - mae: 0.7856 - mean_pred: 0.4598 - mae_t1: 0.0524 - val_loss: 78.7780 - val_mae: 1.6412 - val_mean_pred: 2.2835 - val_mae_t1: 0.1094\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 65.9602 - mae: 1.3742 - mean_pred: 1.8596 - mae_t1: 0.0916 - val_loss: 73.0996 - val_mae: 1.5229 - val_mean_pred: 0.9010 - val_mae_t1: 0.1015\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 62.6235 - mae: 1.3047 - mean_pred: 0.8471 - mae_t1: 0.0870 - val_loss: 33.4194 - val_mae: 0.6962 - val_mean_pred: 1.1619 - val_mae_t1: 0.0464\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 36.5143 - mae: 0.7607 - mean_pred: 1.1131 - mae_t1: 0.0507 - val_loss: 48.9088 - val_mae: 1.0189 - val_mean_pred: 0.4016 - val_mae_t1: 0.0679\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 48.1585 - mae: 1.0033 - mean_pred: 0.4371 - mae_t1: 0.0669 - val_loss: 44.4703 - val_mae: 0.9265 - val_mean_pred: 0.7763 - val_mae_t1: 0.0618\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 44.6331 - mae: 0.9299 - mean_pred: 0.9078 - mae_t1: 0.0620 - val_loss: 89.1929 - val_mae: 1.8582 - val_mean_pred: 2.2248 - val_mae_t1: 0.1239\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 102.2424 - mae: 2.1300 - mean_pred: 2.2945 - mae_t1: 0.1420 - val_loss: 37.0501 - val_mae: 0.7719 - val_mean_pred: 0.5914 - val_mae_t1: 0.0515\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 43.6274 - mae: 0.9089 - mean_pred: 0.5758 - mae_t1: 0.0606 - val_loss: 42.2288 - val_mae: 0.8798 - val_mean_pred: 0.4302 - val_mae_t1: 0.0587\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 41.6671 - mae: 0.8681 - mean_pred: 0.6772 - mae_t1: 0.0579 - val_loss: 37.2308 - val_mae: 0.7756 - val_mean_pred: 1.1391 - val_mae_t1: 0.0517\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 43.6659 - mae: 0.9097 - mean_pred: 1.1397 - mae_t1: 0.0606 - val_loss: 42.0655 - val_mae: 0.8764 - val_mean_pred: 1.2316 - val_mae_t1: 0.0584\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 36.9712 - mae: 0.7702 - mean_pred: 1.0315 - mae_t1: 0.0513 - val_loss: 55.7803 - val_mae: 1.1621 - val_mean_pred: 1.1065 - val_mae_t1: 0.0775\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 49.5053 - mae: 1.0314 - mean_pred: 0.9613 - mae_t1: 0.0688 - val_loss: 36.8045 - val_mae: 0.7668 - val_mean_pred: 0.5785 - val_mae_t1: 0.0511\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 44.3925 - mae: 0.9248 - mean_pred: 0.7444 - mae_t1: 0.0617 - val_loss: 36.5234 - val_mae: 0.7609 - val_mean_pred: 0.4135 - val_mae_t1: 0.0507\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 41.0582 - mae: 0.8554 - mean_pred: 0.4222 - mae_t1: 0.0570 - val_loss: 36.8637 - val_mae: 0.7680 - val_mean_pred: 0.5636 - val_mae_t1: 0.0512\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 36.6212 - mae: 0.7629 - mean_pred: 0.5441 - mae_t1: 0.0509 - val_loss: 34.5385 - val_mae: 0.7196 - val_mean_pred: 0.8487 - val_mae_t1: 0.0480\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 34.6621 - mae: 0.7221 - mean_pred: 0.7974 - mae_t1: 0.0481 - val_loss: 31.4223 - val_mae: 0.6546 - val_mean_pred: 0.7781 - val_mae_t1: 0.0436\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 32.9451 - mae: 0.6864 - mean_pred: 0.6573 - mae_t1: 0.0458 - val_loss: 36.3803 - val_mae: 0.7579 - val_mean_pred: 0.5726 - val_mae_t1: 0.0505\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 35.7814 - mae: 0.7454 - mean_pred: 0.4831 - mae_t1: 0.0497 - val_loss: 50.5075 - val_mae: 1.0522 - val_mean_pred: 0.7172 - val_mae_t1: 0.0701\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 50.6227 - mae: 1.0546 - mean_pred: 0.8269 - mae_t1: 0.0703 - val_loss: 38.1636 - val_mae: 0.7951 - val_mean_pred: 0.5243 - val_mae_t1: 0.0530\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 32.7493 - mae: 0.6823 - mean_pred: 0.5027 - mae_t1: 0.0455 - val_loss: 36.0895 - val_mae: 0.7519 - val_mean_pred: 0.8278 - val_mae_t1: 0.0501\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 33.1280 - mae: 0.6902 - mean_pred: 0.8067 - mae_t1: 0.0460 - val_loss: 43.4128 - val_mae: 0.9044 - val_mean_pred: 1.4067 - val_mae_t1: 0.0603\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 47.5035 - mae: 0.9897 - mean_pred: 1.3407 - mae_t1: 0.0660 - val_loss: 43.4132 - val_mae: 0.9044 - val_mean_pred: 1.1796 - val_mae_t1: 0.0603\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 45.6442 - mae: 0.9509 - mean_pred: 1.0414 - mae_t1: 0.0634 - val_loss: 39.3568 - val_mae: 0.8199 - val_mean_pred: 0.6117 - val_mae_t1: 0.0547\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 42.7604 - mae: 0.8908 - mean_pred: 0.7484 - mae_t1: 0.0594 - val_loss: 54.6156 - val_mae: 1.1378 - val_mean_pred: 0.9993 - val_mae_t1: 0.0759\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 46.5676 - mae: 0.9702 - mean_pred: 0.9030 - mae_t1: 0.0647 - val_loss: 40.6100 - val_mae: 0.8460 - val_mean_pred: 0.9871 - val_mae_t1: 0.0564\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 35.9277 - mae: 0.7485 - mean_pred: 1.0563 - mae_t1: 0.0499 - val_loss: 41.5917 - val_mae: 0.8665 - val_mean_pred: 1.2951 - val_mae_t1: 0.0578\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 36.9730 - mae: 0.7703 - mean_pred: 1.1682 - mae_t1: 0.0514 - val_loss: 31.0326 - val_mae: 0.6465 - val_mean_pred: 0.9788 - val_mae_t1: 0.0431\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 31.6821 - mae: 0.6600 - mean_pred: 0.8712 - mae_t1: 0.0440 - val_loss: 30.1677 - val_mae: 0.6285 - val_mean_pred: 0.6579 - val_mae_t1: 0.0419\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 27.6218 - mae: 0.5755 - mean_pred: 0.5834 - mae_t1: 0.0384 - val_loss: 27.7317 - val_mae: 0.5777 - val_mean_pred: 0.7093 - val_mae_t1: 0.0385\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 26.0508 - mae: 0.5427 - mean_pred: 0.6867 - mae_t1: 0.0362 - val_loss: 26.2200 - val_mae: 0.5462 - val_mean_pred: 0.7755 - val_mae_t1: 0.0364\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 25.1724 - mae: 0.5244 - mean_pred: 0.6629 - mae_t1: 0.0350 - val_loss: 26.2142 - val_mae: 0.5461 - val_mean_pred: 0.7305 - val_mae_t1: 0.0364\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 24.8170 - mae: 0.5170 - mean_pred: 0.6760 - mae_t1: 0.0345 - val_loss: 27.6649 - val_mae: 0.5764 - val_mean_pred: 0.7881 - val_mae_t1: 0.0384\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 23.4606 - mae: 0.4888 - mean_pred: 0.6447 - mae_t1: 0.0326 - val_loss: 28.5655 - val_mae: 0.5951 - val_mean_pred: 0.7164 - val_mae_t1: 0.0397\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 23.7768 - mae: 0.4953 - mean_pred: 0.6648 - mae_t1: 0.0330 - val_loss: 30.0836 - val_mae: 0.6267 - val_mean_pred: 0.8979 - val_mae_t1: 0.0418\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 25.3598 - mae: 0.5283 - mean_pred: 0.8185 - mae_t1: 0.0352 - val_loss: 26.9311 - val_mae: 0.5611 - val_mean_pred: 0.9904 - val_mae_t1: 0.0374\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 24.5247 - mae: 0.5109 - mean_pred: 0.9269 - mae_t1: 0.0341 - val_loss: 27.6951 - val_mae: 0.5770 - val_mean_pred: 0.9275 - val_mae_t1: 0.0385\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 27.3733 - mae: 0.5703 - mean_pred: 0.8141 - mae_t1: 0.0380 - val_loss: 31.8223 - val_mae: 0.6630 - val_mean_pred: 0.5733 - val_mae_t1: 0.0442\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 30.3742 - mae: 0.6328 - mean_pred: 0.5433 - mae_t1: 0.0422 - val_loss: 36.4490 - val_mae: 0.7594 - val_mean_pred: 0.5161 - val_mae_t1: 0.0506\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 37.2800 - mae: 0.7767 - mean_pred: 0.6596 - mae_t1: 0.0518 - val_loss: 31.0107 - val_mae: 0.6461 - val_mean_pred: 0.5658 - val_mae_t1: 0.0431\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 29.5070 - mae: 0.6147 - mean_pred: 0.6271 - mae_t1: 0.0410 - val_loss: 31.7948 - val_mae: 0.6624 - val_mean_pred: 1.0277 - val_mae_t1: 0.0442\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 29.2460 - mae: 0.6093 - mean_pred: 0.9958 - mae_t1: 0.0406 - val_loss: 52.0165 - val_mae: 1.0837 - val_mean_pred: 1.3450 - val_mae_t1: 0.0722\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 58.2670 - mae: 1.2139 - mean_pred: 1.3774 - mae_t1: 0.0809 - val_loss: 32.0593 - val_mae: 0.6679 - val_mean_pred: 0.8971 - val_mae_t1: 0.0445\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 36.2111 - mae: 0.7544 - mean_pred: 0.8657 - mae_t1: 0.0503 - val_loss: 45.1912 - val_mae: 0.9415 - val_mean_pred: 0.8170 - val_mae_t1: 0.0628\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 36.8326 - mae: 0.7673 - mean_pred: 0.7540 - mae_t1: 0.0512 - val_loss: 45.1186 - val_mae: 0.9400 - val_mean_pred: 0.8543 - val_mae_t1: 0.0627\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 37.5729 - mae: 0.7828 - mean_pred: 0.7722 - mae_t1: 0.0522 - val_loss: 37.6990 - val_mae: 0.7854 - val_mean_pred: 0.5583 - val_mae_t1: 0.0524\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 32.9028 - mae: 0.6855 - mean_pred: 0.5318 - mae_t1: 0.0457 - val_loss: 29.3949 - val_mae: 0.6124 - val_mean_pred: 0.6449 - val_mae_t1: 0.0408\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 26.9672 - mae: 0.5618 - mean_pred: 0.6418 - mae_t1: 0.0375 - val_loss: 30.5353 - val_mae: 0.6362 - val_mean_pred: 0.8751 - val_mae_t1: 0.0424\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 31.1404 - mae: 0.6488 - mean_pred: 0.8350 - mae_t1: 0.0433 - val_loss: 33.5555 - val_mae: 0.6991 - val_mean_pred: 1.1385 - val_mae_t1: 0.0466\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 41.5061 - mae: 0.8647 - mean_pred: 1.0862 - mae_t1: 0.0576 - val_loss: 32.9463 - val_mae: 0.6864 - val_mean_pred: 1.1746 - val_mae_t1: 0.0458\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 35.3152 - mae: 0.7357 - mean_pred: 1.0755 - mae_t1: 0.0490 - val_loss: 32.9574 - val_mae: 0.6866 - val_mean_pred: 1.0450 - val_mae_t1: 0.0458\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 30.5618 - mae: 0.6367 - mean_pred: 0.8669 - mae_t1: 0.0424 - val_loss: 36.9462 - val_mae: 0.7697 - val_mean_pred: 0.8744 - val_mae_t1: 0.0513\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 35.1193 - mae: 0.7317 - mean_pred: 0.7534 - mae_t1: 0.0488 - val_loss: 32.3746 - val_mae: 0.6745 - val_mean_pred: 0.6600 - val_mae_t1: 0.0450\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 31.5496 - mae: 0.6573 - mean_pred: 0.6256 - mae_t1: 0.0438 - val_loss: 25.0664 - val_mae: 0.5222 - val_mean_pred: 0.6638 - val_mae_t1: 0.0348\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 26.4228 - mae: 0.5505 - mean_pred: 0.6330 - mae_t1: 0.0367 - val_loss: 25.4648 - val_mae: 0.5305 - val_mean_pred: 0.8492 - val_mae_t1: 0.0354\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 27.9011 - mae: 0.5813 - mean_pred: 0.7922 - mae_t1: 0.0388 - val_loss: 39.8302 - val_mae: 0.8298 - val_mean_pred: 0.9815 - val_mae_t1: 0.0553\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 48.4539 - mae: 1.0095 - mean_pred: 1.0524 - mae_t1: 0.0673 - val_loss: 27.7926 - val_mae: 0.5790 - val_mean_pred: 0.7574 - val_mae_t1: 0.0386\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 30.5325 - mae: 0.6361 - mean_pred: 0.7828 - mae_t1: 0.0424 - val_loss: 45.6154 - val_mae: 0.9503 - val_mean_pred: 1.0388 - val_mae_t1: 0.0634\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 37.5973 - mae: 0.7833 - mean_pred: 0.8593 - mae_t1: 0.0522 - val_loss: 34.0049 - val_mae: 0.7084 - val_mean_pred: 0.8007 - val_mae_t1: 0.0472\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 32.8068 - mae: 0.6835 - mean_pred: 0.7876 - mae_t1: 0.0456 - val_loss: 32.7830 - val_mae: 0.6830 - val_mean_pred: 0.8983 - val_mae_t1: 0.0455\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 30.2504 - mae: 0.6302 - mean_pred: 0.8152 - mae_t1: 0.0420 - val_loss: 28.3621 - val_mae: 0.5909 - val_mean_pred: 0.7741 - val_mae_t1: 0.0394\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 34.3577 - mae: 0.7158 - mean_pred: 0.7813 - mae_t1: 0.0477 - val_loss: 33.5915 - val_mae: 0.6998 - val_mean_pred: 0.4732 - val_mae_t1: 0.0467\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 35.1431 - mae: 0.7321 - mean_pred: 0.5706 - mae_t1: 0.0488 - val_loss: 45.7897 - val_mae: 0.9540 - val_mean_pred: 0.8587 - val_mae_t1: 0.0636\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 36.4814 - mae: 0.7600 - mean_pred: 0.6004 - mae_t1: 0.0507 - val_loss: 30.7895 - val_mae: 0.6414 - val_mean_pred: 0.6145 - val_mae_t1: 0.0428\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 32.0565 - mae: 0.6678 - mean_pred: 0.6579 - mae_t1: 0.0445 - val_loss: 27.6208 - val_mae: 0.5754 - val_mean_pred: 0.9122 - val_mae_t1: 0.0384\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 27.2106 - mae: 0.5669 - mean_pred: 0.8954 - mae_t1: 0.0378 - val_loss: 30.2114 - val_mae: 0.6294 - val_mean_pred: 1.0005 - val_mae_t1: 0.0420\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 31.6687 - mae: 0.6598 - mean_pred: 0.9174 - mae_t1: 0.0440 - val_loss: 28.1014 - val_mae: 0.5854 - val_mean_pred: 0.8202 - val_mae_t1: 0.0390\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 24.5527 - mae: 0.5115 - mean_pred: 0.7375 - mae_t1: 0.0341 - val_loss: 28.2169 - val_mae: 0.5879 - val_mean_pred: 0.7689 - val_mae_t1: 0.0392\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 23.6704 - mae: 0.4931 - mean_pred: 0.7254 - mae_t1: 0.0329 - val_loss: 30.6509 - val_mae: 0.6386 - val_mean_pred: 0.7689 - val_mae_t1: 0.0426\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 25.1220 - mae: 0.5234 - mean_pred: 0.7212 - mae_t1: 0.0349 - val_loss: 32.7371 - val_mae: 0.6820 - val_mean_pred: 0.7983 - val_mae_t1: 0.0455\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 29.8959 - mae: 0.6228 - mean_pred: 0.7846 - mae_t1: 0.0415 - val_loss: 35.3640 - val_mae: 0.7368 - val_mean_pred: 0.8383 - val_mae_t1: 0.0491\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 37.8071 - mae: 0.7876 - mean_pred: 0.8558 - mae_t1: 0.0525 - val_loss: 32.0455 - val_mae: 0.6676 - val_mean_pred: 0.6376 - val_mae_t1: 0.0445\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 31.6345 - mae: 0.6591 - mean_pred: 0.6898 - mae_t1: 0.0439 - val_loss: 46.2900 - val_mae: 0.9644 - val_mean_pred: 0.9234 - val_mae_t1: 0.0643\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 36.9215 - mae: 0.7692 - mean_pred: 0.7405 - mae_t1: 0.0513 - val_loss: 32.8499 - val_mae: 0.6844 - val_mean_pred: 0.5641 - val_mae_t1: 0.0456\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 28.5270 - mae: 0.5943 - mean_pred: 0.5607 - mae_t1: 0.0396 - val_loss: 32.8999 - val_mae: 0.6854 - val_mean_pred: 0.8217 - val_mae_t1: 0.0457\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 27.6913 - mae: 0.5769 - mean_pred: 0.8462 - mae_t1: 0.0385 - val_loss: 40.4776 - val_mae: 0.8433 - val_mean_pred: 1.2014 - val_mae_t1: 0.0562\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 33.2213 - mae: 0.6921 - mean_pred: 1.1267 - mae_t1: 0.0461 - val_loss: 38.2628 - val_mae: 0.7971 - val_mean_pred: 1.1579 - val_mae_t1: 0.0531\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 28.7436 - mae: 0.5988 - mean_pred: 1.0058 - mae_t1: 0.0399 - val_loss: 38.8238 - val_mae: 0.8088 - val_mean_pred: 1.0261 - val_mae_t1: 0.0539\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 32.8698 - mae: 0.6848 - mean_pred: 0.9075 - mae_t1: 0.0457 - val_loss: 45.5633 - val_mae: 0.9492 - val_mean_pred: 0.9713 - val_mae_t1: 0.0633\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 51.5017 - mae: 1.0730 - mean_pred: 1.1156 - mae_t1: 0.0715 - val_loss: 46.5410 - val_mae: 0.9696 - val_mean_pred: 0.9216 - val_mae_t1: 0.0646\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 36.6729 - mae: 0.7640 - mean_pred: 0.7418 - mae_t1: 0.0509 - val_loss: 44.3440 - val_mae: 0.9238 - val_mean_pred: 0.7523 - val_mae_t1: 0.0616\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 40.0649 - mae: 0.8347 - mean_pred: 0.7010 - mae_t1: 0.0556 - val_loss: 29.7856 - val_mae: 0.6205 - val_mean_pred: 0.6354 - val_mae_t1: 0.0414\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 25.6705 - mae: 0.5348 - mean_pred: 0.6051 - mae_t1: 0.0357 - val_loss: 32.2737 - val_mae: 0.6724 - val_mean_pred: 0.8802 - val_mae_t1: 0.0448\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 32.1295 - mae: 0.6694 - mean_pred: 0.8704 - mae_t1: 0.0446 - val_loss: 34.5829 - val_mae: 0.7205 - val_mean_pred: 1.1095 - val_mae_t1: 0.0480\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 41.8373 - mae: 0.8716 - mean_pred: 1.0960 - mae_t1: 0.0581 - val_loss: 32.3150 - val_mae: 0.6732 - val_mean_pred: 1.0079 - val_mae_t1: 0.0449\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 32.0961 - mae: 0.6687 - mean_pred: 0.8968 - mae_t1: 0.0446 - val_loss: 39.7713 - val_mae: 0.8286 - val_mean_pred: 0.8622 - val_mae_t1: 0.0552\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 33.5494 - mae: 0.6989 - mean_pred: 0.7942 - mae_t1: 0.0466 - val_loss: 46.2153 - val_mae: 0.9628 - val_mean_pred: 1.0467 - val_mae_t1: 0.0642\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 50.0555 - mae: 1.0428 - mean_pred: 1.0788 - mae_t1: 0.0695 - val_loss: 28.9547 - val_mae: 0.6032 - val_mean_pred: 0.7846 - val_mae_t1: 0.0402\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 36.2132 - mae: 0.7544 - mean_pred: 0.8776 - mae_t1: 0.0503 - val_loss: 44.7925 - val_mae: 0.9332 - val_mean_pred: 1.1144 - val_mae_t1: 0.0622\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 37.4407 - mae: 0.7800 - mean_pred: 0.9995 - mae_t1: 0.0520 - val_loss: 52.3732 - val_mae: 1.0911 - val_mean_pred: 1.2919 - val_mae_t1: 0.0727\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 47.5203 - mae: 0.9900 - mean_pred: 1.1362 - mae_t1: 0.0660 - val_loss: 30.7288 - val_mae: 0.6402 - val_mean_pred: 1.0930 - val_mae_t1: 0.0427\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 38.0166 - mae: 0.7920 - mean_pred: 1.0479 - mae_t1: 0.0528 - val_loss: 30.7841 - val_mae: 0.6413 - val_mean_pred: 0.9732 - val_mae_t1: 0.0428\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 31.0136 - mae: 0.6461 - mean_pred: 0.8246 - mae_t1: 0.0431 - val_loss: 34.7153 - val_mae: 0.7232 - val_mean_pred: 0.9373 - val_mae_t1: 0.0482\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 31.0937 - mae: 0.6478 - mean_pred: 0.7445 - mae_t1: 0.0432 - val_loss: 38.0975 - val_mae: 0.7937 - val_mean_pred: 0.8666 - val_mae_t1: 0.0529\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 37.8900 - mae: 0.7894 - mean_pred: 0.7724 - mae_t1: 0.0526 - val_loss: 25.9445 - val_mae: 0.5405 - val_mean_pred: 0.7031 - val_mae_t1: 0.0360\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 25.9776 - mae: 0.5412 - mean_pred: 0.6039 - mae_t1: 0.0361 - val_loss: 25.7142 - val_mae: 0.5357 - val_mean_pred: 0.8572 - val_mae_t1: 0.0357\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 28.0510 - mae: 0.5844 - mean_pred: 0.7521 - mae_t1: 0.0390 - val_loss: 25.1913 - val_mae: 0.5248 - val_mean_pred: 0.8918 - val_mae_t1: 0.0350\n",
      "Earliness...\n",
      "0.002001047134399414\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3227129617583219  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▂▁▅▂█▃▃▃▂▃▃▃▂▁▁▁▁▂▄▂▁▂▂▁▂▂▂▁▁▂▂▁▂▂▂▂▂▃▂▁</td></tr><tr><td>mae</td><td>▂▁▅▂█▃▃▃▂▃▃▃▂▁▁▁▁▂▄▂▁▂▂▁▂▂▂▁▁▂▂▁▂▂▂▂▂▃▂▁</td></tr><tr><td>mae_t1</td><td>▂▁▅▂█▃▃▃▂▃▃▃▂▁▁▁▁▂▄▂▁▂▂▁▂▂▂▁▁▂▂▁▂▂▂▂▂▃▂▁</td></tr><tr><td>mean_pred</td><td>▁▂▆▄█▂▃▁▂▃▄▂▄▂▂▂▂▂▅▂▂▃▂▂▂▂▂▃▂▂▂▃▃▂▃▃▃▄▂▂</td></tr><tr><td>val_loss</td><td>▂▆█▄▃▃▃▃▃▃▄▅▂▁▁▁▂▂▂▃▂▂▂▃▄▁▂▂▂▂▂▃▄▄▂▃▄▂▃▁</td></tr><tr><td>val_mae</td><td>▂▆█▄▃▃▃▃▃▃▄▅▂▁▁▁▂▂▂▃▂▂▂▃▄▁▂▂▂▂▂▃▄▄▂▃▄▂▃▁</td></tr><tr><td>val_mae_t1</td><td>▂▆█▄▃▃▃▃▃▃▄▅▂▁▁▁▂▂▂▃▂▂▂▃▄▁▂▂▂▂▂▃▄▄▂▃▄▂▃▁</td></tr><tr><td>val_mean_pred</td><td>▃▂▅▁▃▇▃▂▂▂█▆▆▄▄▆▃▆▅▂▅▇▃▆▇▄▃▆▄▅▂█▆▄▇▅▇▇▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.31668</td></tr><tr><td>AE_2</td><td>0.28198</td></tr><tr><td>AE_3</td><td>0.30926</td></tr><tr><td>MAE</td><td>0.32271</td></tr><tr><td>best_epoch</td><td>56</td></tr><tr><td>best_val_loss</td><td>25.06641</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>28.05101</td></tr><tr><td>mae</td><td>0.5844</td></tr><tr><td>mae_t1</td><td>0.03896</td></tr><tr><td>mean_pred</td><td>0.75215</td></tr><tr><td>val_loss</td><td>25.19126</td></tr><tr><td>val_mae</td><td>0.52482</td></tr><tr><td>val_mae_t1</td><td>0.03499</td></tr><tr><td>val_mean_pred</td><td>0.8918</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">crisp-oath-60</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xga97en\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xga97en</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154415-1xga97en\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154437-1ajvhuk0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ajvhuk0\" target=\"_blank\">genial-puddle-61</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 870us/sample - loss: 2.6704 - mae: 0.7511 - mean_pred: 0.2330 - mae_t1: 0.0501 - val_loss: 1.8123 - val_mae: 0.5097 - val_mean_pred: 0.7223 - val_mae_t1: 0.0340\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 222us/sample - loss: 2.1066 - mae: 0.5925 - mean_pred: 0.7713 - mae_t1: 0.0395 - val_loss: 1.7956 - val_mae: 0.5050 - val_mean_pred: 0.7276 - val_mae_t1: 0.0337\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 2.1028 - mae: 0.5914 - mean_pred: 0.5196 - mae_t1: 0.0394 - val_loss: 2.0974 - val_mae: 0.5899 - val_mean_pred: 0.4617 - val_mae_t1: 0.0393\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 215us/sample - loss: 1.9966 - mae: 0.5616 - mean_pred: 0.4717 - mae_t1: 0.0374 - val_loss: 1.7842 - val_mae: 0.5018 - val_mean_pred: 0.7160 - val_mae_t1: 0.0335\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 1.7904 - mae: 0.5035 - mean_pred: 0.7323 - mae_t1: 0.0336 - val_loss: 1.5909 - val_mae: 0.4474 - val_mean_pred: 0.8629 - val_mae_t1: 0.0298\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.8131 - mae: 0.5099 - mean_pred: 0.8326 - mae_t1: 0.0340 - val_loss: 1.6177 - val_mae: 0.4550 - val_mean_pred: 0.8559 - val_mae_t1: 0.0303\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.7336 - mae: 0.4876 - mean_pred: 0.8140 - mae_t1: 0.0325 - val_loss: 1.6905 - val_mae: 0.4754 - val_mean_pred: 0.7845 - val_mae_t1: 0.0317\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.6746 - mae: 0.4710 - mean_pred: 0.6851 - mae_t1: 0.0314 - val_loss: 1.8549 - val_mae: 0.5217 - val_mean_pred: 0.6513 - val_mae_t1: 0.0348\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.6519 - mae: 0.4646 - mean_pred: 0.6298 - mae_t1: 0.0310 - val_loss: 1.7949 - val_mae: 0.5048 - val_mean_pred: 0.7743 - val_mae_t1: 0.0337\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.5993 - mae: 0.4498 - mean_pred: 0.7362 - mae_t1: 0.0300 - val_loss: 1.7858 - val_mae: 0.5023 - val_mean_pred: 0.9495 - val_mae_t1: 0.0335\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.8204 - mae: 0.5120 - mean_pred: 0.9538 - mae_t1: 0.0341 - val_loss: 2.0568 - val_mae: 0.5785 - val_mean_pred: 1.0231 - val_mae_t1: 0.0386\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.6183 - mae: 0.4552 - mean_pred: 0.9030 - mae_t1: 0.0303 - val_loss: 1.8439 - val_mae: 0.5186 - val_mean_pred: 0.8795 - val_mae_t1: 0.0346\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 1.5260 - mae: 0.4292 - mean_pred: 0.8219 - mae_t1: 0.0286 - val_loss: 1.7819 - val_mae: 0.5012 - val_mean_pred: 0.8899 - val_mae_t1: 0.0334\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.4586 - mae: 0.4102 - mean_pred: 0.8123 - mae_t1: 0.0273 - val_loss: 1.7694 - val_mae: 0.4977 - val_mean_pred: 0.7863 - val_mae_t1: 0.0332\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.4763 - mae: 0.4152 - mean_pred: 0.7093 - mae_t1: 0.0277 - val_loss: 1.5979 - val_mae: 0.4494 - val_mean_pred: 0.8536 - val_mae_t1: 0.0300\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 232us/sample - loss: 1.5401 - mae: 0.4331 - mean_pred: 0.8705 - mae_t1: 0.0289 - val_loss: 1.5110 - val_mae: 0.4250 - val_mean_pred: 0.9218 - val_mae_t1: 0.0283\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 1.4427 - mae: 0.4058 - mean_pred: 0.7970 - mae_t1: 0.0271 - val_loss: 1.7459 - val_mae: 0.4910 - val_mean_pred: 0.7194 - val_mae_t1: 0.0327\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.4911 - mae: 0.4194 - mean_pred: 0.6778 - mae_t1: 0.0280 - val_loss: 1.6436 - val_mae: 0.4623 - val_mean_pred: 0.7933 - val_mae_t1: 0.0308\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.4137 - mae: 0.3976 - mean_pred: 0.7959 - mae_t1: 0.0265 - val_loss: 1.6425 - val_mae: 0.4619 - val_mean_pred: 0.9400 - val_mae_t1: 0.0308\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.3857 - mae: 0.3897 - mean_pred: 0.8570 - mae_t1: 0.0260 - val_loss: 1.6029 - val_mae: 0.4508 - val_mean_pred: 0.8870 - val_mae_t1: 0.0301\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.3975 - mae: 0.3930 - mean_pred: 0.7845 - mae_t1: 0.0262 - val_loss: 1.6032 - val_mae: 0.4509 - val_mean_pred: 0.7929 - val_mae_t1: 0.0301\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.3905 - mae: 0.3911 - mean_pred: 0.7211 - mae_t1: 0.0261 - val_loss: 1.5506 - val_mae: 0.4361 - val_mean_pred: 0.8585 - val_mae_t1: 0.0291\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.3233 - mae: 0.3722 - mean_pred: 0.7973 - mae_t1: 0.0248 - val_loss: 1.5869 - val_mae: 0.4463 - val_mean_pred: 0.8214 - val_mae_t1: 0.0298\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.3595 - mae: 0.3824 - mean_pred: 0.7232 - mae_t1: 0.0255 - val_loss: 1.6404 - val_mae: 0.4614 - val_mean_pred: 0.7856 - val_mae_t1: 0.0308\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.3694 - mae: 0.3851 - mean_pred: 0.8282 - mae_t1: 0.0257 - val_loss: 1.5438 - val_mae: 0.4342 - val_mean_pred: 0.9071 - val_mae_t1: 0.0289\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.3772 - mae: 0.3873 - mean_pred: 0.7696 - mae_t1: 0.0258 - val_loss: 1.7898 - val_mae: 0.5034 - val_mean_pred: 0.6813 - val_mae_t1: 0.0336\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.4525 - mae: 0.4085 - mean_pred: 0.6408 - mae_t1: 0.0272 - val_loss: 1.5511 - val_mae: 0.4363 - val_mean_pred: 0.8306 - val_mae_t1: 0.0291\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 1.2845 - mae: 0.3613 - mean_pred: 0.7964 - mae_t1: 0.0241 - val_loss: 1.4561 - val_mae: 0.4095 - val_mean_pred: 0.9162 - val_mae_t1: 0.0273\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 1.2844 - mae: 0.3612 - mean_pred: 0.8219 - mae_t1: 0.0241 - val_loss: 1.4801 - val_mae: 0.4163 - val_mean_pred: 0.8289 - val_mae_t1: 0.0278\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.2383 - mae: 0.3483 - mean_pred: 0.7489 - mae_t1: 0.0232 - val_loss: 1.4896 - val_mae: 0.4190 - val_mean_pred: 0.9219 - val_mae_t1: 0.0279\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2183 - mae: 0.3427 - mean_pred: 0.8651 - mae_t1: 0.0228 - val_loss: 1.5780 - val_mae: 0.4438 - val_mean_pred: 0.9402 - val_mae_t1: 0.0296\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2086 - mae: 0.3399 - mean_pred: 0.7982 - mae_t1: 0.0227 - val_loss: 1.6334 - val_mae: 0.4594 - val_mean_pred: 0.8911 - val_mae_t1: 0.0306\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.2233 - mae: 0.3440 - mean_pred: 0.8293 - mae_t1: 0.0229 - val_loss: 1.6376 - val_mae: 0.4606 - val_mean_pred: 0.8939 - val_mae_t1: 0.0307\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.2291 - mae: 0.3457 - mean_pred: 0.7447 - mae_t1: 0.0230 - val_loss: 1.7139 - val_mae: 0.4820 - val_mean_pred: 0.7705 - val_mae_t1: 0.0321\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.3251 - mae: 0.3727 - mean_pred: 0.7106 - mae_t1: 0.0248 - val_loss: 1.6913 - val_mae: 0.4757 - val_mean_pred: 0.7765 - val_mae_t1: 0.0317\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.2749 - mae: 0.3586 - mean_pred: 0.7183 - mae_t1: 0.0239 - val_loss: 1.5795 - val_mae: 0.4442 - val_mean_pred: 0.8477 - val_mae_t1: 0.0296\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.1839 - mae: 0.3330 - mean_pred: 0.8254 - mae_t1: 0.0222 - val_loss: 1.6886 - val_mae: 0.4749 - val_mean_pred: 0.9858 - val_mae_t1: 0.0317\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1840 - mae: 0.3330 - mean_pred: 0.8687 - mae_t1: 0.0222 - val_loss: 1.5941 - val_mae: 0.4483 - val_mean_pred: 0.8843 - val_mae_t1: 0.0299\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.1397 - mae: 0.3205 - mean_pred: 0.7837 - mae_t1: 0.0214 - val_loss: 1.5467 - val_mae: 0.4350 - val_mean_pred: 0.8524 - val_mae_t1: 0.0290\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1711 - mae: 0.3294 - mean_pred: 0.7444 - mae_t1: 0.0220 - val_loss: 1.4955 - val_mae: 0.4206 - val_mean_pred: 0.9221 - val_mae_t1: 0.0280\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 1.2128 - mae: 0.3411 - mean_pred: 0.8661 - mae_t1: 0.0227 - val_loss: 1.5515 - val_mae: 0.4364 - val_mean_pred: 0.8284 - val_mae_t1: 0.0291\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.2887 - mae: 0.3624 - mean_pred: 0.6590 - mae_t1: 0.0242 - val_loss: 1.5504 - val_mae: 0.4361 - val_mean_pred: 0.7922 - val_mae_t1: 0.0291\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.2426 - mae: 0.3495 - mean_pred: 0.8311 - mae_t1: 0.0233 - val_loss: 1.7790 - val_mae: 0.5003 - val_mean_pred: 1.0684 - val_mae_t1: 0.0334\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 1.2334 - mae: 0.3469 - mean_pred: 0.8953 - mae_t1: 0.0231 - val_loss: 1.5175 - val_mae: 0.4268 - val_mean_pred: 0.7720 - val_mae_t1: 0.0285\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.3480 - mae: 0.3791 - mean_pred: 0.6507 - mae_t1: 0.0253 - val_loss: 1.5711 - val_mae: 0.4419 - val_mean_pred: 0.7264 - val_mae_t1: 0.0295\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.1815 - mae: 0.3323 - mean_pred: 0.7588 - mae_t1: 0.0222 - val_loss: 1.5404 - val_mae: 0.4332 - val_mean_pred: 0.9567 - val_mae_t1: 0.0289\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.1647 - mae: 0.3276 - mean_pred: 0.8071 - mae_t1: 0.0218 - val_loss: 1.6347 - val_mae: 0.4597 - val_mean_pred: 0.8054 - val_mae_t1: 0.0306\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.2470 - mae: 0.3507 - mean_pred: 0.7034 - mae_t1: 0.0234 - val_loss: 1.6744 - val_mae: 0.4709 - val_mean_pred: 0.7762 - val_mae_t1: 0.0314\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1730 - mae: 0.3299 - mean_pred: 0.7169 - mae_t1: 0.0220 - val_loss: 1.5279 - val_mae: 0.4297 - val_mean_pred: 0.9021 - val_mae_t1: 0.0286\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1175 - mae: 0.3143 - mean_pred: 0.8463 - mae_t1: 0.0210 - val_loss: 1.5137 - val_mae: 0.4257 - val_mean_pred: 0.8996 - val_mae_t1: 0.0284\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.1483 - mae: 0.3230 - mean_pred: 0.7702 - mae_t1: 0.0215 - val_loss: 1.5306 - val_mae: 0.4305 - val_mean_pred: 0.8062 - val_mae_t1: 0.0287\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.1412 - mae: 0.3210 - mean_pred: 0.7686 - mae_t1: 0.0214 - val_loss: 1.5942 - val_mae: 0.4484 - val_mean_pred: 0.9507 - val_mae_t1: 0.0299\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.0663 - mae: 0.2999 - mean_pred: 0.8186 - mae_t1: 0.0200 - val_loss: 1.6278 - val_mae: 0.4578 - val_mean_pred: 0.9780 - val_mae_t1: 0.0305\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.1564 - mae: 0.3252 - mean_pred: 0.8819 - mae_t1: 0.0217 - val_loss: 1.5806 - val_mae: 0.4445 - val_mean_pred: 0.9294 - val_mae_t1: 0.0296\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.1644 - mae: 0.3275 - mean_pred: 0.7692 - mae_t1: 0.0218 - val_loss: 1.6483 - val_mae: 0.4636 - val_mean_pred: 0.8017 - val_mae_t1: 0.0309\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1487 - mae: 0.3231 - mean_pred: 0.7581 - mae_t1: 0.0215 - val_loss: 1.5333 - val_mae: 0.4312 - val_mean_pred: 0.9168 - val_mae_t1: 0.0287\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 1.1584 - mae: 0.3258 - mean_pred: 0.8728 - mae_t1: 0.0217 - val_loss: 1.7301 - val_mae: 0.4866 - val_mean_pred: 0.9683 - val_mae_t1: 0.0324\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.0779 - mae: 0.3032 - mean_pred: 0.8484 - mae_t1: 0.0202 - val_loss: 1.5681 - val_mae: 0.4410 - val_mean_pred: 0.8543 - val_mae_t1: 0.0294\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.1019 - mae: 0.3099 - mean_pred: 0.7454 - mae_t1: 0.0207 - val_loss: 1.6488 - val_mae: 0.4637 - val_mean_pred: 0.7967 - val_mae_t1: 0.0309\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.0671 - mae: 0.3001 - mean_pred: 0.7602 - mae_t1: 0.0200 - val_loss: 1.5391 - val_mae: 0.4329 - val_mean_pred: 0.9784 - val_mae_t1: 0.0289\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.0138 - mae: 0.2851 - mean_pred: 0.8613 - mae_t1: 0.0190 - val_loss: 1.4633 - val_mae: 0.4116 - val_mean_pred: 0.8840 - val_mae_t1: 0.0274\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.0079 - mae: 0.2835 - mean_pred: 0.8053 - mae_t1: 0.0189 - val_loss: 1.5673 - val_mae: 0.4408 - val_mean_pred: 0.8561 - val_mae_t1: 0.0294\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.0254 - mae: 0.2884 - mean_pred: 0.7792 - mae_t1: 0.0192 - val_loss: 1.4985 - val_mae: 0.4215 - val_mean_pred: 0.8484 - val_mae_t1: 0.0281\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.0493 - mae: 0.2951 - mean_pred: 0.7615 - mae_t1: 0.0197 - val_loss: 1.5552 - val_mae: 0.4374 - val_mean_pred: 0.8223 - val_mae_t1: 0.0292\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9841 - mae: 0.2768 - mean_pred: 0.7661 - mae_t1: 0.0185 - val_loss: 1.5556 - val_mae: 0.4375 - val_mean_pred: 0.9633 - val_mae_t1: 0.0292\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.0050 - mae: 0.2827 - mean_pred: 0.8519 - mae_t1: 0.0188 - val_loss: 1.5264 - val_mae: 0.4293 - val_mean_pred: 0.9061 - val_mae_t1: 0.0286\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.0150 - mae: 0.2855 - mean_pred: 0.7916 - mae_t1: 0.0190 - val_loss: 1.5056 - val_mae: 0.4235 - val_mean_pred: 0.8862 - val_mae_t1: 0.0282\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.9564 - mae: 0.2690 - mean_pred: 0.8246 - mae_t1: 0.0179 - val_loss: 1.5798 - val_mae: 0.4443 - val_mean_pred: 0.8967 - val_mae_t1: 0.0296\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 0.9971 - mae: 0.2804 - mean_pred: 0.7657 - mae_t1: 0.0187 - val_loss: 1.6103 - val_mae: 0.4529 - val_mean_pred: 0.8912 - val_mae_t1: 0.0302\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.9711 - mae: 0.2731 - mean_pred: 0.8329 - mae_t1: 0.0182 - val_loss: 1.5499 - val_mae: 0.4359 - val_mean_pred: 0.9337 - val_mae_t1: 0.0291\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 231us/sample - loss: 0.9984 - mae: 0.2808 - mean_pred: 0.7744 - mae_t1: 0.0187 - val_loss: 1.4277 - val_mae: 0.4015 - val_mean_pred: 0.8288 - val_mae_t1: 0.0268\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.0488 - mae: 0.2950 - mean_pred: 0.7868 - mae_t1: 0.0197 - val_loss: 1.5090 - val_mae: 0.4244 - val_mean_pred: 0.9592 - val_mae_t1: 0.0283\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.0605 - mae: 0.2983 - mean_pred: 0.8658 - mae_t1: 0.0199 - val_loss: 1.5685 - val_mae: 0.4412 - val_mean_pred: 0.9500 - val_mae_t1: 0.0294\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 0.9859 - mae: 0.2773 - mean_pred: 0.8300 - mae_t1: 0.0185 - val_loss: 1.6398 - val_mae: 0.4612 - val_mean_pred: 0.8607 - val_mae_t1: 0.0307\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.0079 - mae: 0.2835 - mean_pred: 0.7631 - mae_t1: 0.0189 - val_loss: 1.6347 - val_mae: 0.4598 - val_mean_pred: 0.8886 - val_mae_t1: 0.0307\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 0.9816 - mae: 0.2761 - mean_pred: 0.8304 - mae_t1: 0.0184 - val_loss: 1.5203 - val_mae: 0.4276 - val_mean_pred: 0.9357 - val_mae_t1: 0.0285\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 0.9174 - mae: 0.2580 - mean_pred: 0.8341 - mae_t1: 0.0172 - val_loss: 1.5070 - val_mae: 0.4239 - val_mean_pred: 0.8701 - val_mae_t1: 0.0283\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9207 - mae: 0.2590 - mean_pred: 0.7996 - mae_t1: 0.0173 - val_loss: 1.4926 - val_mae: 0.4198 - val_mean_pred: 0.9182 - val_mae_t1: 0.0280\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.8947 - mae: 0.2516 - mean_pred: 0.8389 - mae_t1: 0.0168 - val_loss: 1.4982 - val_mae: 0.4214 - val_mean_pred: 0.8971 - val_mae_t1: 0.0281\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 0.9173 - mae: 0.2580 - mean_pred: 0.8007 - mae_t1: 0.0172 - val_loss: 1.5222 - val_mae: 0.4281 - val_mean_pred: 0.8456 - val_mae_t1: 0.0285\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.8829 - mae: 0.2483 - mean_pred: 0.7615 - mae_t1: 0.0166 - val_loss: 1.4919 - val_mae: 0.4196 - val_mean_pred: 0.9170 - val_mae_t1: 0.0280\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.9588 - mae: 0.2697 - mean_pred: 0.8516 - mae_t1: 0.0180 - val_loss: 1.4946 - val_mae: 0.4204 - val_mean_pred: 0.9388 - val_mae_t1: 0.0280\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 0.8809 - mae: 0.2478 - mean_pred: 0.8034 - mae_t1: 0.0165 - val_loss: 1.4933 - val_mae: 0.4200 - val_mean_pred: 0.9062 - val_mae_t1: 0.0280\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.8718 - mae: 0.2452 - mean_pred: 0.8336 - mae_t1: 0.0163 - val_loss: 1.5300 - val_mae: 0.4303 - val_mean_pred: 0.9140 - val_mae_t1: 0.0287\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 0.8879 - mae: 0.2497 - mean_pred: 0.7917 - mae_t1: 0.0166 - val_loss: 1.5334 - val_mae: 0.4313 - val_mean_pred: 0.8473 - val_mae_t1: 0.0288\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.0041 - mae: 0.2824 - mean_pred: 0.7741 - mae_t1: 0.0188 - val_loss: 1.4906 - val_mae: 0.4192 - val_mean_pred: 0.8361 - val_mae_t1: 0.0279\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 0.9327 - mae: 0.2623 - mean_pred: 0.7661 - mae_t1: 0.0175 - val_loss: 1.4958 - val_mae: 0.4207 - val_mean_pred: 0.9753 - val_mae_t1: 0.0280\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.9918 - mae: 0.2790 - mean_pred: 0.8981 - mae_t1: 0.0186 - val_loss: 1.4763 - val_mae: 0.4152 - val_mean_pred: 0.9050 - val_mae_t1: 0.0277\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 0.8927 - mae: 0.2511 - mean_pred: 0.7534 - mae_t1: 0.0167 - val_loss: 1.5186 - val_mae: 0.4271 - val_mean_pred: 0.8485 - val_mae_t1: 0.0285\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.8926 - mae: 0.2510 - mean_pred: 0.8058 - mae_t1: 0.0167 - val_loss: 1.5075 - val_mae: 0.4240 - val_mean_pred: 0.9780 - val_mae_t1: 0.0283\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.8976 - mae: 0.2525 - mean_pred: 0.8602 - mae_t1: 0.0168 - val_loss: 1.4701 - val_mae: 0.4135 - val_mean_pred: 0.8465 - val_mae_t1: 0.0276\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.9953 - mae: 0.2799 - mean_pred: 0.7730 - mae_t1: 0.0187 - val_loss: 1.5343 - val_mae: 0.4315 - val_mean_pred: 0.8554 - val_mae_t1: 0.0288\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 0.9695 - mae: 0.2727 - mean_pred: 0.8132 - mae_t1: 0.0182 - val_loss: 1.5990 - val_mae: 0.4497 - val_mean_pred: 0.9459 - val_mae_t1: 0.0300\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.9478 - mae: 0.2666 - mean_pred: 0.7848 - mae_t1: 0.0178 - val_loss: 1.6169 - val_mae: 0.4548 - val_mean_pred: 0.8406 - val_mae_t1: 0.0303\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.9197 - mae: 0.2587 - mean_pred: 0.7990 - mae_t1: 0.0172 - val_loss: 1.6225 - val_mae: 0.4563 - val_mean_pred: 0.9889 - val_mae_t1: 0.0304\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 0.9491 - mae: 0.2669 - mean_pred: 0.8618 - mae_t1: 0.0178 - val_loss: 1.5182 - val_mae: 0.4270 - val_mean_pred: 0.8801 - val_mae_t1: 0.0285\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 0.9023 - mae: 0.2538 - mean_pred: 0.7616 - mae_t1: 0.0169 - val_loss: 1.6267 - val_mae: 0.4575 - val_mean_pred: 0.9442 - val_mae_t1: 0.0305\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.9285 - mae: 0.2612 - mean_pred: 0.8444 - mae_t1: 0.0174 - val_loss: 1.6405 - val_mae: 0.4614 - val_mean_pred: 0.9242 - val_mae_t1: 0.0308\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9028 - mae: 0.2539 - mean_pred: 0.7416 - mae_t1: 0.0169 - val_loss: 1.5288 - val_mae: 0.4300 - val_mean_pred: 0.7918 - val_mae_t1: 0.0287\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 0.8716 - mae: 0.2451 - mean_pred: 0.7663 - mae_t1: 0.0163 - val_loss: 1.5056 - val_mae: 0.4235 - val_mean_pred: 0.9616 - val_mae_t1: 0.0282\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.37041687465318174  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998326…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▆▅▄▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆▅▄▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▄▇▅█▇▇▅▆▆▆▆▇▆▆▆▇▇▆▆▆▇▆▆▇▆▆▆▆▇▇▇▇▇▆▆▆▆▆▆</td></tr><tr><td>val_loss</td><td>▅█▃▅█▅▂▃▃▂▅▁▂▄▂▂▂▂▂▂▂▂▂▃▁▂▂▃▂▃▂▁▁▂▁▂▂▃▃▂</td></tr><tr><td>val_mae</td><td>▅█▃▅█▅▂▃▃▂▅▁▂▄▂▂▂▂▂▂▂▂▂▃▁▂▂▃▂▃▂▁▁▂▁▂▂▃▃▂</td></tr><tr><td>val_mae_t1</td><td>▅█▃▅█▅▂▃▃▂▅▁▂▄▂▂▂▂▂▂▂▂▂▃▁▂▂▃▂▃▂▁▁▂▁▂▂▃▃▂</td></tr><tr><td>val_mean_pred</td><td>▄▁▆▃█▆▇▅▅▅▄▇▇▅▆▆▆▅▇▆▅▇▇▅▆▅▆▆▇▆▆▆▇▇▇▆▆▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.45396</td></tr><tr><td>AE_2</td><td>0.33928</td></tr><tr><td>AE_3</td><td>0.30879</td></tr><tr><td>MAE</td><td>0.37042</td></tr><tr><td>best_epoch</td><td>70</td></tr><tr><td>best_val_loss</td><td>1.42768</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.8716</td></tr><tr><td>mae</td><td>0.24514</td></tr><tr><td>mae_t1</td><td>0.01634</td></tr><tr><td>mean_pred</td><td>0.76631</td></tr><tr><td>val_loss</td><td>1.50563</td></tr><tr><td>val_mae</td><td>0.42346</td></tr><tr><td>val_mae_t1</td><td>0.02823</td></tr><tr><td>val_mean_pred</td><td>0.9616</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">genial-puddle-61</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ajvhuk0\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1ajvhuk0</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154437-1ajvhuk0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154506-3mzt539m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mzt539m\" target=\"_blank\">electric-sky-62</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 723us/sample - loss: 2.6067 - mae: 0.7331 - mean_pred: 0.2344 - mae_t1: 0.0489 - val_loss: 1.7855 - val_mae: 0.5022 - val_mean_pred: 0.5506 - val_mae_t1: 0.0335\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.1247 - mae: 0.5976 - mean_pred: 0.6072 - mae_t1: 0.0398 - val_loss: 2.0254 - val_mae: 0.5696 - val_mean_pred: 1.0379 - val_mae_t1: 0.0380\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 153us/sample - loss: 2.5062 - mae: 0.7049 - mean_pred: 1.0603 - mae_t1: 0.0470 - val_loss: 2.1015 - val_mae: 0.5911 - val_mean_pred: 1.1154 - val_mae_t1: 0.0394\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 2.3242 - mae: 0.6537 - mean_pred: 1.0416 - mae_t1: 0.0436 - val_loss: 1.7210 - val_mae: 0.4840 - val_mean_pred: 0.6963 - val_mae_t1: 0.0323\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9424 - mae: 0.5463 - mean_pred: 0.6107 - mae_t1: 0.0364 - val_loss: 2.3417 - val_mae: 0.6586 - val_mean_pred: 0.3968 - val_mae_t1: 0.0439\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 2.3134 - mae: 0.6506 - mean_pred: 0.3517 - mae_t1: 0.0434 - val_loss: 2.3954 - val_mae: 0.6737 - val_mean_pred: 0.3703 - val_mae_t1: 0.0449\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.2409 - mae: 0.6303 - mean_pred: 0.3556 - mae_t1: 0.0420 - val_loss: 2.0055 - val_mae: 0.5640 - val_mean_pred: 0.5794 - val_mae_t1: 0.0376\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9053 - mae: 0.5359 - mean_pred: 0.5499 - mae_t1: 0.0357 - val_loss: 1.9317 - val_mae: 0.5433 - val_mean_pred: 0.7997 - val_mae_t1: 0.0362\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7910 - mae: 0.5037 - mean_pred: 0.7337 - mae_t1: 0.0336 - val_loss: 1.7424 - val_mae: 0.4900 - val_mean_pred: 0.8878 - val_mae_t1: 0.0327\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.6909 - mae: 0.4756 - mean_pred: 0.7932 - mae_t1: 0.0317 - val_loss: 1.7285 - val_mae: 0.4861 - val_mean_pred: 0.7899 - val_mae_t1: 0.0324\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.7831 - mae: 0.5015 - mean_pred: 0.6792 - mae_t1: 0.0334 - val_loss: 1.8928 - val_mae: 0.5324 - val_mean_pred: 0.6778 - val_mae_t1: 0.0355\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 1.8550 - mae: 0.5217 - mean_pred: 0.6186 - mae_t1: 0.0348 - val_loss: 1.7175 - val_mae: 0.4830 - val_mean_pred: 0.7777 - val_mae_t1: 0.0322\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 1.7051 - mae: 0.4796 - mean_pred: 0.7401 - mae_t1: 0.0320 - val_loss: 1.6223 - val_mae: 0.4563 - val_mean_pred: 0.9978 - val_mae_t1: 0.0304\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7730 - mae: 0.4987 - mean_pred: 0.9517 - mae_t1: 0.0332 - val_loss: 1.8302 - val_mae: 0.5148 - val_mean_pred: 1.1089 - val_mae_t1: 0.0343\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8791 - mae: 0.5285 - mean_pred: 1.0164 - mae_t1: 0.0352 - val_loss: 1.6256 - val_mae: 0.4572 - val_mean_pred: 0.9678 - val_mae_t1: 0.0305\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.6072 - mae: 0.4520 - mean_pred: 0.8525 - mae_t1: 0.0301 - val_loss: 1.6878 - val_mae: 0.4747 - val_mean_pred: 0.7842 - val_mae_t1: 0.0316\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.5721 - mae: 0.4421 - mean_pred: 0.7062 - mae_t1: 0.0295 - val_loss: 1.7130 - val_mae: 0.4818 - val_mean_pred: 0.7782 - val_mae_t1: 0.0321\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.5757 - mae: 0.4432 - mean_pred: 0.7239 - mae_t1: 0.0295 - val_loss: 1.6602 - val_mae: 0.4669 - val_mean_pred: 0.8879 - val_mae_t1: 0.0311\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.5649 - mae: 0.4401 - mean_pred: 0.8326 - mae_t1: 0.0293 - val_loss: 1.7311 - val_mae: 0.4869 - val_mean_pred: 1.0206 - val_mae_t1: 0.0325\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.6875 - mae: 0.4746 - mean_pred: 0.9547 - mae_t1: 0.0316 - val_loss: 1.7107 - val_mae: 0.4811 - val_mean_pred: 1.0473 - val_mae_t1: 0.0321\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6035 - mae: 0.4510 - mean_pred: 0.9496 - mae_t1: 0.0301 - val_loss: 1.6343 - val_mae: 0.4597 - val_mean_pred: 0.9000 - val_mae_t1: 0.0306\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6146 - mae: 0.4541 - mean_pred: 0.7988 - mae_t1: 0.0303 - val_loss: 1.7267 - val_mae: 0.4856 - val_mean_pred: 0.8054 - val_mae_t1: 0.0324\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 1.6327 - mae: 0.4592 - mean_pred: 0.7579 - mae_t1: 0.0306 - val_loss: 1.5723 - val_mae: 0.4422 - val_mean_pred: 0.8985 - val_mae_t1: 0.0295\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4782 - mae: 0.4158 - mean_pred: 0.8705 - mae_t1: 0.0277 - val_loss: 1.6613 - val_mae: 0.4672 - val_mean_pred: 1.0256 - val_mae_t1: 0.0311\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 1.6443 - mae: 0.4625 - mean_pred: 0.9930 - mae_t1: 0.0308 - val_loss: 1.5425 - val_mae: 0.4338 - val_mean_pred: 0.9227 - val_mae_t1: 0.0289\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.4572 - mae: 0.4098 - mean_pred: 0.8457 - mae_t1: 0.0273 - val_loss: 1.7281 - val_mae: 0.4860 - val_mean_pred: 0.6854 - val_mae_t1: 0.0324\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.5694 - mae: 0.4414 - mean_pred: 0.6664 - mae_t1: 0.0294 - val_loss: 1.7644 - val_mae: 0.4962 - val_mean_pred: 0.6748 - val_mae_t1: 0.0331\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.5362 - mae: 0.4321 - mean_pred: 0.6769 - mae_t1: 0.0288 - val_loss: 1.5976 - val_mae: 0.4493 - val_mean_pred: 0.8200 - val_mae_t1: 0.0300\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4664 - mae: 0.4124 - mean_pred: 0.8444 - mae_t1: 0.0275 - val_loss: 1.6213 - val_mae: 0.4560 - val_mean_pred: 0.9987 - val_mae_t1: 0.0304\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.5712 - mae: 0.4419 - mean_pred: 0.9651 - mae_t1: 0.0295 - val_loss: 1.6149 - val_mae: 0.4542 - val_mean_pred: 0.9324 - val_mae_t1: 0.0303\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.5060 - mae: 0.4236 - mean_pred: 0.8845 - mae_t1: 0.0282 - val_loss: 1.6939 - val_mae: 0.4764 - val_mean_pred: 0.7993 - val_mae_t1: 0.0318\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.4262 - mae: 0.4011 - mean_pred: 0.7619 - mae_t1: 0.0267 - val_loss: 1.7222 - val_mae: 0.4844 - val_mean_pred: 0.7568 - val_mae_t1: 0.0323\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.3979 - mae: 0.3932 - mean_pred: 0.7462 - mae_t1: 0.0262 - val_loss: 1.6578 - val_mae: 0.4663 - val_mean_pred: 0.8380 - val_mae_t1: 0.0311\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.3965 - mae: 0.3928 - mean_pred: 0.8252 - mae_t1: 0.0262 - val_loss: 1.6567 - val_mae: 0.4659 - val_mean_pred: 0.9305 - val_mae_t1: 0.0311\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.4862 - mae: 0.4180 - mean_pred: 0.9018 - mae_t1: 0.0279 - val_loss: 1.6615 - val_mae: 0.4673 - val_mean_pred: 0.9286 - val_mae_t1: 0.0312\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4209 - mae: 0.3996 - mean_pred: 0.8608 - mae_t1: 0.0266 - val_loss: 1.8043 - val_mae: 0.5074 - val_mean_pred: 0.8014 - val_mae_t1: 0.0338\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6018 - mae: 0.4505 - mean_pred: 0.7273 - mae_t1: 0.0300 - val_loss: 1.9922 - val_mae: 0.5603 - val_mean_pred: 0.7032 - val_mae_t1: 0.0374\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6925 - mae: 0.4760 - mean_pred: 0.6265 - mae_t1: 0.0317 - val_loss: 1.7775 - val_mae: 0.4999 - val_mean_pred: 0.6827 - val_mae_t1: 0.0333\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.4564 - mae: 0.4096 - mean_pred: 0.6410 - mae_t1: 0.0273 - val_loss: 1.6383 - val_mae: 0.4608 - val_mean_pred: 0.8538 - val_mae_t1: 0.0307\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4140 - mae: 0.3977 - mean_pred: 0.8297 - mae_t1: 0.0265 - val_loss: 1.8566 - val_mae: 0.5222 - val_mean_pred: 1.1024 - val_mae_t1: 0.0348\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.8020 - mae: 0.5068 - mean_pred: 1.0724 - mae_t1: 0.0338 - val_loss: 1.9887 - val_mae: 0.5593 - val_mean_pred: 1.1282 - val_mae_t1: 0.0373\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7690 - mae: 0.4975 - mean_pred: 1.0214 - mae_t1: 0.0332 - val_loss: 1.7023 - val_mae: 0.4788 - val_mean_pred: 0.8814 - val_mae_t1: 0.0319\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.4299 - mae: 0.4022 - mean_pred: 0.8152 - mae_t1: 0.0268 - val_loss: 1.6673 - val_mae: 0.4689 - val_mean_pred: 0.7623 - val_mae_t1: 0.0313\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4097 - mae: 0.3965 - mean_pred: 0.7271 - mae_t1: 0.0264 - val_loss: 1.7126 - val_mae: 0.4817 - val_mean_pred: 0.7432 - val_mae_t1: 0.0321\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4329 - mae: 0.4030 - mean_pred: 0.7107 - mae_t1: 0.0269 - val_loss: 1.7020 - val_mae: 0.4787 - val_mean_pred: 0.7801 - val_mae_t1: 0.0319\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3238 - mae: 0.3723 - mean_pred: 0.7404 - mae_t1: 0.0248 - val_loss: 1.6968 - val_mae: 0.4772 - val_mean_pred: 0.9025 - val_mae_t1: 0.0318\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.4317 - mae: 0.4027 - mean_pred: 0.8430 - mae_t1: 0.0268 - val_loss: 1.6555 - val_mae: 0.4656 - val_mean_pred: 0.9417 - val_mae_t1: 0.0310\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.3602 - mae: 0.3825 - mean_pred: 0.8506 - mae_t1: 0.0255 - val_loss: 1.6623 - val_mae: 0.4675 - val_mean_pred: 0.8780 - val_mae_t1: 0.0312\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.3912 - mae: 0.3913 - mean_pred: 0.8066 - mae_t1: 0.0261 - val_loss: 1.7037 - val_mae: 0.4792 - val_mean_pred: 0.8246 - val_mae_t1: 0.0319\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4009 - mae: 0.3940 - mean_pred: 0.7486 - mae_t1: 0.0263 - val_loss: 1.7499 - val_mae: 0.4921 - val_mean_pred: 0.7346 - val_mae_t1: 0.0328\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4063 - mae: 0.3955 - mean_pred: 0.6685 - mae_t1: 0.0264 - val_loss: 1.7718 - val_mae: 0.4983 - val_mean_pred: 0.7473 - val_mae_t1: 0.0332\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.3603 - mae: 0.3826 - mean_pred: 0.7016 - mae_t1: 0.0255 - val_loss: 1.6011 - val_mae: 0.4503 - val_mean_pred: 0.8147 - val_mae_t1: 0.0300\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2630 - mae: 0.3552 - mean_pred: 0.7405 - mae_t1: 0.0237 - val_loss: 1.6050 - val_mae: 0.4514 - val_mean_pred: 0.8202 - val_mae_t1: 0.0301\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2894 - mae: 0.3626 - mean_pred: 0.7479 - mae_t1: 0.0242 - val_loss: 1.5842 - val_mae: 0.4455 - val_mean_pred: 0.8278 - val_mae_t1: 0.0297\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 1.2518 - mae: 0.3521 - mean_pred: 0.7494 - mae_t1: 0.0235 - val_loss: 1.5362 - val_mae: 0.4321 - val_mean_pred: 0.8932 - val_mae_t1: 0.0288\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2570 - mae: 0.3535 - mean_pred: 0.8229 - mae_t1: 0.0236 - val_loss: 1.7229 - val_mae: 0.4846 - val_mean_pred: 1.0254 - val_mae_t1: 0.0323\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.4782 - mae: 0.4157 - mean_pred: 0.9380 - mae_t1: 0.0277 - val_loss: 1.6788 - val_mae: 0.4722 - val_mean_pred: 0.9910 - val_mae_t1: 0.0315\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 1.3631 - mae: 0.3834 - mean_pred: 0.8792 - mae_t1: 0.0256 - val_loss: 1.5154 - val_mae: 0.4262 - val_mean_pred: 0.8098 - val_mae_t1: 0.0284\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.2982 - mae: 0.3651 - mean_pred: 0.7389 - mae_t1: 0.0243 - val_loss: 1.6090 - val_mae: 0.4525 - val_mean_pred: 0.7063 - val_mae_t1: 0.0302\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 317us/sample - loss: 1.4012 - mae: 0.3941 - mean_pred: 0.6623 - mae_t1: 0.0263 - val_loss: 1.5979 - val_mae: 0.4494 - val_mean_pred: 0.7305 - val_mae_t1: 0.0300\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 1.3069 - mae: 0.3676 - mean_pred: 0.7144 - mae_t1: 0.0245 - val_loss: 1.4893 - val_mae: 0.4189 - val_mean_pred: 0.8784 - val_mae_t1: 0.0279\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 1.2655 - mae: 0.3559 - mean_pred: 0.8330 - mae_t1: 0.0237 - val_loss: 1.5444 - val_mae: 0.4344 - val_mean_pred: 0.9399 - val_mae_t1: 0.0290\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.2738 - mae: 0.3583 - mean_pred: 0.8522 - mae_t1: 0.0239 - val_loss: 1.5697 - val_mae: 0.4415 - val_mean_pred: 0.9003 - val_mae_t1: 0.0294\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2221 - mae: 0.3437 - mean_pred: 0.7877 - mae_t1: 0.0229 - val_loss: 1.6787 - val_mae: 0.4721 - val_mean_pred: 0.8836 - val_mae_t1: 0.0315\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.2976 - mae: 0.3650 - mean_pred: 0.7752 - mae_t1: 0.0243 - val_loss: 1.6677 - val_mae: 0.4691 - val_mean_pred: 0.9421 - val_mae_t1: 0.0313\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2361 - mae: 0.3476 - mean_pred: 0.8358 - mae_t1: 0.0232 - val_loss: 1.5713 - val_mae: 0.4419 - val_mean_pred: 0.9814 - val_mae_t1: 0.0295\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2857 - mae: 0.3616 - mean_pred: 0.8868 - mae_t1: 0.0241 - val_loss: 1.5653 - val_mae: 0.4402 - val_mean_pred: 0.9846 - val_mae_t1: 0.0293\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.2895 - mae: 0.3627 - mean_pred: 0.8857 - mae_t1: 0.0242 - val_loss: 1.5720 - val_mae: 0.4421 - val_mean_pred: 0.9036 - val_mae_t1: 0.0295\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2132 - mae: 0.3412 - mean_pred: 0.7890 - mae_t1: 0.0227 - val_loss: 1.6588 - val_mae: 0.4665 - val_mean_pred: 0.8340 - val_mae_t1: 0.0311\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.2359 - mae: 0.3476 - mean_pred: 0.7684 - mae_t1: 0.0232 - val_loss: 1.5251 - val_mae: 0.4289 - val_mean_pred: 0.9019 - val_mae_t1: 0.0286\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.1845 - mae: 0.3331 - mean_pred: 0.8333 - mae_t1: 0.0222 - val_loss: 1.4958 - val_mae: 0.4207 - val_mean_pred: 0.8786 - val_mae_t1: 0.0280\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2966 - mae: 0.3647 - mean_pred: 0.7875 - mae_t1: 0.0243 - val_loss: 1.5782 - val_mae: 0.4439 - val_mean_pred: 0.7632 - val_mae_t1: 0.0296\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2751 - mae: 0.3586 - mean_pred: 0.6713 - mae_t1: 0.0239 - val_loss: 1.6754 - val_mae: 0.4712 - val_mean_pred: 0.7561 - val_mae_t1: 0.0314\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.3299 - mae: 0.3740 - mean_pred: 0.6802 - mae_t1: 0.0249 - val_loss: 1.5562 - val_mae: 0.4377 - val_mean_pred: 0.8866 - val_mae_t1: 0.0292\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2445 - mae: 0.3500 - mean_pred: 0.7999 - mae_t1: 0.0233 - val_loss: 1.7522 - val_mae: 0.4928 - val_mean_pred: 1.0431 - val_mae_t1: 0.0329\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.4379 - mae: 0.4044 - mean_pred: 0.9572 - mae_t1: 0.0270 - val_loss: 1.7949 - val_mae: 0.5048 - val_mean_pred: 1.0628 - val_mae_t1: 0.0337\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.3574 - mae: 0.3818 - mean_pred: 0.9262 - mae_t1: 0.0255 - val_loss: 1.5510 - val_mae: 0.4362 - val_mean_pred: 0.8253 - val_mae_t1: 0.0291\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2289 - mae: 0.3456 - mean_pred: 0.7157 - mae_t1: 0.0230 - val_loss: 1.7896 - val_mae: 0.5033 - val_mean_pred: 0.6523 - val_mae_t1: 0.0336\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4811 - mae: 0.4166 - mean_pred: 0.5856 - mae_t1: 0.0278 - val_loss: 1.8342 - val_mae: 0.5159 - val_mean_pred: 0.6291 - val_mae_t1: 0.0344\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4709 - mae: 0.4137 - mean_pred: 0.5959 - mae_t1: 0.0276 - val_loss: 1.6558 - val_mae: 0.4657 - val_mean_pred: 0.7894 - val_mae_t1: 0.0310\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2593 - mae: 0.3542 - mean_pred: 0.7445 - mae_t1: 0.0236 - val_loss: 1.6804 - val_mae: 0.4726 - val_mean_pred: 0.9663 - val_mae_t1: 0.0315\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2208 - mae: 0.3433 - mean_pred: 0.8650 - mae_t1: 0.0229 - val_loss: 1.6619 - val_mae: 0.4674 - val_mean_pred: 0.9758 - val_mae_t1: 0.0312\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2212 - mae: 0.3435 - mean_pred: 0.8748 - mae_t1: 0.0229 - val_loss: 1.6147 - val_mae: 0.4541 - val_mean_pred: 0.9419 - val_mae_t1: 0.0303\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2219 - mae: 0.3437 - mean_pred: 0.8538 - mae_t1: 0.0229 - val_loss: 1.5757 - val_mae: 0.4432 - val_mean_pred: 0.8684 - val_mae_t1: 0.0295\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.1889 - mae: 0.3344 - mean_pred: 0.7781 - mae_t1: 0.0223 - val_loss: 1.6249 - val_mae: 0.4570 - val_mean_pred: 0.8227 - val_mae_t1: 0.0305\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2150 - mae: 0.3417 - mean_pred: 0.7559 - mae_t1: 0.0228 - val_loss: 1.6378 - val_mae: 0.4606 - val_mean_pred: 0.8846 - val_mae_t1: 0.0307\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2494 - mae: 0.3514 - mean_pred: 0.8240 - mae_t1: 0.0234 - val_loss: 1.6552 - val_mae: 0.4655 - val_mean_pred: 0.9301 - val_mae_t1: 0.0310\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2276 - mae: 0.3452 - mean_pred: 0.8528 - mae_t1: 0.0230 - val_loss: 1.5847 - val_mae: 0.4457 - val_mean_pred: 0.9167 - val_mae_t1: 0.0297\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.2397 - mae: 0.3487 - mean_pred: 0.8326 - mae_t1: 0.0232 - val_loss: 1.6479 - val_mae: 0.4635 - val_mean_pred: 0.8955 - val_mae_t1: 0.0309\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2665 - mae: 0.3562 - mean_pred: 0.8083 - mae_t1: 0.0237 - val_loss: 1.5458 - val_mae: 0.4347 - val_mean_pred: 0.8938 - val_mae_t1: 0.0290\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2261 - mae: 0.3448 - mean_pred: 0.8273 - mae_t1: 0.0230 - val_loss: 1.6713 - val_mae: 0.4700 - val_mean_pred: 0.9433 - val_mae_t1: 0.0313\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.2858 - mae: 0.3616 - mean_pred: 0.8441 - mae_t1: 0.0241 - val_loss: 1.5757 - val_mae: 0.4432 - val_mean_pred: 0.9015 - val_mae_t1: 0.0295\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.3033 - mae: 0.3665 - mean_pred: 0.7959 - mae_t1: 0.0244 - val_loss: 1.7905 - val_mae: 0.5036 - val_mean_pred: 0.8789 - val_mae_t1: 0.0336\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.4368 - mae: 0.4041 - mean_pred: 0.7835 - mae_t1: 0.0269 - val_loss: 1.5567 - val_mae: 0.4378 - val_mean_pred: 0.8688 - val_mae_t1: 0.0292\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.1481 - mae: 0.3229 - mean_pred: 0.7763 - mae_t1: 0.0215 - val_loss: 1.6272 - val_mae: 0.4576 - val_mean_pred: 0.8346 - val_mae_t1: 0.0305\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 1.3697 - mae: 0.3852 - mean_pred: 0.7292 - mae_t1: 0.0257 - val_loss: 1.7058 - val_mae: 0.4798 - val_mean_pred: 0.7218 - val_mae_t1: 0.0320\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.3132 - mae: 0.3693 - mean_pred: 0.6299 - mae_t1: 0.0246 - val_loss: 1.8177 - val_mae: 0.5112 - val_mean_pred: 0.7205 - val_mae_t1: 0.0341\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.3983 - mae: 0.3933 - mean_pred: 0.6240 - mae_t1: 0.0262 - val_loss: 1.7346 - val_mae: 0.4878 - val_mean_pred: 0.8400 - val_mae_t1: 0.0325\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2331 - mae: 0.3468 - mean_pred: 0.7415 - mae_t1: 0.0231 - val_loss: 1.7945 - val_mae: 0.5047 - val_mean_pred: 1.0151 - val_mae_t1: 0.0336\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2433 - mae: 0.3497 - mean_pred: 0.9112 - mae_t1: 0.0233 - val_loss: 1.8685 - val_mae: 0.5255 - val_mean_pred: 1.0657 - val_mae_t1: 0.0350\n",
      "Earliness...\n",
      "0.0015001296997070312\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3536652998129527  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▇▄▄▃▃▃▃▃▂▃▂▂▂▂▄▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▁</td></tr><tr><td>mae</td><td>█▇▇▄▄▃▃▃▃▃▂▃▂▂▂▂▄▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▁</td></tr><tr><td>mae_t1</td><td>█▇▇▄▄▃▃▃▃▃▂▃▂▂▂▂▄▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▁</td></tr><tr><td>mean_pred</td><td>▁█▂▄▅▅▆▅▇▅▆▅▆▆▆▄█▅▅▆▅▅▆▅▅▆▆▆▆▅▇▄▆▆▆▆▆▆▄▇</td></tr><tr><td>val_loss</td><td>▃▆█▄▄▂▃▂▂▂▃▂▃▂▃▂▅▃▃▃▃▂▃▂▁▂▂▂▂▂▁▄▂▂▂▂▂▂▄▄</td></tr><tr><td>val_mae</td><td>▃▆█▄▄▂▃▂▂▂▃▂▃▂▃▂▅▃▃▃▃▂▃▂▁▂▂▂▂▂▁▄▂▂▂▂▂▂▄▄</td></tr><tr><td>val_mae_t1</td><td>▃▆█▄▄▂▃▂▂▂▃▂▃▂▃▂▅▃▃▃▃▂▃▂▁▂▂▂▂▂▁▄▂▂▂▂▂▂▄▄</td></tr><tr><td>val_mean_pred</td><td>▃█▁▅▄▇▅▆▆▆▄▅▅▆▅▅█▄▆▅▄▅▇▄▆▆▇▅▅▆▅▃▇▆▆▆▆▆▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.47384</td></tr><tr><td>AE_2</td><td>0.34316</td></tr><tr><td>AE_3</td><td>0.31108</td></tr><tr><td>MAE</td><td>0.35367</td></tr><tr><td>best_epoch</td><td>60</td></tr><tr><td>best_val_loss</td><td>1.48928</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.24334</td></tr><tr><td>mae</td><td>0.34969</td></tr><tr><td>mae_t1</td><td>0.02331</td></tr><tr><td>mean_pred</td><td>0.91123</td></tr><tr><td>val_loss</td><td>1.86855</td></tr><tr><td>val_mae</td><td>0.52553</td></tr><tr><td>val_mae_t1</td><td>0.03504</td></tr><tr><td>val_mean_pred</td><td>1.06575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">electric-sky-62</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mzt539m\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mzt539m</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154506-3mzt539m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154530-10e6ulys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/10e6ulys\" target=\"_blank\">confused-river-63</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 862us/sample - loss: 5.4229 - mae: 0.6779 - mean_pred: 0.4116 - mae_t1: 0.0452 - val_loss: 5.3615 - val_mae: 0.6702 - val_mean_pred: 1.3102 - val_mae_t1: 0.0447\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 280us/sample - loss: 6.0810 - mae: 0.7601 - mean_pred: 1.1749 - mae_t1: 0.0507 - val_loss: 4.0577 - val_mae: 0.5072 - val_mean_pred: 0.6379 - val_mae_t1: 0.0338\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 270us/sample - loss: 4.3165 - mae: 0.5396 - mean_pred: 0.6370 - mae_t1: 0.0360 - val_loss: 3.7662 - val_mae: 0.4708 - val_mean_pred: 0.8356 - val_mae_t1: 0.0314\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 4.0211 - mae: 0.5026 - mean_pred: 0.8291 - mae_t1: 0.0335 - val_loss: 4.2547 - val_mae: 0.5318 - val_mean_pred: 0.7410 - val_mae_t1: 0.0355\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 4.0963 - mae: 0.5120 - mean_pred: 0.6210 - mae_t1: 0.0341 - val_loss: 4.2026 - val_mae: 0.5253 - val_mean_pred: 0.7078 - val_mae_t1: 0.0350\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 3.7151 - mae: 0.4644 - mean_pred: 0.7745 - mae_t1: 0.0310 - val_loss: 4.2699 - val_mae: 0.5337 - val_mean_pred: 1.0329 - val_mae_t1: 0.0356\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.5814 - mae: 0.4477 - mean_pred: 0.8724 - mae_t1: 0.0298 - val_loss: 4.1145 - val_mae: 0.5143 - val_mean_pred: 0.7535 - val_mae_t1: 0.0343\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.4099 - mae: 0.4262 - mean_pred: 0.7668 - mae_t1: 0.0284 - val_loss: 4.1167 - val_mae: 0.5146 - val_mean_pred: 1.0431 - val_mae_t1: 0.0343\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6782 - mae: 0.4598 - mean_pred: 0.9102 - mae_t1: 0.0307 - val_loss: 4.4482 - val_mae: 0.5560 - val_mean_pred: 0.6326 - val_mae_t1: 0.0371\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7963 - mae: 0.4745 - mean_pred: 0.5744 - mae_t1: 0.0316 - val_loss: 3.9655 - val_mae: 0.4957 - val_mean_pred: 0.9244 - val_mae_t1: 0.0330\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.9315 - mae: 0.4914 - mean_pred: 1.0193 - mae_t1: 0.0328 - val_loss: 3.8002 - val_mae: 0.4750 - val_mean_pred: 0.9280 - val_mae_t1: 0.0317\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1780 - mae: 0.3973 - mean_pred: 0.8064 - mae_t1: 0.0265 - val_loss: 3.9774 - val_mae: 0.4972 - val_mean_pred: 0.7312 - val_mae_t1: 0.0331\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 3.4059 - mae: 0.4257 - mean_pred: 0.7568 - mae_t1: 0.0284 - val_loss: 4.0969 - val_mae: 0.5121 - val_mean_pred: 1.0252 - val_mae_t1: 0.0341\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6041 - mae: 0.4505 - mean_pred: 0.9633 - mae_t1: 0.0300 - val_loss: 4.2337 - val_mae: 0.5292 - val_mean_pred: 0.6897 - val_mae_t1: 0.0353\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.9467 - mae: 0.4933 - mean_pred: 0.5587 - mae_t1: 0.0329 - val_loss: 4.1868 - val_mae: 0.5234 - val_mean_pred: 0.6730 - val_mae_t1: 0.0349\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3154 - mae: 0.4144 - mean_pred: 0.7927 - mae_t1: 0.0276 - val_loss: 4.0619 - val_mae: 0.5077 - val_mean_pred: 1.0217 - val_mae_t1: 0.0338\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.2132 - mae: 0.4016 - mean_pred: 0.8481 - mae_t1: 0.0268 - val_loss: 4.3060 - val_mae: 0.5383 - val_mean_pred: 0.7837 - val_mae_t1: 0.0359\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.1481 - mae: 0.3935 - mean_pred: 0.7367 - mae_t1: 0.0262 - val_loss: 3.9406 - val_mae: 0.4926 - val_mean_pred: 0.9241 - val_mae_t1: 0.0328\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9814 - mae: 0.3727 - mean_pred: 0.8469 - mae_t1: 0.0248 - val_loss: 3.9070 - val_mae: 0.4884 - val_mean_pred: 0.8261 - val_mae_t1: 0.0326\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.2507 - mae: 0.4063 - mean_pred: 0.7092 - mae_t1: 0.0271 - val_loss: 4.3211 - val_mae: 0.5401 - val_mean_pred: 0.6635 - val_mae_t1: 0.0360\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 3.3851 - mae: 0.4231 - mean_pred: 0.6769 - mae_t1: 0.0282 - val_loss: 3.5840 - val_mae: 0.4480 - val_mean_pred: 0.9308 - val_mae_t1: 0.0299\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 2.8652 - mae: 0.3582 - mean_pred: 0.8092 - mae_t1: 0.0239 - val_loss: 3.4363 - val_mae: 0.4295 - val_mean_pred: 0.8020 - val_mae_t1: 0.0286\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0587 - mae: 0.3823 - mean_pred: 0.7420 - mae_t1: 0.0255 - val_loss: 3.7786 - val_mae: 0.4723 - val_mean_pred: 0.8885 - val_mae_t1: 0.0315\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8131 - mae: 0.3516 - mean_pred: 0.7921 - mae_t1: 0.0234 - val_loss: 3.6903 - val_mae: 0.4613 - val_mean_pred: 0.8838 - val_mae_t1: 0.0308\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1014 - mae: 0.3877 - mean_pred: 0.7243 - mae_t1: 0.0258 - val_loss: 3.9702 - val_mae: 0.4963 - val_mean_pred: 0.8019 - val_mae_t1: 0.0331\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8885 - mae: 0.3611 - mean_pred: 0.7898 - mae_t1: 0.0241 - val_loss: 3.6251 - val_mae: 0.4531 - val_mean_pred: 0.9703 - val_mae_t1: 0.0302\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8253 - mae: 0.3532 - mean_pred: 0.8428 - mae_t1: 0.0235 - val_loss: 3.7947 - val_mae: 0.4743 - val_mean_pred: 0.8753 - val_mae_t1: 0.0316\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.7389 - mae: 0.3424 - mean_pred: 0.8299 - mae_t1: 0.0228 - val_loss: 3.7341 - val_mae: 0.4668 - val_mean_pred: 0.8477 - val_mae_t1: 0.0311\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7812 - mae: 0.3476 - mean_pred: 0.7376 - mae_t1: 0.0232 - val_loss: 4.2237 - val_mae: 0.5280 - val_mean_pred: 0.8417 - val_mae_t1: 0.0352\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8945 - mae: 0.3618 - mean_pred: 0.7986 - mae_t1: 0.0241 - val_loss: 4.1033 - val_mae: 0.5129 - val_mean_pred: 1.0027 - val_mae_t1: 0.0342\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.7802 - mae: 0.3475 - mean_pred: 0.8435 - mae_t1: 0.0232 - val_loss: 3.9211 - val_mae: 0.4901 - val_mean_pred: 0.9599 - val_mae_t1: 0.0327\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.5215 - mae: 0.3152 - mean_pred: 0.8619 - mae_t1: 0.0210 - val_loss: 3.7209 - val_mae: 0.4651 - val_mean_pred: 0.9673 - val_mae_t1: 0.0310\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4534 - mae: 0.3067 - mean_pred: 0.8523 - mae_t1: 0.0204 - val_loss: 3.4941 - val_mae: 0.4368 - val_mean_pred: 0.8567 - val_mae_t1: 0.0291\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5085 - mae: 0.3136 - mean_pred: 0.7833 - mae_t1: 0.0209 - val_loss: 3.9307 - val_mae: 0.4913 - val_mean_pred: 0.9436 - val_mae_t1: 0.0328\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.8469 - mae: 0.3559 - mean_pred: 0.8426 - mae_t1: 0.0237 - val_loss: 3.6412 - val_mae: 0.4552 - val_mean_pred: 0.8844 - val_mae_t1: 0.0303\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8581 - mae: 0.3573 - mean_pred: 0.7062 - mae_t1: 0.0238 - val_loss: 3.6031 - val_mae: 0.4504 - val_mean_pred: 0.8624 - val_mae_t1: 0.0300\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5953 - mae: 0.3244 - mean_pred: 0.8198 - mae_t1: 0.0216 - val_loss: 3.8565 - val_mae: 0.4821 - val_mean_pred: 0.8789 - val_mae_t1: 0.0321\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.9504 - mae: 0.3688 - mean_pred: 0.7130 - mae_t1: 0.0246 - val_loss: 3.7501 - val_mae: 0.4688 - val_mean_pred: 0.7621 - val_mae_t1: 0.0313\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.8527 - mae: 0.3566 - mean_pred: 0.7269 - mae_t1: 0.0238 - val_loss: 4.1139 - val_mae: 0.5142 - val_mean_pred: 1.0740 - val_mae_t1: 0.0343\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 3.6718 - mae: 0.4590 - mean_pred: 1.0352 - mae_t1: 0.0306 - val_loss: 4.5286 - val_mae: 0.5661 - val_mean_pred: 1.0782 - val_mae_t1: 0.0377\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.7411 - mae: 0.3426 - mean_pred: 0.8143 - mae_t1: 0.0228 - val_loss: 3.7605 - val_mae: 0.4701 - val_mean_pred: 0.7973 - val_mae_t1: 0.0313\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.5971 - mae: 0.3246 - mean_pred: 0.7261 - mae_t1: 0.0216 - val_loss: 3.6288 - val_mae: 0.4536 - val_mean_pred: 0.8695 - val_mae_t1: 0.0302\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.5284 - mae: 0.3161 - mean_pred: 0.7149 - mae_t1: 0.0211 - val_loss: 3.7643 - val_mae: 0.4705 - val_mean_pred: 0.8951 - val_mae_t1: 0.0314\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5158 - mae: 0.3145 - mean_pred: 0.8428 - mae_t1: 0.0210 - val_loss: 4.0130 - val_mae: 0.5016 - val_mean_pred: 1.0226 - val_mae_t1: 0.0334\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6899 - mae: 0.3362 - mean_pred: 0.8452 - mae_t1: 0.0224 - val_loss: 4.3868 - val_mae: 0.5484 - val_mean_pred: 0.8361 - val_mae_t1: 0.0366\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.2714 - mae: 0.4089 - mean_pred: 0.7254 - mae_t1: 0.0273 - val_loss: 3.7377 - val_mae: 0.4672 - val_mean_pred: 0.8067 - val_mae_t1: 0.0311\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.3602 - mae: 0.4200 - mean_pred: 0.7861 - mae_t1: 0.0280 - val_loss: 4.3034 - val_mae: 0.5379 - val_mean_pred: 0.8678 - val_mae_t1: 0.0359\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.8078 - mae: 0.4760 - mean_pred: 0.7692 - mae_t1: 0.0317 - val_loss: 3.7227 - val_mae: 0.4653 - val_mean_pred: 0.7762 - val_mae_t1: 0.0310\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.2078 - mae: 0.4010 - mean_pred: 0.7097 - mae_t1: 0.0267 - val_loss: 4.0033 - val_mae: 0.5004 - val_mean_pred: 0.8037 - val_mae_t1: 0.0334\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9501 - mae: 0.3688 - mean_pred: 0.7732 - mae_t1: 0.0246 - val_loss: 4.1927 - val_mae: 0.5241 - val_mean_pred: 0.9575 - val_mae_t1: 0.0349\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.7437 - mae: 0.3430 - mean_pred: 0.7868 - mae_t1: 0.0229 - val_loss: 3.6302 - val_mae: 0.4538 - val_mean_pred: 0.8068 - val_mae_t1: 0.0303\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.7026 - mae: 0.3378 - mean_pred: 0.7558 - mae_t1: 0.0225 - val_loss: 3.5812 - val_mae: 0.4476 - val_mean_pred: 0.8268 - val_mae_t1: 0.0298\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.9484 - mae: 0.3686 - mean_pred: 0.7334 - mae_t1: 0.0246 - val_loss: 3.6345 - val_mae: 0.4543 - val_mean_pred: 0.9175 - val_mae_t1: 0.0303\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7832 - mae: 0.3479 - mean_pred: 0.8523 - mae_t1: 0.0232 - val_loss: 4.2900 - val_mae: 0.5363 - val_mean_pred: 0.9925 - val_mae_t1: 0.0358\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.9737 - mae: 0.3717 - mean_pred: 0.7616 - mae_t1: 0.0248 - val_loss: 4.0739 - val_mae: 0.5092 - val_mean_pred: 0.9640 - val_mae_t1: 0.0339\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.3528 - mae: 0.4191 - mean_pred: 0.9479 - mae_t1: 0.0279 - val_loss: 5.0210 - val_mae: 0.6276 - val_mean_pred: 1.1997 - val_mae_t1: 0.0418\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4551 - mae: 0.4319 - mean_pred: 0.9654 - mae_t1: 0.0288 - val_loss: 3.9885 - val_mae: 0.4986 - val_mean_pred: 0.8732 - val_mae_t1: 0.0332\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7264 - mae: 0.3408 - mean_pred: 0.7449 - mae_t1: 0.0227 - val_loss: 3.7642 - val_mae: 0.4705 - val_mean_pred: 0.8455 - val_mae_t1: 0.0314\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.5226 - mae: 0.3153 - mean_pred: 0.7675 - mae_t1: 0.0210 - val_loss: 3.3990 - val_mae: 0.4249 - val_mean_pred: 0.9124 - val_mae_t1: 0.0283\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 2.4195 - mae: 0.3024 - mean_pred: 0.7825 - mae_t1: 0.0202 - val_loss: 3.2043 - val_mae: 0.4005 - val_mean_pred: 0.8205 - val_mae_t1: 0.0267\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.2435 - mae: 0.2804 - mean_pred: 0.7855 - mae_t1: 0.0187 - val_loss: 3.3477 - val_mae: 0.4185 - val_mean_pred: 0.9493 - val_mae_t1: 0.0279\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.2715 - mae: 0.2839 - mean_pred: 0.8307 - mae_t1: 0.0189 - val_loss: 3.7558 - val_mae: 0.4695 - val_mean_pred: 0.8353 - val_mae_t1: 0.0313\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8346 - mae: 0.3543 - mean_pred: 0.7326 - mae_t1: 0.0236 - val_loss: 3.6563 - val_mae: 0.4570 - val_mean_pred: 0.8678 - val_mae_t1: 0.0305\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.6828 - mae: 0.3353 - mean_pred: 0.8193 - mae_t1: 0.0224 - val_loss: 3.8113 - val_mae: 0.4764 - val_mean_pred: 0.9549 - val_mae_t1: 0.0318\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4201 - mae: 0.3025 - mean_pred: 0.7865 - mae_t1: 0.0202 - val_loss: 3.6695 - val_mae: 0.4587 - val_mean_pred: 0.8356 - val_mae_t1: 0.0306\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4510 - mae: 0.3064 - mean_pred: 0.7076 - mae_t1: 0.0204 - val_loss: 3.4675 - val_mae: 0.4334 - val_mean_pred: 0.8268 - val_mae_t1: 0.0289\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2595 - mae: 0.2824 - mean_pred: 0.7885 - mae_t1: 0.0188 - val_loss: 3.5595 - val_mae: 0.4449 - val_mean_pred: 0.9170 - val_mae_t1: 0.0297\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1505 - mae: 0.2688 - mean_pred: 0.7860 - mae_t1: 0.0179 - val_loss: 3.3612 - val_mae: 0.4201 - val_mean_pred: 0.8466 - val_mae_t1: 0.0280\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1844 - mae: 0.2731 - mean_pred: 0.7611 - mae_t1: 0.0182 - val_loss: 3.5863 - val_mae: 0.4483 - val_mean_pred: 1.0001 - val_mae_t1: 0.0299\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.3506 - mae: 0.2938 - mean_pred: 0.8867 - mae_t1: 0.0196 - val_loss: 3.4136 - val_mae: 0.4267 - val_mean_pred: 0.9464 - val_mae_t1: 0.0284\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1295 - mae: 0.2662 - mean_pred: 0.8026 - mae_t1: 0.0177 - val_loss: 3.5519 - val_mae: 0.4440 - val_mean_pred: 0.8883 - val_mae_t1: 0.0296\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3614 - mae: 0.2952 - mean_pred: 0.8186 - mae_t1: 0.0197 - val_loss: 3.9839 - val_mae: 0.4980 - val_mean_pred: 0.9727 - val_mae_t1: 0.0332\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7428 - mae: 0.3428 - mean_pred: 0.8843 - mae_t1: 0.0229 - val_loss: 3.7739 - val_mae: 0.4717 - val_mean_pred: 0.9648 - val_mae_t1: 0.0314\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.2090 - mae: 0.2761 - mean_pred: 0.7858 - mae_t1: 0.0184 - val_loss: 3.6982 - val_mae: 0.4623 - val_mean_pred: 0.8427 - val_mae_t1: 0.0308\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2906 - mae: 0.2863 - mean_pred: 0.7277 - mae_t1: 0.0191 - val_loss: 3.6214 - val_mae: 0.4527 - val_mean_pred: 0.8906 - val_mae_t1: 0.0302\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.1455 - mae: 0.2682 - mean_pred: 0.7849 - mae_t1: 0.0179 - val_loss: 3.4535 - val_mae: 0.4317 - val_mean_pred: 0.9702 - val_mae_t1: 0.0288\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.0815 - mae: 0.2602 - mean_pred: 0.8397 - mae_t1: 0.0173 - val_loss: 3.5346 - val_mae: 0.4418 - val_mean_pred: 0.8136 - val_mae_t1: 0.0295\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.1243 - mae: 0.2655 - mean_pred: 0.7336 - mae_t1: 0.0177 - val_loss: 3.5274 - val_mae: 0.4409 - val_mean_pred: 0.8960 - val_mae_t1: 0.0294\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0420 - mae: 0.2552 - mean_pred: 0.8401 - mae_t1: 0.0170 - val_loss: 3.5550 - val_mae: 0.4444 - val_mean_pred: 0.8148 - val_mae_t1: 0.0296\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.3580 - mae: 0.2947 - mean_pred: 0.6865 - mae_t1: 0.0196 - val_loss: 3.6668 - val_mae: 0.4584 - val_mean_pred: 0.8337 - val_mae_t1: 0.0306\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2386 - mae: 0.2798 - mean_pred: 0.7867 - mae_t1: 0.0187 - val_loss: 3.7448 - val_mae: 0.4681 - val_mean_pred: 0.9557 - val_mae_t1: 0.0312\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2551 - mae: 0.2819 - mean_pred: 0.7565 - mae_t1: 0.0188 - val_loss: 3.5747 - val_mae: 0.4468 - val_mean_pred: 0.7960 - val_mae_t1: 0.0298\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.4207 - mae: 0.3026 - mean_pred: 0.7168 - mae_t1: 0.0202 - val_loss: 3.3619 - val_mae: 0.4202 - val_mean_pred: 0.8192 - val_mae_t1: 0.0280\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0005 - mae: 0.2501 - mean_pred: 0.7586 - mae_t1: 0.0167 - val_loss: 3.4496 - val_mae: 0.4312 - val_mean_pred: 1.0187 - val_mae_t1: 0.0287\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3388 - mae: 0.2923 - mean_pred: 0.9348 - mae_t1: 0.0195 - val_loss: 3.4489 - val_mae: 0.4311 - val_mean_pred: 0.8362 - val_mae_t1: 0.0287\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.5729 - mae: 0.3216 - mean_pred: 0.6827 - mae_t1: 0.0214 - val_loss: 3.6682 - val_mae: 0.4585 - val_mean_pred: 0.7121 - val_mae_t1: 0.0306\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.5629 - mae: 0.3204 - mean_pred: 0.7113 - mae_t1: 0.0214 - val_loss: 3.7693 - val_mae: 0.4712 - val_mean_pred: 1.0116 - val_mae_t1: 0.0314\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.5576 - mae: 0.3197 - mean_pred: 0.9163 - mae_t1: 0.0213 - val_loss: 3.6218 - val_mae: 0.4527 - val_mean_pred: 0.8772 - val_mae_t1: 0.0302\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2762 - mae: 0.2845 - mean_pred: 0.7355 - mae_t1: 0.0190 - val_loss: 3.7800 - val_mae: 0.4725 - val_mean_pred: 0.8205 - val_mae_t1: 0.0315\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3531 - mae: 0.2941 - mean_pred: 0.8066 - mae_t1: 0.0196 - val_loss: 3.6409 - val_mae: 0.4551 - val_mean_pred: 1.0163 - val_mae_t1: 0.0303\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3605 - mae: 0.2951 - mean_pred: 0.8926 - mae_t1: 0.0197 - val_loss: 3.6761 - val_mae: 0.4595 - val_mean_pred: 0.7850 - val_mae_t1: 0.0306\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.4536 - mae: 0.3067 - mean_pred: 0.6659 - mae_t1: 0.0204 - val_loss: 3.6374 - val_mae: 0.4547 - val_mean_pred: 0.7974 - val_mae_t1: 0.0303\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1010 - mae: 0.2626 - mean_pred: 0.8105 - mae_t1: 0.0175 - val_loss: 3.7301 - val_mae: 0.4663 - val_mean_pred: 1.0149 - val_mae_t1: 0.0311\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1031 - mae: 0.2629 - mean_pred: 0.8226 - mae_t1: 0.0175 - val_loss: 3.7091 - val_mae: 0.4636 - val_mean_pred: 0.8444 - val_mae_t1: 0.0309\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2706 - mae: 0.2838 - mean_pred: 0.7534 - mae_t1: 0.0189 - val_loss: 3.7043 - val_mae: 0.4630 - val_mean_pred: 1.0047 - val_mae_t1: 0.0309\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.2408 - mae: 0.2801 - mean_pred: 0.9190 - mae_t1: 0.0187 - val_loss: 3.7194 - val_mae: 0.4649 - val_mean_pred: 0.9181 - val_mae_t1: 0.0310\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 2.1598 - mae: 0.2700 - mean_pred: 0.7406 - mae_t1: 0.0180 - val_loss: 3.7391 - val_mae: 0.4674 - val_mean_pred: 0.8113 - val_mae_t1: 0.0312\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9373 - mae: 0.2422 - mean_pred: 0.7865 - mae_t1: 0.0161 - val_loss: 4.0611 - val_mae: 0.5076 - val_mean_pred: 1.0358 - val_mae_t1: 0.0338\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.1022 - mae: 0.2628 - mean_pred: 0.8971 - mae_t1: 0.0175 - val_loss: 3.7443 - val_mae: 0.4680 - val_mean_pred: 0.9172 - val_mae_t1: 0.0312\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.9796 - mae: 0.2475 - mean_pred: 0.7770 - mae_t1: 0.0165 - val_loss: 3.5892 - val_mae: 0.4486 - val_mean_pred: 0.8392 - val_mae_t1: 0.0299\n",
      "Earliness...\n",
      "0.002001047134399414\n",
      "____________________________________________________________\n",
      "Test MAE:      0.31778795550905903  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▅▄▄▃▄▃▃▃▃▂▃▃▃▂▄▃▃▃▄▂▂▂▂▁▂▁▁▁▂▁▂▂▂▁▁▁</td></tr><tr><td>mae</td><td>█▆▅▄▅▄▄▃▄▃▃▃▃▂▃▃▃▂▄▃▃▃▄▂▂▂▂▁▂▁▁▁▂▁▂▂▂▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆▅▄▅▄▄▃▄▃▃▃▃▂▃▃▃▂▄▃▃▃▄▂▂▂▂▁▂▁▁▁▂▁▂▂▂▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▄▅▅█▅▅▅▄▅▅▆▆▅▄▅▆▆▅▄▅▆▇▅▅▆▅▅▆▅▆▆▅▅▄▅▄▆▅▅</td></tr><tr><td>val_loss</td><td>█▂▄▄▃▄▃▃▂▂▂▂▃▃▂▄▂▃▂▃▂▄▇▁▁▃▂▂▃▂▂▂▂▁▂▃▂▂▂▂</td></tr><tr><td>val_mae</td><td>█▂▄▄▃▄▃▃▂▂▂▂▃▃▂▄▂▃▂▃▂▄▇▁▁▃▂▂▃▂▂▂▂▁▂▃▂▂▂▂</td></tr><tr><td>val_mae_t1</td><td>█▂▄▄▃▄▃▃▂▂▂▂▃▃▂▄▂▃▂▃▂▄▇▁▁▃▂▂▃▂▂▂▂▁▂▃▂▂▂▂</td></tr><tr><td>val_mean_pred</td><td>█▂▄▄▃▄▄▃▃▂▃▂▃▃▂▅▁▄▁▁▁▄▆▃▃▃▃▄▃▂▁▁▁▄▄▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.36527</td></tr><tr><td>AE_2</td><td>0.2909</td></tr><tr><td>AE_3</td><td>0.30318</td></tr><tr><td>MAE</td><td>0.31779</td></tr><tr><td>best_epoch</td><td>59</td></tr><tr><td>best_val_loss</td><td>3.20428</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.97961</td></tr><tr><td>mae</td><td>0.24745</td></tr><tr><td>mae_t1</td><td>0.0165</td></tr><tr><td>mean_pred</td><td>0.77701</td></tr><tr><td>val_loss</td><td>3.58917</td></tr><tr><td>val_mae</td><td>0.44865</td></tr><tr><td>val_mae_t1</td><td>0.02991</td></tr><tr><td>val_mean_pred</td><td>0.8392</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">confused-river-63</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/10e6ulys\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/10e6ulys</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154530-10e6ulys\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154603-2fb4v0hu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fb4v0hu\" target=\"_blank\">distinctive-fire-64</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 700us/sample - loss: 6.1390 - mae: 0.7674 - mean_pred: 0.2183 - mae_t1: 0.0512 - val_loss: 4.5911 - val_mae: 0.5739 - val_mean_pred: 1.0426 - val_mae_t1: 0.0383\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 155us/sample - loss: 5.8770 - mae: 0.7346 - mean_pred: 1.1222 - mae_t1: 0.0490 - val_loss: 5.3066 - val_mae: 0.6633 - val_mean_pred: 1.2385 - val_mae_t1: 0.0442\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 163us/sample - loss: 5.1784 - mae: 0.6473 - mean_pred: 1.0274 - mae_t1: 0.0432 - val_loss: 4.5814 - val_mae: 0.5727 - val_mean_pred: 0.5080 - val_mae_t1: 0.0382\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 4.8221 - mae: 0.6028 - mean_pred: 0.3773 - mae_t1: 0.0402 - val_loss: 4.8569 - val_mae: 0.6071 - val_mean_pred: 0.4598 - val_mae_t1: 0.0405\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 141us/sample - loss: 4.4699 - mae: 0.5587 - mean_pred: 0.4934 - mae_t1: 0.0372 - val_loss: 4.0702 - val_mae: 0.5088 - val_mean_pred: 0.8209 - val_mae_t1: 0.0339\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 4.1003 - mae: 0.5125 - mean_pred: 0.8259 - mae_t1: 0.0342 - val_loss: 4.2847 - val_mae: 0.5356 - val_mean_pred: 0.8167 - val_mae_t1: 0.0357\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.4230 - mae: 0.5529 - mean_pred: 0.7900 - mae_t1: 0.0369 - val_loss: 4.7508 - val_mae: 0.5939 - val_mean_pred: 0.7331 - val_mae_t1: 0.0396\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 4.5315 - mae: 0.5664 - mean_pred: 0.7504 - mae_t1: 0.0378 - val_loss: 4.0357 - val_mae: 0.5045 - val_mean_pred: 0.7593 - val_mae_t1: 0.0336\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.0030 - mae: 0.5004 - mean_pred: 0.7453 - mae_t1: 0.0334 - val_loss: 4.7999 - val_mae: 0.6000 - val_mean_pred: 0.6893 - val_mae_t1: 0.0400\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 5.1097 - mae: 0.6387 - mean_pred: 0.6422 - mae_t1: 0.0426 - val_loss: 5.3835 - val_mae: 0.6729 - val_mean_pred: 0.4617 - val_mae_t1: 0.0449\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 5.0575 - mae: 0.6322 - mean_pred: 0.4655 - mae_t1: 0.0421 - val_loss: 4.2481 - val_mae: 0.5310 - val_mean_pred: 0.6604 - val_mae_t1: 0.0354\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.9213 - mae: 0.4902 - mean_pred: 0.6954 - mae_t1: 0.0327 - val_loss: 4.5235 - val_mae: 0.5654 - val_mean_pred: 1.0855 - val_mae_t1: 0.0377\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.4066 - mae: 0.5508 - mean_pred: 1.0030 - mae_t1: 0.0367 - val_loss: 4.0372 - val_mae: 0.5046 - val_mean_pred: 1.0257 - val_mae_t1: 0.0336\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 4.0270 - mae: 0.5034 - mean_pred: 0.8933 - mae_t1: 0.0336 - val_loss: 4.0228 - val_mae: 0.5028 - val_mean_pred: 0.6920 - val_mae_t1: 0.0335\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.0734 - mae: 0.5092 - mean_pred: 0.5853 - mae_t1: 0.0339 - val_loss: 4.2646 - val_mae: 0.5331 - val_mean_pred: 0.6564 - val_mae_t1: 0.0355\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.0707 - mae: 0.5088 - mean_pred: 0.6399 - mae_t1: 0.0339 - val_loss: 4.1620 - val_mae: 0.5202 - val_mean_pred: 0.9115 - val_mae_t1: 0.0347\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.8501 - mae: 0.4813 - mean_pred: 0.8290 - mae_t1: 0.0321 - val_loss: 4.2438 - val_mae: 0.5305 - val_mean_pred: 0.9749 - val_mae_t1: 0.0354\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7982 - mae: 0.4748 - mean_pred: 0.8975 - mae_t1: 0.0317 - val_loss: 4.1540 - val_mae: 0.5193 - val_mean_pred: 0.9964 - val_mae_t1: 0.0346\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 3.6685 - mae: 0.4586 - mean_pred: 0.8824 - mae_t1: 0.0306 - val_loss: 3.8955 - val_mae: 0.4869 - val_mean_pred: 0.8592 - val_mae_t1: 0.0325\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 3.4809 - mae: 0.4351 - mean_pred: 0.7415 - mae_t1: 0.0290 - val_loss: 3.8856 - val_mae: 0.4857 - val_mean_pred: 0.8636 - val_mae_t1: 0.0324\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 3.5284 - mae: 0.4411 - mean_pred: 0.7817 - mae_t1: 0.0294 - val_loss: 4.0169 - val_mae: 0.5021 - val_mean_pred: 1.0092 - val_mae_t1: 0.0335\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 3.5582 - mae: 0.4448 - mean_pred: 0.8871 - mae_t1: 0.0297 - val_loss: 3.6260 - val_mae: 0.4532 - val_mean_pred: 0.9533 - val_mae_t1: 0.0302\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.3525 - mae: 0.4191 - mean_pred: 0.8297 - mae_t1: 0.0279 - val_loss: 3.6534 - val_mae: 0.4567 - val_mean_pred: 0.8070 - val_mae_t1: 0.0304\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.8514 - mae: 0.4814 - mean_pred: 0.7151 - mae_t1: 0.0321 - val_loss: 3.7123 - val_mae: 0.4640 - val_mean_pred: 0.7081 - val_mae_t1: 0.0309\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.5144 - mae: 0.4393 - mean_pred: 0.6494 - mae_t1: 0.0293 - val_loss: 4.2852 - val_mae: 0.5356 - val_mean_pred: 0.8845 - val_mae_t1: 0.0357\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 4.1292 - mae: 0.5161 - mean_pred: 0.8198 - mae_t1: 0.0344 - val_loss: 3.8365 - val_mae: 0.4796 - val_mean_pred: 0.9870 - val_mae_t1: 0.0320\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.4391 - mae: 0.4299 - mean_pred: 0.8659 - mae_t1: 0.0287 - val_loss: 3.9814 - val_mae: 0.4977 - val_mean_pred: 0.7857 - val_mae_t1: 0.0332\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 3.5110 - mae: 0.4389 - mean_pred: 0.6458 - mae_t1: 0.0293 - val_loss: 4.5297 - val_mae: 0.5662 - val_mean_pred: 0.5636 - val_mae_t1: 0.0377\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.1643 - mae: 0.5205 - mean_pred: 0.4926 - mae_t1: 0.0347 - val_loss: 4.2778 - val_mae: 0.5347 - val_mean_pred: 0.6729 - val_mae_t1: 0.0356\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.5689 - mae: 0.4461 - mean_pred: 0.6289 - mae_t1: 0.0297 - val_loss: 4.4844 - val_mae: 0.5605 - val_mean_pred: 1.1259 - val_mae_t1: 0.0374\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 4.1649 - mae: 0.5206 - mean_pred: 1.0561 - mae_t1: 0.0347 - val_loss: 4.2688 - val_mae: 0.5336 - val_mean_pred: 1.0514 - val_mae_t1: 0.0356\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.6203 - mae: 0.4525 - mean_pred: 0.8713 - mae_t1: 0.0302 - val_loss: 4.4769 - val_mae: 0.5596 - val_mean_pred: 0.5536 - val_mae_t1: 0.0373\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 4.0951 - mae: 0.5119 - mean_pred: 0.5096 - mae_t1: 0.0341 - val_loss: 4.3590 - val_mae: 0.5449 - val_mean_pred: 0.5854 - val_mae_t1: 0.0363\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.7926 - mae: 0.4741 - mean_pred: 0.5900 - mae_t1: 0.0316 - val_loss: 3.6470 - val_mae: 0.4559 - val_mean_pred: 0.9099 - val_mae_t1: 0.0304\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.2979 - mae: 0.4122 - mean_pred: 0.8652 - mae_t1: 0.0275 - val_loss: 4.0072 - val_mae: 0.5009 - val_mean_pred: 1.0338 - val_mae_t1: 0.0334\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3813 - mae: 0.4227 - mean_pred: 0.8891 - mae_t1: 0.0282 - val_loss: 3.8438 - val_mae: 0.4805 - val_mean_pred: 0.8183 - val_mae_t1: 0.0320\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 3.3668 - mae: 0.4209 - mean_pred: 0.6784 - mae_t1: 0.0281 - val_loss: 4.0092 - val_mae: 0.5012 - val_mean_pred: 0.7776 - val_mae_t1: 0.0334\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.2040 - mae: 0.4005 - mean_pred: 0.6923 - mae_t1: 0.0267 - val_loss: 3.9061 - val_mae: 0.4883 - val_mean_pred: 0.9372 - val_mae_t1: 0.0326\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.4359 - mae: 0.4295 - mean_pred: 0.8419 - mae_t1: 0.0286 - val_loss: 3.6700 - val_mae: 0.4587 - val_mean_pred: 0.9477 - val_mae_t1: 0.0306\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.2383 - mae: 0.4048 - mean_pred: 0.8328 - mae_t1: 0.0270 - val_loss: 3.6995 - val_mae: 0.4624 - val_mean_pred: 0.9034 - val_mae_t1: 0.0308\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 3.2694 - mae: 0.4087 - mean_pred: 0.8207 - mae_t1: 0.0272 - val_loss: 3.4391 - val_mae: 0.4299 - val_mean_pred: 0.8629 - val_mae_t1: 0.0287\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0473 - mae: 0.3809 - mean_pred: 0.7776 - mae_t1: 0.0254 - val_loss: 3.6313 - val_mae: 0.4539 - val_mean_pred: 0.7809 - val_mae_t1: 0.0303\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 3.1457 - mae: 0.3932 - mean_pred: 0.7183 - mae_t1: 0.0262 - val_loss: 3.9041 - val_mae: 0.4880 - val_mean_pred: 0.8872 - val_mae_t1: 0.0325\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.1800 - mae: 0.3975 - mean_pred: 0.8355 - mae_t1: 0.0265 - val_loss: 4.5799 - val_mae: 0.5725 - val_mean_pred: 1.0421 - val_mae_t1: 0.0382\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3756 - mae: 0.4219 - mean_pred: 0.9015 - mae_t1: 0.0281 - val_loss: 4.3150 - val_mae: 0.5394 - val_mean_pred: 0.9001 - val_mae_t1: 0.0360\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0986 - mae: 0.3873 - mean_pred: 0.7693 - mae_t1: 0.0258 - val_loss: 4.2373 - val_mae: 0.5297 - val_mean_pred: 0.8600 - val_mae_t1: 0.0353\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0708 - mae: 0.3839 - mean_pred: 0.7808 - mae_t1: 0.0256 - val_loss: 4.1080 - val_mae: 0.5135 - val_mean_pred: 0.9061 - val_mae_t1: 0.0342\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.1708 - mae: 0.3963 - mean_pred: 0.8210 - mae_t1: 0.0264 - val_loss: 4.0288 - val_mae: 0.5036 - val_mean_pred: 0.9631 - val_mae_t1: 0.0336\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9318 - mae: 0.3665 - mean_pred: 0.8552 - mae_t1: 0.0244 - val_loss: 4.8062 - val_mae: 0.6008 - val_mean_pred: 1.0626 - val_mae_t1: 0.0401\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 3.5579 - mae: 0.4447 - mean_pred: 0.8755 - mae_t1: 0.0296 - val_loss: 3.9053 - val_mae: 0.4882 - val_mean_pred: 0.9022 - val_mae_t1: 0.0325\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9378 - mae: 0.3672 - mean_pred: 0.7412 - mae_t1: 0.0245 - val_loss: 4.1546 - val_mae: 0.5193 - val_mean_pred: 0.8667 - val_mae_t1: 0.0346\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.6025 - mae: 0.4503 - mean_pred: 0.7773 - mae_t1: 0.0300 - val_loss: 3.5152 - val_mae: 0.4394 - val_mean_pred: 0.8877 - val_mae_t1: 0.0293\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.8665 - mae: 0.3583 - mean_pred: 0.7659 - mae_t1: 0.0239 - val_loss: 4.0657 - val_mae: 0.5082 - val_mean_pred: 0.9366 - val_mae_t1: 0.0339\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0171 - mae: 0.3771 - mean_pred: 0.8226 - mae_t1: 0.0251 - val_loss: 3.7818 - val_mae: 0.4727 - val_mean_pred: 1.0437 - val_mae_t1: 0.0315\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1019 - mae: 0.3877 - mean_pred: 0.9442 - mae_t1: 0.0258 - val_loss: 3.6027 - val_mae: 0.4503 - val_mean_pred: 0.9471 - val_mae_t1: 0.0300\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.7949 - mae: 0.3494 - mean_pred: 0.8137 - mae_t1: 0.0233 - val_loss: 3.9391 - val_mae: 0.4924 - val_mean_pred: 0.8693 - val_mae_t1: 0.0328\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9838 - mae: 0.3730 - mean_pred: 0.7872 - mae_t1: 0.0249 - val_loss: 3.8883 - val_mae: 0.4860 - val_mean_pred: 0.9512 - val_mae_t1: 0.0324\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9896 - mae: 0.3737 - mean_pred: 0.8266 - mae_t1: 0.0249 - val_loss: 3.8940 - val_mae: 0.4867 - val_mean_pred: 0.8995 - val_mae_t1: 0.0324\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9797 - mae: 0.3725 - mean_pred: 0.7997 - mae_t1: 0.0248 - val_loss: 3.7990 - val_mae: 0.4749 - val_mean_pred: 0.8070 - val_mae_t1: 0.0317\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.1405 - mae: 0.3926 - mean_pred: 0.6985 - mae_t1: 0.0262 - val_loss: 3.8690 - val_mae: 0.4836 - val_mean_pred: 0.6727 - val_mae_t1: 0.0322\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4160 - mae: 0.4270 - mean_pred: 0.6127 - mae_t1: 0.0285 - val_loss: 3.8552 - val_mae: 0.4819 - val_mean_pred: 0.7576 - val_mae_t1: 0.0321\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.1452 - mae: 0.3932 - mean_pred: 0.7261 - mae_t1: 0.0262 - val_loss: 4.5617 - val_mae: 0.5702 - val_mean_pred: 1.0719 - val_mae_t1: 0.0380\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 3.7394 - mae: 0.4674 - mean_pred: 0.9621 - mae_t1: 0.0312 - val_loss: 4.4153 - val_mae: 0.5519 - val_mean_pred: 1.0755 - val_mae_t1: 0.0368\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.1953 - mae: 0.3994 - mean_pred: 0.8966 - mae_t1: 0.0266 - val_loss: 3.9566 - val_mae: 0.4946 - val_mean_pred: 0.8525 - val_mae_t1: 0.0330\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.1894 - mae: 0.3987 - mean_pred: 0.7317 - mae_t1: 0.0266 - val_loss: 4.2880 - val_mae: 0.5360 - val_mean_pred: 0.6553 - val_mae_t1: 0.0357\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.6721 - mae: 0.4590 - mean_pred: 0.5847 - mae_t1: 0.0306 - val_loss: 4.5769 - val_mae: 0.5721 - val_mean_pred: 0.6013 - val_mae_t1: 0.0381\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.9329 - mae: 0.4916 - mean_pred: 0.5672 - mae_t1: 0.0328 - val_loss: 4.3256 - val_mae: 0.5407 - val_mean_pred: 0.7631 - val_mae_t1: 0.0360\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.8281 - mae: 0.4785 - mean_pred: 0.7414 - mae_t1: 0.0319 - val_loss: 3.7782 - val_mae: 0.4723 - val_mean_pred: 0.9640 - val_mae_t1: 0.0315\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1721 - mae: 0.3965 - mean_pred: 0.8466 - mae_t1: 0.0264 - val_loss: 5.0430 - val_mae: 0.6304 - val_mean_pred: 1.0356 - val_mae_t1: 0.0420\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.9711 - mae: 0.4964 - mean_pred: 0.9253 - mae_t1: 0.0331 - val_loss: 3.4954 - val_mae: 0.4369 - val_mean_pred: 0.9432 - val_mae_t1: 0.0291\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.9755 - mae: 0.3719 - mean_pred: 0.8044 - mae_t1: 0.0248 - val_loss: 3.5273 - val_mae: 0.4409 - val_mean_pred: 0.7738 - val_mae_t1: 0.0294\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4286 - mae: 0.4286 - mean_pred: 0.7173 - mae_t1: 0.0286 - val_loss: 3.5935 - val_mae: 0.4492 - val_mean_pred: 0.8049 - val_mae_t1: 0.0299\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0863 - mae: 0.3858 - mean_pred: 0.7373 - mae_t1: 0.0257 - val_loss: 3.9423 - val_mae: 0.4928 - val_mean_pred: 0.8806 - val_mae_t1: 0.0329\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9840 - mae: 0.3730 - mean_pred: 0.7475 - mae_t1: 0.0249 - val_loss: 3.5851 - val_mae: 0.4481 - val_mean_pred: 0.9162 - val_mae_t1: 0.0299\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.2263 - mae: 0.4033 - mean_pred: 0.8405 - mae_t1: 0.0269 - val_loss: 4.0281 - val_mae: 0.5035 - val_mean_pred: 0.9907 - val_mae_t1: 0.0336\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0196 - mae: 0.3775 - mean_pred: 0.8337 - mae_t1: 0.0252 - val_loss: 3.7843 - val_mae: 0.4730 - val_mean_pred: 0.8878 - val_mae_t1: 0.0315\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.8763 - mae: 0.3595 - mean_pred: 0.7487 - mae_t1: 0.0240 - val_loss: 3.7147 - val_mae: 0.4643 - val_mean_pred: 0.8276 - val_mae_t1: 0.0310\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0593 - mae: 0.3824 - mean_pred: 0.7085 - mae_t1: 0.0255 - val_loss: 3.6684 - val_mae: 0.4586 - val_mean_pred: 0.7993 - val_mae_t1: 0.0306\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.8757 - mae: 0.3595 - mean_pred: 0.7360 - mae_t1: 0.0240 - val_loss: 3.8041 - val_mae: 0.4755 - val_mean_pred: 1.0052 - val_mae_t1: 0.0317\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 2.8486 - mae: 0.3561 - mean_pred: 0.9025 - mae_t1: 0.0237 - val_loss: 4.0842 - val_mae: 0.5105 - val_mean_pred: 1.0499 - val_mae_t1: 0.0340\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.8895 - mae: 0.3612 - mean_pred: 0.8911 - mae_t1: 0.0241 - val_loss: 3.7700 - val_mae: 0.4713 - val_mean_pred: 0.8565 - val_mae_t1: 0.0314\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 2.7840 - mae: 0.3480 - mean_pred: 0.7476 - mae_t1: 0.0232 - val_loss: 3.8688 - val_mae: 0.4836 - val_mean_pred: 0.8308 - val_mae_t1: 0.0322\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7823 - mae: 0.3478 - mean_pred: 0.7420 - mae_t1: 0.0232 - val_loss: 3.8087 - val_mae: 0.4761 - val_mean_pred: 0.9337 - val_mae_t1: 0.0317\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9101 - mae: 0.3638 - mean_pred: 0.8797 - mae_t1: 0.0243 - val_loss: 4.1559 - val_mae: 0.5195 - val_mean_pred: 1.0185 - val_mae_t1: 0.0346\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0369 - mae: 0.3796 - mean_pred: 0.9394 - mae_t1: 0.0253 - val_loss: 4.1013 - val_mae: 0.5127 - val_mean_pred: 1.0125 - val_mae_t1: 0.0342\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.3897 - mae: 0.4237 - mean_pred: 0.9453 - mae_t1: 0.0282 - val_loss: 4.1198 - val_mae: 0.5150 - val_mean_pred: 1.0068 - val_mae_t1: 0.0343\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 3.2403 - mae: 0.4050 - mean_pred: 0.9178 - mae_t1: 0.0270 - val_loss: 3.5971 - val_mae: 0.4496 - val_mean_pred: 0.8561 - val_mae_t1: 0.0300\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.1250 - mae: 0.3906 - mean_pred: 0.8084 - mae_t1: 0.0260 - val_loss: 3.6607 - val_mae: 0.4576 - val_mean_pred: 0.7977 - val_mae_t1: 0.0305\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.8116 - mae: 0.3514 - mean_pred: 0.7360 - mae_t1: 0.0234 - val_loss: 4.1643 - val_mae: 0.5205 - val_mean_pred: 0.9376 - val_mae_t1: 0.0347\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4826 - mae: 0.4353 - mean_pred: 0.8193 - mae_t1: 0.0290 - val_loss: 3.8293 - val_mae: 0.4787 - val_mean_pred: 0.9947 - val_mae_t1: 0.0319\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9236 - mae: 0.3654 - mean_pred: 0.8469 - mae_t1: 0.0244 - val_loss: 4.4071 - val_mae: 0.5509 - val_mean_pred: 0.8421 - val_mae_t1: 0.0367\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.7309 - mae: 0.4664 - mean_pred: 0.7406 - mae_t1: 0.0311 - val_loss: 4.6907 - val_mae: 0.5863 - val_mean_pred: 0.6472 - val_mae_t1: 0.0391\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.8184 - mae: 0.4773 - mean_pred: 0.5967 - mae_t1: 0.0318 - val_loss: 4.3974 - val_mae: 0.5497 - val_mean_pred: 0.8126 - val_mae_t1: 0.0366\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2186 - mae: 0.4023 - mean_pred: 0.7383 - mae_t1: 0.0268 - val_loss: 5.0633 - val_mae: 0.6329 - val_mean_pred: 1.2373 - val_mae_t1: 0.0422\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 4.0800 - mae: 0.5100 - mean_pred: 1.1022 - mae_t1: 0.0340 - val_loss: 5.5773 - val_mae: 0.6972 - val_mean_pred: 1.3020 - val_mae_t1: 0.0465\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 4.0106 - mae: 0.5013 - mean_pred: 1.0697 - mae_t1: 0.0334 - val_loss: 3.9875 - val_mae: 0.4984 - val_mean_pred: 0.8608 - val_mae_t1: 0.0332\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9389 - mae: 0.3674 - mean_pred: 0.7135 - mae_t1: 0.0245 - val_loss: 4.0668 - val_mae: 0.5084 - val_mean_pred: 0.6593 - val_mae_t1: 0.0339\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.2196 - mae: 0.4025 - mean_pred: 0.6084 - mae_t1: 0.0268 - val_loss: 3.8378 - val_mae: 0.4797 - val_mean_pred: 0.7906 - val_mae_t1: 0.0320\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9157 - mae: 0.3645 - mean_pred: 0.7779 - mae_t1: 0.0243 - val_loss: 3.9468 - val_mae: 0.4934 - val_mean_pred: 0.9644 - val_mae_t1: 0.0329\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9347 - mae: 0.3668 - mean_pred: 0.8789 - mae_t1: 0.0245 - val_loss: 3.7759 - val_mae: 0.4720 - val_mean_pred: 0.8067 - val_mae_t1: 0.0315\n",
      "Earliness...\n",
      "0.002000093460083008\n",
      "____________________________________________________________\n",
      "Test MAE:      0.34127130021361096  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▅▆▄▄▃▃▂▄▃▄▃▂▂▂▂▂▁▁▁▁▁▂▂▃▂▂▁▁▁▁▁▂▁▃▂▁▁</td></tr><tr><td>mae</td><td>█▆▄▅▆▄▄▃▃▂▄▃▄▃▂▂▂▂▂▁▁▁▁▁▂▂▃▂▂▁▁▁▁▁▂▁▃▂▁▁</td></tr><tr><td>mae_t1</td><td>█▆▄▅▆▄▄▃▃▂▄▃▄▃▂▂▂▂▂▁▁▁▁▁▂▂▃▂▂▁▁▁▁▁▂▁▃▂▁▁</td></tr><tr><td>mean_pred</td><td>▁█▆▅▃█▅▇▆▆▆▅█▄▇▆▆▆▆▆▅▆▆▆▄▇▄▆▅▅▅▅▅▇▇▅▅▅▅▇</td></tr><tr><td>val_loss</td><td>▆▆▅▄▄▄▄▄▃▂▃▆▅▂▃▂▁▆▄▇▄▂▃▃▃▃▅█▂▂▂▃▃▄▂▄▆█▄▂</td></tr><tr><td>val_mae</td><td>▆▆▅▄▄▄▄▄▃▂▃▆▅▂▃▂▁▆▄▇▄▂▃▃▃▃▅█▂▂▂▃▃▄▂▄▆█▄▂</td></tr><tr><td>val_mae_t1</td><td>▆▆▅▄▄▄▄▄▃▂▃▆▅▂▃▂▁▆▄▇▄▂▃▃▃▃▅█▂▂▂▃▃▄▂▄▆█▄▂</td></tr><tr><td>val_mean_pred</td><td>▆▁▄▃▂▆▅▆▆▄▆▂▆▅▄▅▄▆▄▆▄▆▄▄▃▄▃▆▄▅▄▆▄▆▄▅▂█▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.4036</td></tr><tr><td>AE_2</td><td>0.29657</td></tr><tr><td>AE_3</td><td>0.33171</td></tr><tr><td>MAE</td><td>0.34127</td></tr><tr><td>best_epoch</td><td>40</td></tr><tr><td>best_val_loss</td><td>3.43907</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.93466</td></tr><tr><td>mae</td><td>0.36683</td></tr><tr><td>mae_t1</td><td>0.02446</td></tr><tr><td>mean_pred</td><td>0.8789</td></tr><tr><td>val_loss</td><td>3.77593</td></tr><tr><td>val_mae</td><td>0.47199</td></tr><tr><td>val_mae_t1</td><td>0.03147</td></tr><tr><td>val_mean_pred</td><td>0.80666</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">distinctive-fire-64</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fb4v0hu\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fb4v0hu</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154603-2fb4v0hu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154625-1lxv8k14</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lxv8k14\" target=\"_blank\">copper-snowball-65</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 867us/sample - loss: 8.9297 - mae: 0.7176 - mean_pred: 0.5740 - mae_t1: 0.0478 - val_loss: 6.1474 - val_mae: 0.4940 - val_mean_pred: 0.9851 - val_mae_t1: 0.0329\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 285us/sample - loss: 6.9620 - mae: 0.5595 - mean_pred: 0.6988 - mae_t1: 0.0373 - val_loss: 6.5316 - val_mae: 0.5249 - val_mean_pred: 0.8734 - val_mae_t1: 0.0350\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 247us/sample - loss: 6.4974 - mae: 0.5221 - mean_pred: 0.8720 - mae_t1: 0.0348 - val_loss: 7.5968 - val_mae: 0.6105 - val_mean_pred: 0.5798 - val_mae_t1: 0.0407\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 7.4300 - mae: 0.5971 - mean_pred: 0.4862 - mae_t1: 0.0398 - val_loss: 6.6400 - val_mae: 0.5336 - val_mean_pred: 0.9032 - val_mae_t1: 0.0356\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 7.3135 - mae: 0.5877 - mean_pred: 1.0013 - mae_t1: 0.0392 - val_loss: 7.0679 - val_mae: 0.5680 - val_mean_pred: 0.8655 - val_mae_t1: 0.0379\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 6.1411 - mae: 0.4935 - mean_pred: 0.7575 - mae_t1: 0.0329 - val_loss: 6.6677 - val_mae: 0.5358 - val_mean_pred: 0.8184 - val_mae_t1: 0.0357\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5207 - mae: 0.4436 - mean_pred: 0.7382 - mae_t1: 0.0296 - val_loss: 7.2385 - val_mae: 0.5817 - val_mean_pred: 1.1353 - val_mae_t1: 0.0388\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 7.0169 - mae: 0.5639 - mean_pred: 1.0600 - mae_t1: 0.0376 - val_loss: 6.3380 - val_mae: 0.5093 - val_mean_pred: 0.6741 - val_mae_t1: 0.0340\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 6.4340 - mae: 0.5170 - mean_pred: 0.6519 - mae_t1: 0.0345 - val_loss: 5.2429 - val_mae: 0.4213 - val_mean_pred: 0.8853 - val_mae_t1: 0.0281\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5959 - mae: 0.4497 - mean_pred: 0.8044 - mae_t1: 0.0300 - val_loss: 6.3708 - val_mae: 0.5119 - val_mean_pred: 1.1322 - val_mae_t1: 0.0341\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.2987 - mae: 0.5061 - mean_pred: 0.9650 - mae_t1: 0.0337 - val_loss: 5.8932 - val_mae: 0.4736 - val_mean_pred: 0.7628 - val_mae_t1: 0.0316\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.8921 - mae: 0.4735 - mean_pred: 0.7691 - mae_t1: 0.0316 - val_loss: 7.1371 - val_mae: 0.5735 - val_mean_pred: 0.8008 - val_mae_t1: 0.0382\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.5088 - mae: 0.6034 - mean_pred: 0.7616 - mae_t1: 0.0402 - val_loss: 5.8457 - val_mae: 0.4697 - val_mean_pred: 0.9745 - val_mae_t1: 0.0313\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.4057 - mae: 0.5147 - mean_pred: 0.8326 - mae_t1: 0.0343 - val_loss: 6.3411 - val_mae: 0.5096 - val_mean_pred: 0.7699 - val_mae_t1: 0.0340\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 6.2772 - mae: 0.5044 - mean_pred: 0.7798 - mae_t1: 0.0336 - val_loss: 5.8743 - val_mae: 0.4720 - val_mean_pred: 0.9644 - val_mae_t1: 0.0315\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.2079 - mae: 0.4185 - mean_pred: 0.7333 - mae_t1: 0.0279 - val_loss: 6.1129 - val_mae: 0.4912 - val_mean_pred: 0.7459 - val_mae_t1: 0.0327\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.3776 - mae: 0.4321 - mean_pred: 0.7000 - mae_t1: 0.0288 - val_loss: 6.2838 - val_mae: 0.5049 - val_mean_pred: 0.9404 - val_mae_t1: 0.0337\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 5.2697 - mae: 0.4235 - mean_pred: 0.8787 - mae_t1: 0.0282 - val_loss: 5.9550 - val_mae: 0.4785 - val_mean_pred: 0.8252 - val_mae_t1: 0.0319\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.0553 - mae: 0.4062 - mean_pred: 0.7410 - mae_t1: 0.0271 - val_loss: 6.1348 - val_mae: 0.4930 - val_mean_pred: 0.9603 - val_mae_t1: 0.0329\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0726 - mae: 0.4076 - mean_pred: 0.8998 - mae_t1: 0.0272 - val_loss: 5.3640 - val_mae: 0.4310 - val_mean_pred: 0.7872 - val_mae_t1: 0.0287\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.9810 - mae: 0.4003 - mean_pred: 0.6835 - mae_t1: 0.0267 - val_loss: 6.5474 - val_mae: 0.5261 - val_mean_pred: 0.9904 - val_mae_t1: 0.0351\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.3979 - mae: 0.4338 - mean_pred: 0.9174 - mae_t1: 0.0289 - val_loss: 5.6011 - val_mae: 0.4501 - val_mean_pred: 0.7714 - val_mae_t1: 0.0300\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 5.4558 - mae: 0.4384 - mean_pred: 0.6289 - mae_t1: 0.0292 - val_loss: 6.4713 - val_mae: 0.5200 - val_mean_pred: 0.7514 - val_mae_t1: 0.0347\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 5.0222 - mae: 0.4036 - mean_pred: 0.8414 - mae_t1: 0.0269 - val_loss: 5.9043 - val_mae: 0.4745 - val_mean_pred: 0.8837 - val_mae_t1: 0.0316\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.4759 - mae: 0.4400 - mean_pred: 0.6453 - mae_t1: 0.0293 - val_loss: 6.8291 - val_mae: 0.5488 - val_mean_pred: 0.5923 - val_mae_t1: 0.0366\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.1187 - mae: 0.4917 - mean_pred: 0.6596 - mae_t1: 0.0328 - val_loss: 6.6423 - val_mae: 0.5338 - val_mean_pred: 0.8715 - val_mae_t1: 0.0356\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.6648 - mae: 0.4552 - mean_pred: 0.9010 - mae_t1: 0.0303 - val_loss: 6.6664 - val_mae: 0.5357 - val_mean_pred: 0.9377 - val_mae_t1: 0.0357\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.3039 - mae: 0.5066 - mean_pred: 0.7043 - mae_t1: 0.0338 - val_loss: 7.2932 - val_mae: 0.5861 - val_mean_pred: 0.5882 - val_mae_t1: 0.0391\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.1196 - mae: 0.4114 - mean_pred: 0.6783 - mae_t1: 0.0274 - val_loss: 6.3635 - val_mae: 0.5114 - val_mean_pred: 0.9991 - val_mae_t1: 0.0341\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.6952 - mae: 0.4577 - mean_pred: 0.7951 - mae_t1: 0.0305 - val_loss: 6.3697 - val_mae: 0.5119 - val_mean_pred: 0.7099 - val_mae_t1: 0.0341\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.6122 - mae: 0.3706 - mean_pred: 0.7521 - mae_t1: 0.0247 - val_loss: 7.0108 - val_mae: 0.5634 - val_mean_pred: 1.0837 - val_mae_t1: 0.0376\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0166 - mae: 0.4031 - mean_pred: 0.8933 - mae_t1: 0.0269 - val_loss: 6.0169 - val_mae: 0.4835 - val_mean_pred: 0.8580 - val_mae_t1: 0.0322\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.4417 - mae: 0.3569 - mean_pred: 0.7632 - mae_t1: 0.0238 - val_loss: 5.7145 - val_mae: 0.4592 - val_mean_pred: 0.8481 - val_mae_t1: 0.0306\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.5213 - mae: 0.3633 - mean_pred: 0.8065 - mae_t1: 0.0242 - val_loss: 5.4323 - val_mae: 0.4365 - val_mean_pred: 0.8757 - val_mae_t1: 0.0291\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.2097 - mae: 0.3383 - mean_pred: 0.7510 - mae_t1: 0.0226 - val_loss: 6.1466 - val_mae: 0.4939 - val_mean_pred: 0.8652 - val_mae_t1: 0.0329\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 4.9441 - mae: 0.3973 - mean_pred: 0.7599 - mae_t1: 0.0265 - val_loss: 5.5639 - val_mae: 0.4471 - val_mean_pred: 0.9244 - val_mae_t1: 0.0298\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 4.5205 - mae: 0.3633 - mean_pred: 0.8134 - mae_t1: 0.0242 - val_loss: 5.1784 - val_mae: 0.4161 - val_mean_pred: 0.8371 - val_mae_t1: 0.0277\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 4.4130 - mae: 0.3546 - mean_pred: 0.7275 - mae_t1: 0.0236 - val_loss: 4.7647 - val_mae: 0.3829 - val_mean_pred: 0.7845 - val_mae_t1: 0.0255\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.5028 - mae: 0.3618 - mean_pred: 0.6968 - mae_t1: 0.0241 - val_loss: 5.8212 - val_mae: 0.4678 - val_mean_pred: 0.9531 - val_mae_t1: 0.0312\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 4.5121 - mae: 0.3626 - mean_pred: 0.8878 - mae_t1: 0.0242 - val_loss: 6.2163 - val_mae: 0.4995 - val_mean_pred: 1.0083 - val_mae_t1: 0.0333\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.1380 - mae: 0.3325 - mean_pred: 0.8597 - mae_t1: 0.0222 - val_loss: 5.7395 - val_mae: 0.4612 - val_mean_pred: 0.8111 - val_mae_t1: 0.0307\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.1294 - mae: 0.3318 - mean_pred: 0.7309 - mae_t1: 0.0221 - val_loss: 5.7300 - val_mae: 0.4604 - val_mean_pred: 0.9312 - val_mae_t1: 0.0307\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.8231 - mae: 0.3072 - mean_pred: 0.7974 - mae_t1: 0.0205 - val_loss: 5.4280 - val_mae: 0.4362 - val_mean_pred: 0.8379 - val_mae_t1: 0.0291\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.7507 - mae: 0.3014 - mean_pred: 0.7378 - mae_t1: 0.0201 - val_loss: 5.2316 - val_mae: 0.4204 - val_mean_pred: 0.9067 - val_mae_t1: 0.0280\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7535 - mae: 0.3016 - mean_pred: 0.7764 - mae_t1: 0.0201 - val_loss: 6.0109 - val_mae: 0.4830 - val_mean_pred: 0.8971 - val_mae_t1: 0.0322\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.9983 - mae: 0.3213 - mean_pred: 0.7855 - mae_t1: 0.0214 - val_loss: 6.6782 - val_mae: 0.5366 - val_mean_pred: 1.0714 - val_mae_t1: 0.0358\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 5.1282 - mae: 0.4121 - mean_pred: 0.9522 - mae_t1: 0.0275 - val_loss: 6.7795 - val_mae: 0.5448 - val_mean_pred: 0.8552 - val_mae_t1: 0.0363\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.6364 - mae: 0.3726 - mean_pred: 0.7141 - mae_t1: 0.0248 - val_loss: 6.0806 - val_mae: 0.4886 - val_mean_pred: 0.9412 - val_mae_t1: 0.0326\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.1096 - mae: 0.3302 - mean_pred: 0.8407 - mae_t1: 0.0220 - val_loss: 5.5305 - val_mae: 0.4444 - val_mean_pred: 0.8852 - val_mae_t1: 0.0296\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 4.2900 - mae: 0.3447 - mean_pred: 0.7912 - mae_t1: 0.0230 - val_loss: 6.8232 - val_mae: 0.5483 - val_mean_pred: 0.9192 - val_mae_t1: 0.0366\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 4.7558 - mae: 0.3822 - mean_pred: 0.8056 - mae_t1: 0.0255 - val_loss: 5.9674 - val_mae: 0.4795 - val_mean_pred: 0.9634 - val_mae_t1: 0.0320\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.1849 - mae: 0.3363 - mean_pred: 0.8277 - mae_t1: 0.0224 - val_loss: 5.7510 - val_mae: 0.4621 - val_mean_pred: 0.8532 - val_mae_t1: 0.0308\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.9998 - mae: 0.3214 - mean_pred: 0.7112 - mae_t1: 0.0214 - val_loss: 6.0384 - val_mae: 0.4852 - val_mean_pred: 0.7895 - val_mae_t1: 0.0323\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.0866 - mae: 0.3284 - mean_pred: 0.7283 - mae_t1: 0.0219 - val_loss: 6.1173 - val_mae: 0.4916 - val_mean_pred: 0.9301 - val_mae_t1: 0.0328\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 3.7851 - mae: 0.3042 - mean_pred: 0.8019 - mae_t1: 0.0203 - val_loss: 6.0435 - val_mae: 0.4856 - val_mean_pred: 0.9179 - val_mae_t1: 0.0324\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.5804 - mae: 0.2877 - mean_pred: 0.8411 - mae_t1: 0.0192 - val_loss: 6.2648 - val_mae: 0.5034 - val_mean_pred: 1.0285 - val_mae_t1: 0.0336\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.8909 - mae: 0.3127 - mean_pred: 0.8984 - mae_t1: 0.0208 - val_loss: 6.0903 - val_mae: 0.4894 - val_mean_pred: 0.8169 - val_mae_t1: 0.0326\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.0368 - mae: 0.3244 - mean_pred: 0.7102 - mae_t1: 0.0216 - val_loss: 5.8427 - val_mae: 0.4695 - val_mean_pred: 0.9366 - val_mae_t1: 0.0313\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7276 - mae: 0.2995 - mean_pred: 0.8770 - mae_t1: 0.0200 - val_loss: 5.8966 - val_mae: 0.4738 - val_mean_pred: 0.9056 - val_mae_t1: 0.0316\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.5882 - mae: 0.2883 - mean_pred: 0.7698 - mae_t1: 0.0192 - val_loss: 6.0849 - val_mae: 0.4890 - val_mean_pred: 0.8165 - val_mae_t1: 0.0326\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 4.2043 - mae: 0.3378 - mean_pred: 0.7991 - mae_t1: 0.0225 - val_loss: 6.6134 - val_mae: 0.5314 - val_mean_pred: 0.8984 - val_mae_t1: 0.0354\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.9743 - mae: 0.3194 - mean_pred: 0.8141 - mae_t1: 0.0213 - val_loss: 6.6686 - val_mae: 0.5359 - val_mean_pred: 0.7895 - val_mae_t1: 0.0357\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0860 - mae: 0.4087 - mean_pred: 0.6589 - mae_t1: 0.0272 - val_loss: 7.1859 - val_mae: 0.5774 - val_mean_pred: 0.8644 - val_mae_t1: 0.0385\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.9972 - mae: 0.4016 - mean_pred: 0.8691 - mae_t1: 0.0268 - val_loss: 7.0140 - val_mae: 0.5636 - val_mean_pred: 1.0266 - val_mae_t1: 0.0376\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.0237 - mae: 0.3233 - mean_pred: 0.8098 - mae_t1: 0.0216 - val_loss: 6.2994 - val_mae: 0.5062 - val_mean_pred: 0.7659 - val_mae_t1: 0.0337\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.7859 - mae: 0.3042 - mean_pred: 0.7838 - mae_t1: 0.0203 - val_loss: 6.3058 - val_mae: 0.5067 - val_mean_pred: 0.9117 - val_mae_t1: 0.0338\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 4.0521 - mae: 0.3256 - mean_pred: 0.7649 - mae_t1: 0.0217 - val_loss: 6.3407 - val_mae: 0.5095 - val_mean_pred: 0.8309 - val_mae_t1: 0.0340\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.1336 - mae: 0.3322 - mean_pred: 0.7721 - mae_t1: 0.0221 - val_loss: 8.0594 - val_mae: 0.6476 - val_mean_pred: 0.9266 - val_mae_t1: 0.0432\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.0014 - mae: 0.4019 - mean_pred: 0.7666 - mae_t1: 0.0268 - val_loss: 6.2264 - val_mae: 0.5003 - val_mean_pred: 0.8662 - val_mae_t1: 0.0334\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.0342 - mae: 0.3242 - mean_pred: 0.8094 - mae_t1: 0.0216 - val_loss: 6.4015 - val_mae: 0.5144 - val_mean_pred: 0.9309 - val_mae_t1: 0.0343\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.7200 - mae: 0.2989 - mean_pred: 0.8062 - mae_t1: 0.0199 - val_loss: 6.3768 - val_mae: 0.5124 - val_mean_pred: 0.8595 - val_mae_t1: 0.0342\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7307 - mae: 0.2998 - mean_pred: 0.7554 - mae_t1: 0.0200 - val_loss: 5.9994 - val_mae: 0.4821 - val_mean_pred: 0.9717 - val_mae_t1: 0.0321\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.6885 - mae: 0.2964 - mean_pred: 0.8596 - mae_t1: 0.0198 - val_loss: 6.0530 - val_mae: 0.4864 - val_mean_pred: 0.8076 - val_mae_t1: 0.0324\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7042 - mae: 0.2977 - mean_pred: 0.7662 - mae_t1: 0.0198 - val_loss: 5.8820 - val_mae: 0.4727 - val_mean_pred: 0.9999 - val_mae_t1: 0.0315\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.8942 - mae: 0.3129 - mean_pred: 0.9075 - mae_t1: 0.0209 - val_loss: 6.0345 - val_mae: 0.4849 - val_mean_pred: 0.8451 - val_mae_t1: 0.0323\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.9007 - mae: 0.3134 - mean_pred: 0.6937 - mae_t1: 0.0209 - val_loss: 6.2386 - val_mae: 0.5013 - val_mean_pred: 0.8964 - val_mae_t1: 0.0334\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.8625 - mae: 0.3104 - mean_pred: 0.8382 - mae_t1: 0.0207 - val_loss: 6.4481 - val_mae: 0.5182 - val_mean_pred: 0.9092 - val_mae_t1: 0.0345\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.8342 - mae: 0.3081 - mean_pred: 0.7690 - mae_t1: 0.0205 - val_loss: 5.8836 - val_mae: 0.4728 - val_mean_pred: 0.8971 - val_mae_t1: 0.0315\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.9883 - mae: 0.3205 - mean_pred: 0.8246 - mae_t1: 0.0214 - val_loss: 6.4573 - val_mae: 0.5189 - val_mean_pred: 1.0272 - val_mae_t1: 0.0346\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8530 - mae: 0.3096 - mean_pred: 0.8984 - mae_t1: 0.0206 - val_loss: 5.4276 - val_mae: 0.4361 - val_mean_pred: 0.8047 - val_mae_t1: 0.0291\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6695 - mae: 0.2949 - mean_pred: 0.7561 - mae_t1: 0.0197 - val_loss: 5.6500 - val_mae: 0.4540 - val_mean_pred: 0.9205 - val_mae_t1: 0.0303\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.1235 - mae: 0.3314 - mean_pred: 0.8772 - mae_t1: 0.0221 - val_loss: 6.0245 - val_mae: 0.4841 - val_mean_pred: 0.9952 - val_mae_t1: 0.0323\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 3.7591 - mae: 0.3021 - mean_pred: 0.8350 - mae_t1: 0.0201 - val_loss: 5.9586 - val_mae: 0.4788 - val_mean_pred: 0.7799 - val_mae_t1: 0.0319\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8325 - mae: 0.3080 - mean_pred: 0.6763 - mae_t1: 0.0205 - val_loss: 6.0101 - val_mae: 0.4830 - val_mean_pred: 0.9720 - val_mae_t1: 0.0322\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7210 - mae: 0.2990 - mean_pred: 0.8393 - mae_t1: 0.0199 - val_loss: 6.0752 - val_mae: 0.4882 - val_mean_pred: 0.8416 - val_mae_t1: 0.0325\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.6152 - mae: 0.2905 - mean_pred: 0.7666 - mae_t1: 0.0194 - val_loss: 6.3813 - val_mae: 0.5128 - val_mean_pred: 0.8510 - val_mae_t1: 0.0342\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.6042 - mae: 0.2896 - mean_pred: 0.7563 - mae_t1: 0.0193 - val_loss: 6.2928 - val_mae: 0.5057 - val_mean_pred: 0.9797 - val_mae_t1: 0.0337\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.5608 - mae: 0.2861 - mean_pred: 0.8439 - mae_t1: 0.0191 - val_loss: 5.9382 - val_mae: 0.4772 - val_mean_pred: 1.0171 - val_mae_t1: 0.0318\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8317 - mae: 0.3079 - mean_pred: 0.9335 - mae_t1: 0.0205 - val_loss: 7.0285 - val_mae: 0.5648 - val_mean_pred: 0.8653 - val_mae_t1: 0.0377\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.2851 - mae: 0.3443 - mean_pred: 0.6886 - mae_t1: 0.0230 - val_loss: 6.1513 - val_mae: 0.4943 - val_mean_pred: 0.8400 - val_mae_t1: 0.0330\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.1056 - mae: 0.3299 - mean_pred: 0.8336 - mae_t1: 0.0220 - val_loss: 6.9082 - val_mae: 0.5551 - val_mean_pred: 1.0698 - val_mae_t1: 0.0370\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.9416 - mae: 0.3167 - mean_pred: 0.8374 - mae_t1: 0.0211 - val_loss: 6.2076 - val_mae: 0.4988 - val_mean_pred: 0.7947 - val_mae_t1: 0.0333\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.9587 - mae: 0.3181 - mean_pred: 0.7263 - mae_t1: 0.0212 - val_loss: 6.3666 - val_mae: 0.5116 - val_mean_pred: 0.9612 - val_mae_t1: 0.0341\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.3333 - mae: 0.3482 - mean_pred: 0.9146 - mae_t1: 0.0232 - val_loss: 6.5629 - val_mae: 0.5274 - val_mean_pred: 0.8202 - val_mae_t1: 0.0352\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8459 - mae: 0.3090 - mean_pred: 0.6812 - mae_t1: 0.0206 - val_loss: 6.2861 - val_mae: 0.5051 - val_mean_pred: 0.9313 - val_mae_t1: 0.0337\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6442 - mae: 0.2928 - mean_pred: 0.8370 - mae_t1: 0.0195 - val_loss: 6.3109 - val_mae: 0.5071 - val_mean_pred: 0.9318 - val_mae_t1: 0.0338\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 4.5219 - mae: 0.3634 - mean_pred: 0.7501 - mae_t1: 0.0242 - val_loss: 6.6466 - val_mae: 0.5341 - val_mean_pred: 0.7029 - val_mae_t1: 0.0356\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 4.1737 - mae: 0.3354 - mean_pred: 0.6847 - mae_t1: 0.0224 - val_loss: 6.9191 - val_mae: 0.5560 - val_mean_pred: 1.0847 - val_mae_t1: 0.0371\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 5.5234 - mae: 0.4438 - mean_pred: 1.0695 - mae_t1: 0.0296 - val_loss: 7.3367 - val_mae: 0.5896 - val_mean_pred: 1.0866 - val_mae_t1: 0.0393\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 4.0439 - mae: 0.3250 - mean_pred: 0.7944 - mae_t1: 0.0217 - val_loss: 6.0467 - val_mae: 0.4859 - val_mean_pred: 0.7609 - val_mae_t1: 0.0324\n",
      "Earliness...\n",
      "0.0015001296997070312\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3311492729930615  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▅▅▆▃▃▃▃▄▅▂▂▃▂▂▁▂▂▃▂▁▁▂▃▂▃▁▁▁▂▂▁▁▁▁▂▂▂</td></tr><tr><td>mae</td><td>█▅▄▅▅▆▃▃▃▃▄▅▂▂▃▂▂▁▂▂▃▂▁▁▂▃▂▃▁▁▁▂▂▁▁▁▁▂▂▂</td></tr><tr><td>mae_t1</td><td>█▅▄▅▅▆▃▃▃▃▄▅▂▂▃▂▂▁▂▂▃▂▁▁▂▃▂▃▁▁▁▂▂▁▁▁▁▂▂▂</td></tr><tr><td>mean_pred</td><td>▁▅▄█▇▄▃▅▃▂▂▃▄▄▄▃▅▃▄▅▄▃▅▅▄▅▄▄▄▄▅▅▅▂▄▆▅▆▄▄</td></tr><tr><td>val_loss</td><td>▄█▅▄▃▃▄▃▅▅▅▇▆▂▂▃▃▁▅▂▃▄▄▃▅▆▄▄▃▃▅▅▃▃▄▆▄▅▅▃</td></tr><tr><td>val_mae</td><td>▄█▅▄▃▃▄▃▅▅▅▇▆▂▂▃▃▁▅▂▃▄▄▃▅▆▄▄▃▃▅▅▃▃▄▆▄▅▅▃</td></tr><tr><td>val_mae_t1</td><td>▄█▅▄▃▃▄▃▅▅▅▇▆▂▂▃▃▁▅▂▃▄▄▃▅▆▄▄▃▃▅▅▃▃▄▆▄▅▅▃</td></tr><tr><td>val_mean_pred</td><td>▇▁▄▂▄▆▃▄▇▃▅▁█▅▆▆▄▆█▅▆▆▇▆▅▇▄▅▆▇▆▇▇▆▇▅▄▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.40448</td></tr><tr><td>AE_2</td><td>0.32279</td></tr><tr><td>AE_3</td><td>0.30933</td></tr><tr><td>MAE</td><td>0.33115</td></tr><tr><td>best_epoch</td><td>37</td></tr><tr><td>best_val_loss</td><td>4.76472</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>4.0439</td></tr><tr><td>mae</td><td>0.32496</td></tr><tr><td>mae_t1</td><td>0.02166</td></tr><tr><td>mean_pred</td><td>0.79441</td></tr><tr><td>val_loss</td><td>6.04672</td></tr><tr><td>val_mae</td><td>0.4859</td></tr><tr><td>val_mae_t1</td><td>0.03239</td></tr><tr><td>val_mean_pred</td><td>0.76094</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">copper-snowball-65</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lxv8k14\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1lxv8k14</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154625-1lxv8k14\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154653-xwqursu8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/xwqursu8\" target=\"_blank\">youthful-snowball-66</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 695us/sample - loss: 11.3566 - mae: 0.9126 - mean_pred: 0.1872 - mae_t1: 0.0608 - val_loss: 6.3769 - val_mae: 0.5124 - val_mean_pred: 0.9996 - val_mae_t1: 0.0342\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 9.1358 - mae: 0.7341 - mean_pred: 1.1246 - mae_t1: 0.0489 - val_loss: 5.8081 - val_mae: 0.4667 - val_mean_pred: 0.8790 - val_mae_t1: 0.0311\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 141us/sample - loss: 7.1345 - mae: 0.5733 - mean_pred: 0.7235 - mae_t1: 0.0382 - val_loss: 8.6224 - val_mae: 0.6929 - val_mean_pred: 0.3937 - val_mae_t1: 0.0462\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 7.4262 - mae: 0.5968 - mean_pred: 0.4561 - mae_t1: 0.0398 - val_loss: 7.1245 - val_mae: 0.5725 - val_mean_pred: 1.0699 - val_mae_t1: 0.0382\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 7.9975 - mae: 0.6427 - mean_pred: 1.1101 - mae_t1: 0.0428 - val_loss: 7.0662 - val_mae: 0.5678 - val_mean_pred: 0.6378 - val_mae_t1: 0.0379\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 7.4717 - mae: 0.6004 - mean_pred: 0.4721 - mae_t1: 0.0400 - val_loss: 9.5925 - val_mae: 0.7708 - val_mean_pred: 0.2443 - val_mae_t1: 0.0514\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 135us/sample - loss: 8.5303 - mae: 0.6855 - mean_pred: 0.2902 - mae_t1: 0.0457 - val_loss: 5.7926 - val_mae: 0.4655 - val_mean_pred: 0.8754 - val_mae_t1: 0.0310\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 7.1444 - mae: 0.5741 - mean_pred: 0.9801 - mae_t1: 0.0383 - val_loss: 7.5735 - val_mae: 0.6086 - val_mean_pred: 1.2006 - val_mae_t1: 0.0406\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 7.7053 - mae: 0.6192 - mean_pred: 1.0345 - mae_t1: 0.0413 - val_loss: 8.9120 - val_mae: 0.7161 - val_mean_pred: 0.3471 - val_mae_t1: 0.0477\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 8.8954 - mae: 0.7148 - mean_pred: 0.3218 - mae_t1: 0.0477 - val_loss: 8.9550 - val_mae: 0.7196 - val_mean_pred: 0.3577 - val_mae_t1: 0.0480\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 8.0452 - mae: 0.6465 - mean_pred: 0.4650 - mae_t1: 0.0431 - val_loss: 7.6479 - val_mae: 0.6146 - val_mean_pred: 0.9593 - val_mae_t1: 0.0410\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 7.5364 - mae: 0.6056 - mean_pred: 0.9838 - mae_t1: 0.0404 - val_loss: 8.1880 - val_mae: 0.6580 - val_mean_pred: 1.0130 - val_mae_t1: 0.0439\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 7.2426 - mae: 0.5820 - mean_pred: 0.8997 - mae_t1: 0.0388 - val_loss: 7.0565 - val_mae: 0.5670 - val_mean_pred: 0.6387 - val_mae_t1: 0.0378\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 7.0902 - mae: 0.5697 - mean_pred: 0.6099 - mae_t1: 0.0380 - val_loss: 8.3617 - val_mae: 0.6719 - val_mean_pred: 0.9298 - val_mae_t1: 0.0448\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 8.8225 - mae: 0.7089 - mean_pred: 0.9465 - mae_t1: 0.0473 - val_loss: 7.0941 - val_mae: 0.5701 - val_mean_pred: 1.0393 - val_mae_t1: 0.0380\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 7.1760 - mae: 0.5766 - mean_pred: 0.9080 - mae_t1: 0.0384 - val_loss: 8.8847 - val_mae: 0.7139 - val_mean_pred: 0.7679 - val_mae_t1: 0.0476\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 8.7136 - mae: 0.7002 - mean_pred: 0.6670 - mae_t1: 0.0467 - val_loss: 7.6643 - val_mae: 0.6159 - val_mean_pred: 0.3957 - val_mae_t1: 0.0411\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 7.3514 - mae: 0.5907 - mean_pred: 0.4141 - mae_t1: 0.0394 - val_loss: 6.2704 - val_mae: 0.5039 - val_mean_pred: 0.8091 - val_mae_t1: 0.0336\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 6.6735 - mae: 0.5363 - mean_pred: 0.9017 - mae_t1: 0.0358 - val_loss: 7.8651 - val_mae: 0.6320 - val_mean_pred: 1.1925 - val_mae_t1: 0.0421\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.4223 - mae: 0.5964 - mean_pred: 1.0841 - mae_t1: 0.0398 - val_loss: 6.6480 - val_mae: 0.5342 - val_mean_pred: 0.7054 - val_mae_t1: 0.0356\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 6.4042 - mae: 0.5146 - mean_pred: 0.5797 - mae_t1: 0.0343 - val_loss: 7.9519 - val_mae: 0.6390 - val_mean_pred: 0.5694 - val_mae_t1: 0.0426\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 7.0994 - mae: 0.5705 - mean_pred: 0.5469 - mae_t1: 0.0380 - val_loss: 6.5532 - val_mae: 0.5266 - val_mean_pred: 0.8332 - val_mae_t1: 0.0351\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 5.8244 - mae: 0.4680 - mean_pred: 0.7540 - mae_t1: 0.0312 - val_loss: 6.9796 - val_mae: 0.5609 - val_mean_pred: 1.0389 - val_mae_t1: 0.0374\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.1089 - mae: 0.4909 - mean_pred: 0.9371 - mae_t1: 0.0327 - val_loss: 6.1790 - val_mae: 0.4965 - val_mean_pred: 1.0160 - val_mae_t1: 0.0331\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.6176 - mae: 0.4514 - mean_pred: 0.8360 - mae_t1: 0.0301 - val_loss: 6.7713 - val_mae: 0.5441 - val_mean_pred: 0.6949 - val_mae_t1: 0.0363\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 6.5122 - mae: 0.5233 - mean_pred: 0.5784 - mae_t1: 0.0349 - val_loss: 6.7522 - val_mae: 0.5426 - val_mean_pred: 0.7004 - val_mae_t1: 0.0362\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 5.9270 - mae: 0.4763 - mean_pred: 0.6627 - mae_t1: 0.0318 - val_loss: 7.0382 - val_mae: 0.5656 - val_mean_pred: 1.0784 - val_mae_t1: 0.0377\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.0348 - mae: 0.4849 - mean_pred: 0.9669 - mae_t1: 0.0323 - val_loss: 6.8647 - val_mae: 0.5516 - val_mean_pred: 0.9871 - val_mae_t1: 0.0368\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.6357 - mae: 0.4529 - mean_pred: 0.7998 - mae_t1: 0.0302 - val_loss: 6.8009 - val_mae: 0.5465 - val_mean_pred: 0.8515 - val_mae_t1: 0.0364\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 6.0397 - mae: 0.4853 - mean_pred: 0.7997 - mae_t1: 0.0324 - val_loss: 7.4877 - val_mae: 0.6017 - val_mean_pred: 1.1228 - val_mae_t1: 0.0401\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.3501 - mae: 0.5103 - mean_pred: 1.0348 - mae_t1: 0.0340 - val_loss: 7.2766 - val_mae: 0.5847 - val_mean_pred: 0.8715 - val_mae_t1: 0.0390\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.2308 - mae: 0.5007 - mean_pred: 0.6981 - mae_t1: 0.0334 - val_loss: 7.3837 - val_mae: 0.5933 - val_mean_pred: 0.4780 - val_mae_t1: 0.0396\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 7.0398 - mae: 0.5657 - mean_pred: 0.3815 - mae_t1: 0.0377 - val_loss: 6.7332 - val_mae: 0.5411 - val_mean_pred: 0.6347 - val_mae_t1: 0.0361\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.9189 - mae: 0.4756 - mean_pred: 0.5789 - mae_t1: 0.0317 - val_loss: 6.9027 - val_mae: 0.5547 - val_mean_pred: 0.9551 - val_mae_t1: 0.0370\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.8643 - mae: 0.4712 - mean_pred: 0.8332 - mae_t1: 0.0314 - val_loss: 6.3389 - val_mae: 0.5094 - val_mean_pred: 0.9647 - val_mae_t1: 0.0340\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.4847 - mae: 0.4407 - mean_pred: 0.8165 - mae_t1: 0.0294 - val_loss: 6.2233 - val_mae: 0.5001 - val_mean_pred: 0.8977 - val_mae_t1: 0.0333\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 5.2899 - mae: 0.4251 - mean_pred: 0.7819 - mae_t1: 0.0283 - val_loss: 6.6656 - val_mae: 0.5356 - val_mean_pred: 0.9239 - val_mae_t1: 0.0357\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.6542 - mae: 0.4544 - mean_pred: 0.8416 - mae_t1: 0.0303 - val_loss: 6.2636 - val_mae: 0.5033 - val_mean_pred: 0.9597 - val_mae_t1: 0.0336\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 5.1433 - mae: 0.4133 - mean_pred: 0.8500 - mae_t1: 0.0276 - val_loss: 6.1420 - val_mae: 0.4936 - val_mean_pred: 0.8785 - val_mae_t1: 0.0329\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.0346 - mae: 0.4046 - mean_pred: 0.7878 - mae_t1: 0.0270 - val_loss: 6.2699 - val_mae: 0.5038 - val_mean_pred: 0.8422 - val_mae_t1: 0.0336\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0146 - mae: 0.4030 - mean_pred: 0.7582 - mae_t1: 0.0269 - val_loss: 7.1372 - val_mae: 0.5735 - val_mean_pred: 0.8747 - val_mae_t1: 0.0382\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.8766 - mae: 0.4722 - mean_pred: 0.7646 - mae_t1: 0.0315 - val_loss: 6.5011 - val_mae: 0.5224 - val_mean_pred: 0.8366 - val_mae_t1: 0.0348\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1547 - mae: 0.4142 - mean_pred: 0.7731 - mae_t1: 0.0276 - val_loss: 6.4780 - val_mae: 0.5206 - val_mean_pred: 0.9190 - val_mae_t1: 0.0347\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 5.6379 - mae: 0.4530 - mean_pred: 0.8360 - mae_t1: 0.0302 - val_loss: 6.5860 - val_mae: 0.5292 - val_mean_pred: 1.0146 - val_mae_t1: 0.0353\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 5.4549 - mae: 0.4383 - mean_pred: 0.9012 - mae_t1: 0.0292 - val_loss: 6.5637 - val_mae: 0.5274 - val_mean_pred: 0.9294 - val_mae_t1: 0.0352\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.4547 - mae: 0.4383 - mean_pred: 0.7592 - mae_t1: 0.0292 - val_loss: 7.3538 - val_mae: 0.5909 - val_mean_pred: 0.7636 - val_mae_t1: 0.0394\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 6.6946 - mae: 0.5380 - mean_pred: 0.7029 - mae_t1: 0.0359 - val_loss: 6.8617 - val_mae: 0.5514 - val_mean_pred: 0.7601 - val_mae_t1: 0.0368\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 5.9365 - mae: 0.4770 - mean_pred: 0.6801 - mae_t1: 0.0318 - val_loss: 7.3682 - val_mae: 0.5921 - val_mean_pred: 0.8115 - val_mae_t1: 0.0395\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 7.0655 - mae: 0.5678 - mean_pred: 0.8228 - mae_t1: 0.0379 - val_loss: 6.5057 - val_mae: 0.5228 - val_mean_pred: 0.9126 - val_mae_t1: 0.0349\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.0832 - mae: 0.4085 - mean_pred: 0.8486 - mae_t1: 0.0272 - val_loss: 7.5006 - val_mae: 0.6027 - val_mean_pred: 0.8971 - val_mae_t1: 0.0402\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 6.5752 - mae: 0.5284 - mean_pred: 0.8329 - mae_t1: 0.0352 - val_loss: 6.4835 - val_mae: 0.5210 - val_mean_pred: 0.7178 - val_mae_t1: 0.0347\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.3295 - mae: 0.4283 - mean_pred: 0.6442 - mae_t1: 0.0286 - val_loss: 8.0876 - val_mae: 0.6499 - val_mean_pred: 0.6809 - val_mae_t1: 0.0433\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.6551 - mae: 0.5348 - mean_pred: 0.5774 - mae_t1: 0.0357 - val_loss: 6.9330 - val_mae: 0.5571 - val_mean_pred: 0.6556 - val_mae_t1: 0.0371\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1737 - mae: 0.4157 - mean_pred: 0.6294 - mae_t1: 0.0277 - val_loss: 6.9136 - val_mae: 0.5556 - val_mean_pred: 0.8972 - val_mae_t1: 0.0370\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.6918 - mae: 0.4574 - mean_pred: 0.8446 - mae_t1: 0.0305 - val_loss: 6.5373 - val_mae: 0.5253 - val_mean_pred: 1.0523 - val_mae_t1: 0.0350\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.2772 - mae: 0.4241 - mean_pred: 0.9351 - mae_t1: 0.0283 - val_loss: 7.7996 - val_mae: 0.6268 - val_mean_pred: 0.8330 - val_mae_t1: 0.0418\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.8701 - mae: 0.4717 - mean_pred: 0.6777 - mae_t1: 0.0314 - val_loss: 7.3847 - val_mae: 0.5934 - val_mean_pred: 0.5430 - val_mae_t1: 0.0396\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 6.6569 - mae: 0.5349 - mean_pred: 0.5583 - mae_t1: 0.0357 - val_loss: 7.3296 - val_mae: 0.5890 - val_mean_pred: 0.6471 - val_mae_t1: 0.0393\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.7511 - mae: 0.4621 - mean_pred: 0.6434 - mae_t1: 0.0308 - val_loss: 6.7182 - val_mae: 0.5399 - val_mean_pred: 0.8832 - val_mae_t1: 0.0360\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.6653 - mae: 0.3749 - mean_pred: 0.7869 - mae_t1: 0.0250 - val_loss: 6.6750 - val_mae: 0.5364 - val_mean_pred: 1.0027 - val_mae_t1: 0.0358\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.0732 - mae: 0.4077 - mean_pred: 0.9055 - mae_t1: 0.0272 - val_loss: 6.8583 - val_mae: 0.5511 - val_mean_pred: 1.0029 - val_mae_t1: 0.0367\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.4111 - mae: 0.4348 - mean_pred: 0.8748 - mae_t1: 0.0290 - val_loss: 6.1237 - val_mae: 0.4921 - val_mean_pred: 0.8638 - val_mae_t1: 0.0328\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.7752 - mae: 0.3837 - mean_pred: 0.7671 - mae_t1: 0.0256 - val_loss: 7.1976 - val_mae: 0.5784 - val_mean_pred: 0.8922 - val_mae_t1: 0.0386\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.3853 - mae: 0.4327 - mean_pred: 0.8069 - mae_t1: 0.0288 - val_loss: 6.0318 - val_mae: 0.4847 - val_mean_pred: 0.7969 - val_mae_t1: 0.0323\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.9823 - mae: 0.4004 - mean_pred: 0.7135 - mae_t1: 0.0267 - val_loss: 6.5143 - val_mae: 0.5235 - val_mean_pred: 0.7200 - val_mae_t1: 0.0349\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.9904 - mae: 0.4010 - mean_pred: 0.6571 - mae_t1: 0.0267 - val_loss: 6.3314 - val_mae: 0.5088 - val_mean_pred: 0.9333 - val_mae_t1: 0.0339\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.7726 - mae: 0.3835 - mean_pred: 0.8469 - mae_t1: 0.0256 - val_loss: 7.5131 - val_mae: 0.6037 - val_mean_pred: 0.9918 - val_mae_t1: 0.0402\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 6.5401 - mae: 0.5255 - mean_pred: 0.8796 - mae_t1: 0.0350 - val_loss: 6.3500 - val_mae: 0.5103 - val_mean_pred: 0.8440 - val_mae_t1: 0.0340\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 4.9129 - mae: 0.3948 - mean_pred: 0.7775 - mae_t1: 0.0263 - val_loss: 6.6627 - val_mae: 0.5354 - val_mean_pred: 0.9351 - val_mae_t1: 0.0357\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 5.6179 - mae: 0.4514 - mean_pred: 0.8497 - mae_t1: 0.0301 - val_loss: 5.6501 - val_mae: 0.4540 - val_mean_pred: 0.8617 - val_mae_t1: 0.0303\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.6966 - mae: 0.3774 - mean_pred: 0.7614 - mae_t1: 0.0252 - val_loss: 5.9361 - val_mae: 0.4770 - val_mean_pred: 0.7678 - val_mae_t1: 0.0318\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.7201 - mae: 0.3793 - mean_pred: 0.7121 - mae_t1: 0.0253 - val_loss: 6.3982 - val_mae: 0.5141 - val_mean_pred: 0.9291 - val_mae_t1: 0.0343\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.0284 - mae: 0.4041 - mean_pred: 0.8549 - mae_t1: 0.0269 - val_loss: 6.7930 - val_mae: 0.5459 - val_mean_pred: 1.0918 - val_mae_t1: 0.0364\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 5.1317 - mae: 0.4124 - mean_pred: 0.9793 - mae_t1: 0.0275 - val_loss: 6.5663 - val_mae: 0.5277 - val_mean_pred: 0.9346 - val_mae_t1: 0.0352\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0159 - mae: 0.4031 - mean_pred: 0.7891 - mae_t1: 0.0269 - val_loss: 7.4082 - val_mae: 0.5953 - val_mean_pred: 0.7087 - val_mae_t1: 0.0397\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.4935 - mae: 0.4414 - mean_pred: 0.6577 - mae_t1: 0.0294 - val_loss: 6.8042 - val_mae: 0.5468 - val_mean_pred: 0.8515 - val_mae_t1: 0.0365\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.9996 - mae: 0.4018 - mean_pred: 0.7879 - mae_t1: 0.0268 - val_loss: 8.0808 - val_mae: 0.6494 - val_mean_pred: 1.2310 - val_mae_t1: 0.0433\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.7415 - mae: 0.5417 - mean_pred: 1.1384 - mae_t1: 0.0361 - val_loss: 7.6717 - val_mae: 0.6165 - val_mean_pred: 1.1561 - val_mae_t1: 0.0411\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 6.5020 - mae: 0.5225 - mean_pred: 1.0000 - mae_t1: 0.0348 - val_loss: 6.9656 - val_mae: 0.5597 - val_mean_pred: 0.7193 - val_mae_t1: 0.0373\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 5.0747 - mae: 0.4078 - mean_pred: 0.6403 - mae_t1: 0.0272 - val_loss: 7.8585 - val_mae: 0.6315 - val_mean_pred: 0.6073 - val_mae_t1: 0.0421\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.6511 - mae: 0.4541 - mean_pred: 0.5507 - mae_t1: 0.0303 - val_loss: 6.9017 - val_mae: 0.5546 - val_mean_pred: 0.6435 - val_mae_t1: 0.0370\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.9176 - mae: 0.3952 - mean_pred: 0.6413 - mae_t1: 0.0263 - val_loss: 5.9744 - val_mae: 0.4801 - val_mean_pred: 0.8239 - val_mae_t1: 0.0320\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.7161 - mae: 0.3790 - mean_pred: 0.8205 - mae_t1: 0.0253 - val_loss: 6.9120 - val_mae: 0.5554 - val_mean_pred: 0.8886 - val_mae_t1: 0.0370\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.4450 - mae: 0.4375 - mean_pred: 0.8106 - mae_t1: 0.0292 - val_loss: 6.6119 - val_mae: 0.5313 - val_mean_pred: 0.7385 - val_mae_t1: 0.0354\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.9544 - mae: 0.3981 - mean_pred: 0.6922 - mae_t1: 0.0265 - val_loss: 6.9334 - val_mae: 0.5571 - val_mean_pred: 0.7600 - val_mae_t1: 0.0371\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 6.3961 - mae: 0.5140 - mean_pred: 0.7254 - mae_t1: 0.0343 - val_loss: 6.4509 - val_mae: 0.5184 - val_mean_pred: 0.7658 - val_mae_t1: 0.0346\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.8364 - mae: 0.3886 - mean_pred: 0.7107 - mae_t1: 0.0259 - val_loss: 7.6462 - val_mae: 0.6144 - val_mean_pred: 1.0012 - val_mae_t1: 0.0410\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 6.3228 - mae: 0.5081 - mean_pred: 0.9159 - mae_t1: 0.0339 - val_loss: 6.4639 - val_mae: 0.5194 - val_mean_pred: 1.0321 - val_mae_t1: 0.0346\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.7995 - mae: 0.3857 - mean_pred: 0.8938 - mae_t1: 0.0257 - val_loss: 6.8810 - val_mae: 0.5529 - val_mean_pred: 0.8560 - val_mae_t1: 0.0369\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.1843 - mae: 0.4166 - mean_pred: 0.7711 - mae_t1: 0.0278 - val_loss: 6.6586 - val_mae: 0.5351 - val_mean_pred: 0.8196 - val_mae_t1: 0.0357\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.7861 - mae: 0.3846 - mean_pred: 0.7282 - mae_t1: 0.0256 - val_loss: 6.5346 - val_mae: 0.5251 - val_mean_pred: 0.8678 - val_mae_t1: 0.0350\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.6028 - mae: 0.3699 - mean_pred: 0.7430 - mae_t1: 0.0247 - val_loss: 7.6856 - val_mae: 0.6176 - val_mean_pred: 0.7778 - val_mae_t1: 0.0412\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.3814 - mae: 0.5128 - mean_pred: 0.7173 - mae_t1: 0.0342 - val_loss: 7.1526 - val_mae: 0.5748 - val_mean_pred: 0.6197 - val_mae_t1: 0.0383\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 5.5013 - mae: 0.4421 - mean_pred: 0.5545 - mae_t1: 0.0295 - val_loss: 7.2428 - val_mae: 0.5820 - val_mean_pred: 0.6643 - val_mae_t1: 0.0388\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 5.3881 - mae: 0.4330 - mean_pred: 0.6059 - mae_t1: 0.0289 - val_loss: 6.3445 - val_mae: 0.5098 - val_mean_pred: 0.9344 - val_mae_t1: 0.0340\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.8655 - mae: 0.3910 - mean_pred: 0.8902 - mae_t1: 0.0261 - val_loss: 7.2752 - val_mae: 0.5846 - val_mean_pred: 1.1360 - val_mae_t1: 0.0390\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.5661 - mae: 0.4473 - mean_pred: 1.0151 - mae_t1: 0.0298 - val_loss: 6.3063 - val_mae: 0.5068 - val_mean_pred: 0.9903 - val_mae_t1: 0.0338\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.4950 - mae: 0.3612 - mean_pred: 0.8736 - mae_t1: 0.0241 - val_loss: 6.0877 - val_mae: 0.4892 - val_mean_pred: 0.7592 - val_mae_t1: 0.0326\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.9074 - mae: 0.3943 - mean_pred: 0.6481 - mae_t1: 0.0263 - val_loss: 7.0162 - val_mae: 0.5638 - val_mean_pred: 0.5943 - val_mae_t1: 0.0376\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.5398 - mae: 0.4452 - mean_pred: 0.5593 - mae_t1: 0.0297 - val_loss: 6.2296 - val_mae: 0.5006 - val_mean_pred: 0.7712 - val_mae_t1: 0.0334\n",
      "Earliness...\n",
      "0.0019996166229248047\n",
      "____________________________________________________________\n",
      "Test MAE:      0.31715113202963163  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▄▄▅▄▄▄▃▂▃▂▃▂▂▂▁▂▂▄▃▂▂▂▁▂▁▁▁▂▁▃▁▂▁▁▁▂▂▂</td></tr><tr><td>mae</td><td>█▄▄▄▅▄▄▄▃▂▃▂▃▂▂▂▁▂▂▄▃▂▂▂▁▂▁▁▁▂▁▃▁▂▁▁▁▂▂▂</td></tr><tr><td>mae_t1</td><td>█▄▄▄▅▄▄▄▃▂▃▂▃▂▂▂▁▂▂▄▃▂▂▂▁▂▁▁▁▂▁▃▁▂▁▁▁▂▂▂</td></tr><tr><td>mean_pred</td><td>▁▅▃█▃▇▇▃▄▆▄▇█▄▆▆▆▆▆▆▆▅▇▅▇▆▆▆▅█▆█▅▆▅▇▆▄█▄</td></tr><tr><td>val_loss</td><td>▂▆█▄▄▃▇▂▅▃▃▃▄▃▁▁▃▂▄▂▂▃▅▂▃▁▄▂▂▂▅▃▁▂▄▃▄▃▂▁</td></tr><tr><td>val_mae</td><td>▂▆█▄▄▃▇▂▅▃▃▃▄▃▁▁▃▂▄▂▂▃▅▂▃▁▄▂▂▂▅▃▁▂▄▃▄▃▂▁</td></tr><tr><td>val_mae_t1</td><td>▂▆█▄▄▃▇▂▅▃▃▃▄▃▁▁▃▂▄▂▂▃▅▂▃▁▄▂▂▂▅▃▁▂▄▃▄▃▂▁</td></tr><tr><td>val_mean_pred</td><td>▆▂▁█▆▄▅▅▃▇▄▆▅▆▆▅▅▆▅▆▄▆▅▆▆▅▆▆▆▆█▄▅▅▆▅▅▄▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.36451</td></tr><tr><td>AE_2</td><td>0.3073</td></tr><tr><td>AE_3</td><td>0.3244</td></tr><tr><td>MAE</td><td>0.31715</td></tr><tr><td>best_epoch</td><td>69</td></tr><tr><td>best_val_loss</td><td>5.65009</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.53978</td></tr><tr><td>mae</td><td>0.44516</td></tr><tr><td>mae_t1</td><td>0.02968</td></tr><tr><td>mean_pred</td><td>0.55925</td></tr><tr><td>val_loss</td><td>6.22955</td></tr><tr><td>val_mae</td><td>0.50059</td></tr><tr><td>val_mae_t1</td><td>0.03337</td></tr><tr><td>val_mean_pred</td><td>0.77118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-snowball-66</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/xwqursu8\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/xwqursu8</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154653-xwqursu8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154718-3p3ij3kg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3p3ij3kg\" target=\"_blank\">polished-rain-67</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 863us/sample - loss: 11.4209 - mae: 0.6762 - mean_pred: 0.6062 - mae_t1: 0.0451 - val_loss: 8.2881 - val_mae: 0.4907 - val_mean_pred: 0.9060 - val_mae_t1: 0.0327\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 9.8681 - mae: 0.5843 - mean_pred: 0.6893 - mae_t1: 0.0390 - val_loss: 10.8473 - val_mae: 0.6423 - val_mean_pred: 1.1265 - val_mae_t1: 0.0428\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 9.9178 - mae: 0.5872 - mean_pred: 0.9740 - mae_t1: 0.0391 - val_loss: 10.6637 - val_mae: 0.6314 - val_mean_pred: 0.5398 - val_mae_t1: 0.0421\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.0248 - mae: 0.5344 - mean_pred: 0.7058 - mae_t1: 0.0356 - val_loss: 8.4088 - val_mae: 0.4979 - val_mean_pred: 0.8987 - val_mae_t1: 0.0332\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.3603 - mae: 0.4950 - mean_pred: 0.6771 - mae_t1: 0.0330 - val_loss: 8.6580 - val_mae: 0.5126 - val_mean_pred: 1.0186 - val_mae_t1: 0.0342\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 9.0895 - mae: 0.5382 - mean_pred: 0.9950 - mae_t1: 0.0359 - val_loss: 10.8739 - val_mae: 0.6438 - val_mean_pred: 0.4339 - val_mae_t1: 0.0429\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 10.0252 - mae: 0.5936 - mean_pred: 0.4100 - mae_t1: 0.0396 - val_loss: 10.3522 - val_mae: 0.6130 - val_mean_pred: 1.0595 - val_mae_t1: 0.0409\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.5452 - mae: 0.5060 - mean_pred: 0.9673 - mae_t1: 0.0337 - val_loss: 11.5787 - val_mae: 0.6856 - val_mean_pred: 0.4553 - val_mae_t1: 0.0457\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 9.0601 - mae: 0.5365 - mean_pred: 0.5701 - mae_t1: 0.0358 - val_loss: 9.0400 - val_mae: 0.5353 - val_mean_pred: 0.9718 - val_mae_t1: 0.0357\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.8752 - mae: 0.5255 - mean_pred: 0.8845 - mae_t1: 0.0350 - val_loss: 9.1756 - val_mae: 0.5433 - val_mean_pred: 0.7364 - val_mae_t1: 0.0362\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.2760 - mae: 0.4900 - mean_pred: 0.7209 - mae_t1: 0.0327 - val_loss: 8.6728 - val_mae: 0.5135 - val_mean_pred: 0.9413 - val_mae_t1: 0.0342\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.4365 - mae: 0.5587 - mean_pred: 0.8061 - mae_t1: 0.0372 - val_loss: 10.6760 - val_mae: 0.6321 - val_mean_pred: 0.7146 - val_mae_t1: 0.0421\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 8.8501 - mae: 0.5240 - mean_pred: 0.7253 - mae_t1: 0.0349 - val_loss: 11.9514 - val_mae: 0.7076 - val_mean_pred: 1.2917 - val_mae_t1: 0.0472\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 10.2481 - mae: 0.6068 - mean_pred: 1.1504 - mae_t1: 0.0405 - val_loss: 11.0373 - val_mae: 0.6535 - val_mean_pred: 0.4229 - val_mae_t1: 0.0436\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 10.2979 - mae: 0.6097 - mean_pred: 0.3456 - mae_t1: 0.0406 - val_loss: 8.1597 - val_mae: 0.4831 - val_mean_pred: 0.7950 - val_mae_t1: 0.0322\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 9.8266 - mae: 0.5818 - mean_pred: 0.9809 - mae_t1: 0.0388 - val_loss: 8.4379 - val_mae: 0.4996 - val_mean_pred: 1.0355 - val_mae_t1: 0.0333\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.2872 - mae: 0.4315 - mean_pred: 0.7893 - mae_t1: 0.0288 - val_loss: 9.0897 - val_mae: 0.5382 - val_mean_pred: 1.0684 - val_mae_t1: 0.0359\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 8.3699 - mae: 0.4956 - mean_pred: 1.0483 - mae_t1: 0.0330 - val_loss: 9.5821 - val_mae: 0.5674 - val_mean_pred: 0.6967 - val_mae_t1: 0.0378\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.1162 - mae: 0.5398 - mean_pred: 0.5293 - mae_t1: 0.0360 - val_loss: 9.5206 - val_mae: 0.5637 - val_mean_pred: 0.8074 - val_mae_t1: 0.0376\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.9176 - mae: 0.5280 - mean_pred: 0.8963 - mae_t1: 0.0352 - val_loss: 8.9399 - val_mae: 0.5293 - val_mean_pred: 0.8424 - val_mae_t1: 0.0353\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.2977 - mae: 0.4321 - mean_pred: 0.6763 - mae_t1: 0.0288 - val_loss: 8.2215 - val_mae: 0.4868 - val_mean_pred: 0.8260 - val_mae_t1: 0.0325\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 7.5901 - mae: 0.4494 - mean_pred: 0.9341 - mae_t1: 0.0300 - val_loss: 8.4237 - val_mae: 0.4988 - val_mean_pred: 0.8984 - val_mae_t1: 0.0333\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.5023 - mae: 0.3850 - mean_pred: 0.7436 - mae_t1: 0.0257 - val_loss: 8.5520 - val_mae: 0.5064 - val_mean_pred: 0.7239 - val_mae_t1: 0.0338\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.4690 - mae: 0.3830 - mean_pred: 0.7473 - mae_t1: 0.0255 - val_loss: 8.4037 - val_mae: 0.4976 - val_mean_pred: 0.9756 - val_mae_t1: 0.0332\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.4788 - mae: 0.3836 - mean_pred: 0.8618 - mae_t1: 0.0256 - val_loss: 9.2533 - val_mae: 0.5479 - val_mean_pred: 0.6861 - val_mae_t1: 0.0365\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 6.9126 - mae: 0.4093 - mean_pred: 0.6548 - mae_t1: 0.0273 - val_loss: 7.7516 - val_mae: 0.4590 - val_mean_pred: 0.9229 - val_mae_t1: 0.0306\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.2533 - mae: 0.3703 - mean_pred: 0.8578 - mae_t1: 0.0247 - val_loss: 7.8519 - val_mae: 0.4649 - val_mean_pred: 0.7500 - val_mae_t1: 0.0310\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 6.3593 - mae: 0.3765 - mean_pred: 0.7031 - mae_t1: 0.0251 - val_loss: 7.7401 - val_mae: 0.4583 - val_mean_pred: 0.8306 - val_mae_t1: 0.0306\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.5518 - mae: 0.3879 - mean_pred: 0.7157 - mae_t1: 0.0259 - val_loss: 8.4001 - val_mae: 0.4974 - val_mean_pred: 0.8178 - val_mae_t1: 0.0332\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 5.9974 - mae: 0.3551 - mean_pred: 0.7787 - mae_t1: 0.0237 - val_loss: 7.6988 - val_mae: 0.4559 - val_mean_pred: 0.9878 - val_mae_t1: 0.0304\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 6.2878 - mae: 0.3723 - mean_pred: 0.8689 - mae_t1: 0.0248 - val_loss: 8.7603 - val_mae: 0.5187 - val_mean_pred: 0.6551 - val_mae_t1: 0.0346\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 6.8186 - mae: 0.4037 - mean_pred: 0.6487 - mae_t1: 0.0269 - val_loss: 7.7911 - val_mae: 0.4613 - val_mean_pred: 0.9618 - val_mae_t1: 0.0308\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.1056 - mae: 0.3615 - mean_pred: 0.8919 - mae_t1: 0.0241 - val_loss: 8.5327 - val_mae: 0.5052 - val_mean_pred: 0.8531 - val_mae_t1: 0.0337\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2957 - mae: 0.3728 - mean_pred: 0.7525 - mae_t1: 0.0249 - val_loss: 8.8923 - val_mae: 0.5265 - val_mean_pred: 0.9385 - val_mae_t1: 0.0351\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 8.0164 - mae: 0.4747 - mean_pred: 0.8876 - mae_t1: 0.0316 - val_loss: 10.4203 - val_mae: 0.6170 - val_mean_pred: 0.7339 - val_mae_t1: 0.0411\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 7.9788 - mae: 0.4724 - mean_pred: 0.5813 - mae_t1: 0.0315 - val_loss: 8.1331 - val_mae: 0.4816 - val_mean_pred: 0.8329 - val_mae_t1: 0.0321\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.9532 - mae: 0.4117 - mean_pred: 0.8826 - mae_t1: 0.0274 - val_loss: 8.3998 - val_mae: 0.4974 - val_mean_pred: 0.8547 - val_mae_t1: 0.0332\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.1921 - mae: 0.3666 - mean_pred: 0.7099 - mae_t1: 0.0244 - val_loss: 7.9206 - val_mae: 0.4690 - val_mean_pred: 0.8180 - val_mae_t1: 0.0313\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 5.6797 - mae: 0.3363 - mean_pred: 0.7880 - mae_t1: 0.0224 - val_loss: 7.5227 - val_mae: 0.4454 - val_mean_pred: 0.8759 - val_mae_t1: 0.0297\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 5.6675 - mae: 0.3356 - mean_pred: 0.7523 - mae_t1: 0.0224 - val_loss: 7.3890 - val_mae: 0.4375 - val_mean_pred: 0.9092 - val_mae_t1: 0.0292\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.5531 - mae: 0.3880 - mean_pred: 0.8855 - mae_t1: 0.0259 - val_loss: 7.8091 - val_mae: 0.4624 - val_mean_pred: 0.9084 - val_mae_t1: 0.0308\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 6.0340 - mae: 0.3573 - mean_pred: 0.7831 - mae_t1: 0.0238 - val_loss: 6.8316 - val_mae: 0.4045 - val_mean_pred: 0.9047 - val_mae_t1: 0.0270\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.7710 - mae: 0.3417 - mean_pred: 0.8273 - mae_t1: 0.0228 - val_loss: 7.5152 - val_mae: 0.4450 - val_mean_pred: 0.8886 - val_mae_t1: 0.0297\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.8313 - mae: 0.3453 - mean_pred: 0.7743 - mae_t1: 0.0230 - val_loss: 9.4493 - val_mae: 0.5595 - val_mean_pred: 0.9789 - val_mae_t1: 0.0373\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2739 - mae: 0.3715 - mean_pred: 0.8840 - mae_t1: 0.0248 - val_loss: 8.5135 - val_mae: 0.5041 - val_mean_pred: 0.7399 - val_mae_t1: 0.0336\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.8573 - mae: 0.4060 - mean_pred: 0.6169 - mae_t1: 0.0271 - val_loss: 8.8331 - val_mae: 0.5230 - val_mean_pred: 1.0115 - val_mae_t1: 0.0349\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.0066 - mae: 0.4149 - mean_pred: 0.9596 - mae_t1: 0.0277 - val_loss: 9.0280 - val_mae: 0.5346 - val_mean_pred: 1.0166 - val_mae_t1: 0.0356\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.9944 - mae: 0.4141 - mean_pred: 0.6602 - mae_t1: 0.0276 - val_loss: 9.2146 - val_mae: 0.5456 - val_mean_pred: 0.6201 - val_mae_t1: 0.0364\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.8377 - mae: 0.3457 - mean_pred: 0.6962 - mae_t1: 0.0230 - val_loss: 9.4834 - val_mae: 0.5615 - val_mean_pred: 1.1187 - val_mae_t1: 0.0374\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 6.5514 - mae: 0.3879 - mean_pred: 0.9300 - mae_t1: 0.0259 - val_loss: 8.2579 - val_mae: 0.4890 - val_mean_pred: 0.7150 - val_mae_t1: 0.0326\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.0210 - mae: 0.3565 - mean_pred: 0.6836 - mae_t1: 0.0238 - val_loss: 9.6328 - val_mae: 0.5704 - val_mean_pred: 0.9910 - val_mae_t1: 0.0380\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.4131 - mae: 0.3797 - mean_pred: 0.8787 - mae_t1: 0.0253 - val_loss: 7.5195 - val_mae: 0.4452 - val_mean_pred: 0.8850 - val_mae_t1: 0.0297\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.5254 - mae: 0.3272 - mean_pred: 0.7564 - mae_t1: 0.0218 - val_loss: 8.1061 - val_mae: 0.4800 - val_mean_pred: 0.8426 - val_mae_t1: 0.0320\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.3245 - mae: 0.3153 - mean_pred: 0.7716 - mae_t1: 0.0210 - val_loss: 8.0795 - val_mae: 0.4784 - val_mean_pred: 0.9113 - val_mae_t1: 0.0319\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.4730 - mae: 0.3241 - mean_pred: 0.7650 - mae_t1: 0.0216 - val_loss: 7.8745 - val_mae: 0.4663 - val_mean_pred: 0.7514 - val_mae_t1: 0.0311\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 5.5446 - mae: 0.3283 - mean_pred: 0.6922 - mae_t1: 0.0219 - val_loss: 8.9309 - val_mae: 0.5288 - val_mean_pred: 0.9583 - val_mae_t1: 0.0353\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.0732 - mae: 0.3596 - mean_pred: 0.9056 - mae_t1: 0.0240 - val_loss: 6.9328 - val_mae: 0.4105 - val_mean_pred: 0.8950 - val_mae_t1: 0.0274\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2181 - mae: 0.3682 - mean_pred: 0.7826 - mae_t1: 0.0245 - val_loss: 7.3827 - val_mae: 0.4371 - val_mean_pred: 0.9294 - val_mae_t1: 0.0291\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.8638 - mae: 0.3472 - mean_pred: 0.8164 - mae_t1: 0.0231 - val_loss: 8.3514 - val_mae: 0.4945 - val_mean_pred: 0.8034 - val_mae_t1: 0.0330\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 6.5427 - mae: 0.3874 - mean_pred: 0.6224 - mae_t1: 0.0258 - val_loss: 8.0248 - val_mae: 0.4752 - val_mean_pred: 0.8100 - val_mae_t1: 0.0317\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.1422 - mae: 0.3637 - mean_pred: 0.8169 - mae_t1: 0.0242 - val_loss: 7.4976 - val_mae: 0.4439 - val_mean_pred: 0.9064 - val_mae_t1: 0.0296\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2876 - mae: 0.3723 - mean_pred: 0.7169 - mae_t1: 0.0248 - val_loss: 7.9754 - val_mae: 0.4722 - val_mean_pred: 0.7077 - val_mae_t1: 0.0315\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.2753 - mae: 0.3716 - mean_pred: 0.6594 - mae_t1: 0.0248 - val_loss: 7.7525 - val_mae: 0.4590 - val_mean_pred: 1.0008 - val_mae_t1: 0.0306\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.3118 - mae: 0.3145 - mean_pred: 0.8442 - mae_t1: 0.0210 - val_loss: 7.6679 - val_mae: 0.4540 - val_mean_pred: 0.8509 - val_mae_t1: 0.0303\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5128 - mae: 0.3264 - mean_pred: 0.7140 - mae_t1: 0.0218 - val_loss: 9.2297 - val_mae: 0.5465 - val_mean_pred: 0.9815 - val_mae_t1: 0.0364\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.9268 - mae: 0.3509 - mean_pred: 0.9079 - mae_t1: 0.0234 - val_loss: 8.1844 - val_mae: 0.4846 - val_mean_pred: 0.9026 - val_mae_t1: 0.0323\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.4673 - mae: 0.3237 - mean_pred: 0.7711 - mae_t1: 0.0216 - val_loss: 8.0202 - val_mae: 0.4749 - val_mean_pred: 0.8452 - val_mae_t1: 0.0317\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.1149 - mae: 0.3029 - mean_pred: 0.7502 - mae_t1: 0.0202 - val_loss: 7.5297 - val_mae: 0.4458 - val_mean_pred: 0.9498 - val_mae_t1: 0.0297\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 5.0835 - mae: 0.3010 - mean_pred: 0.8627 - mae_t1: 0.0201 - val_loss: 8.9069 - val_mae: 0.5274 - val_mean_pred: 0.8703 - val_mae_t1: 0.0352\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 5.4879 - mae: 0.3249 - mean_pred: 0.7159 - mae_t1: 0.0217 - val_loss: 8.6740 - val_mae: 0.5136 - val_mean_pred: 1.0504 - val_mae_t1: 0.0342\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 6.0043 - mae: 0.3555 - mean_pred: 0.9897 - mae_t1: 0.0237 - val_loss: 8.2089 - val_mae: 0.4861 - val_mean_pred: 0.8705 - val_mae_t1: 0.0324\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 5.8201 - mae: 0.3446 - mean_pred: 0.6752 - mae_t1: 0.0230 - val_loss: 10.0320 - val_mae: 0.5940 - val_mean_pred: 0.7680 - val_mae_t1: 0.0396\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 6.1420 - mae: 0.3637 - mean_pred: 0.7716 - mae_t1: 0.0242 - val_loss: 10.7541 - val_mae: 0.6368 - val_mean_pred: 1.0043 - val_mae_t1: 0.0425\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 10.4789 - mae: 0.6205 - mean_pred: 0.9299 - mae_t1: 0.0414 - val_loss: 12.1036 - val_mae: 0.7167 - val_mean_pred: 0.9197 - val_mae_t1: 0.0478\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 10.8323 - mae: 0.6414 - mean_pred: 0.8094 - mae_t1: 0.0428 - val_loss: 8.6367 - val_mae: 0.5114 - val_mean_pred: 0.7752 - val_mae_t1: 0.0341\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 11.5976 - mae: 0.6867 - mean_pred: 0.9271 - mae_t1: 0.0458 - val_loss: 7.9903 - val_mae: 0.4731 - val_mean_pred: 0.7794 - val_mae_t1: 0.0315\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 8.3685 - mae: 0.4955 - mean_pred: 0.7708 - mae_t1: 0.0330 - val_loss: 7.9143 - val_mae: 0.4686 - val_mean_pred: 0.7969 - val_mae_t1: 0.0312\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.6514 - mae: 0.4530 - mean_pred: 0.7575 - mae_t1: 0.0302 - val_loss: 9.2240 - val_mae: 0.5462 - val_mean_pred: 0.7425 - val_mae_t1: 0.0364\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.4557 - mae: 0.3822 - mean_pred: 0.7135 - mae_t1: 0.0255 - val_loss: 8.8880 - val_mae: 0.5263 - val_mean_pred: 0.9417 - val_mae_t1: 0.0351\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.9073 - mae: 0.3498 - mean_pred: 0.8547 - mae_t1: 0.0233 - val_loss: 7.5051 - val_mae: 0.4444 - val_mean_pred: 0.8755 - val_mae_t1: 0.0296\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.4678 - mae: 0.3830 - mean_pred: 0.7007 - mae_t1: 0.0255 - val_loss: 8.9624 - val_mae: 0.5307 - val_mean_pred: 0.8324 - val_mae_t1: 0.0354\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.2267 - mae: 0.3687 - mean_pred: 0.8076 - mae_t1: 0.0246 - val_loss: 8.9056 - val_mae: 0.5273 - val_mean_pred: 0.9865 - val_mae_t1: 0.0352\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 6.4167 - mae: 0.3799 - mean_pred: 0.7864 - mae_t1: 0.0253 - val_loss: 8.7271 - val_mae: 0.5167 - val_mean_pred: 0.6974 - val_mae_t1: 0.0344\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.9702 - mae: 0.4127 - mean_pred: 0.6615 - mae_t1: 0.0275 - val_loss: 8.9314 - val_mae: 0.5288 - val_mean_pred: 0.8315 - val_mae_t1: 0.0353\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 5.7576 - mae: 0.3409 - mean_pred: 0.7997 - mae_t1: 0.0227 - val_loss: 7.6634 - val_mae: 0.4538 - val_mean_pred: 0.8803 - val_mae_t1: 0.0303\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 5.2973 - mae: 0.3137 - mean_pred: 0.7066 - mae_t1: 0.0209 - val_loss: 7.6932 - val_mae: 0.4555 - val_mean_pred: 0.7985 - val_mae_t1: 0.0304\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 5.0084 - mae: 0.2966 - mean_pred: 0.8076 - mae_t1: 0.0198 - val_loss: 7.9806 - val_mae: 0.4725 - val_mean_pred: 0.9398 - val_mae_t1: 0.0315\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.1188 - mae: 0.3031 - mean_pred: 0.7355 - mae_t1: 0.0202 - val_loss: 7.7057 - val_mae: 0.4563 - val_mean_pred: 0.7056 - val_mae_t1: 0.0304\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.4112 - mae: 0.3204 - mean_pred: 0.7303 - mae_t1: 0.0214 - val_loss: 8.2103 - val_mae: 0.4861 - val_mean_pred: 0.9649 - val_mae_t1: 0.0324\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.3169 - mae: 0.3740 - mean_pred: 0.9087 - mae_t1: 0.0249 - val_loss: 8.9861 - val_mae: 0.5321 - val_mean_pred: 0.9925 - val_mae_t1: 0.0355\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.2369 - mae: 0.3101 - mean_pred: 0.7839 - mae_t1: 0.0207 - val_loss: 7.9448 - val_mae: 0.4704 - val_mean_pred: 0.8432 - val_mae_t1: 0.0314\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0063 - mae: 0.2964 - mean_pred: 0.7494 - mae_t1: 0.0198 - val_loss: 7.6200 - val_mae: 0.4512 - val_mean_pred: 0.8815 - val_mae_t1: 0.0301\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0742 - mae: 0.3004 - mean_pred: 0.7642 - mae_t1: 0.0200 - val_loss: 7.8820 - val_mae: 0.4667 - val_mean_pred: 0.8528 - val_mae_t1: 0.0311\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.7282 - mae: 0.2800 - mean_pred: 0.7565 - mae_t1: 0.0187 - val_loss: 8.5160 - val_mae: 0.5042 - val_mean_pred: 1.0135 - val_mae_t1: 0.0336\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.9312 - mae: 0.2920 - mean_pred: 0.9021 - mae_t1: 0.0195 - val_loss: 7.7788 - val_mae: 0.4606 - val_mean_pred: 0.8726 - val_mae_t1: 0.0307\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 4.7930 - mae: 0.2838 - mean_pred: 0.7086 - mae_t1: 0.0189 - val_loss: 8.0340 - val_mae: 0.4757 - val_mean_pred: 0.8034 - val_mae_t1: 0.0317\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.3619 - mae: 0.2583 - mean_pred: 0.7806 - mae_t1: 0.0172 - val_loss: 8.1968 - val_mae: 0.4853 - val_mean_pred: 1.0342 - val_mae_t1: 0.0324\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.0836 - mae: 0.3010 - mean_pred: 0.9181 - mae_t1: 0.0201 - val_loss: 7.2023 - val_mae: 0.4265 - val_mean_pred: 0.8770 - val_mae_t1: 0.0284\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.9973 - mae: 0.2959 - mean_pred: 0.7528 - mae_t1: 0.0197 - val_loss: 8.2260 - val_mae: 0.4871 - val_mean_pred: 0.8977 - val_mae_t1: 0.0325\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 5.1148 - mae: 0.3028 - mean_pred: 0.8152 - mae_t1: 0.0202 - val_loss: 7.2916 - val_mae: 0.4317 - val_mean_pred: 0.9616 - val_mae_t1: 0.0288\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.33440580936900355  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▅▅▅▆▅▄▃▄▃▃▃▅▂▃▂▃▂▃▂▂▂▃▂▂▂▂▇▅▃▃▄▂▂▂▁▁▂</td></tr><tr><td>mae</td><td>█▇▆▅▅▅▆▅▄▃▄▃▃▃▅▂▃▂▃▂▃▂▂▂▃▂▂▂▂▇▅▃▃▄▂▂▂▁▁▂</td></tr><tr><td>mae_t1</td><td>█▇▆▅▅▅▆▅▄▃▄▃▃▃▅▂▃▂▃▂▃▂▂▂▃▂▂▂▂▇▅▃▃▄▂▂▂▁▁▂</td></tr><tr><td>mean_pred</td><td>▁▇▇▇▃▃▇█▂▃▂▃▅▄▁▄▆▄▂▃▃▄▃▅▅▅▄▅▂▆▄▃▄▂▄▃▄▄▄▅</td></tr><tr><td>val_loss</td><td>▂▆▆▇▃█▃▄▂▃▂▂▃▃▂▁▂▄▃▄▄▂▃▃▁▂▂▃▅█▂▃▃▃▂▂▁▃▂▁</td></tr><tr><td>val_mae</td><td>▂▆▆▇▃█▃▄▂▃▂▂▃▃▂▁▂▄▃▄▄▂▃▃▁▂▂▃▅█▂▃▃▃▂▂▁▃▂▁</td></tr><tr><td>val_mae_t1</td><td>▂▆▆▇▃█▃▄▂▃▂▂▃▃▂▁▂▄▃▄▄▂▃▃▁▂▂▃▅█▂▃▃▃▂▂▁▃▂▁</td></tr><tr><td>val_mean_pred</td><td>▅▂▁▁▅█▆▃▄▃▅▄▃▅▄▅▅▅▆▇▆▅▅▄▅▄▄▅▄▅▄▅▆▄▅▅▅▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33777</td></tr><tr><td>AE_2</td><td>0.32446</td></tr><tr><td>AE_3</td><td>0.35287</td></tr><tr><td>MAE</td><td>0.33441</td></tr><tr><td>best_epoch</td><td>41</td></tr><tr><td>best_val_loss</td><td>6.83164</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.11476</td></tr><tr><td>mae</td><td>0.30285</td></tr><tr><td>mae_t1</td><td>0.02019</td></tr><tr><td>mean_pred</td><td>0.81517</td></tr><tr><td>val_loss</td><td>7.29156</td></tr><tr><td>val_mae</td><td>0.43174</td></tr><tr><td>val_mae_t1</td><td>0.02878</td></tr><tr><td>val_mean_pred</td><td>0.9616</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">polished-rain-67</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3p3ij3kg\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3p3ij3kg</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154718-3p3ij3kg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154745-2rr8rd9k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2rr8rd9k\" target=\"_blank\">denim-firefly-68</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 695us/sample - loss: 14.7613 - mae: 0.8740 - mean_pred: 0.2230 - mae_t1: 0.0583 - val_loss: 8.7192 - val_mae: 0.5163 - val_mean_pred: 0.7379 - val_mae_t1: 0.0344\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 10.8465 - mae: 0.6422 - mean_pred: 0.7847 - mae_t1: 0.0428 - val_loss: 11.4300 - val_mae: 0.6768 - val_mean_pred: 0.4617 - val_mae_t1: 0.0451\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 145us/sample - loss: 11.5775 - mae: 0.6855 - mean_pred: 0.4962 - mae_t1: 0.0457 - val_loss: 9.3348 - val_mae: 0.5527 - val_mean_pred: 0.6460 - val_mae_t1: 0.0368\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 9.9353 - mae: 0.5883 - mean_pred: 0.7819 - mae_t1: 0.0392 - val_loss: 9.4123 - val_mae: 0.5573 - val_mean_pred: 1.1061 - val_mae_t1: 0.0372\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 9.6588 - mae: 0.5719 - mean_pred: 0.9882 - mae_t1: 0.0381 - val_loss: 10.6833 - val_mae: 0.6326 - val_mean_pred: 0.5921 - val_mae_t1: 0.0422\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 10.8001 - mae: 0.6395 - mean_pred: 0.5422 - mae_t1: 0.0426 - val_loss: 9.0301 - val_mae: 0.5347 - val_mean_pred: 0.5514 - val_mae_t1: 0.0356\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 9.7700 - mae: 0.5785 - mean_pred: 0.5358 - mae_t1: 0.0386 - val_loss: 9.4793 - val_mae: 0.5613 - val_mean_pred: 0.6851 - val_mae_t1: 0.0374\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 9.3063 - mae: 0.5510 - mean_pred: 0.6461 - mae_t1: 0.0367 - val_loss: 9.9419 - val_mae: 0.5887 - val_mean_pred: 0.8482 - val_mae_t1: 0.0392\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 11.3909 - mae: 0.6745 - mean_pred: 0.8493 - mae_t1: 0.0450 - val_loss: 8.9718 - val_mae: 0.5312 - val_mean_pred: 0.8992 - val_mae_t1: 0.0354\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.3950 - mae: 0.5563 - mean_pred: 0.8488 - mae_t1: 0.0371 - val_loss: 11.7680 - val_mae: 0.6968 - val_mean_pred: 0.4831 - val_mae_t1: 0.0465\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 12.3027 - mae: 0.7284 - mean_pred: 0.4680 - mae_t1: 0.0486 - val_loss: 13.0792 - val_mae: 0.7744 - val_mean_pred: 0.3303 - val_mae_t1: 0.0516\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 10.3713 - mae: 0.6141 - mean_pred: 0.3953 - mae_t1: 0.0409 - val_loss: 11.0104 - val_mae: 0.6519 - val_mean_pred: 1.1781 - val_mae_t1: 0.0435\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 10.2478 - mae: 0.6068 - mean_pred: 1.0977 - mae_t1: 0.0405 - val_loss: 10.0495 - val_mae: 0.5950 - val_mean_pred: 0.7237 - val_mae_t1: 0.0397\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.9009 - mae: 0.5862 - mean_pred: 0.5864 - mae_t1: 0.0391 - val_loss: 8.9411 - val_mae: 0.5294 - val_mean_pred: 0.7769 - val_mae_t1: 0.0353\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.6634 - mae: 0.5130 - mean_pred: 0.8491 - mae_t1: 0.0342 - val_loss: 11.0573 - val_mae: 0.6547 - val_mean_pred: 1.2321 - val_mae_t1: 0.0436\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.8095 - mae: 0.5808 - mean_pred: 1.0303 - mae_t1: 0.0387 - val_loss: 13.2416 - val_mae: 0.7840 - val_mean_pred: 0.2441 - val_mae_t1: 0.0523\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 13.8933 - mae: 0.8226 - mean_pred: 0.2170 - mae_t1: 0.0548 - val_loss: 13.3799 - val_mae: 0.7922 - val_mean_pred: 0.2549 - val_mae_t1: 0.0528\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.4005 - mae: 0.6750 - mean_pred: 0.4134 - mae_t1: 0.0450 - val_loss: 9.5693 - val_mae: 0.5666 - val_mean_pred: 1.1030 - val_mae_t1: 0.0378\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.7406 - mae: 0.6360 - mean_pred: 1.0530 - mae_t1: 0.0424 - val_loss: 9.9310 - val_mae: 0.5880 - val_mean_pred: 0.9092 - val_mae_t1: 0.0392\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.5149 - mae: 0.5634 - mean_pred: 0.7454 - mae_t1: 0.0376 - val_loss: 10.4845 - val_mae: 0.6208 - val_mean_pred: 0.5852 - val_mae_t1: 0.0414\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.9317 - mae: 0.5881 - mean_pred: 0.5821 - mae_t1: 0.0392 - val_loss: 9.9933 - val_mae: 0.5917 - val_mean_pred: 0.7922 - val_mae_t1: 0.0394\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.8241 - mae: 0.6409 - mean_pred: 0.8164 - mae_t1: 0.0427 - val_loss: 11.7584 - val_mae: 0.6962 - val_mean_pred: 1.1045 - val_mae_t1: 0.0464\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.6177 - mae: 0.6287 - mean_pred: 1.0700 - mae_t1: 0.0419 - val_loss: 10.6180 - val_mae: 0.6287 - val_mean_pred: 1.0030 - val_mae_t1: 0.0419\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 10.4302 - mae: 0.6176 - mean_pred: 0.8384 - mae_t1: 0.0412 - val_loss: 11.1445 - val_mae: 0.6599 - val_mean_pred: 0.4506 - val_mae_t1: 0.0440\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 10.0946 - mae: 0.5977 - mean_pred: 0.4846 - mae_t1: 0.0398 - val_loss: 8.6170 - val_mae: 0.5102 - val_mean_pred: 0.7396 - val_mae_t1: 0.0340\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.8197 - mae: 0.5222 - mean_pred: 0.8115 - mae_t1: 0.0348 - val_loss: 8.7863 - val_mae: 0.5202 - val_mean_pred: 0.9491 - val_mae_t1: 0.0347\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.9463 - mae: 0.5297 - mean_pred: 0.8007 - mae_t1: 0.0353 - val_loss: 9.7911 - val_mae: 0.5797 - val_mean_pred: 0.5323 - val_mae_t1: 0.0386\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 10.7683 - mae: 0.6376 - mean_pred: 0.5118 - mae_t1: 0.0425 - val_loss: 10.6014 - val_mae: 0.6277 - val_mean_pred: 0.5647 - val_mae_t1: 0.0418\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 9.9162 - mae: 0.5871 - mean_pred: 0.5199 - mae_t1: 0.0391 - val_loss: 8.2138 - val_mae: 0.4863 - val_mean_pred: 0.7329 - val_mae_t1: 0.0324\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 7.7151 - mae: 0.4568 - mean_pred: 0.6515 - mae_t1: 0.0305 - val_loss: 7.8559 - val_mae: 0.4652 - val_mean_pred: 0.8254 - val_mae_t1: 0.0310\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 8.1549 - mae: 0.4829 - mean_pred: 0.7963 - mae_t1: 0.0322 - val_loss: 9.0082 - val_mae: 0.5334 - val_mean_pred: 0.9954 - val_mae_t1: 0.0356\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 8.4530 - mae: 0.5005 - mean_pred: 0.9503 - mae_t1: 0.0334 - val_loss: 9.0272 - val_mae: 0.5345 - val_mean_pred: 0.7762 - val_mae_t1: 0.0356\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.6403 - mae: 0.5116 - mean_pred: 0.6335 - mae_t1: 0.0341 - val_loss: 10.6914 - val_mae: 0.6330 - val_mean_pred: 0.5368 - val_mae_t1: 0.0422\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.9402 - mae: 0.5294 - mean_pred: 0.5014 - mae_t1: 0.0353 - val_loss: 10.8750 - val_mae: 0.6439 - val_mean_pred: 1.1228 - val_mae_t1: 0.0429\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.5430 - mae: 0.5650 - mean_pred: 1.0635 - mae_t1: 0.0377 - val_loss: 12.1343 - val_mae: 0.7185 - val_mean_pred: 1.2818 - val_mae_t1: 0.0479\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.4176 - mae: 0.5576 - mean_pred: 1.0577 - mae_t1: 0.0372 - val_loss: 8.3941 - val_mae: 0.4970 - val_mean_pred: 0.6723 - val_mae_t1: 0.0331\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.3507 - mae: 0.4944 - mean_pred: 0.5814 - mae_t1: 0.0330 - val_loss: 8.1542 - val_mae: 0.4828 - val_mean_pred: 0.7665 - val_mae_t1: 0.0322\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 7.3990 - mae: 0.4381 - mean_pred: 0.8050 - mae_t1: 0.0292 - val_loss: 10.9290 - val_mae: 0.6471 - val_mean_pred: 1.1866 - val_mae_t1: 0.0431\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 9.0105 - mae: 0.5335 - mean_pred: 1.0090 - mae_t1: 0.0356 - val_loss: 8.9234 - val_mae: 0.5284 - val_mean_pred: 0.9331 - val_mae_t1: 0.0352\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 7.1832 - mae: 0.4253 - mean_pred: 0.7761 - mae_t1: 0.0284 - val_loss: 9.5954 - val_mae: 0.5681 - val_mean_pred: 0.9299 - val_mae_t1: 0.0379\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 7.8025 - mae: 0.4620 - mean_pred: 0.8144 - mae_t1: 0.0308 - val_loss: 8.6096 - val_mae: 0.5098 - val_mean_pred: 1.0547 - val_mae_t1: 0.0340\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 7.2259 - mae: 0.4279 - mean_pred: 0.8730 - mae_t1: 0.0285 - val_loss: 8.0805 - val_mae: 0.4785 - val_mean_pred: 0.9500 - val_mae_t1: 0.0319\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.6929 - mae: 0.3963 - mean_pred: 0.7728 - mae_t1: 0.0264 - val_loss: 7.9232 - val_mae: 0.4691 - val_mean_pred: 0.8629 - val_mae_t1: 0.0313\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 6.5047 - mae: 0.3851 - mean_pred: 0.7131 - mae_t1: 0.0257 - val_loss: 8.4996 - val_mae: 0.5033 - val_mean_pred: 0.7451 - val_mae_t1: 0.0336\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.1640 - mae: 0.4242 - mean_pred: 0.6322 - mae_t1: 0.0283 - val_loss: 8.8816 - val_mae: 0.5259 - val_mean_pred: 0.7282 - val_mae_t1: 0.0351\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.3768 - mae: 0.4368 - mean_pred: 0.6836 - mae_t1: 0.0291 - val_loss: 9.7675 - val_mae: 0.5783 - val_mean_pred: 0.9182 - val_mae_t1: 0.0386\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.6884 - mae: 0.4552 - mean_pred: 0.8775 - mae_t1: 0.0303 - val_loss: 11.4441 - val_mae: 0.6776 - val_mean_pred: 1.0338 - val_mae_t1: 0.0452\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 9.1537 - mae: 0.5420 - mean_pred: 0.8546 - mae_t1: 0.0361 - val_loss: 9.7822 - val_mae: 0.5792 - val_mean_pred: 0.9770 - val_mae_t1: 0.0386\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.2293 - mae: 0.4873 - mean_pred: 0.8793 - mae_t1: 0.0325 - val_loss: 10.6113 - val_mae: 0.6283 - val_mean_pred: 1.1345 - val_mae_t1: 0.0419\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 8.5129 - mae: 0.5041 - mean_pred: 0.9656 - mae_t1: 0.0336 - val_loss: 13.0375 - val_mae: 0.7720 - val_mean_pred: 0.9816 - val_mae_t1: 0.0515\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.6784 - mae: 0.6323 - mean_pred: 0.8028 - mae_t1: 0.0422 - val_loss: 10.6351 - val_mae: 0.6297 - val_mean_pred: 0.5738 - val_mae_t1: 0.0420\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.5896 - mae: 0.5678 - mean_pred: 0.5039 - mae_t1: 0.0379 - val_loss: 9.1497 - val_mae: 0.5418 - val_mean_pred: 0.7428 - val_mae_t1: 0.0361\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 10.4897 - mae: 0.6211 - mean_pred: 0.7732 - mae_t1: 0.0414 - val_loss: 8.9508 - val_mae: 0.5300 - val_mean_pred: 1.0283 - val_mae_t1: 0.0353\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 10.0950 - mae: 0.5977 - mean_pred: 1.0520 - mae_t1: 0.0398 - val_loss: 12.1179 - val_mae: 0.7175 - val_mean_pred: 1.2430 - val_mae_t1: 0.0478\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 10.7026 - mae: 0.6337 - mean_pred: 1.1216 - mae_t1: 0.0422 - val_loss: 9.9736 - val_mae: 0.5905 - val_mean_pred: 0.7943 - val_mae_t1: 0.0394\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.6309 - mae: 0.5110 - mean_pred: 0.6296 - mae_t1: 0.0341 - val_loss: 13.8682 - val_mae: 0.8211 - val_mean_pred: 0.4287 - val_mae_t1: 0.0547\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 13.1827 - mae: 0.7806 - mean_pred: 0.4487 - mae_t1: 0.0520 - val_loss: 14.3430 - val_mae: 0.8493 - val_mean_pred: 0.6668 - val_mae_t1: 0.0566\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 13.2686 - mae: 0.7856 - mean_pred: 0.7054 - mae_t1: 0.0524 - val_loss: 10.4804 - val_mae: 0.6205 - val_mean_pred: 1.1342 - val_mae_t1: 0.0414\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 9.4984 - mae: 0.5624 - mean_pred: 0.9851 - mae_t1: 0.0375 - val_loss: 12.0261 - val_mae: 0.7121 - val_mean_pred: 1.1444 - val_mae_t1: 0.0475\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 10.7521 - mae: 0.6366 - mean_pred: 1.0028 - mae_t1: 0.0424 - val_loss: 7.7359 - val_mae: 0.4580 - val_mean_pred: 0.9005 - val_mae_t1: 0.0305\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.2106 - mae: 0.4269 - mean_pred: 0.7568 - mae_t1: 0.0285 - val_loss: 10.8890 - val_mae: 0.6447 - val_mean_pred: 0.7334 - val_mae_t1: 0.0430\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 9.0139 - mae: 0.5337 - mean_pred: 0.6321 - mae_t1: 0.0356 - val_loss: 8.2529 - val_mae: 0.4887 - val_mean_pred: 0.6982 - val_mae_t1: 0.0326\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.6206 - mae: 0.4512 - mean_pred: 0.6763 - mae_t1: 0.0301 - val_loss: 8.6604 - val_mae: 0.5128 - val_mean_pred: 0.9841 - val_mae_t1: 0.0342\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.3376 - mae: 0.4937 - mean_pred: 0.9254 - mae_t1: 0.0329 - val_loss: 9.7021 - val_mae: 0.5745 - val_mean_pred: 1.0882 - val_mae_t1: 0.0383\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.0448 - mae: 0.4763 - mean_pred: 0.9665 - mae_t1: 0.0318 - val_loss: 9.1191 - val_mae: 0.5399 - val_mean_pred: 0.8394 - val_mae_t1: 0.0360\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.3940 - mae: 0.4970 - mean_pred: 0.7889 - mae_t1: 0.0331 - val_loss: 9.3339 - val_mae: 0.5527 - val_mean_pred: 0.7598 - val_mae_t1: 0.0368\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 8.2038 - mae: 0.4858 - mean_pred: 0.8046 - mae_t1: 0.0324 - val_loss: 9.5504 - val_mae: 0.5655 - val_mean_pred: 0.9165 - val_mae_t1: 0.0377\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.3041 - mae: 0.4917 - mean_pred: 0.8700 - mae_t1: 0.0328 - val_loss: 10.2888 - val_mae: 0.6092 - val_mean_pred: 0.7619 - val_mae_t1: 0.0406\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.8392 - mae: 0.5234 - mean_pred: 0.6882 - mae_t1: 0.0349 - val_loss: 10.2035 - val_mae: 0.6042 - val_mean_pred: 0.6674 - val_mae_t1: 0.0403\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.2886 - mae: 0.4908 - mean_pred: 0.6297 - mae_t1: 0.0327 - val_loss: 10.2492 - val_mae: 0.6069 - val_mean_pred: 1.0004 - val_mae_t1: 0.0405\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.4127 - mae: 0.5573 - mean_pred: 0.9173 - mae_t1: 0.0372 - val_loss: 13.1650 - val_mae: 0.7795 - val_mean_pred: 1.2973 - val_mae_t1: 0.0520\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 11.0204 - mae: 0.6525 - mean_pred: 1.0900 - mae_t1: 0.0435 - val_loss: 10.5762 - val_mae: 0.6262 - val_mean_pred: 1.2031 - val_mae_t1: 0.0417\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.9323 - mae: 0.5289 - mean_pred: 0.9808 - mae_t1: 0.0353 - val_loss: 9.7006 - val_mae: 0.5744 - val_mean_pred: 0.9416 - val_mae_t1: 0.0383\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.3332 - mae: 0.4342 - mean_pred: 0.7540 - mae_t1: 0.0289 - val_loss: 8.2532 - val_mae: 0.4887 - val_mean_pred: 0.6902 - val_mae_t1: 0.0326\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.9914 - mae: 0.5324 - mean_pred: 0.6152 - mae_t1: 0.0355 - val_loss: 8.6301 - val_mae: 0.5110 - val_mean_pred: 0.5481 - val_mae_t1: 0.0341\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1396 - mae: 0.4820 - mean_pred: 0.5353 - mae_t1: 0.0321 - val_loss: 8.0865 - val_mae: 0.4788 - val_mean_pred: 0.8510 - val_mae_t1: 0.0319\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.3358 - mae: 0.4344 - mean_pred: 0.7910 - mae_t1: 0.0290 - val_loss: 9.3822 - val_mae: 0.5555 - val_mean_pred: 1.1123 - val_mae_t1: 0.0370\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.6684 - mae: 0.4541 - mean_pred: 0.9697 - mae_t1: 0.0303 - val_loss: 10.1827 - val_mae: 0.6029 - val_mean_pred: 0.9914 - val_mae_t1: 0.0402\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.4019 - mae: 0.4383 - mean_pred: 0.8338 - mae_t1: 0.0292 - val_loss: 9.7333 - val_mae: 0.5763 - val_mean_pred: 0.6981 - val_mae_t1: 0.0384\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 7.2639 - mae: 0.4301 - mean_pred: 0.5904 - mae_t1: 0.0287 - val_loss: 11.0692 - val_mae: 0.6554 - val_mean_pred: 0.5909 - val_mae_t1: 0.0437\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.8903 - mae: 0.4672 - mean_pred: 0.5817 - mae_t1: 0.0311 - val_loss: 10.9545 - val_mae: 0.6486 - val_mean_pred: 0.7834 - val_mae_t1: 0.0432\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.8250 - mae: 0.5225 - mean_pred: 0.8422 - mae_t1: 0.0348 - val_loss: 10.7258 - val_mae: 0.6351 - val_mean_pred: 1.0652 - val_mae_t1: 0.0423\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.9231 - mae: 0.5283 - mean_pred: 1.0272 - mae_t1: 0.0352 - val_loss: 11.7097 - val_mae: 0.6933 - val_mean_pred: 1.0600 - val_mae_t1: 0.0462\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 8.1901 - mae: 0.4849 - mean_pred: 0.9278 - mae_t1: 0.0323 - val_loss: 11.2608 - val_mae: 0.6668 - val_mean_pred: 0.6504 - val_mae_t1: 0.0445\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.0711 - mae: 0.5963 - mean_pred: 0.6898 - mae_t1: 0.0398 - val_loss: 11.1420 - val_mae: 0.6597 - val_mean_pred: 0.4588 - val_mae_t1: 0.0440\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.8587 - mae: 0.5245 - mean_pred: 0.4917 - mae_t1: 0.0350 - val_loss: 13.6821 - val_mae: 0.8101 - val_mean_pred: 0.7185 - val_mae_t1: 0.0540\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 9.9901 - mae: 0.5915 - mean_pred: 0.6389 - mae_t1: 0.0394 - val_loss: 9.2503 - val_mae: 0.5477 - val_mean_pred: 0.7136 - val_mae_t1: 0.0365\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 8.1510 - mae: 0.4826 - mean_pred: 0.7649 - mae_t1: 0.0322 - val_loss: 9.7540 - val_mae: 0.5775 - val_mean_pred: 0.8492 - val_mae_t1: 0.0385\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 7.8224 - mae: 0.4632 - mean_pred: 0.8495 - mae_t1: 0.0309 - val_loss: 11.8184 - val_mae: 0.6998 - val_mean_pred: 0.9044 - val_mae_t1: 0.0467\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 9.3829 - mae: 0.5556 - mean_pred: 0.7770 - mae_t1: 0.0370 - val_loss: 9.3897 - val_mae: 0.5560 - val_mean_pred: 0.6877 - val_mae_t1: 0.0371\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.3854 - mae: 0.4373 - mean_pred: 0.6414 - mae_t1: 0.0292 - val_loss: 9.3603 - val_mae: 0.5542 - val_mean_pred: 0.6367 - val_mae_t1: 0.0369\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.4165 - mae: 0.4983 - mean_pred: 0.6131 - mae_t1: 0.0332 - val_loss: 9.3567 - val_mae: 0.5540 - val_mean_pred: 0.6402 - val_mae_t1: 0.0369\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 7.2285 - mae: 0.4280 - mean_pred: 0.6028 - mae_t1: 0.0285 - val_loss: 7.7438 - val_mae: 0.4585 - val_mean_pred: 0.7973 - val_mae_t1: 0.0306\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.1032 - mae: 0.4206 - mean_pred: 0.7701 - mae_t1: 0.0280 - val_loss: 8.7643 - val_mae: 0.5189 - val_mean_pred: 0.8901 - val_mae_t1: 0.0346\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.7239 - mae: 0.4573 - mean_pred: 0.8507 - mae_t1: 0.0305 - val_loss: 8.8863 - val_mae: 0.5262 - val_mean_pred: 0.9575 - val_mae_t1: 0.0351\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.1919 - mae: 0.4850 - mean_pred: 0.8833 - mae_t1: 0.0323 - val_loss: 7.8060 - val_mae: 0.4622 - val_mean_pred: 0.8877 - val_mae_t1: 0.0308\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.5192 - mae: 0.3860 - mean_pred: 0.8151 - mae_t1: 0.0257 - val_loss: 8.1837 - val_mae: 0.4846 - val_mean_pred: 0.7870 - val_mae_t1: 0.0323\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 7.1045 - mae: 0.4207 - mean_pred: 0.7859 - mae_t1: 0.0280 - val_loss: 8.6486 - val_mae: 0.5121 - val_mean_pred: 0.8207 - val_mae_t1: 0.0341\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 6.9875 - mae: 0.4137 - mean_pred: 0.8075 - mae_t1: 0.0276 - val_loss: 8.1613 - val_mae: 0.4832 - val_mean_pred: 0.8773 - val_mae_t1: 0.0322\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 6.4778 - mae: 0.3836 - mean_pred: 0.8411 - mae_t1: 0.0256 - val_loss: 8.0600 - val_mae: 0.4772 - val_mean_pred: 0.7918 - val_mae_t1: 0.0318\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3267660023501506  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▅▃▆▄▄▅▄▄▃▅▂▃▃▃▂▁▂▂▅▄▃▄▂▃▂▃▅▂▂▂▃▂▄▂▃▂▁▁</td></tr><tr><td>mae</td><td>█▅▅▃▆▄▄▅▄▄▃▅▂▃▃▃▂▁▂▂▅▄▃▄▂▃▂▃▅▂▂▂▃▂▄▂▃▂▁▁</td></tr><tr><td>mae_t1</td><td>█▅▅▃▆▄▄▅▄▄▃▅▂▃▃▃▂▁▂▂▅▄▃▄▂▃▂▃▅▂▂▂▃▂▄▂▃▂▁▁</td></tr><tr><td>mean_pred</td><td>▁▃▄▄▃█▇▃▄█▆▃▆▃█▇▆▅▅▆▆█▄▇▅▇▆▅█▅▆▆▆▇▄▆▄▅▆▆</td></tr><tr><td>val_loss</td><td>▂▃▂▃▇▃▇▃▃▄▂▄▂▄▁▂▂▂▃▄▄▆█▆▄▃▃▄▄▁▃▃▄▅▂▆▃▂▁▁</td></tr><tr><td>val_mae</td><td>▂▃▂▃▇▃▇▃▃▄▂▄▂▄▁▂▂▂▃▄▄▆█▆▄▃▃▄▄▁▃▃▄▅▂▆▃▂▁▁</td></tr><tr><td>val_mae_t1</td><td>▂▃▂▃▇▃▇▃▃▄▂▄▂▄▁▂▂▂▃▄▄▆█▆▄▃▃▄▄▁▃▃▄▅▂▆▃▂▁▁</td></tr><tr><td>val_mean_pred</td><td>▄▄▃▅▂▄▁▇▅▆▆▃▆▇▄▆▇▅▆▇▃█▂▇▄▇▆▄█▄▇▄▇▄▄▆▄▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.4255</td></tr><tr><td>AE_2</td><td>0.32842</td></tr><tr><td>AE_3</td><td>0.30655</td></tr><tr><td>MAE</td><td>0.32677</td></tr><tr><td>best_epoch</td><td>59</td></tr><tr><td>best_val_loss</td><td>7.73588</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>6.47779</td></tr><tr><td>mae</td><td>0.38355</td></tr><tr><td>mae_t1</td><td>0.02557</td></tr><tr><td>mean_pred</td><td>0.84112</td></tr><tr><td>val_loss</td><td>8.05996</td></tr><tr><td>val_mae</td><td>0.47723</td></tr><tr><td>val_mae_t1</td><td>0.03182</td></tr><tr><td>val_mean_pred</td><td>0.7918</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">denim-firefly-68</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2rr8rd9k\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2rr8rd9k</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154745-2rr8rd9k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154808-rjsqqfe9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/rjsqqfe9\" target=\"_blank\">fiery-waterfall-69</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 856us/sample - loss: 13.7652 - mae: 0.6452 - mean_pred: 0.6341 - mae_t1: 0.0430 - val_loss: 13.0844 - val_mae: 0.6133 - val_mean_pred: 0.4860 - val_mae_t1: 0.0409\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 291us/sample - loss: 13.0589 - mae: 0.6121 - mean_pred: 0.6561 - mae_t1: 0.0408 - val_loss: 17.1648 - val_mae: 0.8046 - val_mean_pred: 1.4174 - val_mae_t1: 0.0536\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 242us/sample - loss: 15.8862 - mae: 0.7447 - mean_pred: 0.9907 - mae_t1: 0.0496 - val_loss: 19.4384 - val_mae: 0.9112 - val_mean_pred: 0.7390 - val_mae_t1: 0.0607\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 21.3391 - mae: 1.0003 - mean_pred: 0.9236 - mae_t1: 0.0667 - val_loss: 19.5601 - val_mae: 0.9169 - val_mean_pred: 0.1231 - val_mae_t1: 0.0611\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 18.2593 - mae: 0.8559 - mean_pred: 0.3531 - mae_t1: 0.0571 - val_loss: 17.2984 - val_mae: 0.8109 - val_mean_pred: 1.2816 - val_mae_t1: 0.0541\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 16.6549 - mae: 0.7807 - mean_pred: 1.0559 - mae_t1: 0.0520 - val_loss: 13.9158 - val_mae: 0.6523 - val_mean_pred: 0.8774 - val_mae_t1: 0.0435\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 13.3194 - mae: 0.6243 - mean_pred: 0.7760 - mae_t1: 0.0416 - val_loss: 12.9837 - val_mae: 0.6086 - val_mean_pred: 1.1447 - val_mae_t1: 0.0406\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 13.8879 - mae: 0.6510 - mean_pred: 0.9113 - mae_t1: 0.0434 - val_loss: 16.2538 - val_mae: 0.7619 - val_mean_pred: 0.2893 - val_mae_t1: 0.0508\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 13.0453 - mae: 0.6115 - mean_pred: 0.5230 - mae_t1: 0.0408 - val_loss: 11.8976 - val_mae: 0.5577 - val_mean_pred: 1.1047 - val_mae_t1: 0.0372\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.1390 - mae: 0.5221 - mean_pred: 0.6982 - mae_t1: 0.0348 - val_loss: 11.9349 - val_mae: 0.5594 - val_mean_pred: 0.6097 - val_mae_t1: 0.0373\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 11.7864 - mae: 0.5525 - mean_pred: 0.6640 - mae_t1: 0.0368 - val_loss: 11.6887 - val_mae: 0.5479 - val_mean_pred: 1.1340 - val_mae_t1: 0.0365\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 10.8802 - mae: 0.5100 - mean_pred: 0.8925 - mae_t1: 0.0340 - val_loss: 10.9783 - val_mae: 0.5146 - val_mean_pred: 0.7704 - val_mae_t1: 0.0343\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 9.7616 - mae: 0.4576 - mean_pred: 0.7755 - mae_t1: 0.0305 - val_loss: 10.7905 - val_mae: 0.5058 - val_mean_pred: 0.9078 - val_mae_t1: 0.0337\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.6638 - mae: 0.4530 - mean_pred: 0.6797 - mae_t1: 0.0302 - val_loss: 13.1144 - val_mae: 0.6147 - val_mean_pred: 1.0226 - val_mae_t1: 0.0410\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.6002 - mae: 0.4969 - mean_pred: 0.8958 - mae_t1: 0.0331 - val_loss: 12.0659 - val_mae: 0.5656 - val_mean_pred: 0.7065 - val_mae_t1: 0.0377\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 10.2113 - mae: 0.4787 - mean_pred: 0.6854 - mae_t1: 0.0319 - val_loss: 11.2997 - val_mae: 0.5297 - val_mean_pred: 0.8745 - val_mae_t1: 0.0353\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.0441 - mae: 0.4708 - mean_pred: 0.6349 - mae_t1: 0.0314 - val_loss: 12.1552 - val_mae: 0.5698 - val_mean_pred: 0.9363 - val_mae_t1: 0.0380\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 10.0460 - mae: 0.4709 - mean_pred: 0.9490 - mae_t1: 0.0314 - val_loss: 10.2132 - val_mae: 0.4787 - val_mean_pred: 0.8390 - val_mae_t1: 0.0319\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 9.3476 - mae: 0.4382 - mean_pred: 0.6441 - mae_t1: 0.0292 - val_loss: 10.1175 - val_mae: 0.4743 - val_mean_pred: 0.8690 - val_mae_t1: 0.0316\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.9867 - mae: 0.3744 - mean_pred: 0.8225 - mae_t1: 0.0250 - val_loss: 10.4245 - val_mae: 0.4886 - val_mean_pred: 0.9483 - val_mae_t1: 0.0326\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 7.8502 - mae: 0.3680 - mean_pred: 0.7772 - mae_t1: 0.0245 - val_loss: 9.6701 - val_mae: 0.4533 - val_mean_pred: 0.8834 - val_mae_t1: 0.0302\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 8.6965 - mae: 0.4076 - mean_pred: 0.7579 - mae_t1: 0.0272 - val_loss: 10.2125 - val_mae: 0.4787 - val_mean_pred: 0.9492 - val_mae_t1: 0.0319\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.4335 - mae: 0.4891 - mean_pred: 0.9101 - mae_t1: 0.0326 - val_loss: 12.6463 - val_mae: 0.5928 - val_mean_pred: 0.7225 - val_mae_t1: 0.0395\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.2737 - mae: 0.5285 - mean_pred: 0.5202 - mae_t1: 0.0352 - val_loss: 12.0641 - val_mae: 0.5655 - val_mean_pred: 0.7555 - val_mae_t1: 0.0377\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.2325 - mae: 0.4796 - mean_pred: 0.8541 - mae_t1: 0.0320 - val_loss: 11.3357 - val_mae: 0.5314 - val_mean_pred: 1.0164 - val_mae_t1: 0.0354\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 8.6628 - mae: 0.4061 - mean_pred: 0.8160 - mae_t1: 0.0271 - val_loss: 12.5349 - val_mae: 0.5876 - val_mean_pred: 0.6192 - val_mae_t1: 0.0392\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.5772 - mae: 0.4489 - mean_pred: 0.6221 - mae_t1: 0.0299 - val_loss: 15.9480 - val_mae: 0.7476 - val_mean_pred: 1.3118 - val_mae_t1: 0.0498\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 12.9530 - mae: 0.6072 - mean_pred: 1.1046 - mae_t1: 0.0405 - val_loss: 16.6273 - val_mae: 0.7794 - val_mean_pred: 0.9535 - val_mae_t1: 0.0520\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 17.4662 - mae: 0.8187 - mean_pred: 0.8788 - mae_t1: 0.0546 - val_loss: 19.3155 - val_mae: 0.9054 - val_mean_pred: 0.6809 - val_mae_t1: 0.0604\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 21.0519 - mae: 0.9868 - mean_pred: 0.9304 - mae_t1: 0.0658 - val_loss: 9.9444 - val_mae: 0.4661 - val_mean_pred: 0.7721 - val_mae_t1: 0.0311\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 16.2784 - mae: 0.7630 - mean_pred: 0.9227 - mae_t1: 0.0509 - val_loss: 11.4462 - val_mae: 0.5365 - val_mean_pred: 0.7081 - val_mae_t1: 0.0358\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 11.1430 - mae: 0.5223 - mean_pred: 0.6537 - mae_t1: 0.0348 - val_loss: 13.3788 - val_mae: 0.6271 - val_mean_pred: 1.1097 - val_mae_t1: 0.0418\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.4233 - mae: 0.5355 - mean_pred: 1.0342 - mae_t1: 0.0357 - val_loss: 12.2350 - val_mae: 0.5735 - val_mean_pred: 0.9098 - val_mae_t1: 0.0382\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.4897 - mae: 0.4448 - mean_pred: 0.7163 - mae_t1: 0.0297 - val_loss: 13.0402 - val_mae: 0.6113 - val_mean_pred: 0.8480 - val_mae_t1: 0.0408\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 11.8801 - mae: 0.5569 - mean_pred: 0.7482 - mae_t1: 0.0371 - val_loss: 14.0736 - val_mae: 0.6597 - val_mean_pred: 0.8337 - val_mae_t1: 0.0440\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 10.4622 - mae: 0.4904 - mean_pred: 0.6689 - mae_t1: 0.0327 - val_loss: 12.7737 - val_mae: 0.5988 - val_mean_pred: 0.8912 - val_mae_t1: 0.0399\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 11.7850 - mae: 0.5524 - mean_pred: 0.8208 - mae_t1: 0.0368 - val_loss: 15.1371 - val_mae: 0.7096 - val_mean_pred: 0.9055 - val_mae_t1: 0.0473\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 9.9047 - mae: 0.4643 - mean_pred: 0.7086 - mae_t1: 0.0310 - val_loss: 10.6964 - val_mae: 0.5014 - val_mean_pred: 0.8535 - val_mae_t1: 0.0334\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 9.3947 - mae: 0.4404 - mean_pred: 0.8230 - mae_t1: 0.0294 - val_loss: 11.1375 - val_mae: 0.5221 - val_mean_pred: 0.7383 - val_mae_t1: 0.0348\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 10.0885 - mae: 0.4729 - mean_pred: 0.5539 - mae_t1: 0.0315 - val_loss: 10.7952 - val_mae: 0.5060 - val_mean_pred: 0.5843 - val_mae_t1: 0.0337\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 9.3661 - mae: 0.4390 - mean_pred: 0.6700 - mae_t1: 0.0293 - val_loss: 12.7405 - val_mae: 0.5972 - val_mean_pred: 1.0637 - val_mae_t1: 0.0398\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 9.1154 - mae: 0.4273 - mean_pred: 0.8206 - mae_t1: 0.0285 - val_loss: 10.5140 - val_mae: 0.4928 - val_mean_pred: 0.7307 - val_mae_t1: 0.0329\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 8.1103 - mae: 0.3802 - mean_pred: 0.6603 - mae_t1: 0.0253 - val_loss: 11.2355 - val_mae: 0.5267 - val_mean_pred: 1.0117 - val_mae_t1: 0.0351\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 8.5768 - mae: 0.4020 - mean_pred: 0.9769 - mae_t1: 0.0268 - val_loss: 10.0436 - val_mae: 0.4708 - val_mean_pred: 0.9511 - val_mae_t1: 0.0314\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.2324 - mae: 0.3390 - mean_pred: 0.7113 - mae_t1: 0.0226 - val_loss: 9.6728 - val_mae: 0.4534 - val_mean_pred: 0.9193 - val_mae_t1: 0.0302\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.5343 - mae: 0.3532 - mean_pred: 0.9170 - mae_t1: 0.0235 - val_loss: 10.6735 - val_mae: 0.5003 - val_mean_pred: 0.9347 - val_mae_t1: 0.0334\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.7537 - mae: 0.4103 - mean_pred: 0.6831 - mae_t1: 0.0274 - val_loss: 12.8102 - val_mae: 0.6005 - val_mean_pred: 0.5874 - val_mae_t1: 0.0400\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.9748 - mae: 0.4676 - mean_pred: 0.5589 - mae_t1: 0.0312 - val_loss: 12.1267 - val_mae: 0.5684 - val_mean_pred: 0.9626 - val_mae_t1: 0.0379\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 10.2169 - mae: 0.4789 - mean_pred: 0.9683 - mae_t1: 0.0319 - val_loss: 12.1642 - val_mae: 0.5702 - val_mean_pred: 1.0509 - val_mae_t1: 0.0380\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.1654 - mae: 0.3828 - mean_pred: 0.7634 - mae_t1: 0.0255 - val_loss: 9.9257 - val_mae: 0.4653 - val_mean_pred: 0.7769 - val_mae_t1: 0.0310\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.0899 - mae: 0.3792 - mean_pred: 0.7944 - mae_t1: 0.0253 - val_loss: 11.5594 - val_mae: 0.5418 - val_mean_pred: 1.1077 - val_mae_t1: 0.0361\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 10.3967 - mae: 0.4873 - mean_pred: 1.0046 - mae_t1: 0.0325 - val_loss: 13.0316 - val_mae: 0.6109 - val_mean_pred: 0.7755 - val_mae_t1: 0.0407\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.9937 - mae: 0.4685 - mean_pred: 0.5975 - mae_t1: 0.0312 - val_loss: 11.8148 - val_mae: 0.5538 - val_mean_pred: 0.6713 - val_mae_t1: 0.0369\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 8.4183 - mae: 0.3946 - mean_pred: 0.7554 - mae_t1: 0.0263 - val_loss: 14.1836 - val_mae: 0.6649 - val_mean_pred: 1.2239 - val_mae_t1: 0.0443\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.1858 - mae: 0.3837 - mean_pred: 0.9141 - mae_t1: 0.0256 - val_loss: 12.0460 - val_mae: 0.5647 - val_mean_pred: 0.8142 - val_mae_t1: 0.0376\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.7009 - mae: 0.4079 - mean_pred: 0.6424 - mae_t1: 0.0272 - val_loss: 10.1592 - val_mae: 0.4762 - val_mean_pred: 0.9069 - val_mae_t1: 0.0317\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.7283 - mae: 0.3154 - mean_pred: 0.8373 - mae_t1: 0.0210 - val_loss: 9.8772 - val_mae: 0.4630 - val_mean_pred: 0.9341 - val_mae_t1: 0.0309\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.7207 - mae: 0.3150 - mean_pred: 0.7477 - mae_t1: 0.0210 - val_loss: 10.0294 - val_mae: 0.4701 - val_mean_pred: 0.7949 - val_mae_t1: 0.0313\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.2614 - mae: 0.3404 - mean_pred: 0.8125 - mae_t1: 0.0227 - val_loss: 12.4898 - val_mae: 0.5855 - val_mean_pred: 1.0300 - val_mae_t1: 0.0390\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.4139 - mae: 0.3475 - mean_pred: 0.8914 - mae_t1: 0.0232 - val_loss: 12.0195 - val_mae: 0.5634 - val_mean_pred: 0.8864 - val_mae_t1: 0.0376\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.7369 - mae: 0.3627 - mean_pred: 0.7455 - mae_t1: 0.0242 - val_loss: 11.5763 - val_mae: 0.5426 - val_mean_pred: 0.8926 - val_mae_t1: 0.0362\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 8.9207 - mae: 0.4182 - mean_pred: 0.8165 - mae_t1: 0.0279 - val_loss: 13.4830 - val_mae: 0.6320 - val_mean_pred: 1.0154 - val_mae_t1: 0.0421\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 9.3074 - mae: 0.4363 - mean_pred: 0.9264 - mae_t1: 0.0291 - val_loss: 12.0456 - val_mae: 0.5646 - val_mean_pred: 0.8998 - val_mae_t1: 0.0376\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.6673 - mae: 0.4063 - mean_pred: 0.7234 - mae_t1: 0.0271 - val_loss: 10.3880 - val_mae: 0.4869 - val_mean_pred: 0.6911 - val_mae_t1: 0.0325\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 8.0958 - mae: 0.3795 - mean_pred: 0.6603 - mae_t1: 0.0253 - val_loss: 10.3055 - val_mae: 0.4831 - val_mean_pred: 0.9609 - val_mae_t1: 0.0322\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 7.3809 - mae: 0.3460 - mean_pred: 0.9005 - mae_t1: 0.0231 - val_loss: 10.5870 - val_mae: 0.4963 - val_mean_pred: 0.9552 - val_mae_t1: 0.0331\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 7.1347 - mae: 0.3344 - mean_pred: 0.7221 - mae_t1: 0.0223 - val_loss: 9.9392 - val_mae: 0.4659 - val_mean_pred: 0.7612 - val_mae_t1: 0.0311\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 6.8419 - mae: 0.3207 - mean_pred: 0.7121 - mae_t1: 0.0214 - val_loss: 10.5132 - val_mae: 0.4928 - val_mean_pred: 0.9324 - val_mae_t1: 0.0329\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.2194 - mae: 0.3384 - mean_pred: 0.8055 - mae_t1: 0.0226 - val_loss: 11.9487 - val_mae: 0.5601 - val_mean_pred: 0.8961 - val_mae_t1: 0.0373\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.8825 - mae: 0.3226 - mean_pred: 0.7553 - mae_t1: 0.0215 - val_loss: 10.9196 - val_mae: 0.5119 - val_mean_pred: 0.9276 - val_mae_t1: 0.0341\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 6.8994 - mae: 0.3234 - mean_pred: 0.7404 - mae_t1: 0.0216 - val_loss: 9.5621 - val_mae: 0.4482 - val_mean_pred: 0.8935 - val_mae_t1: 0.0299\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.1946 - mae: 0.3372 - mean_pred: 0.8558 - mae_t1: 0.0225 - val_loss: 9.7390 - val_mae: 0.4565 - val_mean_pred: 0.9353 - val_mae_t1: 0.0304\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 7.5175 - mae: 0.3524 - mean_pred: 0.7419 - mae_t1: 0.0235 - val_loss: 13.1340 - val_mae: 0.6157 - val_mean_pred: 0.8909 - val_mae_t1: 0.0410\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 8.9041 - mae: 0.4174 - mean_pred: 0.7547 - mae_t1: 0.0278 - val_loss: 11.8922 - val_mae: 0.5574 - val_mean_pred: 1.0480 - val_mae_t1: 0.0372\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.5937 - mae: 0.3560 - mean_pred: 0.8394 - mae_t1: 0.0237 - val_loss: 10.4643 - val_mae: 0.4905 - val_mean_pred: 0.9284 - val_mae_t1: 0.0327\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.9167 - mae: 0.3242 - mean_pred: 0.8182 - mae_t1: 0.0216 - val_loss: 12.8296 - val_mae: 0.6014 - val_mean_pred: 0.9163 - val_mae_t1: 0.0401\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 7.6544 - mae: 0.3588 - mean_pred: 0.7523 - mae_t1: 0.0239 - val_loss: 10.2361 - val_mae: 0.4798 - val_mean_pred: 0.8747 - val_mae_t1: 0.0320\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.6762 - mae: 0.3129 - mean_pred: 0.7989 - mae_t1: 0.0209 - val_loss: 12.8227 - val_mae: 0.6011 - val_mean_pred: 1.0307 - val_mae_t1: 0.0401\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 9.7534 - mae: 0.4572 - mean_pred: 0.9284 - mae_t1: 0.0305 - val_loss: 14.9543 - val_mae: 0.7010 - val_mean_pred: 1.0688 - val_mae_t1: 0.0467\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 8.9275 - mae: 0.4185 - mean_pred: 0.8122 - mae_t1: 0.0279 - val_loss: 8.5677 - val_mae: 0.4016 - val_mean_pred: 0.8060 - val_mae_t1: 0.0268\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 8.5598 - mae: 0.4012 - mean_pred: 0.7580 - mae_t1: 0.0267 - val_loss: 10.6711 - val_mae: 0.5002 - val_mean_pred: 1.0342 - val_mae_t1: 0.0333\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 8.7818 - mae: 0.4116 - mean_pred: 0.9778 - mae_t1: 0.0274 - val_loss: 11.2172 - val_mae: 0.5258 - val_mean_pred: 1.0117 - val_mae_t1: 0.0351\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 7.1235 - mae: 0.3339 - mean_pred: 0.7413 - mae_t1: 0.0223 - val_loss: 9.6036 - val_mae: 0.4502 - val_mean_pred: 0.8352 - val_mae_t1: 0.0300\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.5504 - mae: 0.3071 - mean_pred: 0.8266 - mae_t1: 0.0205 - val_loss: 11.2395 - val_mae: 0.5269 - val_mean_pred: 1.0774 - val_mae_t1: 0.0351\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.4032 - mae: 0.3002 - mean_pred: 0.8724 - mae_t1: 0.0200 - val_loss: 10.2942 - val_mae: 0.4825 - val_mean_pred: 0.8098 - val_mae_t1: 0.0322\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.8348 - mae: 0.3204 - mean_pred: 0.7301 - mae_t1: 0.0214 - val_loss: 9.4825 - val_mae: 0.4445 - val_mean_pred: 0.9387 - val_mae_t1: 0.0296\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.9982 - mae: 0.3280 - mean_pred: 0.8977 - mae_t1: 0.0219 - val_loss: 11.9396 - val_mae: 0.5597 - val_mean_pred: 0.9870 - val_mae_t1: 0.0373\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 8.3568 - mae: 0.3917 - mean_pred: 0.8191 - mae_t1: 0.0261 - val_loss: 11.7205 - val_mae: 0.5494 - val_mean_pred: 0.8040 - val_mae_t1: 0.0366\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 7.1323 - mae: 0.3343 - mean_pred: 0.6928 - mae_t1: 0.0223 - val_loss: 10.3632 - val_mae: 0.4858 - val_mean_pred: 0.8943 - val_mae_t1: 0.0324\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 7.0139 - mae: 0.3288 - mean_pred: 0.8302 - mae_t1: 0.0219 - val_loss: 10.4569 - val_mae: 0.4902 - val_mean_pred: 0.9517 - val_mae_t1: 0.0327\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 7.9337 - mae: 0.3719 - mean_pred: 0.8468 - mae_t1: 0.0248 - val_loss: 9.9527 - val_mae: 0.4665 - val_mean_pred: 0.8762 - val_mae_t1: 0.0311\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 6.4359 - mae: 0.3017 - mean_pred: 0.8424 - mae_t1: 0.0201 - val_loss: 11.1388 - val_mae: 0.5221 - val_mean_pred: 0.9485 - val_mae_t1: 0.0348\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 6.2179 - mae: 0.2915 - mean_pred: 0.7886 - mae_t1: 0.0194 - val_loss: 10.2461 - val_mae: 0.4803 - val_mean_pred: 0.8514 - val_mae_t1: 0.0320\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 6.0205 - mae: 0.2822 - mean_pred: 0.7896 - mae_t1: 0.0188 - val_loss: 11.1111 - val_mae: 0.5208 - val_mean_pred: 1.0025 - val_mae_t1: 0.0347\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.0827 - mae: 0.2851 - mean_pred: 0.8661 - mae_t1: 0.0190 - val_loss: 10.4082 - val_mae: 0.4879 - val_mean_pred: 0.9033 - val_mae_t1: 0.0325\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.1249 - mae: 0.2871 - mean_pred: 0.7767 - mae_t1: 0.0191 - val_loss: 11.4570 - val_mae: 0.5370 - val_mean_pred: 0.9779 - val_mae_t1: 0.0358\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 6.2948 - mae: 0.2951 - mean_pred: 0.8916 - mae_t1: 0.0197 - val_loss: 10.8351 - val_mae: 0.5079 - val_mean_pred: 1.0123 - val_mae_t1: 0.0339\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 6.2033 - mae: 0.2908 - mean_pred: 0.8777 - mae_t1: 0.0194 - val_loss: 10.2913 - val_mae: 0.4824 - val_mean_pred: 0.8754 - val_mae_t1: 0.0322\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.7243 - mae: 0.2683 - mean_pred: 0.7933 - mae_t1: 0.0179 - val_loss: 9.8620 - val_mae: 0.4623 - val_mean_pred: 0.8628 - val_mae_t1: 0.0308\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.5931 - mae: 0.2622 - mean_pred: 0.7552 - mae_t1: 0.0175 - val_loss: 10.4302 - val_mae: 0.4889 - val_mean_pred: 0.9433 - val_mae_t1: 0.0326\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.31988415403905196  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.071 MB of 0.071 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆██▆▅▄▄▄▂▄▃▆█▃▄▃▃▃▂▄▃▃▃▂▂▃▂▂▂▃▂▄▃▂▂▂▂▁▁▁</td></tr><tr><td>mae</td><td>▆██▆▅▄▄▄▂▄▃▆█▃▄▃▃▃▂▄▃▃▃▂▂▃▂▂▂▃▂▄▃▂▂▂▂▁▁▁</td></tr><tr><td>mae_t1</td><td>▆██▆▅▄▄▄▂▄▃▆█▃▄▃▃▃▂▄▃▃▃▂▂▃▂▂▂▃▂▄▃▂▂▂▂▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▆▇▅▁▃▂▆▃▅▄█▅▂▂▄▂▆▅▆▃▃▁▄▃▂▂▄▄▃▃▅▆▄▅▂▄▃▅▃</td></tr><tr><td>val_loss</td><td>▃█▄▆▂▂▂▁▁▃▃▆▂▃▃▂▃▁▂▃▂▄▁▃▂▂▁▃▁▃▁▅▂▂▃▁▂▂▂▂</td></tr><tr><td>val_mae</td><td>▃█▄▆▂▂▂▁▁▃▃▆▂▃▃▂▃▁▂▃▂▄▁▃▂▂▁▃▁▃▁▅▂▂▃▁▂▂▂▂</td></tr><tr><td>val_mae_t1</td><td>▃█▄▆▂▂▂▁▁▃▃▆▂▃▃▂▃▁▂▃▂▄▁▃▂▂▁▃▁▃▁▅▂▂▃▁▂▂▂▂</td></tr><tr><td>val_mean_pred</td><td>▂▄▅▁▇▆▅▅▅▄▃▆▄▅▆▄▇▆▆▇▇█▆▇▆▄▅▆▆▇▅▇▆▇▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.39037</td></tr><tr><td>AE_2</td><td>0.28437</td></tr><tr><td>AE_3</td><td>0.37604</td></tr><tr><td>MAE</td><td>0.31988</td></tr><tr><td>best_epoch</td><td>79</td></tr><tr><td>best_val_loss</td><td>8.56771</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.59309</td></tr><tr><td>mae</td><td>0.26218</td></tr><tr><td>mae_t1</td><td>0.01748</td></tr><tr><td>mean_pred</td><td>0.75524</td></tr><tr><td>val_loss</td><td>10.4302</td></tr><tr><td>val_mae</td><td>0.48892</td></tr><tr><td>val_mae_t1</td><td>0.03259</td></tr><tr><td>val_mean_pred</td><td>0.94328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">fiery-waterfall-69</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/rjsqqfe9\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/rjsqqfe9</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154808-rjsqqfe9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154842-tijiqvb8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/tijiqvb8\" target=\"_blank\">misunderstood-sea-70</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 702us/sample - loss: 18.2898 - mae: 0.8573 - mean_pred: 0.2430 - mae_t1: 0.0572 - val_loss: 13.0712 - val_mae: 0.6127 - val_mean_pred: 1.1799 - val_mae_t1: 0.0408\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 14.1757 - mae: 0.6645 - mean_pred: 1.0592 - mae_t1: 0.0443 - val_loss: 12.2466 - val_mae: 0.5741 - val_mean_pred: 0.5134 - val_mae_t1: 0.0383\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 163us/sample - loss: 12.9125 - mae: 0.6053 - mean_pred: 0.4586 - mae_t1: 0.0404 - val_loss: 11.8926 - val_mae: 0.5575 - val_mean_pred: 0.6278 - val_mae_t1: 0.0372\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 11.7605 - mae: 0.5513 - mean_pred: 0.5926 - mae_t1: 0.0368 - val_loss: 14.0109 - val_mae: 0.6568 - val_mean_pred: 1.2649 - val_mae_t1: 0.0438\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 129us/sample - loss: 15.7726 - mae: 0.7393 - mean_pred: 1.2028 - mae_t1: 0.0493 - val_loss: 11.6049 - val_mae: 0.5440 - val_mean_pred: 0.7080 - val_mae_t1: 0.0363\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 11.4147 - mae: 0.5351 - mean_pred: 0.6888 - mae_t1: 0.0357 - val_loss: 12.0212 - val_mae: 0.5635 - val_mean_pred: 0.6276 - val_mae_t1: 0.0376\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 12.5021 - mae: 0.5860 - mean_pred: 0.6868 - mae_t1: 0.0391 - val_loss: 15.5346 - val_mae: 0.7282 - val_mean_pred: 0.2793 - val_mae_t1: 0.0485\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 15.7350 - mae: 0.7376 - mean_pred: 0.2443 - mae_t1: 0.0492 - val_loss: 13.6695 - val_mae: 0.6408 - val_mean_pred: 0.4600 - val_mae_t1: 0.0427\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 12.0528 - mae: 0.5650 - mean_pred: 0.5558 - mae_t1: 0.0377 - val_loss: 13.7755 - val_mae: 0.6457 - val_mean_pred: 1.1494 - val_mae_t1: 0.0430\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 12.1113 - mae: 0.5677 - mean_pred: 0.9158 - mae_t1: 0.0378 - val_loss: 12.0046 - val_mae: 0.5627 - val_mean_pred: 0.6138 - val_mae_t1: 0.0375\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 12.0271 - mae: 0.5638 - mean_pred: 0.5425 - mae_t1: 0.0376 - val_loss: 14.3985 - val_mae: 0.6749 - val_mean_pred: 0.4812 - val_mae_t1: 0.0450\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 14.2564 - mae: 0.6683 - mean_pred: 0.4615 - mae_t1: 0.0446 - val_loss: 12.4819 - val_mae: 0.5851 - val_mean_pred: 0.7759 - val_mae_t1: 0.0390\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 13.0481 - mae: 0.6116 - mean_pred: 0.8500 - mae_t1: 0.0408 - val_loss: 15.1329 - val_mae: 0.7094 - val_mean_pred: 1.1504 - val_mae_t1: 0.0473\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 14.6985 - mae: 0.6890 - mean_pred: 0.9450 - mae_t1: 0.0459 - val_loss: 13.4170 - val_mae: 0.6289 - val_mean_pred: 0.8797 - val_mae_t1: 0.0419\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 11.4383 - mae: 0.5362 - mean_pred: 0.8377 - mae_t1: 0.0357 - val_loss: 13.8419 - val_mae: 0.6488 - val_mean_pred: 0.8071 - val_mae_t1: 0.0433\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 11.3215 - mae: 0.5307 - mean_pred: 0.6802 - mae_t1: 0.0354 - val_loss: 12.9659 - val_mae: 0.6078 - val_mean_pred: 0.8725 - val_mae_t1: 0.0405\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 11.9142 - mae: 0.5585 - mean_pred: 0.8631 - mae_t1: 0.0372 - val_loss: 12.6332 - val_mae: 0.5922 - val_mean_pred: 0.9831 - val_mae_t1: 0.0395\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 10.9965 - mae: 0.5155 - mean_pred: 0.8384 - mae_t1: 0.0344 - val_loss: 11.0960 - val_mae: 0.5201 - val_mean_pred: 0.8222 - val_mae_t1: 0.0347\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 11.1410 - mae: 0.5222 - mean_pred: 0.8535 - mae_t1: 0.0348 - val_loss: 14.3629 - val_mae: 0.6733 - val_mean_pred: 1.1298 - val_mae_t1: 0.0449\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 13.1950 - mae: 0.6185 - mean_pred: 0.9982 - mae_t1: 0.0412 - val_loss: 13.1260 - val_mae: 0.6153 - val_mean_pred: 0.6597 - val_mae_t1: 0.0410\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 13.9965 - mae: 0.6561 - mean_pred: 0.6259 - mae_t1: 0.0437 - val_loss: 16.2609 - val_mae: 0.7622 - val_mean_pred: 0.3194 - val_mae_t1: 0.0508\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 15.4678 - mae: 0.7251 - mean_pred: 0.2719 - mae_t1: 0.0483 - val_loss: 12.9592 - val_mae: 0.6075 - val_mean_pred: 0.6269 - val_mae_t1: 0.0405\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.4723 - mae: 0.5378 - mean_pred: 0.7100 - mae_t1: 0.0359 - val_loss: 13.9393 - val_mae: 0.6534 - val_mean_pred: 1.1207 - val_mae_t1: 0.0436\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 12.2434 - mae: 0.5739 - mean_pred: 0.9990 - mae_t1: 0.0383 - val_loss: 14.0042 - val_mae: 0.6564 - val_mean_pred: 0.7253 - val_mae_t1: 0.0438\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 13.3398 - mae: 0.6253 - mean_pred: 0.7066 - mae_t1: 0.0417 - val_loss: 12.0236 - val_mae: 0.5636 - val_mean_pred: 0.6670 - val_mae_t1: 0.0376\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 12.1875 - mae: 0.5713 - mean_pred: 0.6848 - mae_t1: 0.0381 - val_loss: 12.2754 - val_mae: 0.5754 - val_mean_pred: 1.0006 - val_mae_t1: 0.0384\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.1791 - mae: 0.5240 - mean_pred: 0.9106 - mae_t1: 0.0349 - val_loss: 13.3871 - val_mae: 0.6275 - val_mean_pred: 0.8397 - val_mae_t1: 0.0418\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.5921 - mae: 0.5434 - mean_pred: 0.5928 - mae_t1: 0.0362 - val_loss: 13.5585 - val_mae: 0.6356 - val_mean_pred: 0.5979 - val_mae_t1: 0.0424\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 13.2903 - mae: 0.6230 - mean_pred: 0.5895 - mae_t1: 0.0415 - val_loss: 11.3133 - val_mae: 0.5303 - val_mean_pred: 0.6481 - val_mae_t1: 0.0354\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 11.3532 - mae: 0.5322 - mean_pred: 0.5393 - mae_t1: 0.0355 - val_loss: 12.8581 - val_mae: 0.6027 - val_mean_pred: 0.5765 - val_mae_t1: 0.0402\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 12.1554 - mae: 0.5698 - mean_pred: 0.4675 - mae_t1: 0.0380 - val_loss: 11.3944 - val_mae: 0.5341 - val_mean_pred: 0.7661 - val_mae_t1: 0.0356\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 11.2345 - mae: 0.5266 - mean_pred: 0.7533 - mae_t1: 0.0351 - val_loss: 11.0100 - val_mae: 0.5161 - val_mean_pred: 0.8222 - val_mae_t1: 0.0344\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 11.0812 - mae: 0.5194 - mean_pred: 0.6943 - mae_t1: 0.0346 - val_loss: 12.1024 - val_mae: 0.5673 - val_mean_pred: 0.5901 - val_mae_t1: 0.0378\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 10.9951 - mae: 0.5154 - mean_pred: 0.5952 - mae_t1: 0.0344 - val_loss: 11.4836 - val_mae: 0.5383 - val_mean_pred: 0.9824 - val_mae_t1: 0.0359\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 10.3857 - mae: 0.4868 - mean_pred: 0.9621 - mae_t1: 0.0325 - val_loss: 10.8814 - val_mae: 0.5101 - val_mean_pred: 0.9299 - val_mae_t1: 0.0340\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.0080 - mae: 0.4691 - mean_pred: 0.7611 - mae_t1: 0.0313 - val_loss: 13.9150 - val_mae: 0.6523 - val_mean_pred: 0.4657 - val_mae_t1: 0.0435\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 13.0404 - mae: 0.6113 - mean_pred: 0.4934 - mae_t1: 0.0408 - val_loss: 12.9455 - val_mae: 0.6068 - val_mean_pred: 0.7290 - val_mae_t1: 0.0405\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.3560 - mae: 0.4854 - mean_pred: 0.7493 - mae_t1: 0.0324 - val_loss: 13.2511 - val_mae: 0.6211 - val_mean_pred: 1.0866 - val_mae_t1: 0.0414\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 12.2760 - mae: 0.5754 - mean_pred: 0.9921 - mae_t1: 0.0384 - val_loss: 14.4807 - val_mae: 0.6788 - val_mean_pred: 0.7827 - val_mae_t1: 0.0453\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 14.8514 - mae: 0.6962 - mean_pred: 0.7576 - mae_t1: 0.0464 - val_loss: 16.7969 - val_mae: 0.7874 - val_mean_pred: 0.5543 - val_mae_t1: 0.0525\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 13.0189 - mae: 0.6103 - mean_pred: 0.5550 - mae_t1: 0.0407 - val_loss: 15.9632 - val_mae: 0.7483 - val_mean_pred: 0.8878 - val_mae_t1: 0.0499\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 15.0588 - mae: 0.7059 - mean_pred: 0.9441 - mae_t1: 0.0471 - val_loss: 20.6063 - val_mae: 0.9659 - val_mean_pred: 1.5581 - val_mae_t1: 0.0644\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 20.8961 - mae: 0.9795 - mean_pred: 1.4961 - mae_t1: 0.0653 - val_loss: 20.7215 - val_mae: 0.9713 - val_mean_pred: 1.2987 - val_mae_t1: 0.0648\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 16.3847 - mae: 0.7680 - mean_pred: 1.0528 - mae_t1: 0.0512 - val_loss: 16.7914 - val_mae: 0.7871 - val_mean_pred: 0.6930 - val_mae_t1: 0.0525\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 20.4709 - mae: 0.9596 - mean_pred: 0.8710 - mae_t1: 0.0640 - val_loss: 21.5347 - val_mae: 1.0094 - val_mean_pred: 0.9636 - val_mae_t1: 0.0673\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 20.2103 - mae: 0.9474 - mean_pred: 0.8297 - mae_t1: 0.0632 - val_loss: 16.4310 - val_mae: 0.7702 - val_mean_pred: 0.2830 - val_mae_t1: 0.0513\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 14.7416 - mae: 0.6910 - mean_pred: 0.3425 - mae_t1: 0.0461 - val_loss: 12.7497 - val_mae: 0.5976 - val_mean_pred: 0.8042 - val_mae_t1: 0.0398\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 11.8058 - mae: 0.5534 - mean_pred: 0.8642 - mae_t1: 0.0369 - val_loss: 17.6686 - val_mae: 0.8282 - val_mean_pred: 1.4424 - val_mae_t1: 0.0552\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 14.7376 - mae: 0.6908 - mean_pred: 1.2450 - mae_t1: 0.0461 - val_loss: 12.0628 - val_mae: 0.5654 - val_mean_pred: 0.9831 - val_mae_t1: 0.0377\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 10.1606 - mae: 0.4763 - mean_pred: 0.7881 - mae_t1: 0.0318 - val_loss: 14.4243 - val_mae: 0.6761 - val_mean_pred: 0.6037 - val_mae_t1: 0.0451\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 12.2244 - mae: 0.5730 - mean_pred: 0.6053 - mae_t1: 0.0382 - val_loss: 12.5054 - val_mae: 0.5862 - val_mean_pred: 0.6817 - val_mae_t1: 0.0391\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 11.2419 - mae: 0.5270 - mean_pred: 0.7571 - mae_t1: 0.0351 - val_loss: 11.3026 - val_mae: 0.5298 - val_mean_pred: 0.9551 - val_mae_t1: 0.0353\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 10.3886 - mae: 0.4870 - mean_pred: 0.9106 - mae_t1: 0.0325 - val_loss: 11.4550 - val_mae: 0.5370 - val_mean_pred: 0.9861 - val_mae_t1: 0.0358\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 9.8081 - mae: 0.4598 - mean_pred: 0.8418 - mae_t1: 0.0307 - val_loss: 11.4387 - val_mae: 0.5362 - val_mean_pred: 0.7689 - val_mae_t1: 0.0357\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 11.2420 - mae: 0.5270 - mean_pred: 0.6260 - mae_t1: 0.0351 - val_loss: 13.4979 - val_mae: 0.6327 - val_mean_pred: 0.6432 - val_mae_t1: 0.0422\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 11.0666 - mae: 0.5187 - mean_pred: 0.5639 - mae_t1: 0.0346 - val_loss: 11.4182 - val_mae: 0.5352 - val_mean_pred: 0.7586 - val_mae_t1: 0.0357\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 9.5715 - mae: 0.4487 - mean_pred: 0.7066 - mae_t1: 0.0299 - val_loss: 12.0041 - val_mae: 0.5627 - val_mean_pred: 0.9657 - val_mae_t1: 0.0375\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 9.6611 - mae: 0.4529 - mean_pred: 0.8239 - mae_t1: 0.0302 - val_loss: 12.1419 - val_mae: 0.5692 - val_mean_pred: 0.9102 - val_mae_t1: 0.0379\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 9.4783 - mae: 0.4443 - mean_pred: 0.7948 - mae_t1: 0.0296 - val_loss: 12.0171 - val_mae: 0.5633 - val_mean_pred: 0.8669 - val_mae_t1: 0.0376\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 8.8331 - mae: 0.4141 - mean_pred: 0.7594 - mae_t1: 0.0276 - val_loss: 11.6877 - val_mae: 0.5479 - val_mean_pred: 0.8489 - val_mae_t1: 0.0365\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 9.1730 - mae: 0.4300 - mean_pred: 0.7953 - mae_t1: 0.0287 - val_loss: 13.1298 - val_mae: 0.6155 - val_mean_pred: 0.9734 - val_mae_t1: 0.0410\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.4696 - mae: 0.4908 - mean_pred: 0.8894 - mae_t1: 0.0327 - val_loss: 11.4352 - val_mae: 0.5360 - val_mean_pred: 0.8089 - val_mae_t1: 0.0357\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 9.2485 - mae: 0.4335 - mean_pred: 0.7319 - mae_t1: 0.0289 - val_loss: 11.8861 - val_mae: 0.5572 - val_mean_pred: 0.6289 - val_mae_t1: 0.0371\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 9.5274 - mae: 0.4466 - mean_pred: 0.5959 - mae_t1: 0.0298 - val_loss: 15.2696 - val_mae: 0.7158 - val_mean_pred: 0.8469 - val_mae_t1: 0.0477\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 13.5134 - mae: 0.6334 - mean_pred: 0.7963 - mae_t1: 0.0422 - val_loss: 10.2062 - val_mae: 0.4784 - val_mean_pred: 0.8925 - val_mae_t1: 0.0319\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.5944 - mae: 0.4029 - mean_pred: 0.8237 - mae_t1: 0.0269 - val_loss: 11.2915 - val_mae: 0.5293 - val_mean_pred: 0.9933 - val_mae_t1: 0.0353\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 9.2237 - mae: 0.4324 - mean_pred: 0.9227 - mae_t1: 0.0288 - val_loss: 11.6246 - val_mae: 0.5449 - val_mean_pred: 0.8378 - val_mae_t1: 0.0363\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 10.1505 - mae: 0.4758 - mean_pred: 0.7050 - mae_t1: 0.0317 - val_loss: 13.8808 - val_mae: 0.6507 - val_mean_pred: 0.5419 - val_mae_t1: 0.0434\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 11.5978 - mae: 0.5436 - mean_pred: 0.5853 - mae_t1: 0.0362 - val_loss: 12.8091 - val_mae: 0.6004 - val_mean_pred: 0.6393 - val_mae_t1: 0.0400\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.0131 - mae: 0.4694 - mean_pred: 0.6594 - mae_t1: 0.0313 - val_loss: 11.5906 - val_mae: 0.5433 - val_mean_pred: 0.8746 - val_mae_t1: 0.0362\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.9816 - mae: 0.4210 - mean_pred: 0.8166 - mae_t1: 0.0281 - val_loss: 12.5655 - val_mae: 0.5890 - val_mean_pred: 0.9129 - val_mae_t1: 0.0393\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 10.2164 - mae: 0.4789 - mean_pred: 0.8599 - mae_t1: 0.0319 - val_loss: 11.6814 - val_mae: 0.5476 - val_mean_pred: 0.7588 - val_mae_t1: 0.0365\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 10.5853 - mae: 0.4962 - mean_pred: 0.6936 - mae_t1: 0.0331 - val_loss: 13.1483 - val_mae: 0.6163 - val_mean_pred: 0.5887 - val_mae_t1: 0.0411\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 10.5279 - mae: 0.4935 - mean_pred: 0.5539 - mae_t1: 0.0329 - val_loss: 13.8325 - val_mae: 0.6484 - val_mean_pred: 0.6996 - val_mae_t1: 0.0432\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 14.1543 - mae: 0.6635 - mean_pred: 0.7332 - mae_t1: 0.0442 - val_loss: 10.6185 - val_mae: 0.4977 - val_mean_pred: 0.7576 - val_mae_t1: 0.0332\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 9.3126 - mae: 0.4365 - mean_pred: 0.7338 - mae_t1: 0.0291 - val_loss: 11.8781 - val_mae: 0.5568 - val_mean_pred: 0.9337 - val_mae_t1: 0.0371\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 9.3890 - mae: 0.4401 - mean_pred: 0.8748 - mae_t1: 0.0293 - val_loss: 10.3577 - val_mae: 0.4855 - val_mean_pred: 0.9814 - val_mae_t1: 0.0324\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 9.5523 - mae: 0.4478 - mean_pred: 0.8983 - mae_t1: 0.0299 - val_loss: 13.2549 - val_mae: 0.6213 - val_mean_pred: 0.8514 - val_mae_t1: 0.0414\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 11.5878 - mae: 0.5432 - mean_pred: 0.7881 - mae_t1: 0.0362 - val_loss: 13.1846 - val_mae: 0.6180 - val_mean_pred: 0.6447 - val_mae_t1: 0.0412\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 10.6018 - mae: 0.4970 - mean_pred: 0.6159 - mae_t1: 0.0331 - val_loss: 11.1252 - val_mae: 0.5215 - val_mean_pred: 0.6686 - val_mae_t1: 0.0348\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 8.8396 - mae: 0.4144 - mean_pred: 0.6642 - mae_t1: 0.0276 - val_loss: 10.4836 - val_mae: 0.4914 - val_mean_pred: 0.8541 - val_mae_t1: 0.0328\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 8.0461 - mae: 0.3772 - mean_pred: 0.7508 - mae_t1: 0.0251 - val_loss: 10.9044 - val_mae: 0.5111 - val_mean_pred: 0.9007 - val_mae_t1: 0.0341\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.4669 - mae: 0.3969 - mean_pred: 0.7715 - mae_t1: 0.0265 - val_loss: 11.6752 - val_mae: 0.5473 - val_mean_pred: 0.8632 - val_mae_t1: 0.0365\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 8.7679 - mae: 0.4110 - mean_pred: 0.7228 - mae_t1: 0.0274 - val_loss: 12.0813 - val_mae: 0.5663 - val_mean_pred: 0.9107 - val_mae_t1: 0.0378\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 8.6885 - mae: 0.4073 - mean_pred: 0.8402 - mae_t1: 0.0272 - val_loss: 13.0551 - val_mae: 0.6120 - val_mean_pred: 1.0920 - val_mae_t1: 0.0408\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 9.4548 - mae: 0.4432 - mean_pred: 0.9624 - mae_t1: 0.0295 - val_loss: 10.1890 - val_mae: 0.4776 - val_mean_pred: 0.9201 - val_mae_t1: 0.0318\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 8.3122 - mae: 0.3896 - mean_pred: 0.8380 - mae_t1: 0.0260 - val_loss: 10.0132 - val_mae: 0.4694 - val_mean_pred: 0.7390 - val_mae_t1: 0.0313\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 8.4901 - mae: 0.3980 - mean_pred: 0.6446 - mae_t1: 0.0265 - val_loss: 9.9405 - val_mae: 0.4660 - val_mean_pred: 0.7059 - val_mae_t1: 0.0311\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 8.3515 - mae: 0.3915 - mean_pred: 0.6461 - mae_t1: 0.0261 - val_loss: 9.5236 - val_mae: 0.4464 - val_mean_pred: 0.9773 - val_mae_t1: 0.0298\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 8.5943 - mae: 0.4029 - mean_pred: 0.9053 - mae_t1: 0.0269 - val_loss: 12.8380 - val_mae: 0.6018 - val_mean_pred: 1.1001 - val_mae_t1: 0.0401\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 10.0817 - mae: 0.4726 - mean_pred: 0.9208 - mae_t1: 0.0315 - val_loss: 9.4386 - val_mae: 0.4424 - val_mean_pred: 0.7977 - val_mae_t1: 0.0295\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 8.1436 - mae: 0.3817 - mean_pred: 0.7235 - mae_t1: 0.0254 - val_loss: 9.8857 - val_mae: 0.4634 - val_mean_pred: 0.8338 - val_mae_t1: 0.0309\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 7.9921 - mae: 0.3746 - mean_pred: 0.7806 - mae_t1: 0.0250 - val_loss: 11.3023 - val_mae: 0.5298 - val_mean_pred: 1.0018 - val_mae_t1: 0.0353\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 8.8199 - mae: 0.4134 - mean_pred: 0.9159 - mae_t1: 0.0276 - val_loss: 10.4338 - val_mae: 0.4891 - val_mean_pred: 0.9504 - val_mae_t1: 0.0326\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 9.8018 - mae: 0.4595 - mean_pred: 0.9118 - mae_t1: 0.0306 - val_loss: 10.4259 - val_mae: 0.4887 - val_mean_pred: 0.9279 - val_mae_t1: 0.0326\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 9.5759 - mae: 0.4489 - mean_pred: 0.8750 - mae_t1: 0.0299 - val_loss: 13.6523 - val_mae: 0.6399 - val_mean_pred: 0.8463 - val_mae_t1: 0.0427\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 10.1907 - mae: 0.4777 - mean_pred: 0.6980 - mae_t1: 0.0318 - val_loss: 12.3462 - val_mae: 0.5787 - val_mean_pred: 0.5364 - val_mae_t1: 0.0386\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 10.2467 - mae: 0.4803 - mean_pred: 0.5455 - mae_t1: 0.0320 - val_loss: 13.0100 - val_mae: 0.6098 - val_mean_pred: 0.6193 - val_mae_t1: 0.0407\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 10.6036 - mae: 0.4970 - mean_pred: 0.6019 - mae_t1: 0.0331 - val_loss: 12.0974 - val_mae: 0.5671 - val_mean_pred: 0.8781 - val_mae_t1: 0.0378\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 8.6907 - mae: 0.4074 - mean_pred: 0.8027 - mae_t1: 0.0272 - val_loss: 13.8986 - val_mae: 0.6515 - val_mean_pred: 1.0591 - val_mae_t1: 0.0434\n",
      "Earliness...\n",
      "0.0024995803833007812\n",
      "____________________________________________________________\n",
      "Test MAE:      0.36050974479660386  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▇▄▃▅▃▄▃▃▄▃▃▃▃▃▂▃▄▆█▅▃▂▃▂▂▂▂▃▂▂▂▃▁▁▁▁▁▁▂▁</td></tr><tr><td>mae</td><td>▇▄▃▅▃▄▃▃▄▃▃▃▃▃▂▃▄▆█▅▃▂▃▂▂▂▂▃▂▂▂▃▁▁▁▁▁▁▂▁</td></tr><tr><td>mae_t1</td><td>▇▄▃▅▃▄▃▃▄▃▃▃▃▃▂▃▄▆█▅▃▂▃▂▂▂▂▃▂▂▂▃▁▁▁▁▁▁▂▁</td></tr><tr><td>mean_pred</td><td>▁▃▄▁▃▅▄▅▄▄▄▃▃▃▅▆▃▇▅█▄▅▃▅▅▃▆▃▅▃▅▅▅▄▅▄▄▆▄▅</td></tr><tr><td>val_loss</td><td>▄▃▃▅▆▆▄▃▇▅▄▅▃▃▅▆▇██▃▄▃▃▃▄▇▃▄▃▅▂▅▂▃▁▁▁▂▄▅</td></tr><tr><td>val_mae</td><td>▄▃▃▅▆▆▄▃▇▅▄▅▃▃▅▆▇██▃▄▃▃▃▄▇▃▄▃▅▂▅▂▃▁▁▁▂▄▅</td></tr><tr><td>val_mae_t1</td><td>▄▃▃▅▆▆▄▃▇▅▄▅▃▃▅▆▇██▃▄▃▃▃▄▇▃▄▃▅▂▅▂▃▁▁▁▂▄▅</td></tr><tr><td>val_mean_pred</td><td>█▄▄▂▃█▆▅▁█▇▃▅▆▂▅▆▄▁▆▄▅▅▆▆▅▅▄▅▄▆▄▆▆▅▆▅▆▃▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33685</td></tr><tr><td>AE_2</td><td>0.35047</td></tr><tr><td>AE_3</td><td>0.29892</td></tr><tr><td>MAE</td><td>0.36051</td></tr><tr><td>best_epoch</td><td>90</td></tr><tr><td>best_val_loss</td><td>9.43861</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>8.69068</td></tr><tr><td>mae</td><td>0.40738</td></tr><tr><td>mae_t1</td><td>0.02716</td></tr><tr><td>mean_pred</td><td>0.80274</td></tr><tr><td>val_loss</td><td>13.89859</td></tr><tr><td>val_mae</td><td>0.6515</td></tr><tr><td>val_mae_t1</td><td>0.04343</td></tr><tr><td>val_mean_pred</td><td>1.05911</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misunderstood-sea-70</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/tijiqvb8\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/tijiqvb8</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154842-tijiqvb8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154904-2b5ckqva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2b5ckqva\" target=\"_blank\">eager-meadow-71</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 866us/sample - loss: 1.8833 - mae: 0.8239 - mean_pred: 0.1484 - mae_t1: 0.0549 - val_loss: 1.3966 - val_mae: 0.6110 - val_mean_pred: 0.3978 - val_mae_t1: 0.0407\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.3708 - mae: 0.5997 - mean_pred: 0.4983 - mae_t1: 0.0400 - val_loss: 1.1648 - val_mae: 0.5096 - val_mean_pred: 0.8510 - val_mae_t1: 0.0340\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.3552 - mae: 0.5929 - mean_pred: 0.8483 - mae_t1: 0.0395 - val_loss: 1.1788 - val_mae: 0.5157 - val_mean_pred: 0.9724 - val_mae_t1: 0.0344\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 1.2247 - mae: 0.5358 - mean_pred: 0.8555 - mae_t1: 0.0357 - val_loss: 1.1446 - val_mae: 0.5008 - val_mean_pred: 0.8339 - val_mae_t1: 0.0334\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.1838 - mae: 0.5179 - mean_pred: 0.7158 - mae_t1: 0.0345 - val_loss: 1.1415 - val_mae: 0.4994 - val_mean_pred: 0.7891 - val_mae_t1: 0.0333\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 1.1010 - mae: 0.4817 - mean_pred: 0.7346 - mae_t1: 0.0321 - val_loss: 1.1202 - val_mae: 0.4901 - val_mean_pred: 0.8326 - val_mae_t1: 0.0327\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.0631 - mae: 0.4651 - mean_pred: 0.7066 - mae_t1: 0.0310 - val_loss: 1.1889 - val_mae: 0.5201 - val_mean_pred: 0.7202 - val_mae_t1: 0.0347\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.0594 - mae: 0.4635 - mean_pred: 0.6163 - mae_t1: 0.0309 - val_loss: 1.1856 - val_mae: 0.5187 - val_mean_pred: 0.7258 - val_mae_t1: 0.0346\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.0169 - mae: 0.4449 - mean_pred: 0.6897 - mae_t1: 0.0297 - val_loss: 1.1389 - val_mae: 0.4983 - val_mean_pred: 0.8208 - val_mae_t1: 0.0332\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.9888 - mae: 0.4326 - mean_pred: 0.7539 - mae_t1: 0.0288 - val_loss: 1.1498 - val_mae: 0.5030 - val_mean_pred: 0.8012 - val_mae_t1: 0.0335\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.0214 - mae: 0.4469 - mean_pred: 0.7301 - mae_t1: 0.0298 - val_loss: 1.1306 - val_mae: 0.4946 - val_mean_pred: 0.8263 - val_mae_t1: 0.0330\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.9695 - mae: 0.4242 - mean_pred: 0.8087 - mae_t1: 0.0283 - val_loss: 1.1398 - val_mae: 0.4987 - val_mean_pred: 0.8399 - val_mae_t1: 0.0332\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.9539 - mae: 0.4173 - mean_pred: 0.7418 - mae_t1: 0.0278 - val_loss: 1.1528 - val_mae: 0.5043 - val_mean_pred: 0.7502 - val_mae_t1: 0.0336\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 0.9277 - mae: 0.4059 - mean_pred: 0.7791 - mae_t1: 0.0271 - val_loss: 1.0960 - val_mae: 0.4795 - val_mean_pred: 0.8952 - val_mae_t1: 0.0320\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.9190 - mae: 0.4021 - mean_pred: 0.8085 - mae_t1: 0.0268 - val_loss: 1.1696 - val_mae: 0.5117 - val_mean_pred: 0.7791 - val_mae_t1: 0.0341\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.9065 - mae: 0.3966 - mean_pred: 0.7797 - mae_t1: 0.0264 - val_loss: 1.1368 - val_mae: 0.4974 - val_mean_pred: 0.9033 - val_mae_t1: 0.0332\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.9316 - mae: 0.4076 - mean_pred: 0.8604 - mae_t1: 0.0272 - val_loss: 1.1493 - val_mae: 0.5028 - val_mean_pred: 0.7897 - val_mae_t1: 0.0335\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.9567 - mae: 0.4186 - mean_pred: 0.6720 - mae_t1: 0.0279 - val_loss: 1.1813 - val_mae: 0.5168 - val_mean_pred: 0.7693 - val_mae_t1: 0.0345\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 228us/sample - loss: 0.9230 - mae: 0.4038 - mean_pred: 0.8025 - mae_t1: 0.0269 - val_loss: 1.0714 - val_mae: 0.4687 - val_mean_pred: 0.9721 - val_mae_t1: 0.0312\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.9184 - mae: 0.4018 - mean_pred: 0.8488 - mae_t1: 0.0268 - val_loss: 1.0883 - val_mae: 0.4761 - val_mean_pred: 0.8164 - val_mae_t1: 0.0317\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 0.8834 - mae: 0.3865 - mean_pred: 0.7335 - mae_t1: 0.0258 - val_loss: 1.0018 - val_mae: 0.4383 - val_mean_pred: 0.8643 - val_mae_t1: 0.0292\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 0.9396 - mae: 0.4111 - mean_pred: 0.8451 - mae_t1: 0.0274 - val_loss: 0.9936 - val_mae: 0.4347 - val_mean_pred: 0.9163 - val_mae_t1: 0.0290\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 0.8628 - mae: 0.3775 - mean_pred: 0.8216 - mae_t1: 0.0252 - val_loss: 1.0738 - val_mae: 0.4698 - val_mean_pred: 0.7686 - val_mae_t1: 0.0313\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.8999 - mae: 0.3937 - mean_pred: 0.6835 - mae_t1: 0.0262 - val_loss: 1.0307 - val_mae: 0.4509 - val_mean_pred: 0.8256 - val_mae_t1: 0.0301\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.9260 - mae: 0.4051 - mean_pred: 0.8588 - mae_t1: 0.0270 - val_loss: 1.0521 - val_mae: 0.4603 - val_mean_pred: 0.9258 - val_mae_t1: 0.0307\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.8566 - mae: 0.3748 - mean_pred: 0.8038 - mae_t1: 0.0250 - val_loss: 1.1246 - val_mae: 0.4920 - val_mean_pred: 0.7968 - val_mae_t1: 0.0328\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.8891 - mae: 0.3890 - mean_pred: 0.8166 - mae_t1: 0.0259 - val_loss: 1.0943 - val_mae: 0.4787 - val_mean_pred: 0.9763 - val_mae_t1: 0.0319\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.8776 - mae: 0.3840 - mean_pred: 0.8580 - mae_t1: 0.0256 - val_loss: 1.0842 - val_mae: 0.4743 - val_mean_pred: 0.8537 - val_mae_t1: 0.0316\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.8502 - mae: 0.3719 - mean_pred: 0.7903 - mae_t1: 0.0248 - val_loss: 1.0354 - val_mae: 0.4530 - val_mean_pred: 0.9338 - val_mae_t1: 0.0302\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.8787 - mae: 0.3844 - mean_pred: 0.9100 - mae_t1: 0.0256 - val_loss: 1.0448 - val_mae: 0.4571 - val_mean_pred: 0.9150 - val_mae_t1: 0.0305\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.8278 - mae: 0.3622 - mean_pred: 0.7829 - mae_t1: 0.0241 - val_loss: 1.0719 - val_mae: 0.4689 - val_mean_pred: 0.7773 - val_mae_t1: 0.0313\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.8174 - mae: 0.3576 - mean_pred: 0.7196 - mae_t1: 0.0238 - val_loss: 1.0350 - val_mae: 0.4528 - val_mean_pred: 0.8960 - val_mae_t1: 0.0302\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.8090 - mae: 0.3539 - mean_pred: 0.8359 - mae_t1: 0.0236 - val_loss: 1.0179 - val_mae: 0.4453 - val_mean_pred: 0.9263 - val_mae_t1: 0.0297\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.8002 - mae: 0.3501 - mean_pred: 0.8202 - mae_t1: 0.0233 - val_loss: 0.9944 - val_mae: 0.4350 - val_mean_pred: 0.9172 - val_mae_t1: 0.0290\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.8109 - mae: 0.3548 - mean_pred: 0.8792 - mae_t1: 0.0237 - val_loss: 1.0109 - val_mae: 0.4423 - val_mean_pred: 0.9168 - val_mae_t1: 0.0295\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.7996 - mae: 0.3498 - mean_pred: 0.8077 - mae_t1: 0.0233 - val_loss: 1.0241 - val_mae: 0.4481 - val_mean_pred: 0.8050 - val_mae_t1: 0.0299\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.8264 - mae: 0.3615 - mean_pred: 0.7384 - mae_t1: 0.0241 - val_loss: 1.0441 - val_mae: 0.4568 - val_mean_pred: 0.8604 - val_mae_t1: 0.0305\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 0.8014 - mae: 0.3506 - mean_pred: 0.8291 - mae_t1: 0.0234 - val_loss: 1.0376 - val_mae: 0.4539 - val_mean_pred: 0.9287 - val_mae_t1: 0.0303\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.8427 - mae: 0.3687 - mean_pred: 0.8185 - mae_t1: 0.0246 - val_loss: 1.0505 - val_mae: 0.4596 - val_mean_pred: 0.8903 - val_mae_t1: 0.0306\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 0.7970 - mae: 0.3487 - mean_pred: 0.7940 - mae_t1: 0.0232 - val_loss: 1.0308 - val_mae: 0.4510 - val_mean_pred: 0.8948 - val_mae_t1: 0.0301\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 0.7780 - mae: 0.3404 - mean_pred: 0.8049 - mae_t1: 0.0227 - val_loss: 1.0270 - val_mae: 0.4493 - val_mean_pred: 0.8730 - val_mae_t1: 0.0300\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7833 - mae: 0.3427 - mean_pred: 0.7920 - mae_t1: 0.0228 - val_loss: 1.0593 - val_mae: 0.4634 - val_mean_pred: 0.9163 - val_mae_t1: 0.0309\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.7817 - mae: 0.3420 - mean_pred: 0.8265 - mae_t1: 0.0228 - val_loss: 1.0396 - val_mae: 0.4548 - val_mean_pred: 0.8636 - val_mae_t1: 0.0303\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7991 - mae: 0.3496 - mean_pred: 0.7604 - mae_t1: 0.0233 - val_loss: 1.0141 - val_mae: 0.4437 - val_mean_pred: 0.8909 - val_mae_t1: 0.0296\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 0.7935 - mae: 0.3471 - mean_pred: 0.7880 - mae_t1: 0.0231 - val_loss: 0.9797 - val_mae: 0.4286 - val_mean_pred: 0.8653 - val_mae_t1: 0.0286\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7568 - mae: 0.3311 - mean_pred: 0.7723 - mae_t1: 0.0221 - val_loss: 0.9850 - val_mae: 0.4309 - val_mean_pred: 0.8781 - val_mae_t1: 0.0287\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.7499 - mae: 0.3281 - mean_pred: 0.7556 - mae_t1: 0.0219 - val_loss: 0.9974 - val_mae: 0.4364 - val_mean_pred: 0.8521 - val_mae_t1: 0.0291\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7343 - mae: 0.3213 - mean_pred: 0.7757 - mae_t1: 0.0214 - val_loss: 1.0239 - val_mae: 0.4479 - val_mean_pred: 0.9024 - val_mae_t1: 0.0299\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 0.7468 - mae: 0.3267 - mean_pred: 0.8008 - mae_t1: 0.0218 - val_loss: 1.0611 - val_mae: 0.4642 - val_mean_pred: 0.9544 - val_mae_t1: 0.0309\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7723 - mae: 0.3379 - mean_pred: 0.8690 - mae_t1: 0.0225 - val_loss: 1.0348 - val_mae: 0.4527 - val_mean_pred: 0.8104 - val_mae_t1: 0.0302\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.8372 - mae: 0.3663 - mean_pred: 0.6711 - mae_t1: 0.0244 - val_loss: 1.0476 - val_mae: 0.4583 - val_mean_pred: 0.7643 - val_mae_t1: 0.0306\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7663 - mae: 0.3353 - mean_pred: 0.7799 - mae_t1: 0.0224 - val_loss: 1.1233 - val_mae: 0.4915 - val_mean_pred: 1.0558 - val_mae_t1: 0.0328\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.8988 - mae: 0.3932 - mean_pred: 0.9631 - mae_t1: 0.0262 - val_loss: 1.0240 - val_mae: 0.4480 - val_mean_pred: 0.9219 - val_mae_t1: 0.0299\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.7683 - mae: 0.3361 - mean_pred: 0.7555 - mae_t1: 0.0224 - val_loss: 1.0182 - val_mae: 0.4455 - val_mean_pred: 0.8118 - val_mae_t1: 0.0297\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.7448 - mae: 0.3259 - mean_pred: 0.7378 - mae_t1: 0.0217 - val_loss: 0.9909 - val_mae: 0.4335 - val_mean_pred: 0.8814 - val_mae_t1: 0.0289\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 0.7135 - mae: 0.3122 - mean_pred: 0.7509 - mae_t1: 0.0208 - val_loss: 0.9769 - val_mae: 0.4274 - val_mean_pred: 0.9215 - val_mae_t1: 0.0285\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7309 - mae: 0.3198 - mean_pred: 0.8419 - mae_t1: 0.0213 - val_loss: 0.9831 - val_mae: 0.4301 - val_mean_pred: 0.9215 - val_mae_t1: 0.0287\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.7149 - mae: 0.3128 - mean_pred: 0.7853 - mae_t1: 0.0209 - val_loss: 1.0010 - val_mae: 0.4379 - val_mean_pred: 0.8102 - val_mae_t1: 0.0292\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.7181 - mae: 0.3142 - mean_pred: 0.7554 - mae_t1: 0.0209 - val_loss: 0.9864 - val_mae: 0.4315 - val_mean_pred: 0.8576 - val_mae_t1: 0.0288\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 0.7272 - mae: 0.3181 - mean_pred: 0.7852 - mae_t1: 0.0212 - val_loss: 0.9856 - val_mae: 0.4312 - val_mean_pred: 0.8730 - val_mae_t1: 0.0287\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 0.7296 - mae: 0.3192 - mean_pred: 0.8438 - mae_t1: 0.0213 - val_loss: 0.9724 - val_mae: 0.4254 - val_mean_pred: 0.8782 - val_mae_t1: 0.0284\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.7213 - mae: 0.3156 - mean_pred: 0.7859 - mae_t1: 0.0210 - val_loss: 1.0246 - val_mae: 0.4483 - val_mean_pred: 0.8274 - val_mae_t1: 0.0299\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.7565 - mae: 0.3310 - mean_pred: 0.7980 - mae_t1: 0.0221 - val_loss: 0.9985 - val_mae: 0.4368 - val_mean_pred: 0.9391 - val_mae_t1: 0.0291\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.7602 - mae: 0.3326 - mean_pred: 0.8121 - mae_t1: 0.0222 - val_loss: 1.0346 - val_mae: 0.4526 - val_mean_pred: 0.8027 - val_mae_t1: 0.0302\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.7339 - mae: 0.3211 - mean_pred: 0.7119 - mae_t1: 0.0214 - val_loss: 1.0398 - val_mae: 0.4549 - val_mean_pred: 0.8940 - val_mae_t1: 0.0303\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.7293 - mae: 0.3191 - mean_pred: 0.8378 - mae_t1: 0.0213 - val_loss: 1.0133 - val_mae: 0.4433 - val_mean_pred: 0.9718 - val_mae_t1: 0.0296\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 0.6953 - mae: 0.3042 - mean_pred: 0.8366 - mae_t1: 0.0203 - val_loss: 0.9656 - val_mae: 0.4225 - val_mean_pred: 0.8914 - val_mae_t1: 0.0282\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 0.6703 - mae: 0.2933 - mean_pred: 0.8185 - mae_t1: 0.0196 - val_loss: 0.9585 - val_mae: 0.4193 - val_mean_pred: 0.9087 - val_mae_t1: 0.0280\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 0.6772 - mae: 0.2963 - mean_pred: 0.7822 - mae_t1: 0.0198 - val_loss: 0.9357 - val_mae: 0.4094 - val_mean_pred: 0.8501 - val_mae_t1: 0.0273\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6678 - mae: 0.2922 - mean_pred: 0.7785 - mae_t1: 0.0195 - val_loss: 0.9546 - val_mae: 0.4176 - val_mean_pred: 0.9371 - val_mae_t1: 0.0278\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.7092 - mae: 0.3103 - mean_pred: 0.8556 - mae_t1: 0.0207 - val_loss: 0.9549 - val_mae: 0.4178 - val_mean_pred: 0.8880 - val_mae_t1: 0.0279\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 0.6914 - mae: 0.3025 - mean_pred: 0.7751 - mae_t1: 0.0202 - val_loss: 0.9787 - val_mae: 0.4282 - val_mean_pred: 0.8214 - val_mae_t1: 0.0285\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6665 - mae: 0.2916 - mean_pred: 0.7645 - mae_t1: 0.0194 - val_loss: 0.9765 - val_mae: 0.4272 - val_mean_pred: 0.9378 - val_mae_t1: 0.0285\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6596 - mae: 0.2886 - mean_pred: 0.8203 - mae_t1: 0.0192 - val_loss: 0.9574 - val_mae: 0.4188 - val_mean_pred: 0.9079 - val_mae_t1: 0.0279\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 0.6504 - mae: 0.2845 - mean_pred: 0.8056 - mae_t1: 0.0190 - val_loss: 0.9342 - val_mae: 0.4087 - val_mean_pred: 0.9073 - val_mae_t1: 0.0272\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6551 - mae: 0.2866 - mean_pred: 0.8236 - mae_t1: 0.0191 - val_loss: 0.9444 - val_mae: 0.4132 - val_mean_pred: 0.8348 - val_mae_t1: 0.0275\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.6812 - mae: 0.2980 - mean_pred: 0.7333 - mae_t1: 0.0199 - val_loss: 0.9797 - val_mae: 0.4286 - val_mean_pred: 0.8162 - val_mae_t1: 0.0286\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6886 - mae: 0.3012 - mean_pred: 0.7629 - mae_t1: 0.0201 - val_loss: 1.0666 - val_mae: 0.4666 - val_mean_pred: 1.0062 - val_mae_t1: 0.0311\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.7432 - mae: 0.3252 - mean_pred: 0.9223 - mae_t1: 0.0217 - val_loss: 0.9721 - val_mae: 0.4253 - val_mean_pred: 0.9205 - val_mae_t1: 0.0284\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7094 - mae: 0.3104 - mean_pred: 0.7783 - mae_t1: 0.0207 - val_loss: 1.0191 - val_mae: 0.4458 - val_mean_pred: 0.8462 - val_mae_t1: 0.0297\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 0.6863 - mae: 0.3002 - mean_pred: 0.8206 - mae_t1: 0.0200 - val_loss: 0.9911 - val_mae: 0.4336 - val_mean_pred: 0.9305 - val_mae_t1: 0.0289\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6648 - mae: 0.2909 - mean_pred: 0.8225 - mae_t1: 0.0194 - val_loss: 1.0617 - val_mae: 0.4645 - val_mean_pred: 0.8015 - val_mae_t1: 0.0310\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7158 - mae: 0.3131 - mean_pred: 0.7156 - mae_t1: 0.0209 - val_loss: 1.0498 - val_mae: 0.4593 - val_mean_pred: 0.8634 - val_mae_t1: 0.0306\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.7270 - mae: 0.3181 - mean_pred: 0.8425 - mae_t1: 0.0212 - val_loss: 1.0443 - val_mae: 0.4569 - val_mean_pred: 0.9512 - val_mae_t1: 0.0305\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6482 - mae: 0.2836 - mean_pred: 0.7934 - mae_t1: 0.0189 - val_loss: 1.0151 - val_mae: 0.4441 - val_mean_pred: 0.8122 - val_mae_t1: 0.0296\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.6584 - mae: 0.2881 - mean_pred: 0.7597 - mae_t1: 0.0192 - val_loss: 0.9761 - val_mae: 0.4270 - val_mean_pred: 0.9504 - val_mae_t1: 0.0285\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 0.7156 - mae: 0.3131 - mean_pred: 0.8921 - mae_t1: 0.0209 - val_loss: 1.0139 - val_mae: 0.4436 - val_mean_pred: 0.9396 - val_mae_t1: 0.0296\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6589 - mae: 0.2883 - mean_pred: 0.8249 - mae_t1: 0.0192 - val_loss: 0.9974 - val_mae: 0.4364 - val_mean_pred: 0.8629 - val_mae_t1: 0.0291\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6520 - mae: 0.2853 - mean_pred: 0.7792 - mae_t1: 0.0190 - val_loss: 1.0072 - val_mae: 0.4406 - val_mean_pred: 0.9124 - val_mae_t1: 0.0294\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.6568 - mae: 0.2873 - mean_pred: 0.8427 - mae_t1: 0.0192 - val_loss: 1.0145 - val_mae: 0.4438 - val_mean_pred: 0.8875 - val_mae_t1: 0.0296\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.6409 - mae: 0.2804 - mean_pred: 0.7410 - mae_t1: 0.0187 - val_loss: 1.0064 - val_mae: 0.4403 - val_mean_pred: 0.8158 - val_mae_t1: 0.0294\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6661 - mae: 0.2914 - mean_pred: 0.8208 - mae_t1: 0.0194 - val_loss: 0.9964 - val_mae: 0.4359 - val_mean_pred: 0.9058 - val_mae_t1: 0.0291\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.6817 - mae: 0.2982 - mean_pred: 0.7976 - mae_t1: 0.0199 - val_loss: 1.0201 - val_mae: 0.4463 - val_mean_pred: 0.7825 - val_mae_t1: 0.0298\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6642 - mae: 0.2906 - mean_pred: 0.7340 - mae_t1: 0.0194 - val_loss: 1.0483 - val_mae: 0.4586 - val_mean_pred: 0.8560 - val_mae_t1: 0.0306\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 0.6496 - mae: 0.2842 - mean_pred: 0.7648 - mae_t1: 0.0189 - val_loss: 1.0487 - val_mae: 0.4588 - val_mean_pred: 0.9718 - val_mae_t1: 0.0306\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6885 - mae: 0.3012 - mean_pred: 0.8698 - mae_t1: 0.0201 - val_loss: 1.0240 - val_mae: 0.4480 - val_mean_pred: 0.9645 - val_mae_t1: 0.0299\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6117 - mae: 0.2676 - mean_pred: 0.7980 - mae_t1: 0.0178 - val_loss: 1.0089 - val_mae: 0.4414 - val_mean_pred: 0.7976 - val_mae_t1: 0.0294\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6190 - mae: 0.2708 - mean_pred: 0.7232 - mae_t1: 0.0181 - val_loss: 0.9691 - val_mae: 0.4240 - val_mean_pred: 0.8902 - val_mae_t1: 0.0283\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6107 - mae: 0.2672 - mean_pred: 0.8295 - mae_t1: 0.0178 - val_loss: 0.9759 - val_mae: 0.4269 - val_mean_pred: 0.9067 - val_mae_t1: 0.0285\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.5987 - mae: 0.2619 - mean_pred: 0.7848 - mae_t1: 0.0175 - val_loss: 1.0101 - val_mae: 0.4419 - val_mean_pred: 0.8213 - val_mae_t1: 0.0295\n",
      "Earliness...\n",
      "0.0014996528625488281\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3134582066303576  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▇▆▅▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▇▇▇▇▇▇▆█▇▇█▇▇▆▇▇</td></tr><tr><td>val_loss</td><td>█▅▄▅▄▄▄▅▂▃▄▃▃▂▂▃▂▂▂▃▃▂▂▂▂▃▁▁▂▁▂▂▃▃▂▂▂▃▂▂</td></tr><tr><td>val_mae</td><td>█▅▄▅▄▄▄▅▂▃▄▃▃▂▂▃▂▂▂▃▃▂▂▂▂▃▁▁▂▁▂▂▃▃▂▂▂▃▂▂</td></tr><tr><td>val_mae_t1</td><td>█▅▄▅▄▄▄▅▂▃▄▃▃▂▂▃▂▂▂▃▃▂▂▂▂▃▁▁▂▁▂▂▃▃▂▂▂▃▂▂</td></tr><tr><td>val_mean_pred</td><td>▁█▆▅▆▅▇▆▇▆▆▇▆▇▆▇▇▇▇█▅▆▇▇▇▆▇▇▆▇▆▇▆██▇▇▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.40081</td></tr><tr><td>AE_2</td><td>0.29986</td></tr><tr><td>AE_3</td><td>0.26452</td></tr><tr><td>MAE</td><td>0.31346</td></tr><tr><td>best_epoch</td><td>74</td></tr><tr><td>best_val_loss</td><td>0.93424</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.59869</td></tr><tr><td>mae</td><td>0.26193</td></tr><tr><td>mae_t1</td><td>0.01746</td></tr><tr><td>mean_pred</td><td>0.78484</td></tr><tr><td>val_loss</td><td>1.01009</td></tr><tr><td>val_mae</td><td>0.44191</td></tr><tr><td>val_mae_t1</td><td>0.02946</td></tr><tr><td>val_mean_pred</td><td>0.82133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">eager-meadow-71</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2b5ckqva\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2b5ckqva</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154904-2b5ckqva\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154931-1xgivayg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xgivayg\" target=\"_blank\">vague-smoke-72</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 708us/sample - loss: 1.6878 - mae: 0.7384 - mean_pred: 0.2695 - mae_t1: 0.0492 - val_loss: 1.4694 - val_mae: 0.6429 - val_mean_pred: 0.3884 - val_mae_t1: 0.0429\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 1.5080 - mae: 0.6598 - mean_pred: 0.4071 - mae_t1: 0.0440 - val_loss: 1.2036 - val_mae: 0.5266 - val_mean_pred: 0.6450 - val_mae_t1: 0.0351\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 161us/sample - loss: 1.4457 - mae: 0.6325 - mean_pred: 0.6536 - mae_t1: 0.0422 - val_loss: 1.2103 - val_mae: 0.5295 - val_mean_pred: 0.7955 - val_mae_t1: 0.0353\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 158us/sample - loss: 1.4425 - mae: 0.6311 - mean_pred: 0.7332 - mae_t1: 0.0421 - val_loss: 1.1437 - val_mae: 0.5004 - val_mean_pred: 0.7210 - val_mae_t1: 0.0334\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 1.3173 - mae: 0.5763 - mean_pred: 0.6729 - mae_t1: 0.0384 - val_loss: 1.1456 - val_mae: 0.5012 - val_mean_pred: 0.6847 - val_mae_t1: 0.0334\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 1.2544 - mae: 0.5488 - mean_pred: 0.6139 - mae_t1: 0.0366 - val_loss: 1.2053 - val_mae: 0.5273 - val_mean_pred: 0.6435 - val_mae_t1: 0.0352\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 1.2323 - mae: 0.5391 - mean_pred: 0.5633 - mae_t1: 0.0359 - val_loss: 1.2188 - val_mae: 0.5332 - val_mean_pred: 0.6863 - val_mae_t1: 0.0355\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.1947 - mae: 0.5227 - mean_pred: 0.5997 - mae_t1: 0.0348 - val_loss: 1.1709 - val_mae: 0.5123 - val_mean_pred: 0.8202 - val_mae_t1: 0.0342\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.1671 - mae: 0.5106 - mean_pred: 0.7303 - mae_t1: 0.0340 - val_loss: 1.2106 - val_mae: 0.5296 - val_mean_pred: 0.9013 - val_mae_t1: 0.0353\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.1602 - mae: 0.5076 - mean_pred: 0.7498 - mae_t1: 0.0338 - val_loss: 1.1907 - val_mae: 0.5210 - val_mean_pred: 0.8153 - val_mae_t1: 0.0347\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.1312 - mae: 0.4949 - mean_pred: 0.6885 - mae_t1: 0.0330 - val_loss: 1.1596 - val_mae: 0.5073 - val_mean_pred: 0.8329 - val_mae_t1: 0.0338\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.1141 - mae: 0.4874 - mean_pred: 0.7440 - mae_t1: 0.0325 - val_loss: 1.1449 - val_mae: 0.5009 - val_mean_pred: 0.8344 - val_mae_t1: 0.0334\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 143us/sample - loss: 1.1214 - mae: 0.4906 - mean_pred: 0.7321 - mae_t1: 0.0327 - val_loss: 1.1423 - val_mae: 0.4997 - val_mean_pred: 0.7646 - val_mae_t1: 0.0333\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 1.1028 - mae: 0.4825 - mean_pred: 0.7245 - mae_t1: 0.0322 - val_loss: 1.1122 - val_mae: 0.4866 - val_mean_pred: 0.7801 - val_mae_t1: 0.0324\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.0711 - mae: 0.4686 - mean_pred: 0.7381 - mae_t1: 0.0312 - val_loss: 1.1967 - val_mae: 0.5236 - val_mean_pred: 0.7278 - val_mae_t1: 0.0349\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.1287 - mae: 0.4938 - mean_pred: 0.7176 - mae_t1: 0.0329 - val_loss: 1.2183 - val_mae: 0.5330 - val_mean_pred: 0.7551 - val_mae_t1: 0.0355\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.1352 - mae: 0.4966 - mean_pred: 0.7672 - mae_t1: 0.0331 - val_loss: 1.1505 - val_mae: 0.5034 - val_mean_pred: 0.7989 - val_mae_t1: 0.0336\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0581 - mae: 0.4629 - mean_pred: 0.7990 - mae_t1: 0.0309 - val_loss: 1.1779 - val_mae: 0.5153 - val_mean_pred: 0.8128 - val_mae_t1: 0.0344\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.0502 - mae: 0.4595 - mean_pred: 0.8111 - mae_t1: 0.0306 - val_loss: 1.2223 - val_mae: 0.5347 - val_mean_pred: 0.7996 - val_mae_t1: 0.0356\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 1.0442 - mae: 0.4568 - mean_pred: 0.7718 - mae_t1: 0.0305 - val_loss: 1.2412 - val_mae: 0.5430 - val_mean_pred: 0.7887 - val_mae_t1: 0.0362\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.0328 - mae: 0.4518 - mean_pred: 0.7761 - mae_t1: 0.0301 - val_loss: 1.1967 - val_mae: 0.5235 - val_mean_pred: 0.8105 - val_mae_t1: 0.0349\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.0161 - mae: 0.4445 - mean_pred: 0.7737 - mae_t1: 0.0296 - val_loss: 1.2012 - val_mae: 0.5255 - val_mean_pred: 0.7734 - val_mae_t1: 0.0350\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.0479 - mae: 0.4584 - mean_pred: 0.7428 - mae_t1: 0.0306 - val_loss: 1.1435 - val_mae: 0.5003 - val_mean_pred: 0.7901 - val_mae_t1: 0.0334\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 0.9938 - mae: 0.4348 - mean_pred: 0.7495 - mae_t1: 0.0290 - val_loss: 1.0667 - val_mae: 0.4667 - val_mean_pred: 0.8590 - val_mae_t1: 0.0311\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 129us/sample - loss: 0.9515 - mae: 0.4163 - mean_pred: 0.8142 - mae_t1: 0.0278 - val_loss: 1.0625 - val_mae: 0.4649 - val_mean_pred: 0.9309 - val_mae_t1: 0.0310\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.9807 - mae: 0.4291 - mean_pred: 0.8567 - mae_t1: 0.0286 - val_loss: 1.0770 - val_mae: 0.4712 - val_mean_pred: 0.9600 - val_mae_t1: 0.0314\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0208 - mae: 0.4466 - mean_pred: 0.9011 - mae_t1: 0.0298 - val_loss: 1.0867 - val_mae: 0.4754 - val_mean_pred: 1.0017 - val_mae_t1: 0.0317\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 0.9996 - mae: 0.4373 - mean_pred: 0.9132 - mae_t1: 0.0292 - val_loss: 1.0472 - val_mae: 0.4581 - val_mean_pred: 0.8782 - val_mae_t1: 0.0305\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.9856 - mae: 0.4312 - mean_pred: 0.7688 - mae_t1: 0.0287 - val_loss: 1.1503 - val_mae: 0.5033 - val_mean_pred: 0.7471 - val_mae_t1: 0.0336\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.0301 - mae: 0.4507 - mean_pred: 0.6583 - mae_t1: 0.0300 - val_loss: 1.1277 - val_mae: 0.4934 - val_mean_pred: 0.7467 - val_mae_t1: 0.0329\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 0.9486 - mae: 0.4150 - mean_pred: 0.6696 - mae_t1: 0.0277 - val_loss: 1.1277 - val_mae: 0.4934 - val_mean_pred: 0.8281 - val_mae_t1: 0.0329\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.9307 - mae: 0.4072 - mean_pred: 0.7401 - mae_t1: 0.0271 - val_loss: 1.1235 - val_mae: 0.4915 - val_mean_pred: 0.8748 - val_mae_t1: 0.0328\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.9205 - mae: 0.4027 - mean_pred: 0.7796 - mae_t1: 0.0268 - val_loss: 1.1072 - val_mae: 0.4844 - val_mean_pred: 0.8730 - val_mae_t1: 0.0323\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.8883 - mae: 0.3887 - mean_pred: 0.7798 - mae_t1: 0.0259 - val_loss: 1.1197 - val_mae: 0.4899 - val_mean_pred: 0.8134 - val_mae_t1: 0.0327\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8992 - mae: 0.3934 - mean_pred: 0.7204 - mae_t1: 0.0262 - val_loss: 1.1269 - val_mae: 0.4930 - val_mean_pred: 0.7856 - val_mae_t1: 0.0329\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.8829 - mae: 0.3863 - mean_pred: 0.7326 - mae_t1: 0.0258 - val_loss: 1.0833 - val_mae: 0.4739 - val_mean_pred: 0.8828 - val_mae_t1: 0.0316\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8633 - mae: 0.3777 - mean_pred: 0.8187 - mae_t1: 0.0252 - val_loss: 1.0818 - val_mae: 0.4733 - val_mean_pred: 0.9521 - val_mae_t1: 0.0316\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8948 - mae: 0.3915 - mean_pred: 0.8805 - mae_t1: 0.0261 - val_loss: 1.0550 - val_mae: 0.4616 - val_mean_pred: 0.9058 - val_mae_t1: 0.0308\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8737 - mae: 0.3822 - mean_pred: 0.8072 - mae_t1: 0.0255 - val_loss: 1.1089 - val_mae: 0.4851 - val_mean_pred: 0.7973 - val_mae_t1: 0.0323\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.9022 - mae: 0.3947 - mean_pred: 0.7319 - mae_t1: 0.0263 - val_loss: 1.1025 - val_mae: 0.4823 - val_mean_pred: 0.8039 - val_mae_t1: 0.0322\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8867 - mae: 0.3879 - mean_pred: 0.7462 - mae_t1: 0.0259 - val_loss: 1.1001 - val_mae: 0.4813 - val_mean_pred: 0.8018 - val_mae_t1: 0.0321\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8789 - mae: 0.3845 - mean_pred: 0.7324 - mae_t1: 0.0256 - val_loss: 1.0967 - val_mae: 0.4798 - val_mean_pred: 0.7957 - val_mae_t1: 0.0320\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.8795 - mae: 0.3848 - mean_pred: 0.7383 - mae_t1: 0.0257 - val_loss: 1.0754 - val_mae: 0.4705 - val_mean_pred: 0.8172 - val_mae_t1: 0.0314\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8560 - mae: 0.3745 - mean_pred: 0.7330 - mae_t1: 0.0250 - val_loss: 1.0699 - val_mae: 0.4681 - val_mean_pred: 0.8042 - val_mae_t1: 0.0312\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8471 - mae: 0.3706 - mean_pred: 0.7236 - mae_t1: 0.0247 - val_loss: 1.0669 - val_mae: 0.4668 - val_mean_pred: 0.8370 - val_mae_t1: 0.0311\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.8504 - mae: 0.3721 - mean_pred: 0.7453 - mae_t1: 0.0248 - val_loss: 1.0523 - val_mae: 0.4604 - val_mean_pred: 0.8749 - val_mae_t1: 0.0307\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.8493 - mae: 0.3716 - mean_pred: 0.7784 - mae_t1: 0.0248 - val_loss: 1.0921 - val_mae: 0.4778 - val_mean_pred: 0.9010 - val_mae_t1: 0.0319\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.8715 - mae: 0.3813 - mean_pred: 0.8037 - mae_t1: 0.0254 - val_loss: 1.0827 - val_mae: 0.4737 - val_mean_pred: 0.8820 - val_mae_t1: 0.0316\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.8370 - mae: 0.3662 - mean_pred: 0.7760 - mae_t1: 0.0244 - val_loss: 1.1522 - val_mae: 0.5041 - val_mean_pred: 0.8411 - val_mae_t1: 0.0336\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.9253 - mae: 0.4048 - mean_pred: 0.7331 - mae_t1: 0.0270 - val_loss: 1.1365 - val_mae: 0.4972 - val_mean_pred: 0.8120 - val_mae_t1: 0.0331\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.8474 - mae: 0.3707 - mean_pred: 0.7374 - mae_t1: 0.0247 - val_loss: 1.0938 - val_mae: 0.4785 - val_mean_pred: 0.8432 - val_mae_t1: 0.0319\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8243 - mae: 0.3606 - mean_pred: 0.7646 - mae_t1: 0.0240 - val_loss: 1.0639 - val_mae: 0.4655 - val_mean_pred: 0.8546 - val_mae_t1: 0.0310\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 0.8157 - mae: 0.3569 - mean_pred: 0.7813 - mae_t1: 0.0238 - val_loss: 1.0361 - val_mae: 0.4533 - val_mean_pred: 0.9127 - val_mae_t1: 0.0302\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 0.8361 - mae: 0.3658 - mean_pred: 0.8330 - mae_t1: 0.0244 - val_loss: 1.0237 - val_mae: 0.4479 - val_mean_pred: 0.9234 - val_mae_t1: 0.0299\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8646 - mae: 0.3783 - mean_pred: 0.8164 - mae_t1: 0.0252 - val_loss: 1.0494 - val_mae: 0.4591 - val_mean_pred: 0.8464 - val_mae_t1: 0.0306\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8488 - mae: 0.3713 - mean_pred: 0.7501 - mae_t1: 0.0248 - val_loss: 1.0402 - val_mae: 0.4551 - val_mean_pred: 0.8036 - val_mae_t1: 0.0303\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 0.8444 - mae: 0.3694 - mean_pred: 0.7278 - mae_t1: 0.0246 - val_loss: 1.0097 - val_mae: 0.4418 - val_mean_pred: 0.8762 - val_mae_t1: 0.0295\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8800 - mae: 0.3850 - mean_pred: 0.8337 - mae_t1: 0.0257 - val_loss: 1.0166 - val_mae: 0.4448 - val_mean_pred: 0.9361 - val_mae_t1: 0.0297\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8570 - mae: 0.3749 - mean_pred: 0.8365 - mae_t1: 0.0250 - val_loss: 1.0511 - val_mae: 0.4599 - val_mean_pred: 0.7680 - val_mae_t1: 0.0307\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 0.8828 - mae: 0.3862 - mean_pred: 0.6758 - mae_t1: 0.0257 - val_loss: 1.1154 - val_mae: 0.4880 - val_mean_pred: 0.6951 - val_mae_t1: 0.0325\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8770 - mae: 0.3837 - mean_pred: 0.6503 - mae_t1: 0.0256 - val_loss: 1.1112 - val_mae: 0.4862 - val_mean_pred: 0.8094 - val_mae_t1: 0.0324\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.9194 - mae: 0.4022 - mean_pred: 0.7685 - mae_t1: 0.0268 - val_loss: 1.1700 - val_mae: 0.5119 - val_mean_pred: 0.9365 - val_mae_t1: 0.0341\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.9388 - mae: 0.4107 - mean_pred: 0.8765 - mae_t1: 0.0274 - val_loss: 1.0900 - val_mae: 0.4769 - val_mean_pred: 0.9689 - val_mae_t1: 0.0318\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8759 - mae: 0.3832 - mean_pred: 0.8674 - mae_t1: 0.0255 - val_loss: 1.0371 - val_mae: 0.4537 - val_mean_pred: 0.8628 - val_mae_t1: 0.0302\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.8214 - mae: 0.3594 - mean_pred: 0.7753 - mae_t1: 0.0240 - val_loss: 1.0818 - val_mae: 0.4733 - val_mean_pred: 0.7733 - val_mae_t1: 0.0316\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8579 - mae: 0.3753 - mean_pred: 0.6867 - mae_t1: 0.0250 - val_loss: 1.0573 - val_mae: 0.4626 - val_mean_pred: 0.7716 - val_mae_t1: 0.0308\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 0.8188 - mae: 0.3582 - mean_pred: 0.7196 - mae_t1: 0.0239 - val_loss: 0.9970 - val_mae: 0.4362 - val_mean_pred: 0.9176 - val_mae_t1: 0.0291\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.8385 - mae: 0.3669 - mean_pred: 0.8332 - mae_t1: 0.0245 - val_loss: 1.0012 - val_mae: 0.4380 - val_mean_pred: 0.9360 - val_mae_t1: 0.0292\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 0.8502 - mae: 0.3720 - mean_pred: 0.8517 - mae_t1: 0.0248 - val_loss: 0.9880 - val_mae: 0.4323 - val_mean_pred: 0.8947 - val_mae_t1: 0.0288\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8400 - mae: 0.3675 - mean_pred: 0.8022 - mae_t1: 0.0245 - val_loss: 0.9972 - val_mae: 0.4363 - val_mean_pred: 0.8787 - val_mae_t1: 0.0291\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.8465 - mae: 0.3703 - mean_pred: 0.8202 - mae_t1: 0.0247 - val_loss: 1.0246 - val_mae: 0.4483 - val_mean_pred: 0.9261 - val_mae_t1: 0.0299\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 0.8388 - mae: 0.3670 - mean_pred: 0.8353 - mae_t1: 0.0245 - val_loss: 1.0184 - val_mae: 0.4455 - val_mean_pred: 0.8450 - val_mae_t1: 0.0297\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8291 - mae: 0.3627 - mean_pred: 0.7522 - mae_t1: 0.0242 - val_loss: 0.9992 - val_mae: 0.4372 - val_mean_pred: 0.8481 - val_mae_t1: 0.0291\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.7978 - mae: 0.3490 - mean_pred: 0.7785 - mae_t1: 0.0233 - val_loss: 1.0874 - val_mae: 0.4758 - val_mean_pred: 0.9687 - val_mae_t1: 0.0317\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8828 - mae: 0.3862 - mean_pred: 0.8774 - mae_t1: 0.0257 - val_loss: 1.0543 - val_mae: 0.4612 - val_mean_pred: 0.9087 - val_mae_t1: 0.0307\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.7916 - mae: 0.3463 - mean_pred: 0.7812 - mae_t1: 0.0231 - val_loss: 1.0973 - val_mae: 0.4801 - val_mean_pred: 0.7430 - val_mae_t1: 0.0320\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8530 - mae: 0.3732 - mean_pred: 0.6582 - mae_t1: 0.0249 - val_loss: 1.0725 - val_mae: 0.4692 - val_mean_pred: 0.7871 - val_mae_t1: 0.0313\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8030 - mae: 0.3513 - mean_pred: 0.7427 - mae_t1: 0.0234 - val_loss: 1.0453 - val_mae: 0.4573 - val_mean_pred: 0.9724 - val_mae_t1: 0.0305\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8494 - mae: 0.3716 - mean_pred: 0.8991 - mae_t1: 0.0248 - val_loss: 1.0390 - val_mae: 0.4545 - val_mean_pred: 0.9568 - val_mae_t1: 0.0303\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.8128 - mae: 0.3556 - mean_pred: 0.8588 - mae_t1: 0.0237 - val_loss: 1.0403 - val_mae: 0.4551 - val_mean_pred: 0.8306 - val_mae_t1: 0.0303\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.7809 - mae: 0.3417 - mean_pred: 0.7624 - mae_t1: 0.0228 - val_loss: 1.0547 - val_mae: 0.4614 - val_mean_pred: 0.8524 - val_mae_t1: 0.0308\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7743 - mae: 0.3387 - mean_pred: 0.7827 - mae_t1: 0.0226 - val_loss: 1.0895 - val_mae: 0.4767 - val_mean_pred: 0.9534 - val_mae_t1: 0.0318\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.8158 - mae: 0.3569 - mean_pred: 0.8743 - mae_t1: 0.0238 - val_loss: 1.1113 - val_mae: 0.4862 - val_mean_pred: 0.9422 - val_mae_t1: 0.0324\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.7851 - mae: 0.3435 - mean_pred: 0.8135 - mae_t1: 0.0229 - val_loss: 1.1440 - val_mae: 0.5005 - val_mean_pred: 0.7814 - val_mae_t1: 0.0334\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.8249 - mae: 0.3609 - mean_pred: 0.6929 - mae_t1: 0.0241 - val_loss: 1.1769 - val_mae: 0.5149 - val_mean_pred: 0.7137 - val_mae_t1: 0.0343\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.8811 - mae: 0.3855 - mean_pred: 0.6248 - mae_t1: 0.0257 - val_loss: 1.2094 - val_mae: 0.5291 - val_mean_pred: 0.6891 - val_mae_t1: 0.0353\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8912 - mae: 0.3899 - mean_pred: 0.6157 - mae_t1: 0.0260 - val_loss: 1.1138 - val_mae: 0.4873 - val_mean_pred: 0.8195 - val_mae_t1: 0.0325\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.7815 - mae: 0.3419 - mean_pred: 0.7506 - mae_t1: 0.0228 - val_loss: 1.1698 - val_mae: 0.5118 - val_mean_pred: 1.0320 - val_mae_t1: 0.0341\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8793 - mae: 0.3847 - mean_pred: 0.9301 - mae_t1: 0.0256 - val_loss: 1.1740 - val_mae: 0.5136 - val_mean_pred: 1.0598 - val_mae_t1: 0.0342\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8520 - mae: 0.3728 - mean_pred: 0.9152 - mae_t1: 0.0249 - val_loss: 1.0871 - val_mae: 0.4756 - val_mean_pred: 0.8712 - val_mae_t1: 0.0317\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8241 - mae: 0.3605 - mean_pred: 0.7319 - mae_t1: 0.0240 - val_loss: 1.1334 - val_mae: 0.4958 - val_mean_pred: 0.7215 - val_mae_t1: 0.0331\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8453 - mae: 0.3698 - mean_pred: 0.6447 - mae_t1: 0.0247 - val_loss: 1.1054 - val_mae: 0.4836 - val_mean_pred: 0.7556 - val_mae_t1: 0.0322\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.8764 - mae: 0.3834 - mean_pred: 0.7050 - mae_t1: 0.0256 - val_loss: 1.0419 - val_mae: 0.4558 - val_mean_pred: 0.8388 - val_mae_t1: 0.0304\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.8300 - mae: 0.3631 - mean_pred: 0.7791 - mae_t1: 0.0242 - val_loss: 1.0295 - val_mae: 0.4504 - val_mean_pred: 0.8829 - val_mae_t1: 0.0300\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.8103 - mae: 0.3545 - mean_pred: 0.7895 - mae_t1: 0.0236 - val_loss: 1.1086 - val_mae: 0.4850 - val_mean_pred: 0.9165 - val_mae_t1: 0.0323\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 0.8640 - mae: 0.3780 - mean_pred: 0.8318 - mae_t1: 0.0252 - val_loss: 1.0944 - val_mae: 0.4788 - val_mean_pred: 0.9780 - val_mae_t1: 0.0319\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 0.8470 - mae: 0.3706 - mean_pred: 0.8695 - mae_t1: 0.0247 - val_loss: 0.9765 - val_mae: 0.4272 - val_mean_pred: 0.8756 - val_mae_t1: 0.0285\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.8507 - mae: 0.3722 - mean_pred: 0.7792 - mae_t1: 0.0248 - val_loss: 0.9841 - val_mae: 0.4305 - val_mean_pred: 0.8130 - val_mae_t1: 0.0287\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.8330 - mae: 0.3644 - mean_pred: 0.7413 - mae_t1: 0.0243 - val_loss: 0.9824 - val_mae: 0.4298 - val_mean_pred: 0.9035 - val_mae_t1: 0.0287\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.7549 - mae: 0.3303 - mean_pred: 0.8159 - mae_t1: 0.0220 - val_loss: 1.0755 - val_mae: 0.4705 - val_mean_pred: 1.0204 - val_mae_t1: 0.0314\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.37749797523602685  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▂▂▂▂▁</td></tr><tr><td>mae</td><td>█▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▂▂▂▂▁</td></tr><tr><td>mae_t1</td><td>█▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▂▂▂▂▁</td></tr><tr><td>mean_pred</td><td>▁▅▅▄▅▆▆▇▆▆▇█▅▆▆▇▆▆▆▆▆▇▆▇▅▇▆▇▇▆▅█▆▇▅█▅▆▇▇</td></tr><tr><td>val_loss</td><td>█▄▄▄▄▃▄▄▄▃▂▂▃▃▃▃▃▂▂▃▃▂▂▂▃▂▁▁▂▃▂▂▃▃▃▄▃▂▁▂</td></tr><tr><td>val_mae</td><td>█▄▄▄▄▃▄▄▄▃▂▂▃▃▃▃▃▂▂▃▃▂▂▂▃▂▁▁▂▃▂▂▃▃▃▄▃▂▁▂</td></tr><tr><td>val_mae_t1</td><td>█▄▄▄▄▃▄▄▄▃▂▂▃▃▃▃▃▂▂▃▃▂▂▂▃▂▁▁▂▃▂▂▃▃▃▄▃▂▁▂</td></tr><tr><td>val_mean_pred</td><td>▁▅▄▆▆▅▅▅▅▅▇▆▆▅▆▅▅▅▆▆▆▇▅▅▅▆▇▆▆▇▅▇▇▅▅█▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.56141</td></tr><tr><td>AE_2</td><td>0.35808</td></tr><tr><td>AE_3</td><td>0.28693</td></tr><tr><td>MAE</td><td>0.3775</td></tr><tr><td>best_epoch</td><td>96</td></tr><tr><td>best_val_loss</td><td>0.9765</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.75489</td></tr><tr><td>mae</td><td>0.33027</td></tr><tr><td>mae_t1</td><td>0.02202</td></tr><tr><td>mean_pred</td><td>0.81591</td></tr><tr><td>val_loss</td><td>1.0755</td></tr><tr><td>val_mae</td><td>0.47053</td></tr><tr><td>val_mae_t1</td><td>0.03137</td></tr><tr><td>val_mean_pred</td><td>1.02043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vague-smoke-72</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xgivayg\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1xgivayg</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154931-1xgivayg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_154953-11z8nt2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/11z8nt2t\" target=\"_blank\">balmy-sponge-73</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 888us/sample - loss: 3.4813 - mae: 0.6769 - mean_pred: 0.4917 - mae_t1: 0.0451 - val_loss: 2.3676 - val_mae: 0.4604 - val_mean_pred: 0.7757 - val_mae_t1: 0.0307\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 278us/sample - loss: 2.8992 - mae: 0.5637 - mean_pred: 0.7331 - mae_t1: 0.0376 - val_loss: 2.6040 - val_mae: 0.5063 - val_mean_pred: 0.8342 - val_mae_t1: 0.0338\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 244us/sample - loss: 2.7762 - mae: 0.5398 - mean_pred: 0.8240 - mae_t1: 0.0360 - val_loss: 2.5570 - val_mae: 0.4972 - val_mean_pred: 0.8178 - val_mae_t1: 0.0331\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.6871 - mae: 0.5225 - mean_pred: 0.7225 - mae_t1: 0.0348 - val_loss: 2.3846 - val_mae: 0.4637 - val_mean_pred: 0.8216 - val_mae_t1: 0.0309\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.8691 - mae: 0.5579 - mean_pred: 0.9382 - mae_t1: 0.0372 - val_loss: 2.8288 - val_mae: 0.5500 - val_mean_pred: 0.7021 - val_mae_t1: 0.0367\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.8056 - mae: 0.5455 - mean_pred: 0.5526 - mae_t1: 0.0364 - val_loss: 2.8153 - val_mae: 0.5474 - val_mean_pred: 0.6021 - val_mae_t1: 0.0365\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.7812 - mae: 0.5408 - mean_pred: 0.6952 - mae_t1: 0.0361 - val_loss: 2.7890 - val_mae: 0.5423 - val_mean_pred: 0.9611 - val_mae_t1: 0.0362\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.5197 - mae: 0.4899 - mean_pred: 0.8951 - mae_t1: 0.0327 - val_loss: 2.5164 - val_mae: 0.4893 - val_mean_pred: 0.8661 - val_mae_t1: 0.0326\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.3381 - mae: 0.4546 - mean_pred: 0.7323 - mae_t1: 0.0303 - val_loss: 2.6539 - val_mae: 0.5160 - val_mean_pred: 0.6819 - val_mae_t1: 0.0344\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.3189 - mae: 0.4509 - mean_pred: 0.6507 - mae_t1: 0.0301 - val_loss: 2.4016 - val_mae: 0.4670 - val_mean_pred: 0.8098 - val_mae_t1: 0.0311\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 2.4183 - mae: 0.4702 - mean_pred: 0.9134 - mae_t1: 0.0313 - val_loss: 2.3493 - val_mae: 0.4568 - val_mean_pred: 0.9004 - val_mae_t1: 0.0305\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.1253 - mae: 0.4133 - mean_pred: 0.7745 - mae_t1: 0.0276 - val_loss: 2.8090 - val_mae: 0.5462 - val_mean_pred: 0.6123 - val_mae_t1: 0.0364\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.4429 - mae: 0.4750 - mean_pred: 0.6023 - mae_t1: 0.0317 - val_loss: 2.4887 - val_mae: 0.4839 - val_mean_pred: 0.7794 - val_mae_t1: 0.0323\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.1629 - mae: 0.4206 - mean_pred: 0.8416 - mae_t1: 0.0280 - val_loss: 2.4825 - val_mae: 0.4827 - val_mean_pred: 0.9511 - val_mae_t1: 0.0322\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 215us/sample - loss: 2.2968 - mae: 0.4466 - mean_pred: 0.8233 - mae_t1: 0.0298 - val_loss: 2.3332 - val_mae: 0.4537 - val_mean_pred: 0.8434 - val_mae_t1: 0.0302\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 222us/sample - loss: 2.4785 - mae: 0.4819 - mean_pred: 0.8569 - mae_t1: 0.0321 - val_loss: 2.2622 - val_mae: 0.4399 - val_mean_pred: 0.7670 - val_mae_t1: 0.0293\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.2051 - mae: 0.4288 - mean_pred: 0.7238 - mae_t1: 0.0286 - val_loss: 2.4939 - val_mae: 0.4849 - val_mean_pred: 0.8315 - val_mae_t1: 0.0323\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.1815 - mae: 0.4242 - mean_pred: 0.8844 - mae_t1: 0.0283 - val_loss: 2.4534 - val_mae: 0.4771 - val_mean_pred: 0.9057 - val_mae_t1: 0.0318\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.1102 - mae: 0.4103 - mean_pred: 0.7609 - mae_t1: 0.0274 - val_loss: 2.5367 - val_mae: 0.4932 - val_mean_pred: 0.7116 - val_mae_t1: 0.0329\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 216us/sample - loss: 2.0383 - mae: 0.3963 - mean_pred: 0.7294 - mae_t1: 0.0264 - val_loss: 2.2361 - val_mae: 0.4348 - val_mean_pred: 0.9515 - val_mae_t1: 0.0290\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.0204 - mae: 0.3928 - mean_pred: 0.8658 - mae_t1: 0.0262 - val_loss: 2.4188 - val_mae: 0.4703 - val_mean_pred: 0.8179 - val_mae_t1: 0.0314\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0884 - mae: 0.4061 - mean_pred: 0.7481 - mae_t1: 0.0271 - val_loss: 2.2901 - val_mae: 0.4453 - val_mean_pred: 0.9252 - val_mae_t1: 0.0297\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 2.1371 - mae: 0.4156 - mean_pred: 0.9256 - mae_t1: 0.0277 - val_loss: 2.5293 - val_mae: 0.4918 - val_mean_pred: 0.8977 - val_mae_t1: 0.0328\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 1.9742 - mae: 0.3839 - mean_pred: 0.7949 - mae_t1: 0.0256 - val_loss: 2.5328 - val_mae: 0.4925 - val_mean_pred: 0.8019 - val_mae_t1: 0.0328\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.9330 - mae: 0.3759 - mean_pred: 0.7487 - mae_t1: 0.0251 - val_loss: 2.3567 - val_mae: 0.4582 - val_mean_pred: 0.8583 - val_mae_t1: 0.0305\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9932 - mae: 0.3876 - mean_pred: 0.8663 - mae_t1: 0.0258 - val_loss: 2.3426 - val_mae: 0.4555 - val_mean_pred: 0.9022 - val_mae_t1: 0.0304\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.9106 - mae: 0.3715 - mean_pred: 0.8127 - mae_t1: 0.0248 - val_loss: 2.2627 - val_mae: 0.4400 - val_mean_pred: 0.8196 - val_mae_t1: 0.0293\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.9385 - mae: 0.3769 - mean_pred: 0.7441 - mae_t1: 0.0251 - val_loss: 2.3477 - val_mae: 0.4565 - val_mean_pred: 0.7913 - val_mae_t1: 0.0304\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.8172 - mae: 0.3534 - mean_pred: 0.7614 - mae_t1: 0.0236 - val_loss: 2.3657 - val_mae: 0.4600 - val_mean_pred: 0.9081 - val_mae_t1: 0.0307\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8475 - mae: 0.3592 - mean_pred: 0.8474 - mae_t1: 0.0239 - val_loss: 2.3886 - val_mae: 0.4644 - val_mean_pred: 0.9705 - val_mae_t1: 0.0310\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.8195 - mae: 0.3538 - mean_pred: 0.8787 - mae_t1: 0.0236 - val_loss: 2.3349 - val_mae: 0.4540 - val_mean_pred: 0.8224 - val_mae_t1: 0.0303\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 1.7950 - mae: 0.3490 - mean_pred: 0.7235 - mae_t1: 0.0233 - val_loss: 2.1678 - val_mae: 0.4215 - val_mean_pred: 0.8436 - val_mae_t1: 0.0281\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.7552 - mae: 0.3413 - mean_pred: 0.8080 - mae_t1: 0.0228 - val_loss: 2.1563 - val_mae: 0.4193 - val_mean_pred: 0.9090 - val_mae_t1: 0.0280\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.8170 - mae: 0.3533 - mean_pred: 0.8575 - mae_t1: 0.0236 - val_loss: 2.2430 - val_mae: 0.4361 - val_mean_pred: 0.9004 - val_mae_t1: 0.0291\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9477 - mae: 0.3787 - mean_pred: 0.8032 - mae_t1: 0.0252 - val_loss: 2.3716 - val_mae: 0.4611 - val_mean_pred: 0.7832 - val_mae_t1: 0.0307\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 1.7652 - mae: 0.3432 - mean_pred: 0.7759 - mae_t1: 0.0229 - val_loss: 2.3915 - val_mae: 0.4650 - val_mean_pred: 0.9370 - val_mae_t1: 0.0310\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 1.7914 - mae: 0.3483 - mean_pred: 0.8639 - mae_t1: 0.0232 - val_loss: 2.3662 - val_mae: 0.4601 - val_mean_pred: 0.8476 - val_mae_t1: 0.0307\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.7421 - mae: 0.3387 - mean_pred: 0.7543 - mae_t1: 0.0226 - val_loss: 2.3029 - val_mae: 0.4478 - val_mean_pred: 0.9326 - val_mae_t1: 0.0299\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 1.7923 - mae: 0.3485 - mean_pred: 0.8753 - mae_t1: 0.0232 - val_loss: 2.1292 - val_mae: 0.4140 - val_mean_pred: 0.8865 - val_mae_t1: 0.0276\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.6833 - mae: 0.3273 - mean_pred: 0.7139 - mae_t1: 0.0218 - val_loss: 2.2593 - val_mae: 0.4393 - val_mean_pred: 0.7234 - val_mae_t1: 0.0293\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.8539 - mae: 0.3605 - mean_pred: 0.6325 - mae_t1: 0.0240 - val_loss: 2.3034 - val_mae: 0.4479 - val_mean_pred: 0.8464 - val_mae_t1: 0.0299\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 1.7717 - mae: 0.3445 - mean_pred: 0.7994 - mae_t1: 0.0230 - val_loss: 2.0904 - val_mae: 0.4065 - val_mean_pred: 0.9182 - val_mae_t1: 0.0271\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6632 - mae: 0.3234 - mean_pred: 0.7622 - mae_t1: 0.0216 - val_loss: 2.1123 - val_mae: 0.4107 - val_mean_pred: 0.7594 - val_mae_t1: 0.0274\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.7957 - mae: 0.3492 - mean_pred: 0.6829 - mae_t1: 0.0233 - val_loss: 2.1465 - val_mae: 0.4174 - val_mean_pred: 0.8574 - val_mae_t1: 0.0278\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.6831 - mae: 0.3273 - mean_pred: 0.8435 - mae_t1: 0.0218 - val_loss: 2.3656 - val_mae: 0.4600 - val_mean_pred: 1.0300 - val_mae_t1: 0.0307\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.8107 - mae: 0.3521 - mean_pred: 0.8588 - mae_t1: 0.0235 - val_loss: 2.3960 - val_mae: 0.4659 - val_mean_pred: 0.8361 - val_mae_t1: 0.0311\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7824 - mae: 0.3466 - mean_pred: 0.8022 - mae_t1: 0.0231 - val_loss: 2.3163 - val_mae: 0.4504 - val_mean_pred: 0.9612 - val_mae_t1: 0.0300\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.7860 - mae: 0.3473 - mean_pred: 0.8404 - mae_t1: 0.0232 - val_loss: 2.3569 - val_mae: 0.4583 - val_mean_pred: 0.8202 - val_mae_t1: 0.0306\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.6504 - mae: 0.3209 - mean_pred: 0.7594 - mae_t1: 0.0214 - val_loss: 2.3328 - val_mae: 0.4536 - val_mean_pred: 0.9905 - val_mae_t1: 0.0302\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.5497 - mae: 0.3013 - mean_pred: 0.8637 - mae_t1: 0.0201 - val_loss: 2.1650 - val_mae: 0.4210 - val_mean_pred: 0.8405 - val_mae_t1: 0.0281\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.6245 - mae: 0.3159 - mean_pred: 0.7470 - mae_t1: 0.0211 - val_loss: 2.2759 - val_mae: 0.4425 - val_mean_pred: 0.7927 - val_mae_t1: 0.0295\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 1.6443 - mae: 0.3197 - mean_pred: 0.7884 - mae_t1: 0.0213 - val_loss: 1.9717 - val_mae: 0.3834 - val_mean_pred: 0.8700 - val_mae_t1: 0.0256\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7030 - mae: 0.3311 - mean_pred: 0.7638 - mae_t1: 0.0221 - val_loss: 2.2215 - val_mae: 0.4320 - val_mean_pred: 0.7943 - val_mae_t1: 0.0288\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.5587 - mae: 0.3031 - mean_pred: 0.7322 - mae_t1: 0.0202 - val_loss: 2.3849 - val_mae: 0.4637 - val_mean_pred: 1.0023 - val_mae_t1: 0.0309\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7301 - mae: 0.3364 - mean_pred: 0.8916 - mae_t1: 0.0224 - val_loss: 2.2929 - val_mae: 0.4458 - val_mean_pred: 0.8733 - val_mae_t1: 0.0297\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.6078 - mae: 0.3126 - mean_pred: 0.7465 - mae_t1: 0.0208 - val_loss: 2.3768 - val_mae: 0.4622 - val_mean_pred: 0.8319 - val_mae_t1: 0.0308\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.5799 - mae: 0.3072 - mean_pred: 0.7740 - mae_t1: 0.0205 - val_loss: 2.2147 - val_mae: 0.4306 - val_mean_pred: 0.8911 - val_mae_t1: 0.0287\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.6289 - mae: 0.3167 - mean_pred: 0.8128 - mae_t1: 0.0211 - val_loss: 2.3015 - val_mae: 0.4475 - val_mean_pred: 0.9285 - val_mae_t1: 0.0298\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 1.6531 - mae: 0.3214 - mean_pred: 0.8565 - mae_t1: 0.0214 - val_loss: 2.1687 - val_mae: 0.4217 - val_mean_pred: 0.8621 - val_mae_t1: 0.0281\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.5231 - mae: 0.2962 - mean_pred: 0.8137 - mae_t1: 0.0197 - val_loss: 2.0826 - val_mae: 0.4050 - val_mean_pred: 0.9423 - val_mae_t1: 0.0270\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.4535 - mae: 0.2826 - mean_pred: 0.8482 - mae_t1: 0.0188 - val_loss: 2.2031 - val_mae: 0.4284 - val_mean_pred: 0.8401 - val_mae_t1: 0.0286\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.5473 - mae: 0.3009 - mean_pred: 0.7358 - mae_t1: 0.0201 - val_loss: 2.1195 - val_mae: 0.4121 - val_mean_pred: 0.8603 - val_mae_t1: 0.0275\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.3841 - mae: 0.2691 - mean_pred: 0.7876 - mae_t1: 0.0179 - val_loss: 2.3093 - val_mae: 0.4490 - val_mean_pred: 0.8952 - val_mae_t1: 0.0299\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.5460 - mae: 0.3006 - mean_pred: 0.8305 - mae_t1: 0.0200 - val_loss: 2.2036 - val_mae: 0.4285 - val_mean_pred: 0.9094 - val_mae_t1: 0.0286\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.3843 - mae: 0.2692 - mean_pred: 0.8078 - mae_t1: 0.0179 - val_loss: 2.1338 - val_mae: 0.4149 - val_mean_pred: 0.8811 - val_mae_t1: 0.0277\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.3427 - mae: 0.2611 - mean_pred: 0.7912 - mae_t1: 0.0174 - val_loss: 2.0723 - val_mae: 0.4030 - val_mean_pred: 0.9017 - val_mae_t1: 0.0269\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.3265 - mae: 0.2579 - mean_pred: 0.8143 - mae_t1: 0.0172 - val_loss: 2.1330 - val_mae: 0.4147 - val_mean_pred: 0.9070 - val_mae_t1: 0.0276\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4671 - mae: 0.2853 - mean_pred: 0.8182 - mae_t1: 0.0190 - val_loss: 2.3741 - val_mae: 0.4616 - val_mean_pred: 0.8084 - val_mae_t1: 0.0308\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.5668 - mae: 0.3046 - mean_pred: 0.7058 - mae_t1: 0.0203 - val_loss: 2.1999 - val_mae: 0.4278 - val_mean_pred: 0.8730 - val_mae_t1: 0.0285\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 1.4741 - mae: 0.2866 - mean_pred: 0.8458 - mae_t1: 0.0191 - val_loss: 2.0631 - val_mae: 0.4012 - val_mean_pred: 0.8822 - val_mae_t1: 0.0267\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.4575 - mae: 0.2834 - mean_pred: 0.7520 - mae_t1: 0.0189 - val_loss: 2.1394 - val_mae: 0.4160 - val_mean_pred: 0.7780 - val_mae_t1: 0.0277\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.3437 - mae: 0.2613 - mean_pred: 0.7666 - mae_t1: 0.0174 - val_loss: 2.1528 - val_mae: 0.4186 - val_mean_pred: 0.9822 - val_mae_t1: 0.0279\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.5099 - mae: 0.2936 - mean_pred: 0.8755 - mae_t1: 0.0196 - val_loss: 2.6001 - val_mae: 0.5056 - val_mean_pred: 0.7600 - val_mae_t1: 0.0337\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.8670 - mae: 0.3630 - mean_pred: 0.6235 - mae_t1: 0.0242 - val_loss: 2.6113 - val_mae: 0.5078 - val_mean_pred: 0.6695 - val_mae_t1: 0.0339\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.6268 - mae: 0.3163 - mean_pred: 0.7499 - mae_t1: 0.0211 - val_loss: 2.3668 - val_mae: 0.4602 - val_mean_pred: 1.0147 - val_mae_t1: 0.0307\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.7592 - mae: 0.3421 - mean_pred: 0.9637 - mae_t1: 0.0228 - val_loss: 2.3866 - val_mae: 0.4641 - val_mean_pred: 0.8166 - val_mae_t1: 0.0309\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.7787 - mae: 0.3459 - mean_pred: 0.6598 - mae_t1: 0.0231 - val_loss: 2.7497 - val_mae: 0.5347 - val_mean_pred: 0.6510 - val_mae_t1: 0.0356\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.7652 - mae: 0.3432 - mean_pred: 0.6822 - mae_t1: 0.0229 - val_loss: 2.5144 - val_mae: 0.4889 - val_mean_pred: 0.9953 - val_mae_t1: 0.0326\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.8589 - mae: 0.3614 - mean_pred: 0.9293 - mae_t1: 0.0241 - val_loss: 2.3751 - val_mae: 0.4618 - val_mean_pred: 0.9543 - val_mae_t1: 0.0308\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.6505 - mae: 0.3209 - mean_pred: 0.7871 - mae_t1: 0.0214 - val_loss: 2.5225 - val_mae: 0.4905 - val_mean_pred: 0.7156 - val_mae_t1: 0.0327\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.6602 - mae: 0.3228 - mean_pred: 0.6703 - mae_t1: 0.0215 - val_loss: 2.3113 - val_mae: 0.4494 - val_mean_pred: 0.8787 - val_mae_t1: 0.0300\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.4799 - mae: 0.2878 - mean_pred: 0.8018 - mae_t1: 0.0192 - val_loss: 2.2447 - val_mae: 0.4365 - val_mean_pred: 0.9055 - val_mae_t1: 0.0291\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.3599 - mae: 0.2644 - mean_pred: 0.7987 - mae_t1: 0.0176 - val_loss: 2.0936 - val_mae: 0.4071 - val_mean_pred: 0.8972 - val_mae_t1: 0.0271\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3704 - mae: 0.2665 - mean_pred: 0.8181 - mae_t1: 0.0178 - val_loss: 2.1751 - val_mae: 0.4229 - val_mean_pred: 0.8877 - val_mae_t1: 0.0282\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.3957 - mae: 0.2714 - mean_pred: 0.8198 - mae_t1: 0.0181 - val_loss: 2.3619 - val_mae: 0.4593 - val_mean_pred: 0.9484 - val_mae_t1: 0.0306\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5503 - mae: 0.3014 - mean_pred: 0.8424 - mae_t1: 0.0201 - val_loss: 2.3151 - val_mae: 0.4502 - val_mean_pred: 0.9527 - val_mae_t1: 0.0300\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.5476 - mae: 0.3009 - mean_pred: 0.8309 - mae_t1: 0.0201 - val_loss: 2.1505 - val_mae: 0.4181 - val_mean_pred: 0.9132 - val_mae_t1: 0.0279\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3277 - mae: 0.2582 - mean_pred: 0.7960 - mae_t1: 0.0172 - val_loss: 1.9946 - val_mae: 0.3878 - val_mean_pred: 0.8641 - val_mae_t1: 0.0259\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.3532 - mae: 0.2631 - mean_pred: 0.8158 - mae_t1: 0.0175 - val_loss: 1.9936 - val_mae: 0.3876 - val_mean_pred: 0.8783 - val_mae_t1: 0.0258\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3333 - mae: 0.2593 - mean_pred: 0.7588 - mae_t1: 0.0173 - val_loss: 2.1447 - val_mae: 0.4170 - val_mean_pred: 0.8404 - val_mae_t1: 0.0278\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.2764 - mae: 0.2482 - mean_pred: 0.8076 - mae_t1: 0.0165 - val_loss: 2.1549 - val_mae: 0.4190 - val_mean_pred: 1.0030 - val_mae_t1: 0.0279\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.3424 - mae: 0.2610 - mean_pred: 0.8519 - mae_t1: 0.0174 - val_loss: 2.1744 - val_mae: 0.4228 - val_mean_pred: 0.8353 - val_mae_t1: 0.0282\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.3108 - mae: 0.2549 - mean_pred: 0.7563 - mae_t1: 0.0170 - val_loss: 2.1871 - val_mae: 0.4253 - val_mean_pred: 0.8983 - val_mae_t1: 0.0284\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.2386 - mae: 0.2408 - mean_pred: 0.8384 - mae_t1: 0.0161 - val_loss: 2.2937 - val_mae: 0.4460 - val_mean_pred: 0.9294 - val_mae_t1: 0.0297\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.2918 - mae: 0.2512 - mean_pred: 0.8394 - mae_t1: 0.0167 - val_loss: 2.1709 - val_mae: 0.4221 - val_mean_pred: 0.9222 - val_mae_t1: 0.0281\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.2950 - mae: 0.2518 - mean_pred: 0.8165 - mae_t1: 0.0168 - val_loss: 2.1802 - val_mae: 0.4239 - val_mean_pred: 0.8842 - val_mae_t1: 0.0283\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.3369 - mae: 0.2600 - mean_pred: 0.7966 - mae_t1: 0.0173 - val_loss: 2.0543 - val_mae: 0.3994 - val_mean_pred: 0.8765 - val_mae_t1: 0.0266\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.2343 - mae: 0.2400 - mean_pred: 0.7813 - mae_t1: 0.0160 - val_loss: 2.1814 - val_mae: 0.4242 - val_mean_pred: 0.9255 - val_mae_t1: 0.0283\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.2940 - mae: 0.2516 - mean_pred: 0.8512 - mae_t1: 0.0168 - val_loss: 2.1420 - val_mae: 0.4165 - val_mean_pred: 0.8976 - val_mae_t1: 0.0278\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.2323 - mae: 0.2396 - mean_pred: 0.7887 - mae_t1: 0.0160 - val_loss: 2.1857 - val_mae: 0.4250 - val_mean_pred: 0.8032 - val_mae_t1: 0.0283\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3048844127394766  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▆▅▅▅▅▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▃▃▃▂▁▂▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▆▆▅▅▅▅▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▃▃▃▂▁▂▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆▆▅▅▅▅▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▃▃▃▂▁▂▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▆▂▇█▃▇▇▇█▇▅▇▇▆▇▃▄▇▅▅▅▅▇▇▆▆▄▅▃▄█▆▆▆▆▇▇▆▆</td></tr><tr><td>val_loss</td><td>▄▆█▅▄▅▃▅▅▆▄▄▄▃▄▂▄▂▄▄▃▄▄▂▃▃▂▃▂▆▇▄▃▃▂▁▃▄▂▃</td></tr><tr><td>val_mae</td><td>▄▆█▅▄▅▃▅▅▆▄▄▄▃▄▂▄▂▄▄▃▄▄▂▃▃▂▃▂▆▇▄▃▃▂▁▃▄▂▃</td></tr><tr><td>val_mae_t1</td><td>▄▆█▅▄▅▃▅▅▆▄▄▄▃▄▂▄▂▄▄▃▄▄▂▃▃▂▃▂▆▇▄▃▃▂▁▃▄▂▃</td></tr><tr><td>val_mean_pred</td><td>▄▅▁▆▆▄▄▆▅▆▆▄▅▆▇▆▅▅▅█▄█▅▆▅▆▆▆█▂▂▇▆▆▆▆▅▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33488</td></tr><tr><td>AE_2</td><td>0.30346</td></tr><tr><td>AE_3</td><td>0.32059</td></tr><tr><td>MAE</td><td>0.30488</td></tr><tr><td>best_epoch</td><td>51</td></tr><tr><td>best_val_loss</td><td>1.97167</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.2323</td></tr><tr><td>mae</td><td>0.23961</td></tr><tr><td>mae_t1</td><td>0.01597</td></tr><tr><td>mean_pred</td><td>0.78873</td></tr><tr><td>val_loss</td><td>2.18569</td></tr><tr><td>val_mae</td><td>0.425</td></tr><tr><td>val_mae_t1</td><td>0.02833</td></tr><tr><td>val_mean_pred</td><td>0.80322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">balmy-sponge-73</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/11z8nt2t\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/11z8nt2t</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_154953-11z8nt2t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155019-2bt40uam</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2bt40uam\" target=\"_blank\">misty-moon-74</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 714us/sample - loss: 4.2927 - mae: 0.8347 - mean_pred: 0.1636 - mae_t1: 0.0556 - val_loss: 3.0674 - val_mae: 0.5964 - val_mean_pred: 0.4881 - val_mae_t1: 0.0398\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 159us/sample - loss: 3.1737 - mae: 0.6171 - mean_pred: 0.5968 - mae_t1: 0.0411 - val_loss: 3.1246 - val_mae: 0.6076 - val_mean_pred: 1.1300 - val_mae_t1: 0.0405\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 153us/sample - loss: 3.8956 - mae: 0.7575 - mean_pred: 1.1867 - mae_t1: 0.0505 - val_loss: 2.9721 - val_mae: 0.5779 - val_mean_pred: 1.1450 - val_mae_t1: 0.0385\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 156us/sample - loss: 3.2580 - mae: 0.6335 - mean_pred: 1.0343 - mae_t1: 0.0422 - val_loss: 2.6548 - val_mae: 0.5162 - val_mean_pred: 0.6107 - val_mae_t1: 0.0344\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 2.8945 - mae: 0.5628 - mean_pred: 0.5358 - mae_t1: 0.0375 - val_loss: 3.1673 - val_mae: 0.6159 - val_mean_pred: 0.4123 - val_mae_t1: 0.0411\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 141us/sample - loss: 3.0516 - mae: 0.5934 - mean_pred: 0.4128 - mae_t1: 0.0396 - val_loss: 2.5858 - val_mae: 0.5028 - val_mean_pred: 0.6071 - val_mae_t1: 0.0335\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 2.6582 - mae: 0.5169 - mean_pred: 0.6140 - mae_t1: 0.0345 - val_loss: 2.5551 - val_mae: 0.4968 - val_mean_pred: 0.8875 - val_mae_t1: 0.0331\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 2.6880 - mae: 0.5227 - mean_pred: 0.8326 - mae_t1: 0.0348 - val_loss: 2.4422 - val_mae: 0.4749 - val_mean_pred: 0.9411 - val_mae_t1: 0.0317\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 2.5957 - mae: 0.5047 - mean_pred: 0.8434 - mae_t1: 0.0336 - val_loss: 2.3489 - val_mae: 0.4567 - val_mean_pred: 0.8236 - val_mae_t1: 0.0304\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.4717 - mae: 0.4806 - mean_pred: 0.7256 - mae_t1: 0.0320 - val_loss: 2.4765 - val_mae: 0.4815 - val_mean_pred: 0.7266 - val_mae_t1: 0.0321\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.4747 - mae: 0.4812 - mean_pred: 0.6703 - mae_t1: 0.0321 - val_loss: 2.3782 - val_mae: 0.4624 - val_mean_pred: 0.8352 - val_mae_t1: 0.0308\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.4077 - mae: 0.4682 - mean_pred: 0.8007 - mae_t1: 0.0312 - val_loss: 2.3788 - val_mae: 0.4625 - val_mean_pred: 0.9210 - val_mae_t1: 0.0308\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.3499 - mae: 0.4569 - mean_pred: 0.8459 - mae_t1: 0.0305 - val_loss: 2.5025 - val_mae: 0.4866 - val_mean_pred: 0.8768 - val_mae_t1: 0.0324\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.3024 - mae: 0.4477 - mean_pred: 0.8340 - mae_t1: 0.0298 - val_loss: 2.5853 - val_mae: 0.5027 - val_mean_pred: 0.8439 - val_mae_t1: 0.0335\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.2690 - mae: 0.4412 - mean_pred: 0.7836 - mae_t1: 0.0294 - val_loss: 2.6883 - val_mae: 0.5227 - val_mean_pred: 0.8535 - val_mae_t1: 0.0348\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.3499 - mae: 0.4569 - mean_pred: 0.8668 - mae_t1: 0.0305 - val_loss: 2.8159 - val_mae: 0.5475 - val_mean_pred: 0.9775 - val_mae_t1: 0.0365\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.6116 - mae: 0.5078 - mean_pred: 0.9543 - mae_t1: 0.0339 - val_loss: 2.7475 - val_mae: 0.5342 - val_mean_pred: 0.7689 - val_mae_t1: 0.0356\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.4095 - mae: 0.4685 - mean_pred: 0.6926 - mae_t1: 0.0312 - val_loss: 3.0208 - val_mae: 0.5874 - val_mean_pred: 0.5912 - val_mae_t1: 0.0392\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.4583 - mae: 0.4780 - mean_pred: 0.5604 - mae_t1: 0.0319 - val_loss: 2.6534 - val_mae: 0.5159 - val_mean_pred: 0.7211 - val_mae_t1: 0.0344\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.1766 - mae: 0.4232 - mean_pred: 0.6903 - mae_t1: 0.0282 - val_loss: 2.4253 - val_mae: 0.4716 - val_mean_pred: 0.9386 - val_mae_t1: 0.0314\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 133us/sample - loss: 2.2958 - mae: 0.4464 - mean_pred: 0.8800 - mae_t1: 0.0298 - val_loss: 2.3355 - val_mae: 0.4541 - val_mean_pred: 0.9609 - val_mae_t1: 0.0303\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 2.1864 - mae: 0.4251 - mean_pred: 0.8473 - mae_t1: 0.0283 - val_loss: 2.1755 - val_mae: 0.4230 - val_mean_pred: 0.8481 - val_mae_t1: 0.0282\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 2.0887 - mae: 0.4061 - mean_pred: 0.7779 - mae_t1: 0.0271 - val_loss: 2.1363 - val_mae: 0.4154 - val_mean_pred: 0.8585 - val_mae_t1: 0.0277\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.1196 - mae: 0.4121 - mean_pred: 0.7939 - mae_t1: 0.0275 - val_loss: 2.1572 - val_mae: 0.4195 - val_mean_pred: 0.8310 - val_mae_t1: 0.0280\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 2.1271 - mae: 0.4136 - mean_pred: 0.7568 - mae_t1: 0.0276 - val_loss: 2.0914 - val_mae: 0.4067 - val_mean_pred: 0.8596 - val_mae_t1: 0.0271\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.1064 - mae: 0.4096 - mean_pred: 0.8247 - mae_t1: 0.0273 - val_loss: 2.1423 - val_mae: 0.4166 - val_mean_pred: 0.8503 - val_mae_t1: 0.0278\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.1025 - mae: 0.4088 - mean_pred: 0.7654 - mae_t1: 0.0273 - val_loss: 2.4375 - val_mae: 0.4740 - val_mean_pred: 0.7316 - val_mae_t1: 0.0316\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 2.1514 - mae: 0.4183 - mean_pred: 0.7136 - mae_t1: 0.0279 - val_loss: 2.4437 - val_mae: 0.4752 - val_mean_pred: 0.8890 - val_mae_t1: 0.0317\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.2239 - mae: 0.4324 - mean_pred: 0.8562 - mae_t1: 0.0288 - val_loss: 2.7023 - val_mae: 0.5254 - val_mean_pred: 0.9723 - val_mae_t1: 0.0350\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.4574 - mae: 0.4778 - mean_pred: 0.8899 - mae_t1: 0.0319 - val_loss: 2.4909 - val_mae: 0.4843 - val_mean_pred: 0.9012 - val_mae_t1: 0.0323\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0670 - mae: 0.4019 - mean_pred: 0.8409 - mae_t1: 0.0268 - val_loss: 2.6472 - val_mae: 0.5147 - val_mean_pred: 0.8136 - val_mae_t1: 0.0343\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.1601 - mae: 0.4200 - mean_pred: 0.7491 - mae_t1: 0.0280 - val_loss: 2.8530 - val_mae: 0.5548 - val_mean_pred: 0.6954 - val_mae_t1: 0.0370\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.3665 - mae: 0.4602 - mean_pred: 0.6710 - mae_t1: 0.0307 - val_loss: 3.0493 - val_mae: 0.5929 - val_mean_pred: 0.8506 - val_mae_t1: 0.0395\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5690 - mae: 0.4995 - mean_pred: 0.8451 - mae_t1: 0.0333 - val_loss: 2.4858 - val_mae: 0.4833 - val_mean_pred: 1.0208 - val_mae_t1: 0.0322\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3180 - mae: 0.4507 - mean_pred: 1.0021 - mae_t1: 0.0300 - val_loss: 2.5660 - val_mae: 0.4989 - val_mean_pred: 0.9737 - val_mae_t1: 0.0333\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.3350 - mae: 0.4540 - mean_pred: 0.9170 - mae_t1: 0.0303 - val_loss: 2.4608 - val_mae: 0.4785 - val_mean_pred: 0.7717 - val_mae_t1: 0.0319\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.1667 - mae: 0.4213 - mean_pred: 0.7082 - mae_t1: 0.0281 - val_loss: 2.8587 - val_mae: 0.5559 - val_mean_pred: 0.7333 - val_mae_t1: 0.0371\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.4000 - mae: 0.4667 - mean_pred: 0.7109 - mae_t1: 0.0311 - val_loss: 2.4036 - val_mae: 0.4674 - val_mean_pred: 0.8097 - val_mae_t1: 0.0312\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0766 - mae: 0.4038 - mean_pred: 0.7854 - mae_t1: 0.0269 - val_loss: 2.6077 - val_mae: 0.5070 - val_mean_pred: 0.8292 - val_mae_t1: 0.0338\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.4520 - mae: 0.4768 - mean_pred: 0.7913 - mae_t1: 0.0318 - val_loss: 2.6345 - val_mae: 0.5123 - val_mean_pred: 0.7067 - val_mae_t1: 0.0342\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.2686 - mae: 0.4411 - mean_pred: 0.6550 - mae_t1: 0.0294 - val_loss: 2.7167 - val_mae: 0.5283 - val_mean_pred: 0.6695 - val_mae_t1: 0.0352\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.2060 - mae: 0.4290 - mean_pred: 0.6558 - mae_t1: 0.0286 - val_loss: 2.4216 - val_mae: 0.4709 - val_mean_pred: 0.9289 - val_mae_t1: 0.0314\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.1068 - mae: 0.4097 - mean_pred: 0.9137 - mae_t1: 0.0273 - val_loss: 2.6057 - val_mae: 0.5067 - val_mean_pred: 1.0649 - val_mae_t1: 0.0338\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.1769 - mae: 0.4233 - mean_pred: 0.9370 - mae_t1: 0.0282 - val_loss: 2.4062 - val_mae: 0.4679 - val_mean_pred: 0.8826 - val_mae_t1: 0.0312\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.9916 - mae: 0.3872 - mean_pred: 0.7828 - mae_t1: 0.0258 - val_loss: 2.4359 - val_mae: 0.4737 - val_mean_pred: 0.8275 - val_mae_t1: 0.0316\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.0339 - mae: 0.3955 - mean_pred: 0.7203 - mae_t1: 0.0264 - val_loss: 2.6424 - val_mae: 0.5138 - val_mean_pred: 0.8763 - val_mae_t1: 0.0343\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.1350 - mae: 0.4151 - mean_pred: 0.7718 - mae_t1: 0.0277 - val_loss: 2.3322 - val_mae: 0.4535 - val_mean_pred: 0.9758 - val_mae_t1: 0.0302\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 2.0009 - mae: 0.3891 - mean_pred: 0.8509 - mae_t1: 0.0259 - val_loss: 2.1837 - val_mae: 0.4246 - val_mean_pred: 0.9049 - val_mae_t1: 0.0283\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.0306 - mae: 0.3948 - mean_pred: 0.7695 - mae_t1: 0.0263 - val_loss: 2.3172 - val_mae: 0.4506 - val_mean_pred: 0.7475 - val_mae_t1: 0.0300\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.0001 - mae: 0.3889 - mean_pred: 0.6869 - mae_t1: 0.0259 - val_loss: 2.2662 - val_mae: 0.4406 - val_mean_pred: 0.8936 - val_mae_t1: 0.0294\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.9405 - mae: 0.3773 - mean_pred: 0.8551 - mae_t1: 0.0252 - val_loss: 2.4867 - val_mae: 0.4835 - val_mean_pred: 1.0552 - val_mae_t1: 0.0322\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.0176 - mae: 0.3923 - mean_pred: 0.9346 - mae_t1: 0.0262 - val_loss: 2.5311 - val_mae: 0.4922 - val_mean_pred: 0.9780 - val_mae_t1: 0.0328\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9709 - mae: 0.3832 - mean_pred: 0.8655 - mae_t1: 0.0255 - val_loss: 2.5845 - val_mae: 0.5025 - val_mean_pred: 0.9585 - val_mae_t1: 0.0335\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.9362 - mae: 0.3765 - mean_pred: 0.8260 - mae_t1: 0.0251 - val_loss: 2.6023 - val_mae: 0.5060 - val_mean_pred: 0.8411 - val_mae_t1: 0.0337\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9584 - mae: 0.3808 - mean_pred: 0.7004 - mae_t1: 0.0254 - val_loss: 2.8027 - val_mae: 0.5450 - val_mean_pred: 0.6701 - val_mae_t1: 0.0363\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.2680 - mae: 0.4410 - mean_pred: 0.5655 - mae_t1: 0.0294 - val_loss: 3.0136 - val_mae: 0.5860 - val_mean_pred: 0.6028 - val_mae_t1: 0.0391\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3675 - mae: 0.4603 - mean_pred: 0.5476 - mae_t1: 0.0307 - val_loss: 2.7904 - val_mae: 0.5426 - val_mean_pred: 0.7407 - val_mae_t1: 0.0362\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0341 - mae: 0.3955 - mean_pred: 0.6485 - mae_t1: 0.0264 - val_loss: 2.6560 - val_mae: 0.5165 - val_mean_pred: 0.9756 - val_mae_t1: 0.0344\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.9765 - mae: 0.3843 - mean_pred: 0.8356 - mae_t1: 0.0256 - val_loss: 2.7653 - val_mae: 0.5377 - val_mean_pred: 1.1141 - val_mae_t1: 0.0358\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.0997 - mae: 0.4083 - mean_pred: 0.9341 - mae_t1: 0.0272 - val_loss: 2.2877 - val_mae: 0.4448 - val_mean_pred: 0.9559 - val_mae_t1: 0.0297\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8705 - mae: 0.3637 - mean_pred: 0.7834 - mae_t1: 0.0242 - val_loss: 2.4762 - val_mae: 0.4815 - val_mean_pred: 0.7094 - val_mae_t1: 0.0321\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.0517 - mae: 0.3989 - mean_pred: 0.6022 - mae_t1: 0.0266 - val_loss: 2.6944 - val_mae: 0.5239 - val_mean_pred: 0.6804 - val_mae_t1: 0.0349\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.1521 - mae: 0.4185 - mean_pred: 0.6245 - mae_t1: 0.0279 - val_loss: 2.3759 - val_mae: 0.4620 - val_mean_pred: 0.8568 - val_mae_t1: 0.0308\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8081 - mae: 0.3516 - mean_pred: 0.7756 - mae_t1: 0.0234 - val_loss: 2.3734 - val_mae: 0.4615 - val_mean_pred: 0.9853 - val_mae_t1: 0.0308\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8735 - mae: 0.3643 - mean_pred: 0.8676 - mae_t1: 0.0243 - val_loss: 2.4181 - val_mae: 0.4702 - val_mean_pred: 0.9532 - val_mae_t1: 0.0313\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.8549 - mae: 0.3607 - mean_pred: 0.8152 - mae_t1: 0.0240 - val_loss: 2.3452 - val_mae: 0.4560 - val_mean_pred: 0.7633 - val_mae_t1: 0.0304\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.9876 - mae: 0.3865 - mean_pred: 0.6470 - mae_t1: 0.0258 - val_loss: 2.3838 - val_mae: 0.4635 - val_mean_pred: 0.6236 - val_mae_t1: 0.0309\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.3091 - mae: 0.4490 - mean_pred: 0.6113 - mae_t1: 0.0299 - val_loss: 2.3099 - val_mae: 0.4491 - val_mean_pred: 0.6878 - val_mae_t1: 0.0299\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3113 - mae: 0.4494 - mean_pred: 0.6675 - mae_t1: 0.0300 - val_loss: 2.1621 - val_mae: 0.4204 - val_mean_pred: 0.8535 - val_mae_t1: 0.0280\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 1.9040 - mae: 0.3702 - mean_pred: 0.7917 - mae_t1: 0.0247 - val_loss: 2.5029 - val_mae: 0.4867 - val_mean_pred: 0.9941 - val_mae_t1: 0.0324\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9686 - mae: 0.3828 - mean_pred: 0.8395 - mae_t1: 0.0255 - val_loss: 2.2630 - val_mae: 0.4400 - val_mean_pred: 0.8837 - val_mae_t1: 0.0293\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8162 - mae: 0.3531 - mean_pred: 0.7449 - mae_t1: 0.0235 - val_loss: 2.5235 - val_mae: 0.4907 - val_mean_pred: 0.7658 - val_mae_t1: 0.0327\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2625 - mae: 0.4399 - mean_pred: 0.6613 - mae_t1: 0.0293 - val_loss: 2.8746 - val_mae: 0.5590 - val_mean_pred: 0.6473 - val_mae_t1: 0.0373\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 2.5668 - mae: 0.4991 - mean_pred: 0.6102 - mae_t1: 0.0333 - val_loss: 2.9072 - val_mae: 0.5653 - val_mean_pred: 0.6149 - val_mae_t1: 0.0377\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.4544 - mae: 0.4772 - mean_pred: 0.5910 - mae_t1: 0.0318 - val_loss: 2.6800 - val_mae: 0.5211 - val_mean_pred: 0.7148 - val_mae_t1: 0.0347\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.0568 - mae: 0.3999 - mean_pred: 0.6595 - mae_t1: 0.0267 - val_loss: 2.7311 - val_mae: 0.5310 - val_mean_pred: 0.9391 - val_mae_t1: 0.0354\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9163 - mae: 0.3726 - mean_pred: 0.8159 - mae_t1: 0.0248 - val_loss: 2.5569 - val_mae: 0.4972 - val_mean_pred: 1.0201 - val_mae_t1: 0.0331\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.8274 - mae: 0.3553 - mean_pred: 0.8661 - mae_t1: 0.0237 - val_loss: 2.3652 - val_mae: 0.4599 - val_mean_pred: 0.9377 - val_mae_t1: 0.0307\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7430 - mae: 0.3389 - mean_pred: 0.7839 - mae_t1: 0.0226 - val_loss: 2.4461 - val_mae: 0.4756 - val_mean_pred: 0.8410 - val_mae_t1: 0.0317\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8598 - mae: 0.3616 - mean_pred: 0.6984 - mae_t1: 0.0241 - val_loss: 2.4625 - val_mae: 0.4788 - val_mean_pred: 0.7975 - val_mae_t1: 0.0319\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.7870 - mae: 0.3475 - mean_pred: 0.6841 - mae_t1: 0.0232 - val_loss: 2.3487 - val_mae: 0.4567 - val_mean_pred: 0.8665 - val_mae_t1: 0.0304\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.6733 - mae: 0.3254 - mean_pred: 0.7502 - mae_t1: 0.0217 - val_loss: 2.4282 - val_mae: 0.4721 - val_mean_pred: 0.9727 - val_mae_t1: 0.0315\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7532 - mae: 0.3409 - mean_pred: 0.8433 - mae_t1: 0.0227 - val_loss: 2.4478 - val_mae: 0.4760 - val_mean_pred: 0.9580 - val_mae_t1: 0.0317\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7308 - mae: 0.3365 - mean_pred: 0.8198 - mae_t1: 0.0224 - val_loss: 2.5409 - val_mae: 0.4941 - val_mean_pred: 0.7937 - val_mae_t1: 0.0329\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.9020 - mae: 0.3698 - mean_pred: 0.6725 - mae_t1: 0.0247 - val_loss: 2.5749 - val_mae: 0.5007 - val_mean_pred: 0.7310 - val_mae_t1: 0.0334\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.8328 - mae: 0.3564 - mean_pred: 0.6559 - mae_t1: 0.0238 - val_loss: 2.3451 - val_mae: 0.4560 - val_mean_pred: 0.8980 - val_mae_t1: 0.0304\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7067 - mae: 0.3319 - mean_pred: 0.8140 - mae_t1: 0.0221 - val_loss: 2.5153 - val_mae: 0.4891 - val_mean_pred: 1.0349 - val_mae_t1: 0.0326\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7478 - mae: 0.3399 - mean_pred: 0.8720 - mae_t1: 0.0227 - val_loss: 2.4131 - val_mae: 0.4692 - val_mean_pred: 0.9277 - val_mae_t1: 0.0313\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.6644 - mae: 0.3236 - mean_pred: 0.7727 - mae_t1: 0.0216 - val_loss: 2.4898 - val_mae: 0.4841 - val_mean_pred: 0.8485 - val_mae_t1: 0.0323\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.7803 - mae: 0.3462 - mean_pred: 0.7338 - mae_t1: 0.0231 - val_loss: 2.3445 - val_mae: 0.4559 - val_mean_pred: 0.9217 - val_mae_t1: 0.0304\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.6991 - mae: 0.3304 - mean_pred: 0.8223 - mae_t1: 0.0220 - val_loss: 2.4591 - val_mae: 0.4782 - val_mean_pred: 0.9767 - val_mae_t1: 0.0319\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.8691 - mae: 0.3634 - mean_pred: 0.8537 - mae_t1: 0.0242 - val_loss: 2.2771 - val_mae: 0.4428 - val_mean_pred: 0.8916 - val_mae_t1: 0.0295\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.6671 - mae: 0.3242 - mean_pred: 0.7754 - mae_t1: 0.0216 - val_loss: 2.5259 - val_mae: 0.4911 - val_mean_pred: 0.8672 - val_mae_t1: 0.0327\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9574 - mae: 0.3806 - mean_pred: 0.7497 - mae_t1: 0.0254 - val_loss: 2.4746 - val_mae: 0.4812 - val_mean_pred: 0.8882 - val_mae_t1: 0.0321\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8052 - mae: 0.3510 - mean_pred: 0.7707 - mae_t1: 0.0234 - val_loss: 2.4669 - val_mae: 0.4797 - val_mean_pred: 0.9769 - val_mae_t1: 0.0320\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.1573 - mae: 0.4195 - mean_pred: 0.8841 - mae_t1: 0.0280 - val_loss: 2.7345 - val_mae: 0.5317 - val_mean_pred: 1.0543 - val_mae_t1: 0.0354\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2191 - mae: 0.4315 - mean_pred: 0.9285 - mae_t1: 0.0288 - val_loss: 2.4311 - val_mae: 0.4727 - val_mean_pred: 0.9496 - val_mae_t1: 0.0315\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.8157 - mae: 0.3531 - mean_pred: 0.7768 - mae_t1: 0.0235 - val_loss: 2.8917 - val_mae: 0.5623 - val_mean_pred: 0.8250 - val_mae_t1: 0.0375\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.2517 - mae: 0.4378 - mean_pred: 0.6906 - mae_t1: 0.0292 - val_loss: 2.6465 - val_mae: 0.5146 - val_mean_pred: 0.8383 - val_mae_t1: 0.0343\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9067 - mae: 0.3708 - mean_pred: 0.7196 - mae_t1: 0.0247 - val_loss: 2.4165 - val_mae: 0.4699 - val_mean_pred: 0.8972 - val_mae_t1: 0.0313\n",
      "Earliness...\n",
      "0.0019998550415039062\n",
      "____________________________________________________________\n",
      "Test MAE:      0.341067712011858  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▃▃▃▂▂▂▂▃▃▂▃▂▂▂▂▂▃▂▂▁▂▃▁▃▂▁▁▁▁▁▂▂▂▂</td></tr><tr><td>mae</td><td>█▇▅▄▃▃▃▃▃▂▂▂▂▃▃▂▃▂▂▂▂▂▃▂▂▁▂▃▁▃▂▁▁▁▁▁▂▂▂▂</td></tr><tr><td>mae_t1</td><td>█▇▅▄▃▃▃▃▃▂▂▂▂▃▃▂▃▂▂▂▂▂▃▂▂▁▂▃▁▃▂▁▁▁▁▁▂▂▂▂</td></tr><tr><td>mean_pred</td><td>▁█▃▆▄▆▆▅▆▅▆▅▆▆▆▅▄▆▅▅▆▆▄▆▅▅▄▄▅▄▅▅▅▅▅▅▆▅▆▅</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▄▆█▂▁▁▃▅▄▃▅▅▃▅▂▄▅█▆▄▃▃▁▄▇▄▃▃▄▄▄▂▄▃▃</td></tr><tr><td>val_mae</td><td>█▇▄▃▃▄▆█▂▁▁▃▅▄▃▅▅▃▅▂▄▅█▆▄▃▃▁▄▇▄▃▃▄▄▄▂▄▃▃</td></tr><tr><td>val_mae_t1</td><td>█▇▄▃▃▄▆█▂▁▁▃▅▄▃▅▅▃▅▂▄▅█▆▄▃▃▁▄▇▄▃▃▄▄▄▂▄▃▃</td></tr><tr><td>val_mean_pred</td><td>▁█▂▆▅▅▆▂▆▅▅▅▄▇▄▅▃▅▅▄▇▅▂█▃▆▂▅▄▂▇▅▆▄▇▅▅▅▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33391</td></tr><tr><td>AE_2</td><td>0.32523</td></tr><tr><td>AE_3</td><td>0.31605</td></tr><tr><td>MAE</td><td>0.34107</td></tr><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>2.09137</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.90675</td></tr><tr><td>mae</td><td>0.37076</td></tr><tr><td>mae_t1</td><td>0.02472</td></tr><tr><td>mean_pred</td><td>0.71962</td></tr><tr><td>val_loss</td><td>2.4165</td></tr><tr><td>val_mae</td><td>0.46987</td></tr><tr><td>val_mae_t1</td><td>0.03132</td></tr><tr><td>val_mean_pred</td><td>0.8972</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misty-moon-74</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2bt40uam\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2bt40uam</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155019-2bt40uam\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155043-37xclchg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/37xclchg\" target=\"_blank\">misty-terrain-75</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 864us/sample - loss: 5.9881 - mae: 0.7485 - mean_pred: 0.3003 - mae_t1: 0.0499 - val_loss: 4.1956 - val_mae: 0.5244 - val_mean_pred: 1.0648 - val_mae_t1: 0.0350\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 283us/sample - loss: 4.8759 - mae: 0.6095 - mean_pred: 0.9510 - mae_t1: 0.0406 - val_loss: 4.4334 - val_mae: 0.5542 - val_mean_pred: 0.6758 - val_mae_t1: 0.0369\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 268us/sample - loss: 4.4835 - mae: 0.5604 - mean_pred: 0.6246 - mae_t1: 0.0374 - val_loss: 3.9317 - val_mae: 0.4915 - val_mean_pred: 1.0228 - val_mae_t1: 0.0328\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 4.7287 - mae: 0.5911 - mean_pred: 1.0421 - mae_t1: 0.0394 - val_loss: 3.9646 - val_mae: 0.4956 - val_mean_pred: 0.7313 - val_mae_t1: 0.0330\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 4.4254 - mae: 0.5532 - mean_pred: 0.4904 - mae_t1: 0.0369 - val_loss: 4.7639 - val_mae: 0.5955 - val_mean_pred: 0.5340 - val_mae_t1: 0.0397\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 4.0147 - mae: 0.5018 - mean_pred: 0.6086 - mae_t1: 0.0335 - val_loss: 4.1457 - val_mae: 0.5182 - val_mean_pred: 1.0088 - val_mae_t1: 0.0345\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.7313 - mae: 0.4664 - mean_pred: 0.8728 - mae_t1: 0.0311 - val_loss: 4.0049 - val_mae: 0.5006 - val_mean_pred: 0.7564 - val_mae_t1: 0.0334\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 3.6487 - mae: 0.4561 - mean_pred: 0.6599 - mae_t1: 0.0304 - val_loss: 3.8865 - val_mae: 0.4858 - val_mean_pred: 0.8988 - val_mae_t1: 0.0324\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 3.6167 - mae: 0.4521 - mean_pred: 0.8914 - mae_t1: 0.0301 - val_loss: 3.7337 - val_mae: 0.4667 - val_mean_pred: 0.9152 - val_mae_t1: 0.0311\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 3.4632 - mae: 0.4329 - mean_pred: 0.8572 - mae_t1: 0.0289 - val_loss: 3.7936 - val_mae: 0.4742 - val_mean_pred: 0.6894 - val_mae_t1: 0.0316\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 3.5703 - mae: 0.4463 - mean_pred: 0.7117 - mae_t1: 0.0298 - val_loss: 3.3326 - val_mae: 0.4166 - val_mean_pred: 0.8590 - val_mae_t1: 0.0278\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3492 - mae: 0.4186 - mean_pred: 0.8424 - mae_t1: 0.0279 - val_loss: 3.5080 - val_mae: 0.4385 - val_mean_pred: 0.8385 - val_mae_t1: 0.0292\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7655 - mae: 0.4707 - mean_pred: 0.7439 - mae_t1: 0.0314 - val_loss: 4.1408 - val_mae: 0.5176 - val_mean_pred: 0.6958 - val_mae_t1: 0.0345\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.8411 - mae: 0.4801 - mean_pred: 0.6914 - mae_t1: 0.0320 - val_loss: 4.2993 - val_mae: 0.5374 - val_mean_pred: 0.9872 - val_mae_t1: 0.0358\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 4.3372 - mae: 0.5421 - mean_pred: 0.9709 - mae_t1: 0.0361 - val_loss: 4.0376 - val_mae: 0.5047 - val_mean_pred: 0.8352 - val_mae_t1: 0.0336\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7242 - mae: 0.4655 - mean_pred: 0.6897 - mae_t1: 0.0310 - val_loss: 4.3646 - val_mae: 0.5456 - val_mean_pred: 0.7736 - val_mae_t1: 0.0364\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.4246 - mae: 0.4281 - mean_pred: 0.8021 - mae_t1: 0.0285 - val_loss: 3.9564 - val_mae: 0.4945 - val_mean_pred: 0.9948 - val_mae_t1: 0.0330\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.2324 - mae: 0.4041 - mean_pred: 0.8410 - mae_t1: 0.0269 - val_loss: 3.8746 - val_mae: 0.4843 - val_mean_pred: 0.7279 - val_mae_t1: 0.0323\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.5174 - mae: 0.4397 - mean_pred: 0.6913 - mae_t1: 0.0293 - val_loss: 3.5265 - val_mae: 0.4408 - val_mean_pred: 0.8794 - val_mae_t1: 0.0294\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6923 - mae: 0.4615 - mean_pred: 0.8673 - mae_t1: 0.0308 - val_loss: 3.9008 - val_mae: 0.4876 - val_mean_pred: 0.8132 - val_mae_t1: 0.0325\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.5404 - mae: 0.4426 - mean_pred: 0.6529 - mae_t1: 0.0295 - val_loss: 3.9988 - val_mae: 0.4999 - val_mean_pred: 0.7161 - val_mae_t1: 0.0333\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6613 - mae: 0.4577 - mean_pred: 0.7217 - mae_t1: 0.0305 - val_loss: 3.8252 - val_mae: 0.4782 - val_mean_pred: 0.8607 - val_mae_t1: 0.0319\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.3184 - mae: 0.4148 - mean_pred: 0.6934 - mae_t1: 0.0277 - val_loss: 3.7253 - val_mae: 0.4657 - val_mean_pred: 0.8886 - val_mae_t1: 0.0310\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 3.1043 - mae: 0.3880 - mean_pred: 0.8462 - mae_t1: 0.0259 - val_loss: 3.4834 - val_mae: 0.4354 - val_mean_pred: 0.8541 - val_mae_t1: 0.0290\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0821 - mae: 0.3853 - mean_pred: 0.7052 - mae_t1: 0.0257 - val_loss: 3.5308 - val_mae: 0.4413 - val_mean_pred: 0.8152 - val_mae_t1: 0.0294\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9489 - mae: 0.3686 - mean_pred: 0.7870 - mae_t1: 0.0246 - val_loss: 3.8054 - val_mae: 0.4757 - val_mean_pred: 0.9216 - val_mae_t1: 0.0317\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1172 - mae: 0.3897 - mean_pred: 0.7594 - mae_t1: 0.0260 - val_loss: 3.8892 - val_mae: 0.4861 - val_mean_pred: 0.8626 - val_mae_t1: 0.0324\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.2608 - mae: 0.4076 - mean_pred: 0.7969 - mae_t1: 0.0272 - val_loss: 3.9384 - val_mae: 0.4923 - val_mean_pred: 0.9260 - val_mae_t1: 0.0328\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0103 - mae: 0.3763 - mean_pred: 0.7418 - mae_t1: 0.0251 - val_loss: 3.9064 - val_mae: 0.4883 - val_mean_pred: 0.9339 - val_mae_t1: 0.0326\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.9743 - mae: 0.3718 - mean_pred: 0.8811 - mae_t1: 0.0248 - val_loss: 3.7581 - val_mae: 0.4698 - val_mean_pred: 0.7626 - val_mae_t1: 0.0313\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.2076 - mae: 0.4009 - mean_pred: 0.6239 - mae_t1: 0.0267 - val_loss: 3.8927 - val_mae: 0.4866 - val_mean_pred: 0.6994 - val_mae_t1: 0.0324\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 3.1100 - mae: 0.3888 - mean_pred: 0.7230 - mae_t1: 0.0259 - val_loss: 3.8572 - val_mae: 0.4822 - val_mean_pred: 0.8993 - val_mae_t1: 0.0321\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9634 - mae: 0.3704 - mean_pred: 0.7406 - mae_t1: 0.0247 - val_loss: 4.1173 - val_mae: 0.5147 - val_mean_pred: 0.8228 - val_mae_t1: 0.0343\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.3101 - mae: 0.4138 - mean_pred: 0.7916 - mae_t1: 0.0276 - val_loss: 3.8573 - val_mae: 0.4822 - val_mean_pred: 0.9476 - val_mae_t1: 0.0321\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8648 - mae: 0.4831 - mean_pred: 0.8895 - mae_t1: 0.0322 - val_loss: 3.7482 - val_mae: 0.4685 - val_mean_pred: 0.8128 - val_mae_t1: 0.0312\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3286 - mae: 0.4161 - mean_pred: 0.7177 - mae_t1: 0.0277 - val_loss: 4.0943 - val_mae: 0.5118 - val_mean_pred: 0.9235 - val_mae_t1: 0.0341\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 3.1143 - mae: 0.3893 - mean_pred: 0.8980 - mae_t1: 0.0260 - val_loss: 3.8978 - val_mae: 0.4872 - val_mean_pred: 0.9239 - val_mae_t1: 0.0325\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.3417 - mae: 0.4177 - mean_pred: 0.7259 - mae_t1: 0.0278 - val_loss: 4.1428 - val_mae: 0.5179 - val_mean_pred: 0.7744 - val_mae_t1: 0.0345\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.1711 - mae: 0.3964 - mean_pred: 0.7554 - mae_t1: 0.0264 - val_loss: 3.7847 - val_mae: 0.4731 - val_mean_pred: 0.9752 - val_mae_t1: 0.0315\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.7553 - mae: 0.3444 - mean_pred: 0.7880 - mae_t1: 0.0230 - val_loss: 3.7142 - val_mae: 0.4643 - val_mean_pred: 0.8497 - val_mae_t1: 0.0310\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8143 - mae: 0.3518 - mean_pred: 0.7711 - mae_t1: 0.0235 - val_loss: 4.4259 - val_mae: 0.5532 - val_mean_pred: 1.1346 - val_mae_t1: 0.0369\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 3.0794 - mae: 0.3849 - mean_pred: 0.9597 - mae_t1: 0.0257 - val_loss: 3.6807 - val_mae: 0.4601 - val_mean_pred: 0.8579 - val_mae_t1: 0.0307\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6930 - mae: 0.3366 - mean_pred: 0.7072 - mae_t1: 0.0224 - val_loss: 3.4728 - val_mae: 0.4341 - val_mean_pred: 0.8832 - val_mae_t1: 0.0289\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.7439 - mae: 0.3430 - mean_pred: 0.8705 - mae_t1: 0.0229 - val_loss: 3.4216 - val_mae: 0.4277 - val_mean_pred: 0.9013 - val_mae_t1: 0.0285\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8007 - mae: 0.3501 - mean_pred: 0.6662 - mae_t1: 0.0233 - val_loss: 3.7120 - val_mae: 0.4640 - val_mean_pred: 0.7222 - val_mae_t1: 0.0309\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.7497 - mae: 0.3437 - mean_pred: 0.7130 - mae_t1: 0.0229 - val_loss: 4.4229 - val_mae: 0.5529 - val_mean_pred: 1.1357 - val_mae_t1: 0.0369\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.4522 - mae: 0.4315 - mean_pred: 0.9882 - mae_t1: 0.0288 - val_loss: 3.7214 - val_mae: 0.4652 - val_mean_pred: 0.8330 - val_mae_t1: 0.0310\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.5401 - mae: 0.4425 - mean_pred: 0.6208 - mae_t1: 0.0295 - val_loss: 4.3644 - val_mae: 0.5455 - val_mean_pred: 0.5487 - val_mae_t1: 0.0364\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4661 - mae: 0.4333 - mean_pred: 0.5484 - mae_t1: 0.0289 - val_loss: 3.6235 - val_mae: 0.4529 - val_mean_pred: 0.9150 - val_mae_t1: 0.0302\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8894 - mae: 0.3612 - mean_pred: 0.8862 - mae_t1: 0.0241 - val_loss: 3.6938 - val_mae: 0.4617 - val_mean_pred: 0.9957 - val_mae_t1: 0.0308\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.8443 - mae: 0.3555 - mean_pred: 0.7638 - mae_t1: 0.0237 - val_loss: 3.6653 - val_mae: 0.4582 - val_mean_pred: 0.7092 - val_mae_t1: 0.0305\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 2.9435 - mae: 0.3679 - mean_pred: 0.6360 - mae_t1: 0.0245 - val_loss: 3.2965 - val_mae: 0.4121 - val_mean_pred: 0.8558 - val_mae_t1: 0.0275\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.6824 - mae: 0.3353 - mean_pred: 0.7822 - mae_t1: 0.0224 - val_loss: 3.3954 - val_mae: 0.4244 - val_mean_pred: 0.9273 - val_mae_t1: 0.0283\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.4795 - mae: 0.3099 - mean_pred: 0.7842 - mae_t1: 0.0207 - val_loss: 3.2921 - val_mae: 0.4115 - val_mean_pred: 0.8677 - val_mae_t1: 0.0274\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5461 - mae: 0.3183 - mean_pred: 0.7858 - mae_t1: 0.0212 - val_loss: 3.6891 - val_mae: 0.4611 - val_mean_pred: 0.9739 - val_mae_t1: 0.0307\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.7020 - mae: 0.3378 - mean_pred: 0.8684 - mae_t1: 0.0225 - val_loss: 3.7969 - val_mae: 0.4746 - val_mean_pred: 0.9785 - val_mae_t1: 0.0316\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4561 - mae: 0.3070 - mean_pred: 0.8200 - mae_t1: 0.0205 - val_loss: 3.7153 - val_mae: 0.4644 - val_mean_pred: 0.9765 - val_mae_t1: 0.0310\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5563 - mae: 0.3195 - mean_pred: 0.8944 - mae_t1: 0.0213 - val_loss: 3.6292 - val_mae: 0.4537 - val_mean_pred: 0.8991 - val_mae_t1: 0.0302\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5540 - mae: 0.3193 - mean_pred: 0.7104 - mae_t1: 0.0213 - val_loss: 3.5099 - val_mae: 0.4387 - val_mean_pred: 0.7627 - val_mae_t1: 0.0292\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 2.9323 - mae: 0.3665 - mean_pred: 0.6882 - mae_t1: 0.0244 - val_loss: 3.2197 - val_mae: 0.4025 - val_mean_pred: 0.8373 - val_mae_t1: 0.0268\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2896 - mae: 0.2862 - mean_pred: 0.7734 - mae_t1: 0.0191 - val_loss: 4.2295 - val_mae: 0.5287 - val_mean_pred: 1.0841 - val_mae_t1: 0.0352\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.7438 - mae: 0.3430 - mean_pred: 0.9003 - mae_t1: 0.0229 - val_loss: 3.5987 - val_mae: 0.4498 - val_mean_pred: 0.7918 - val_mae_t1: 0.0300\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.8437 - mae: 0.3555 - mean_pred: 0.6380 - mae_t1: 0.0237 - val_loss: 3.8138 - val_mae: 0.4767 - val_mean_pred: 0.7806 - val_mae_t1: 0.0318\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.4699 - mae: 0.3087 - mean_pred: 0.7913 - mae_t1: 0.0206 - val_loss: 4.0970 - val_mae: 0.5121 - val_mean_pred: 1.0862 - val_mae_t1: 0.0341\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.3872 - mae: 0.2984 - mean_pred: 0.8617 - mae_t1: 0.0199 - val_loss: 3.5599 - val_mae: 0.4450 - val_mean_pred: 0.7981 - val_mae_t1: 0.0297\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.4211 - mae: 0.3026 - mean_pred: 0.7254 - mae_t1: 0.0202 - val_loss: 3.5102 - val_mae: 0.4388 - val_mean_pred: 0.9727 - val_mae_t1: 0.0293\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.4808 - mae: 0.3101 - mean_pred: 0.9049 - mae_t1: 0.0207 - val_loss: 3.3429 - val_mae: 0.4179 - val_mean_pred: 0.8978 - val_mae_t1: 0.0279\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6120 - mae: 0.3265 - mean_pred: 0.6936 - mae_t1: 0.0218 - val_loss: 3.5317 - val_mae: 0.4415 - val_mean_pred: 0.7570 - val_mae_t1: 0.0294\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.4225 - mae: 0.3028 - mean_pred: 0.7615 - mae_t1: 0.0202 - val_loss: 3.5264 - val_mae: 0.4408 - val_mean_pred: 0.9975 - val_mae_t1: 0.0294\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3715 - mae: 0.2964 - mean_pred: 0.8259 - mae_t1: 0.0198 - val_loss: 3.3095 - val_mae: 0.4137 - val_mean_pred: 0.8651 - val_mae_t1: 0.0276\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.2618 - mae: 0.2827 - mean_pred: 0.8107 - mae_t1: 0.0188 - val_loss: 3.2984 - val_mae: 0.4123 - val_mean_pred: 0.9167 - val_mae_t1: 0.0275\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2399 - mae: 0.2800 - mean_pred: 0.7556 - mae_t1: 0.0187 - val_loss: 3.4658 - val_mae: 0.4332 - val_mean_pred: 0.8167 - val_mae_t1: 0.0289\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.3591 - mae: 0.2949 - mean_pred: 0.8171 - mae_t1: 0.0197 - val_loss: 3.8010 - val_mae: 0.4751 - val_mean_pred: 1.0133 - val_mae_t1: 0.0317\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.2304 - mae: 0.2788 - mean_pred: 0.8777 - mae_t1: 0.0186 - val_loss: 3.3538 - val_mae: 0.4192 - val_mean_pred: 0.8046 - val_mae_t1: 0.0279\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 2.3425 - mae: 0.2928 - mean_pred: 0.6983 - mae_t1: 0.0195 - val_loss: 3.1191 - val_mae: 0.3899 - val_mean_pred: 0.8014 - val_mae_t1: 0.0260\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.5416 - mae: 0.3177 - mean_pred: 0.7747 - mae_t1: 0.0212 - val_loss: 3.5452 - val_mae: 0.4432 - val_mean_pred: 0.9457 - val_mae_t1: 0.0295\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 2.5433 - mae: 0.3179 - mean_pred: 0.8766 - mae_t1: 0.0212 - val_loss: 4.0098 - val_mae: 0.5012 - val_mean_pred: 0.9321 - val_mae_t1: 0.0334\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.6587 - mae: 0.3323 - mean_pred: 0.8074 - mae_t1: 0.0222 - val_loss: 3.8905 - val_mae: 0.4863 - val_mean_pred: 0.8939 - val_mae_t1: 0.0324\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.3865 - mae: 0.2983 - mean_pred: 0.8224 - mae_t1: 0.0199 - val_loss: 3.7763 - val_mae: 0.4720 - val_mean_pred: 0.9897 - val_mae_t1: 0.0315\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.3273 - mae: 0.2909 - mean_pred: 0.8595 - mae_t1: 0.0194 - val_loss: 3.9653 - val_mae: 0.4957 - val_mean_pred: 0.9297 - val_mae_t1: 0.0330\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.6339 - mae: 0.3292 - mean_pred: 0.8551 - mae_t1: 0.0219 - val_loss: 3.6593 - val_mae: 0.4574 - val_mean_pred: 0.8796 - val_mae_t1: 0.0305\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6292 - mae: 0.3287 - mean_pred: 0.7362 - mae_t1: 0.0219 - val_loss: 3.5621 - val_mae: 0.4453 - val_mean_pred: 0.7504 - val_mae_t1: 0.0297\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3743 - mae: 0.2968 - mean_pred: 0.6881 - mae_t1: 0.0198 - val_loss: 3.7060 - val_mae: 0.4633 - val_mean_pred: 1.0533 - val_mae_t1: 0.0309\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.7846 - mae: 0.3481 - mean_pred: 0.9430 - mae_t1: 0.0232 - val_loss: 3.3851 - val_mae: 0.4231 - val_mean_pred: 0.9759 - val_mae_t1: 0.0282\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.2231 - mae: 0.2779 - mean_pred: 0.7881 - mae_t1: 0.0185 - val_loss: 3.3777 - val_mae: 0.4222 - val_mean_pred: 0.8169 - val_mae_t1: 0.0281\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3053 - mae: 0.2882 - mean_pred: 0.7605 - mae_t1: 0.0192 - val_loss: 3.7044 - val_mae: 0.4630 - val_mean_pred: 0.9740 - val_mae_t1: 0.0309\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.3128 - mae: 0.2891 - mean_pred: 0.8601 - mae_t1: 0.0193 - val_loss: 3.3243 - val_mae: 0.4155 - val_mean_pred: 0.8702 - val_mae_t1: 0.0277\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2362 - mae: 0.2795 - mean_pred: 0.7304 - mae_t1: 0.0186 - val_loss: 3.1617 - val_mae: 0.3952 - val_mean_pred: 0.8218 - val_mae_t1: 0.0263\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1604 - mae: 0.2700 - mean_pred: 0.8262 - mae_t1: 0.0180 - val_loss: 3.5525 - val_mae: 0.4441 - val_mean_pred: 0.9675 - val_mae_t1: 0.0296\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1140 - mae: 0.2643 - mean_pred: 0.8243 - mae_t1: 0.0176 - val_loss: 3.2875 - val_mae: 0.4109 - val_mean_pred: 0.8693 - val_mae_t1: 0.0274\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 2.0004 - mae: 0.2500 - mean_pred: 0.7951 - mae_t1: 0.0167 - val_loss: 3.5509 - val_mae: 0.4439 - val_mean_pred: 0.9369 - val_mae_t1: 0.0296\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.9608 - mae: 0.2451 - mean_pred: 0.7913 - mae_t1: 0.0163 - val_loss: 3.4219 - val_mae: 0.4277 - val_mean_pred: 0.9359 - val_mae_t1: 0.0285\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9954 - mae: 0.2494 - mean_pred: 0.8595 - mae_t1: 0.0166 - val_loss: 3.4014 - val_mae: 0.4252 - val_mean_pred: 0.9347 - val_mae_t1: 0.0283\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0034 - mae: 0.2504 - mean_pred: 0.8085 - mae_t1: 0.0167 - val_loss: 3.4755 - val_mae: 0.4344 - val_mean_pred: 0.8764 - val_mae_t1: 0.0290\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1031 - mae: 0.2629 - mean_pred: 0.8219 - mae_t1: 0.0175 - val_loss: 3.6468 - val_mae: 0.4558 - val_mean_pred: 0.9739 - val_mae_t1: 0.0304\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.1423 - mae: 0.2678 - mean_pred: 0.8307 - mae_t1: 0.0179 - val_loss: 3.3620 - val_mae: 0.4202 - val_mean_pred: 0.8969 - val_mae_t1: 0.0280\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 2.0936 - mae: 0.2617 - mean_pred: 0.8146 - mae_t1: 0.0174 - val_loss: 3.4545 - val_mae: 0.4318 - val_mean_pred: 0.9141 - val_mae_t1: 0.0288\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.2367 - mae: 0.2796 - mean_pred: 0.8073 - mae_t1: 0.0186 - val_loss: 3.6337 - val_mae: 0.4542 - val_mean_pred: 0.8847 - val_mae_t1: 0.0303\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.2613 - mae: 0.2827 - mean_pred: 0.8008 - mae_t1: 0.0188 - val_loss: 3.8272 - val_mae: 0.4784 - val_mean_pred: 0.8977 - val_mae_t1: 0.0319\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.0656 - mae: 0.2582 - mean_pred: 0.7999 - mae_t1: 0.0172 - val_loss: 3.5878 - val_mae: 0.4485 - val_mean_pred: 0.9254 - val_mae_t1: 0.0299\n",
      "Earliness...\n",
      "0.0015003681182861328\n",
      "____________________________________________________________\n",
      "Test MAE:      0.31802100733706795  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▅▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▄▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▅▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▄▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▅▄▄▄▄▃▄▃▃▃▃▃▃▃▂▂▂▄▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▅▄▅▅▆▅▇▅▅▆▆▅▆▆▆▆▇▅▄▆▆▇▅▆▆█▆▆▇▇▇▆█▇▇▆▇▇▆</td></tr><tr><td>val_loss</td><td>▇▅▆▅▁▆█▅▅▄▄▅▅▄▆▄█▂█▃▃▁▄▂▇▆▁▂▂▁▅▄▃▂▁▃▂▂▂▃</td></tr><tr><td>val_mae</td><td>▇▅▆▅▁▆█▅▅▄▄▅▅▄▆▄█▂█▃▃▁▄▂▇▆▁▂▂▁▅▄▃▂▁▃▂▂▂▃</td></tr><tr><td>val_mae_t1</td><td>▇▅▆▅▁▆█▅▅▄▄▅▅▄▆▄█▂█▃▃▁▄▂▇▆▁▂▂▁▅▄▃▂▁▃▂▂▂▃</td></tr><tr><td>val_mean_pred</td><td>▇▆▆▄▄▁▂▂▁▄▅▅▁▅▅▅█▄█▄▁▄▅▂▇▇▄▆▃▃▅▆▂▅▄▅▅▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.37832</td></tr><tr><td>AE_2</td><td>0.22972</td></tr><tr><td>AE_3</td><td>0.30751</td></tr><tr><td>MAE</td><td>0.31802</td></tr><tr><td>best_epoch</td><td>74</td></tr><tr><td>best_val_loss</td><td>3.11913</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.06556</td></tr><tr><td>mae</td><td>0.2582</td></tr><tr><td>mae_t1</td><td>0.01721</td></tr><tr><td>mean_pred</td><td>0.79987</td></tr><tr><td>val_loss</td><td>3.58783</td></tr><tr><td>val_mae</td><td>0.44848</td></tr><tr><td>val_mae_t1</td><td>0.0299</td></tr><tr><td>val_mean_pred</td><td>0.9254</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misty-terrain-75</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/37xclchg\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/37xclchg</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155043-37xclchg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155109-14vnk6cl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/14vnk6cl\" target=\"_blank\">prime-morning-76</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 711us/sample - loss: 6.0713 - mae: 0.7589 - mean_pred: 0.2254 - mae_t1: 0.0506 - val_loss: 4.1252 - val_mae: 0.5157 - val_mean_pred: 0.8914 - val_mae_t1: 0.0344\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 163us/sample - loss: 5.2221 - mae: 0.6528 - mean_pred: 0.9368 - mae_t1: 0.0435 - val_loss: 5.7528 - val_mae: 0.7191 - val_mean_pred: 1.3196 - val_mae_t1: 0.0479\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 6.1675 - mae: 0.7709 - mean_pred: 1.1829 - mae_t1: 0.0514 - val_loss: 4.0595 - val_mae: 0.5074 - val_mean_pred: 0.8329 - val_mae_t1: 0.0338\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 159us/sample - loss: 4.6076 - mae: 0.5759 - mean_pred: 0.7115 - mae_t1: 0.0384 - val_loss: 3.8507 - val_mae: 0.4813 - val_mean_pred: 0.7746 - val_mae_t1: 0.0321\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 4.5608 - mae: 0.5701 - mean_pred: 0.7626 - mae_t1: 0.0380 - val_loss: 4.6863 - val_mae: 0.5858 - val_mean_pred: 0.9530 - val_mae_t1: 0.0391\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 5.1327 - mae: 0.6416 - mean_pred: 0.8497 - mae_t1: 0.0428 - val_loss: 3.8583 - val_mae: 0.4823 - val_mean_pred: 0.6732 - val_mae_t1: 0.0322\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.3999 - mae: 0.5500 - mean_pred: 0.5825 - mae_t1: 0.0367 - val_loss: 4.7188 - val_mae: 0.5899 - val_mean_pred: 0.5910 - val_mae_t1: 0.0393\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.8461 - mae: 0.6058 - mean_pred: 0.5868 - mae_t1: 0.0404 - val_loss: 4.0665 - val_mae: 0.5083 - val_mean_pred: 0.7066 - val_mae_t1: 0.0339\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.0526 - mae: 0.5066 - mean_pred: 0.6532 - mae_t1: 0.0338 - val_loss: 4.0505 - val_mae: 0.5063 - val_mean_pred: 0.8395 - val_mae_t1: 0.0338\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 4.0382 - mae: 0.5048 - mean_pred: 0.7912 - mae_t1: 0.0337 - val_loss: 4.2025 - val_mae: 0.5253 - val_mean_pred: 0.9032 - val_mae_t1: 0.0350\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.8974 - mae: 0.4872 - mean_pred: 0.7938 - mae_t1: 0.0325 - val_loss: 4.0103 - val_mae: 0.5013 - val_mean_pred: 0.7684 - val_mae_t1: 0.0334\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.0569 - mae: 0.5071 - mean_pred: 0.6891 - mae_t1: 0.0338 - val_loss: 4.6996 - val_mae: 0.5874 - val_mean_pred: 0.8571 - val_mae_t1: 0.0392\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 4.7819 - mae: 0.5977 - mean_pred: 0.8135 - mae_t1: 0.0398 - val_loss: 3.7722 - val_mae: 0.4715 - val_mean_pred: 0.9512 - val_mae_t1: 0.0314\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.8257 - mae: 0.4782 - mean_pred: 0.8658 - mae_t1: 0.0319 - val_loss: 5.0053 - val_mae: 0.6257 - val_mean_pred: 0.8519 - val_mae_t1: 0.0417\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.9118 - mae: 0.6140 - mean_pred: 0.7673 - mae_t1: 0.0409 - val_loss: 4.9452 - val_mae: 0.6181 - val_mean_pred: 0.4901 - val_mae_t1: 0.0412\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.8948 - mae: 0.6118 - mean_pred: 0.4204 - mae_t1: 0.0408 - val_loss: 6.8162 - val_mae: 0.8520 - val_mean_pred: 0.2942 - val_mae_t1: 0.0568\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 6.0239 - mae: 0.7530 - mean_pred: 0.3277 - mae_t1: 0.0502 - val_loss: 5.1185 - val_mae: 0.6398 - val_mean_pred: 0.4778 - val_mae_t1: 0.0427\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.1960 - mae: 0.5245 - mean_pred: 0.5162 - mae_t1: 0.0350 - val_loss: 3.8635 - val_mae: 0.4829 - val_mean_pred: 0.9625 - val_mae_t1: 0.0322\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.0947 - mae: 0.5118 - mean_pred: 0.9689 - mae_t1: 0.0341 - val_loss: 3.9305 - val_mae: 0.4913 - val_mean_pred: 1.0283 - val_mae_t1: 0.0328\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7723 - mae: 0.4715 - mean_pred: 0.9200 - mae_t1: 0.0314 - val_loss: 4.2936 - val_mae: 0.5367 - val_mean_pred: 0.7014 - val_mae_t1: 0.0358\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.6770 - mae: 0.4596 - mean_pred: 0.6075 - mae_t1: 0.0306 - val_loss: 4.8346 - val_mae: 0.6043 - val_mean_pred: 0.6807 - val_mae_t1: 0.0403\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.9541 - mae: 0.4943 - mean_pred: 0.5945 - mae_t1: 0.0330 - val_loss: 4.9892 - val_mae: 0.6237 - val_mean_pred: 0.9008 - val_mae_t1: 0.0416\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.9153 - mae: 0.4894 - mean_pred: 0.8059 - mae_t1: 0.0326 - val_loss: 5.2177 - val_mae: 0.6522 - val_mean_pred: 1.1275 - val_mae_t1: 0.0435\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 4.1071 - mae: 0.5134 - mean_pred: 0.9786 - mae_t1: 0.0342 - val_loss: 4.4394 - val_mae: 0.5549 - val_mean_pred: 0.9028 - val_mae_t1: 0.0370\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.6376 - mae: 0.4547 - mean_pred: 0.7480 - mae_t1: 0.0303 - val_loss: 4.1354 - val_mae: 0.5169 - val_mean_pred: 0.7006 - val_mae_t1: 0.0345\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.5338 - mae: 0.4417 - mean_pred: 0.6538 - mae_t1: 0.0294 - val_loss: 3.8772 - val_mae: 0.4847 - val_mean_pred: 0.9555 - val_mae_t1: 0.0323\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.6431 - mae: 0.4554 - mean_pred: 0.9401 - mae_t1: 0.0304 - val_loss: 3.7742 - val_mae: 0.4718 - val_mean_pred: 0.9429 - val_mae_t1: 0.0315\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.4946 - mae: 0.4368 - mean_pred: 0.7926 - mae_t1: 0.0291 - val_loss: 4.3860 - val_mae: 0.5482 - val_mean_pred: 0.5942 - val_mae_t1: 0.0365\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.2408 - mae: 0.5301 - mean_pred: 0.5322 - mae_t1: 0.0353 - val_loss: 4.6065 - val_mae: 0.5758 - val_mean_pred: 0.5774 - val_mae_t1: 0.0384\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.8952 - mae: 0.4869 - mean_pred: 0.5263 - mae_t1: 0.0325 - val_loss: 4.0859 - val_mae: 0.5107 - val_mean_pred: 0.9060 - val_mae_t1: 0.0340\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.4675 - mae: 0.4334 - mean_pred: 0.8402 - mae_t1: 0.0289 - val_loss: 4.8208 - val_mae: 0.6026 - val_mean_pred: 1.1662 - val_mae_t1: 0.0402\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.9802 - mae: 0.4975 - mean_pred: 1.0200 - mae_t1: 0.0332 - val_loss: 3.9927 - val_mae: 0.4991 - val_mean_pred: 0.9091 - val_mae_t1: 0.0333\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.3044 - mae: 0.4130 - mean_pred: 0.7501 - mae_t1: 0.0275 - val_loss: 4.5158 - val_mae: 0.5645 - val_mean_pred: 0.7029 - val_mae_t1: 0.0376\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.8625 - mae: 0.4828 - mean_pred: 0.6291 - mae_t1: 0.0322 - val_loss: 4.1044 - val_mae: 0.5131 - val_mean_pred: 0.8213 - val_mae_t1: 0.0342\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.3426 - mae: 0.4178 - mean_pred: 0.7872 - mae_t1: 0.0279 - val_loss: 4.1183 - val_mae: 0.5148 - val_mean_pred: 1.0249 - val_mae_t1: 0.0343\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6562 - mae: 0.4570 - mean_pred: 0.9402 - mae_t1: 0.0305 - val_loss: 3.8373 - val_mae: 0.4797 - val_mean_pred: 0.9664 - val_mae_t1: 0.0320\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.4252 - mae: 0.4281 - mean_pred: 0.8594 - mae_t1: 0.0285 - val_loss: 3.8999 - val_mae: 0.4875 - val_mean_pred: 0.8684 - val_mae_t1: 0.0325\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 3.3155 - mae: 0.4144 - mean_pred: 0.7725 - mae_t1: 0.0276 - val_loss: 3.6779 - val_mae: 0.4597 - val_mean_pred: 0.8701 - val_mae_t1: 0.0306\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 3.5131 - mae: 0.4391 - mean_pred: 0.8200 - mae_t1: 0.0293 - val_loss: 3.5381 - val_mae: 0.4423 - val_mean_pred: 0.8619 - val_mae_t1: 0.0295\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1887 - mae: 0.3986 - mean_pred: 0.7682 - mae_t1: 0.0266 - val_loss: 4.7558 - val_mae: 0.5945 - val_mean_pred: 0.9479 - val_mae_t1: 0.0396\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.9557 - mae: 0.4945 - mean_pred: 0.8569 - mae_t1: 0.0330 - val_loss: 3.9574 - val_mae: 0.4947 - val_mean_pred: 0.9748 - val_mae_t1: 0.0330\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.2136 - mae: 0.4017 - mean_pred: 0.8382 - mae_t1: 0.0268 - val_loss: 3.5420 - val_mae: 0.4428 - val_mean_pred: 0.8217 - val_mae_t1: 0.0295\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 3.7243 - mae: 0.4655 - mean_pred: 0.7793 - mae_t1: 0.0310 - val_loss: 3.4880 - val_mae: 0.4360 - val_mean_pred: 0.7565 - val_mae_t1: 0.0291\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3557 - mae: 0.4195 - mean_pred: 0.7122 - mae_t1: 0.0280 - val_loss: 5.3872 - val_mae: 0.6734 - val_mean_pred: 0.8794 - val_mae_t1: 0.0449\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.0202 - mae: 0.6275 - mean_pred: 0.8580 - mae_t1: 0.0418 - val_loss: 5.1787 - val_mae: 0.6473 - val_mean_pred: 0.9292 - val_mae_t1: 0.0432\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.9566 - mae: 0.4946 - mean_pred: 0.8008 - mae_t1: 0.0330 - val_loss: 3.5187 - val_mae: 0.4398 - val_mean_pred: 0.7478 - val_mae_t1: 0.0293\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6882 - mae: 0.4610 - mean_pred: 0.6626 - mae_t1: 0.0307 - val_loss: 4.0838 - val_mae: 0.5105 - val_mean_pred: 0.7416 - val_mae_t1: 0.0340\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 4.0435 - mae: 0.5054 - mean_pred: 0.6732 - mae_t1: 0.0337 - val_loss: 3.9240 - val_mae: 0.4905 - val_mean_pred: 0.9416 - val_mae_t1: 0.0327\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4479 - mae: 0.4310 - mean_pred: 0.8630 - mae_t1: 0.0287 - val_loss: 4.4515 - val_mae: 0.5564 - val_mean_pred: 1.0750 - val_mae_t1: 0.0371\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.3161 - mae: 0.4145 - mean_pred: 0.8586 - mae_t1: 0.0276 - val_loss: 3.8014 - val_mae: 0.4752 - val_mean_pred: 0.8049 - val_mae_t1: 0.0317\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 3.2155 - mae: 0.4019 - mean_pred: 0.6786 - mae_t1: 0.0268 - val_loss: 4.1368 - val_mae: 0.5171 - val_mean_pred: 0.7250 - val_mae_t1: 0.0345\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4151 - mae: 0.4269 - mean_pred: 0.6404 - mae_t1: 0.0285 - val_loss: 4.1345 - val_mae: 0.5168 - val_mean_pred: 0.7344 - val_mae_t1: 0.0345\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2080 - mae: 0.4010 - mean_pred: 0.6314 - mae_t1: 0.0267 - val_loss: 3.9205 - val_mae: 0.4901 - val_mean_pred: 0.9435 - val_mae_t1: 0.0327\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0157 - mae: 0.3770 - mean_pred: 0.8596 - mae_t1: 0.0251 - val_loss: 4.0117 - val_mae: 0.5015 - val_mean_pred: 1.0530 - val_mae_t1: 0.0334\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0853 - mae: 0.3857 - mean_pred: 0.8790 - mae_t1: 0.0257 - val_loss: 3.8576 - val_mae: 0.4822 - val_mean_pred: 0.7287 - val_mae_t1: 0.0321\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3531 - mae: 0.4191 - mean_pred: 0.6007 - mae_t1: 0.0279 - val_loss: 4.3694 - val_mae: 0.5462 - val_mean_pred: 0.6704 - val_mae_t1: 0.0364\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4532 - mae: 0.4316 - mean_pred: 0.5934 - mae_t1: 0.0288 - val_loss: 3.9069 - val_mae: 0.4884 - val_mean_pred: 0.9712 - val_mae_t1: 0.0326\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.1307 - mae: 0.3913 - mean_pred: 0.9063 - mae_t1: 0.0261 - val_loss: 5.0258 - val_mae: 0.6282 - val_mean_pred: 1.2158 - val_mae_t1: 0.0419\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.7644 - mae: 0.4705 - mean_pred: 1.0425 - mae_t1: 0.0314 - val_loss: 4.1135 - val_mae: 0.5142 - val_mean_pred: 0.9699 - val_mae_t1: 0.0343\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1172 - mae: 0.3897 - mean_pred: 0.7950 - mae_t1: 0.0260 - val_loss: 3.5566 - val_mae: 0.4446 - val_mean_pred: 0.7190 - val_mae_t1: 0.0296\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3571 - mae: 0.4196 - mean_pred: 0.6505 - mae_t1: 0.0280 - val_loss: 3.7977 - val_mae: 0.4747 - val_mean_pred: 0.7330 - val_mae_t1: 0.0316\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.5832 - mae: 0.4479 - mean_pred: 0.7034 - mae_t1: 0.0299 - val_loss: 3.6115 - val_mae: 0.4514 - val_mean_pred: 0.7733 - val_mae_t1: 0.0301\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.0484 - mae: 0.3810 - mean_pred: 0.7232 - mae_t1: 0.0254 - val_loss: 3.8668 - val_mae: 0.4834 - val_mean_pred: 0.7883 - val_mae_t1: 0.0322\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9961 - mae: 0.3745 - mean_pred: 0.7339 - mae_t1: 0.0250 - val_loss: 3.5777 - val_mae: 0.4472 - val_mean_pred: 0.8618 - val_mae_t1: 0.0298\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.9112 - mae: 0.3639 - mean_pred: 0.8014 - mae_t1: 0.0243 - val_loss: 3.7925 - val_mae: 0.4741 - val_mean_pred: 0.9268 - val_mae_t1: 0.0316\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.1542 - mae: 0.3943 - mean_pred: 0.8600 - mae_t1: 0.0263 - val_loss: 3.9130 - val_mae: 0.4891 - val_mean_pred: 0.9481 - val_mae_t1: 0.0326\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 85us/sample - loss: 3.0247 - mae: 0.3781 - mean_pred: 0.8215 - mae_t1: 0.0252 - val_loss: 4.3981 - val_mae: 0.5498 - val_mean_pred: 0.8227 - val_mae_t1: 0.0367\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.2933 - mae: 0.4117 - mean_pred: 0.7201 - mae_t1: 0.0274 - val_loss: 4.0082 - val_mae: 0.5010 - val_mean_pred: 0.7576 - val_mae_t1: 0.0334\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.0548 - mae: 0.3819 - mean_pred: 0.6932 - mae_t1: 0.0255 - val_loss: 3.6768 - val_mae: 0.4596 - val_mean_pred: 0.8706 - val_mae_t1: 0.0306\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.8226 - mae: 0.3528 - mean_pred: 0.7936 - mae_t1: 0.0235 - val_loss: 3.7183 - val_mae: 0.4648 - val_mean_pred: 1.0197 - val_mae_t1: 0.0310\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 2.9439 - mae: 0.3680 - mean_pred: 0.8940 - mae_t1: 0.0245 - val_loss: 3.4607 - val_mae: 0.4326 - val_mean_pred: 0.9766 - val_mae_t1: 0.0288\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 2.9918 - mae: 0.3740 - mean_pred: 0.8555 - mae_t1: 0.0249 - val_loss: 3.2919 - val_mae: 0.4115 - val_mean_pred: 0.7964 - val_mae_t1: 0.0274\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.1441 - mae: 0.3930 - mean_pred: 0.6886 - mae_t1: 0.0262 - val_loss: 3.4983 - val_mae: 0.4373 - val_mean_pred: 0.6489 - val_mae_t1: 0.0292\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4655 - mae: 0.4332 - mean_pred: 0.6014 - mae_t1: 0.0289 - val_loss: 3.4285 - val_mae: 0.4286 - val_mean_pred: 0.7419 - val_mae_t1: 0.0286\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.1315 - mae: 0.3914 - mean_pred: 0.7127 - mae_t1: 0.0261 - val_loss: 3.8343 - val_mae: 0.4793 - val_mean_pred: 1.0221 - val_mae_t1: 0.0320\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3190 - mae: 0.4149 - mean_pred: 0.9269 - mae_t1: 0.0277 - val_loss: 4.0558 - val_mae: 0.5070 - val_mean_pred: 1.0347 - val_mae_t1: 0.0338\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 2.8836 - mae: 0.3604 - mean_pred: 0.8715 - mae_t1: 0.0240 - val_loss: 3.6233 - val_mae: 0.4529 - val_mean_pred: 0.8040 - val_mae_t1: 0.0302\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9450 - mae: 0.3681 - mean_pred: 0.7272 - mae_t1: 0.0245 - val_loss: 3.5182 - val_mae: 0.4398 - val_mean_pred: 0.7879 - val_mae_t1: 0.0293\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6335 - mae: 0.3292 - mean_pred: 0.7361 - mae_t1: 0.0219 - val_loss: 3.9134 - val_mae: 0.4892 - val_mean_pred: 0.8528 - val_mae_t1: 0.0326\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9135 - mae: 0.3642 - mean_pred: 0.7387 - mae_t1: 0.0243 - val_loss: 3.6397 - val_mae: 0.4550 - val_mean_pred: 0.7570 - val_mae_t1: 0.0303\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7505 - mae: 0.3438 - mean_pred: 0.7190 - mae_t1: 0.0229 - val_loss: 3.5457 - val_mae: 0.4432 - val_mean_pred: 0.8596 - val_mae_t1: 0.0295\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9663 - mae: 0.3708 - mean_pred: 0.8292 - mae_t1: 0.0247 - val_loss: 3.7769 - val_mae: 0.4721 - val_mean_pred: 0.9670 - val_mae_t1: 0.0315\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0859 - mae: 0.3857 - mean_pred: 0.8888 - mae_t1: 0.0257 - val_loss: 3.7311 - val_mae: 0.4664 - val_mean_pred: 0.9270 - val_mae_t1: 0.0311\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.7453 - mae: 0.3432 - mean_pred: 0.8102 - mae_t1: 0.0229 - val_loss: 3.6255 - val_mae: 0.4532 - val_mean_pred: 0.8246 - val_mae_t1: 0.0302\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8548 - mae: 0.3569 - mean_pred: 0.7818 - mae_t1: 0.0238 - val_loss: 3.4043 - val_mae: 0.4255 - val_mean_pred: 0.8951 - val_mae_t1: 0.0284\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.5901 - mae: 0.3238 - mean_pred: 0.8407 - mae_t1: 0.0216 - val_loss: 3.5256 - val_mae: 0.4407 - val_mean_pred: 0.9374 - val_mae_t1: 0.0294\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.6210 - mae: 0.3276 - mean_pred: 0.8533 - mae_t1: 0.0218 - val_loss: 3.3965 - val_mae: 0.4246 - val_mean_pred: 0.7943 - val_mae_t1: 0.0283\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9448 - mae: 0.3681 - mean_pred: 0.7261 - mae_t1: 0.0245 - val_loss: 3.8098 - val_mae: 0.4762 - val_mean_pred: 0.6569 - val_mae_t1: 0.0317\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.2325 - mae: 0.4041 - mean_pred: 0.6359 - mae_t1: 0.0269 - val_loss: 3.7736 - val_mae: 0.4717 - val_mean_pred: 0.7721 - val_mae_t1: 0.0314\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8574 - mae: 0.3572 - mean_pred: 0.7290 - mae_t1: 0.0238 - val_loss: 4.4578 - val_mae: 0.5572 - val_mean_pred: 1.1071 - val_mae_t1: 0.0371\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.5499 - mae: 0.4437 - mean_pred: 1.0034 - mae_t1: 0.0296 - val_loss: 4.8072 - val_mae: 0.6009 - val_mean_pred: 1.2133 - val_mae_t1: 0.0401\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4527 - mae: 0.4316 - mean_pred: 1.0311 - mae_t1: 0.0288 - val_loss: 3.4254 - val_mae: 0.4282 - val_mean_pred: 0.8468 - val_mae_t1: 0.0285\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7698 - mae: 0.3462 - mean_pred: 0.7023 - mae_t1: 0.0231 - val_loss: 4.0323 - val_mae: 0.5040 - val_mean_pred: 0.6004 - val_mae_t1: 0.0336\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 3.5212 - mae: 0.4401 - mean_pred: 0.5791 - mae_t1: 0.0293 - val_loss: 3.7785 - val_mae: 0.4723 - val_mean_pred: 0.6615 - val_mae_t1: 0.0315\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.2507 - mae: 0.4063 - mean_pred: 0.6140 - mae_t1: 0.0271 - val_loss: 3.6289 - val_mae: 0.4536 - val_mean_pred: 0.7625 - val_mae_t1: 0.0302\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9721 - mae: 0.3715 - mean_pred: 0.6792 - mae_t1: 0.0248 - val_loss: 3.4111 - val_mae: 0.4264 - val_mean_pred: 0.9146 - val_mae_t1: 0.0284\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.5560 - mae: 0.3195 - mean_pred: 0.7937 - mae_t1: 0.0213 - val_loss: 3.4672 - val_mae: 0.4334 - val_mean_pred: 0.9064 - val_mae_t1: 0.0289\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.6453 - mae: 0.3307 - mean_pred: 0.7659 - mae_t1: 0.0220 - val_loss: 3.4168 - val_mae: 0.4271 - val_mean_pred: 0.8195 - val_mae_t1: 0.0285\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.4859 - mae: 0.3107 - mean_pred: 0.7185 - mae_t1: 0.0207 - val_loss: 3.5682 - val_mae: 0.4460 - val_mean_pred: 0.9202 - val_mae_t1: 0.0297\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.4992 - mae: 0.3124 - mean_pred: 0.8063 - mae_t1: 0.0208 - val_loss: 3.7040 - val_mae: 0.4630 - val_mean_pred: 0.9570 - val_mae_t1: 0.0309\n",
      "Earliness...\n",
      "0.0015001296997070312\n",
      "____________________________________________________________\n",
      "Test MAE:      0.29913916742127805  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▆▅▄▅▆▄▃▄▃▃▃▄▃▃▄▃▄▃▂▂▃▃▃▂▂▂▂▃▂▁▂▁▁▂▃▃▁▁</td></tr><tr><td>mae</td><td>██▆▅▄▅▆▄▃▄▃▃▃▄▃▃▄▃▄▃▂▂▃▃▃▂▂▂▂▃▂▁▂▁▁▂▃▃▁▁</td></tr><tr><td>mae_t1</td><td>██▆▅▄▅▆▄▃▄▃▃▃▄▃▃▄▃▄▃▂▂▃▃▃▂▂▂▂▃▂▁▂▁▁▂▃▃▁▁</td></tr><tr><td>mean_pred</td><td>▁█▆▄▅▅▂▃▄▅▄▅▅▄▆▅▆▅▅▆▄▆▄▇▄▅▅▄▆▄▆▅▅▅▆▄▇▄▅▅</td></tr><tr><td>val_loss</td><td>▃▃▂▃▂▂█▂▄▅▂▃▄▃▂▁▂▅▁▃▃▂▃▃▂▂▃▂▁▁▂▂▂▂▁▂▁▂▁▂</td></tr><tr><td>val_mae</td><td>▃▃▂▃▂▂█▂▄▅▂▃▄▃▂▁▂▅▁▃▃▂▃▃▂▂▃▂▁▁▂▂▂▂▁▂▁▂▁▂</td></tr><tr><td>val_mae_t1</td><td>▃▃▂▃▂▂█▂▄▅▂▃▄▃▂▁▂▅▁▃▃▂▃▃▂▂▃▂▁▁▂▂▂▂▁▂▁▂▁▂</td></tr><tr><td>val_mean_pred</td><td>▆▅▄▄▅▆▁▆▄█▆▃█▅▆▆▆▆▅▇▄▇▄▆▅▆▅▆▅▅▅▅▆▅▅▅▅▄▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.35318</td></tr><tr><td>AE_2</td><td>0.29461</td></tr><tr><td>AE_3</td><td>0.28852</td></tr><tr><td>MAE</td><td>0.29914</td></tr><tr><td>best_epoch</td><td>71</td></tr><tr><td>best_val_loss</td><td>3.29186</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.49921</td></tr><tr><td>mae</td><td>0.3124</td></tr><tr><td>mae_t1</td><td>0.02083</td></tr><tr><td>mean_pred</td><td>0.80629</td></tr><tr><td>val_loss</td><td>3.704</td></tr><tr><td>val_mae</td><td>0.463</td></tr><tr><td>val_mae_t1</td><td>0.03087</td></tr><tr><td>val_mean_pred</td><td>0.95703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">prime-morning-76</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/14vnk6cl\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/14vnk6cl</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155109-14vnk6cl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155139-2onpankt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2onpankt\" target=\"_blank\">youthful-elevator-77</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 878us/sample - loss: 8.1629 - mae: 0.7518 - mean_pred: 0.5268 - mae_t1: 0.0501 - val_loss: 6.9068 - val_mae: 0.6362 - val_mean_pred: 1.1194 - val_mae_t1: 0.0424\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 272us/sample - loss: 7.1299 - mae: 0.6567 - mean_pred: 0.7890 - mae_t1: 0.0438 - val_loss: 7.4485 - val_mae: 0.6860 - val_mean_pred: 0.3356 - val_mae_t1: 0.0457\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 281us/sample - loss: 7.5971 - mae: 0.6997 - mean_pred: 0.3987 - mae_t1: 0.0466 - val_loss: 5.5439 - val_mae: 0.5106 - val_mean_pred: 0.6796 - val_mae_t1: 0.0340\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 5.9916 - mae: 0.5519 - mean_pred: 0.5697 - mae_t1: 0.0368 - val_loss: 6.0221 - val_mae: 0.5547 - val_mean_pred: 0.6320 - val_mae_t1: 0.0370\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 220us/sample - loss: 5.8179 - mae: 0.5359 - mean_pred: 0.5314 - mae_t1: 0.0357 - val_loss: 5.3031 - val_mae: 0.4884 - val_mean_pred: 0.8083 - val_mae_t1: 0.0326\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 5.3703 - mae: 0.4946 - mean_pred: 0.8379 - mae_t1: 0.0330 - val_loss: 5.4939 - val_mae: 0.5060 - val_mean_pred: 0.9713 - val_mae_t1: 0.0337\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 4.8159 - mae: 0.4436 - mean_pred: 0.8425 - mae_t1: 0.0296 - val_loss: 5.6307 - val_mae: 0.5186 - val_mean_pred: 0.6856 - val_mae_t1: 0.0346\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 4.8695 - mae: 0.4485 - mean_pred: 0.6804 - mae_t1: 0.0299 - val_loss: 5.2242 - val_mae: 0.4812 - val_mean_pred: 0.9158 - val_mae_t1: 0.0321\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 4.9731 - mae: 0.4580 - mean_pred: 0.7955 - mae_t1: 0.0305 - val_loss: 5.4776 - val_mae: 0.5045 - val_mean_pred: 0.8203 - val_mae_t1: 0.0336\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 4.9486 - mae: 0.4558 - mean_pred: 0.7349 - mae_t1: 0.0304 - val_loss: 4.9075 - val_mae: 0.4520 - val_mean_pred: 0.8622 - val_mae_t1: 0.0301\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.2601 - mae: 0.4845 - mean_pred: 0.9472 - mae_t1: 0.0323 - val_loss: 5.1045 - val_mae: 0.4702 - val_mean_pred: 0.7657 - val_mae_t1: 0.0313\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.7283 - mae: 0.4355 - mean_pred: 0.7200 - mae_t1: 0.0290 - val_loss: 5.1181 - val_mae: 0.4714 - val_mean_pred: 0.8526 - val_mae_t1: 0.0314\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.2724 - mae: 0.3935 - mean_pred: 0.7635 - mae_t1: 0.0262 - val_loss: 5.1219 - val_mae: 0.4718 - val_mean_pred: 0.8569 - val_mae_t1: 0.0315\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.3550 - mae: 0.4011 - mean_pred: 0.7930 - mae_t1: 0.0267 - val_loss: 5.4332 - val_mae: 0.5004 - val_mean_pred: 0.6742 - val_mae_t1: 0.0334\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.7084 - mae: 0.4337 - mean_pred: 0.6027 - mae_t1: 0.0289 - val_loss: 5.2617 - val_mae: 0.4846 - val_mean_pred: 0.8568 - val_mae_t1: 0.0323\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.2598 - mae: 0.3924 - mean_pred: 0.8069 - mae_t1: 0.0262 - val_loss: 5.5130 - val_mae: 0.5078 - val_mean_pred: 0.9082 - val_mae_t1: 0.0339\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.3701 - mae: 0.4025 - mean_pred: 0.8059 - mae_t1: 0.0268 - val_loss: 5.7274 - val_mae: 0.5275 - val_mean_pred: 0.8389 - val_mae_t1: 0.0352\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.4849 - mae: 0.4131 - mean_pred: 0.7979 - mae_t1: 0.0275 - val_loss: 5.5016 - val_mae: 0.5067 - val_mean_pred: 0.9254 - val_mae_t1: 0.0338\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.3115 - mae: 0.3971 - mean_pred: 0.8415 - mae_t1: 0.0265 - val_loss: 5.5041 - val_mae: 0.5070 - val_mean_pred: 0.8544 - val_mae_t1: 0.0338\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 4.3206 - mae: 0.3979 - mean_pred: 0.8201 - mae_t1: 0.0265 - val_loss: 5.3731 - val_mae: 0.4949 - val_mean_pred: 0.8597 - val_mae_t1: 0.0330\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 4.2260 - mae: 0.3892 - mean_pred: 0.7317 - mae_t1: 0.0259 - val_loss: 5.9733 - val_mae: 0.5502 - val_mean_pred: 1.1032 - val_mae_t1: 0.0367\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 4.6793 - mae: 0.4310 - mean_pred: 0.9627 - mae_t1: 0.0287 - val_loss: 4.7269 - val_mae: 0.4354 - val_mean_pred: 0.7313 - val_mae_t1: 0.0290\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.2005 - mae: 0.3869 - mean_pred: 0.6568 - mae_t1: 0.0258 - val_loss: 5.0264 - val_mae: 0.4630 - val_mean_pred: 0.9551 - val_mae_t1: 0.0309\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.8534 - mae: 0.3549 - mean_pred: 0.7903 - mae_t1: 0.0237 - val_loss: 5.2292 - val_mae: 0.4816 - val_mean_pred: 0.9929 - val_mae_t1: 0.0321\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.2005 - mae: 0.3869 - mean_pred: 0.8836 - mae_t1: 0.0258 - val_loss: 5.1631 - val_mae: 0.4755 - val_mean_pred: 0.7842 - val_mae_t1: 0.0317\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.2022 - mae: 0.3870 - mean_pred: 0.6972 - mae_t1: 0.0258 - val_loss: 5.4216 - val_mae: 0.4994 - val_mean_pred: 0.9255 - val_mae_t1: 0.0333\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 4.1218 - mae: 0.3796 - mean_pred: 0.8287 - mae_t1: 0.0253 - val_loss: 5.1493 - val_mae: 0.4743 - val_mean_pred: 0.9633 - val_mae_t1: 0.0316\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.8604 - mae: 0.3556 - mean_pred: 0.8835 - mae_t1: 0.0237 - val_loss: 4.7387 - val_mae: 0.4365 - val_mean_pred: 0.7749 - val_mae_t1: 0.0291\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 4.0860 - mae: 0.3763 - mean_pred: 0.6311 - mae_t1: 0.0251 - val_loss: 4.6175 - val_mae: 0.4253 - val_mean_pred: 0.8070 - val_mae_t1: 0.0284\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 3.9043 - mae: 0.3596 - mean_pred: 0.7804 - mae_t1: 0.0240 - val_loss: 4.6013 - val_mae: 0.4238 - val_mean_pred: 0.7947 - val_mae_t1: 0.0283\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.3346 - mae: 0.3992 - mean_pred: 0.6159 - mae_t1: 0.0266 - val_loss: 4.7718 - val_mae: 0.4395 - val_mean_pred: 0.8173 - val_mae_t1: 0.0293\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.8354 - mae: 0.3533 - mean_pred: 0.8398 - mae_t1: 0.0236 - val_loss: 4.8686 - val_mae: 0.4484 - val_mean_pred: 0.9048 - val_mae_t1: 0.0299\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.9530 - mae: 0.3641 - mean_pred: 0.7050 - mae_t1: 0.0243 - val_loss: 4.7674 - val_mae: 0.4391 - val_mean_pred: 0.8231 - val_mae_t1: 0.0293\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.1284 - mae: 0.3802 - mean_pred: 0.8639 - mae_t1: 0.0253 - val_loss: 5.9322 - val_mae: 0.5464 - val_mean_pred: 0.9374 - val_mae_t1: 0.0364\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.5754 - mae: 0.4214 - mean_pred: 0.7105 - mae_t1: 0.0281 - val_loss: 5.6264 - val_mae: 0.5182 - val_mean_pred: 0.7399 - val_mae_t1: 0.0345\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.2072 - mae: 0.3875 - mean_pred: 0.6760 - mae_t1: 0.0258 - val_loss: 5.0771 - val_mae: 0.4676 - val_mean_pred: 0.9322 - val_mae_t1: 0.0312\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.7790 - mae: 0.3481 - mean_pred: 0.8803 - mae_t1: 0.0232 - val_loss: 4.8489 - val_mae: 0.4466 - val_mean_pred: 0.8599 - val_mae_t1: 0.0298\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4899 - mae: 0.3214 - mean_pred: 0.7672 - mae_t1: 0.0214 - val_loss: 4.8448 - val_mae: 0.4462 - val_mean_pred: 0.9470 - val_mae_t1: 0.0297\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3510 - mae: 0.3086 - mean_pred: 0.8277 - mae_t1: 0.0206 - val_loss: 5.4645 - val_mae: 0.5033 - val_mean_pred: 0.8555 - val_mae_t1: 0.0336\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.5556 - mae: 0.3275 - mean_pred: 0.7832 - mae_t1: 0.0218 - val_loss: 5.8527 - val_mae: 0.5391 - val_mean_pred: 1.0571 - val_mae_t1: 0.0359\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 4.1872 - mae: 0.3857 - mean_pred: 0.9634 - mae_t1: 0.0257 - val_loss: 5.4343 - val_mae: 0.5005 - val_mean_pred: 0.7598 - val_mae_t1: 0.0334\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6095 - mae: 0.3325 - mean_pred: 0.7378 - mae_t1: 0.0222 - val_loss: 5.5314 - val_mae: 0.5095 - val_mean_pred: 1.0144 - val_mae_t1: 0.0340\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.7349 - mae: 0.3440 - mean_pred: 0.9110 - mae_t1: 0.0229 - val_loss: 5.1422 - val_mae: 0.4736 - val_mean_pred: 0.7903 - val_mae_t1: 0.0316\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.5899 - mae: 0.3306 - mean_pred: 0.7525 - mae_t1: 0.0220 - val_loss: 5.0866 - val_mae: 0.4685 - val_mean_pred: 0.8756 - val_mae_t1: 0.0312\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.6721 - mae: 0.3382 - mean_pred: 0.8266 - mae_t1: 0.0225 - val_loss: 5.5615 - val_mae: 0.5122 - val_mean_pred: 0.8409 - val_mae_t1: 0.0341\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.5966 - mae: 0.3313 - mean_pred: 0.7587 - mae_t1: 0.0221 - val_loss: 5.7377 - val_mae: 0.5285 - val_mean_pred: 0.9950 - val_mae_t1: 0.0352\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.9889 - mae: 0.3674 - mean_pred: 0.9301 - mae_t1: 0.0245 - val_loss: 5.5459 - val_mae: 0.5108 - val_mean_pred: 0.8928 - val_mae_t1: 0.0341\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.7310 - mae: 0.3436 - mean_pred: 0.7921 - mae_t1: 0.0229 - val_loss: 4.8804 - val_mae: 0.4495 - val_mean_pred: 0.8928 - val_mae_t1: 0.0300\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.3971 - mae: 0.3129 - mean_pred: 0.7976 - mae_t1: 0.0209 - val_loss: 4.7447 - val_mae: 0.4370 - val_mean_pred: 0.8648 - val_mae_t1: 0.0291\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.6811 - mae: 0.3390 - mean_pred: 0.7445 - mae_t1: 0.0226 - val_loss: 5.2657 - val_mae: 0.4850 - val_mean_pred: 0.7135 - val_mae_t1: 0.0323\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 4.1209 - mae: 0.3796 - mean_pred: 0.5997 - mae_t1: 0.0253 - val_loss: 5.3780 - val_mae: 0.4953 - val_mean_pred: 0.7860 - val_mae_t1: 0.0330\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.5285 - mae: 0.3250 - mean_pred: 0.7084 - mae_t1: 0.0217 - val_loss: 4.9188 - val_mae: 0.4531 - val_mean_pred: 0.9204 - val_mae_t1: 0.0302\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.5168 - mae: 0.3239 - mean_pred: 0.8461 - mae_t1: 0.0216 - val_loss: 4.7423 - val_mae: 0.4368 - val_mean_pred: 0.8609 - val_mae_t1: 0.0291\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.2975 - mae: 0.3037 - mean_pred: 0.7481 - mae_t1: 0.0202 - val_loss: 4.9660 - val_mae: 0.4574 - val_mean_pred: 0.8722 - val_mae_t1: 0.0305\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 3.2354 - mae: 0.2980 - mean_pred: 0.7513 - mae_t1: 0.0199 - val_loss: 4.8764 - val_mae: 0.4491 - val_mean_pred: 0.9070 - val_mae_t1: 0.0299\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1001 - mae: 0.2855 - mean_pred: 0.7708 - mae_t1: 0.0190 - val_loss: 4.8084 - val_mae: 0.4429 - val_mean_pred: 0.9245 - val_mae_t1: 0.0295\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.9348 - mae: 0.2703 - mean_pred: 0.7889 - mae_t1: 0.0180 - val_loss: 4.9608 - val_mae: 0.4569 - val_mean_pred: 0.8533 - val_mae_t1: 0.0305\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.1161 - mae: 0.2870 - mean_pred: 0.7250 - mae_t1: 0.0191 - val_loss: 5.5980 - val_mae: 0.5156 - val_mean_pred: 0.9020 - val_mae_t1: 0.0344\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.5815 - mae: 0.3299 - mean_pred: 0.7619 - mae_t1: 0.0220 - val_loss: 5.6178 - val_mae: 0.5174 - val_mean_pred: 0.9482 - val_mae_t1: 0.0345\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.2886 - mae: 0.3029 - mean_pred: 0.8535 - mae_t1: 0.0202 - val_loss: 5.1654 - val_mae: 0.4758 - val_mean_pred: 0.8615 - val_mae_t1: 0.0317\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.2301 - mae: 0.2975 - mean_pred: 0.7661 - mae_t1: 0.0198 - val_loss: 4.8434 - val_mae: 0.4461 - val_mean_pred: 0.9495 - val_mae_t1: 0.0297\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.2260 - mae: 0.2971 - mean_pred: 0.8015 - mae_t1: 0.0198 - val_loss: 4.6374 - val_mae: 0.4271 - val_mean_pred: 0.9282 - val_mae_t1: 0.0285\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.3235 - mae: 0.3061 - mean_pred: 0.8595 - mae_t1: 0.0204 - val_loss: 4.8298 - val_mae: 0.4448 - val_mean_pred: 0.8760 - val_mae_t1: 0.0297\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0950 - mae: 0.2851 - mean_pred: 0.7344 - mae_t1: 0.0190 - val_loss: 5.4313 - val_mae: 0.5002 - val_mean_pred: 0.9522 - val_mae_t1: 0.0333\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.9407 - mae: 0.2708 - mean_pred: 0.8262 - mae_t1: 0.0181 - val_loss: 5.1993 - val_mae: 0.4789 - val_mean_pred: 0.9880 - val_mae_t1: 0.0319\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.0975 - mae: 0.2853 - mean_pred: 0.8786 - mae_t1: 0.0190 - val_loss: 4.6350 - val_mae: 0.4269 - val_mean_pred: 0.8962 - val_mae_t1: 0.0285\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.0844 - mae: 0.2841 - mean_pred: 0.8398 - mae_t1: 0.0189 - val_loss: 5.1251 - val_mae: 0.4720 - val_mean_pred: 1.0366 - val_mae_t1: 0.0315\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.1045 - mae: 0.2859 - mean_pred: 0.9068 - mae_t1: 0.0191 - val_loss: 5.1416 - val_mae: 0.4736 - val_mean_pred: 0.8937 - val_mae_t1: 0.0316\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9406 - mae: 0.2708 - mean_pred: 0.8113 - mae_t1: 0.0181 - val_loss: 5.5255 - val_mae: 0.5089 - val_mean_pred: 0.9877 - val_mae_t1: 0.0339\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.2513 - mae: 0.2995 - mean_pred: 0.8552 - mae_t1: 0.0200 - val_loss: 5.4266 - val_mae: 0.4998 - val_mean_pred: 0.9662 - val_mae_t1: 0.0333\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3071 - mae: 0.3046 - mean_pred: 0.8461 - mae_t1: 0.0203 - val_loss: 5.0679 - val_mae: 0.4668 - val_mean_pred: 0.8961 - val_mae_t1: 0.0311\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 3.1792 - mae: 0.2928 - mean_pred: 0.8244 - mae_t1: 0.0195 - val_loss: 4.5977 - val_mae: 0.4235 - val_mean_pred: 0.9027 - val_mae_t1: 0.0282\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.9197 - mae: 0.2689 - mean_pred: 0.8039 - mae_t1: 0.0179 - val_loss: 5.0453 - val_mae: 0.4647 - val_mean_pred: 0.9895 - val_mae_t1: 0.0310\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9625 - mae: 0.2729 - mean_pred: 0.8696 - mae_t1: 0.0182 - val_loss: 4.9958 - val_mae: 0.4601 - val_mean_pred: 0.7302 - val_mae_t1: 0.0307\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 3.9265 - mae: 0.3617 - mean_pred: 0.6027 - mae_t1: 0.0241 - val_loss: 4.7820 - val_mae: 0.4404 - val_mean_pred: 0.7343 - val_mae_t1: 0.0294\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1726 - mae: 0.2922 - mean_pred: 0.8202 - mae_t1: 0.0195 - val_loss: 4.9084 - val_mae: 0.4521 - val_mean_pred: 0.9469 - val_mae_t1: 0.0301\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.2603 - mae: 0.3003 - mean_pred: 0.7476 - mae_t1: 0.0200 - val_loss: 5.6002 - val_mae: 0.5158 - val_mean_pred: 0.7130 - val_mae_t1: 0.0344\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.9040 - mae: 0.3596 - mean_pred: 0.6764 - mae_t1: 0.0240 - val_loss: 5.5973 - val_mae: 0.5155 - val_mean_pred: 0.9137 - val_mae_t1: 0.0344\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.4235 - mae: 0.3153 - mean_pred: 0.8190 - mae_t1: 0.0210 - val_loss: 5.1031 - val_mae: 0.4700 - val_mean_pred: 0.8221 - val_mae_t1: 0.0313\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4668 - mae: 0.3193 - mean_pred: 0.7197 - mae_t1: 0.0213 - val_loss: 4.6831 - val_mae: 0.4313 - val_mean_pred: 0.8253 - val_mae_t1: 0.0288\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 3.2746 - mae: 0.3016 - mean_pred: 0.8055 - mae_t1: 0.0201 - val_loss: 4.7554 - val_mae: 0.4380 - val_mean_pred: 0.9336 - val_mae_t1: 0.0292\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.0448 - mae: 0.2804 - mean_pred: 0.8231 - mae_t1: 0.0187 - val_loss: 5.0495 - val_mae: 0.4651 - val_mean_pred: 0.8007 - val_mae_t1: 0.0310\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 3.0409 - mae: 0.2801 - mean_pred: 0.7565 - mae_t1: 0.0187 - val_loss: 5.8363 - val_mae: 0.5376 - val_mean_pred: 0.9463 - val_mae_t1: 0.0358\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.5392 - mae: 0.3260 - mean_pred: 0.7937 - mae_t1: 0.0217 - val_loss: 5.9323 - val_mae: 0.5464 - val_mean_pred: 0.7991 - val_mae_t1: 0.0364\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 4.0043 - mae: 0.3688 - mean_pred: 0.6854 - mae_t1: 0.0246 - val_loss: 5.6227 - val_mae: 0.5179 - val_mean_pred: 0.8184 - val_mae_t1: 0.0345\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 3.4495 - mae: 0.3177 - mean_pred: 0.7899 - mae_t1: 0.0212 - val_loss: 5.6873 - val_mae: 0.5238 - val_mean_pred: 1.0061 - val_mae_t1: 0.0349\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.1831 - mae: 0.2932 - mean_pred: 0.8290 - mae_t1: 0.0195 - val_loss: 5.1222 - val_mae: 0.4718 - val_mean_pred: 0.7274 - val_mae_t1: 0.0315\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.6373 - mae: 0.3350 - mean_pred: 0.6623 - mae_t1: 0.0223 - val_loss: 5.1888 - val_mae: 0.4779 - val_mean_pred: 0.7896 - val_mae_t1: 0.0319\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.1413 - mae: 0.2893 - mean_pred: 0.7676 - mae_t1: 0.0193 - val_loss: 5.1617 - val_mae: 0.4754 - val_mean_pred: 1.0004 - val_mae_t1: 0.0317\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.9939 - mae: 0.2758 - mean_pred: 0.8081 - mae_t1: 0.0184 - val_loss: 4.7866 - val_mae: 0.4409 - val_mean_pred: 0.8853 - val_mae_t1: 0.0294\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.8689 - mae: 0.2642 - mean_pred: 0.8203 - mae_t1: 0.0176 - val_loss: 4.7013 - val_mae: 0.4330 - val_mean_pred: 0.9426 - val_mae_t1: 0.0289\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 2.7180 - mae: 0.2503 - mean_pred: 0.7889 - mae_t1: 0.0167 - val_loss: 4.4518 - val_mae: 0.4100 - val_mean_pred: 0.9109 - val_mae_t1: 0.0273\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9546 - mae: 0.2721 - mean_pred: 0.8507 - mae_t1: 0.0181 - val_loss: 4.5395 - val_mae: 0.4181 - val_mean_pred: 0.9237 - val_mae_t1: 0.0279\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.7771 - mae: 0.2558 - mean_pred: 0.7854 - mae_t1: 0.0171 - val_loss: 5.3952 - val_mae: 0.4969 - val_mean_pred: 0.8243 - val_mae_t1: 0.0331\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.2461 - mae: 0.2990 - mean_pred: 0.7514 - mae_t1: 0.0199 - val_loss: 5.3247 - val_mae: 0.4904 - val_mean_pred: 0.9854 - val_mae_t1: 0.0327\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 3.3594 - mae: 0.3094 - mean_pred: 0.9373 - mae_t1: 0.0206 - val_loss: 4.9267 - val_mae: 0.4538 - val_mean_pred: 0.8002 - val_mae_t1: 0.0303\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.5204 - mae: 0.3242 - mean_pred: 0.6837 - mae_t1: 0.0216 - val_loss: 4.8803 - val_mae: 0.4495 - val_mean_pred: 0.7397 - val_mae_t1: 0.0300\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.0471 - mae: 0.2807 - mean_pred: 0.7524 - mae_t1: 0.0187 - val_loss: 5.5012 - val_mae: 0.5067 - val_mean_pred: 1.0260 - val_mae_t1: 0.0338\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4676 - mae: 0.3194 - mean_pred: 0.8358 - mae_t1: 0.0213 - val_loss: 5.1116 - val_mae: 0.4708 - val_mean_pred: 0.8833 - val_mae_t1: 0.0314\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.2661 - mae: 0.3008 - mean_pred: 0.7873 - mae_t1: 0.0201 - val_loss: 5.2312 - val_mae: 0.4818 - val_mean_pred: 0.9372 - val_mae_t1: 0.0321\n",
      "Earliness...\n",
      "0.0020003318786621094\n",
      "____________________________________________________________\n",
      "Test MAE:      0.33735499945067976  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▄▄▄▃▃▃▃▃▃▂▃▃▃▂▃▂▂▂▃▂▁▂▂▁▁▁▂▁▂▂▁▂▂▂▁▁▂▂</td></tr><tr><td>mae</td><td>█▇▄▄▄▃▃▃▃▃▃▂▃▃▃▂▃▂▂▂▃▂▁▂▂▁▁▁▂▁▂▂▁▂▂▂▁▁▂▂</td></tr><tr><td>mae_t1</td><td>█▇▄▄▄▃▃▃▃▃▃▂▃▃▃▂▃▂▂▂▃▂▁▂▂▁▁▁▂▁▂▂▁▂▂▂▁▁▂▂</td></tr><tr><td>mean_pred</td><td>▃▁▆▄█▆▆▆▅▄▅▇▄▇▄▆█▅▅▆▃▅▆▆▆▅▆▆▆▇▅▆▆▆▆▆▆▆▅▆</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▄▄▅▃▄▂▂▅▃▄▄▃▅▂▄▂▂▄▂▄▃▄▁▃▄▃▃▅▃▃▁▄▂▃</td></tr><tr><td>val_mae</td><td>█▄▄▃▃▃▄▄▅▃▄▂▂▅▃▄▄▃▅▂▄▂▂▄▂▄▃▄▁▃▄▃▃▅▃▃▁▄▂▃</td></tr><tr><td>val_mae_t1</td><td>█▄▄▃▃▃▄▄▅▃▄▂▂▅▃▄▄▃▅▂▄▂▂▄▂▄▃▄▁▃▄▃▃▅▃▃▁▄▂▃</td></tr><tr><td>val_mean_pred</td><td>█▁▆▅▂▄▅▅█▅▅▃▃▅▅▄▂▄▆▄▃▄▅▅▅▅▇▆▅▂▂▃▃▃▂▆▅▃▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.41179</td></tr><tr><td>AE_2</td><td>0.30661</td></tr><tr><td>AE_3</td><td>0.28078</td></tr><tr><td>MAE</td><td>0.33735</td></tr><tr><td>best_epoch</td><td>91</td></tr><tr><td>best_val_loss</td><td>4.45185</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>3.26612</td></tr><tr><td>mae</td><td>0.30083</td></tr><tr><td>mae_t1</td><td>0.02006</td></tr><tr><td>mean_pred</td><td>0.78733</td></tr><tr><td>val_loss</td><td>5.23124</td></tr><tr><td>val_mae</td><td>0.48182</td></tr><tr><td>val_mae_t1</td><td>0.03212</td></tr><tr><td>val_mean_pred</td><td>0.93717</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-elevator-77</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2onpankt\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2onpankt</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155139-2onpankt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155205-3l94z2tt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3l94z2tt\" target=\"_blank\">light-fire-78</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 716us/sample - loss: 9.8722 - mae: 0.9093 - mean_pred: 0.1921 - mae_t1: 0.0606 - val_loss: 5.3921 - val_mae: 0.4966 - val_mean_pred: 0.6515 - val_mae_t1: 0.0331\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 6.5934 - mae: 0.6073 - mean_pred: 0.7263 - mae_t1: 0.0405 - val_loss: 5.4410 - val_mae: 0.5011 - val_mean_pred: 0.7762 - val_mae_t1: 0.0334\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 6.1020 - mae: 0.5620 - mean_pred: 0.6977 - mae_t1: 0.0375 - val_loss: 6.2628 - val_mae: 0.5768 - val_mean_pred: 0.6504 - val_mae_t1: 0.0385\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.9873 - mae: 0.5515 - mean_pred: 0.7334 - mae_t1: 0.0368 - val_loss: 5.7722 - val_mae: 0.5316 - val_mean_pred: 0.8032 - val_mae_t1: 0.0354\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.1531 - mae: 0.5667 - mean_pred: 0.7892 - mae_t1: 0.0378 - val_loss: 6.6709 - val_mae: 0.6144 - val_mean_pred: 0.5157 - val_mae_t1: 0.0410\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 6.4006 - mae: 0.5895 - mean_pred: 0.5109 - mae_t1: 0.0393 - val_loss: 5.5006 - val_mae: 0.5066 - val_mean_pred: 0.7395 - val_mae_t1: 0.0338\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 142us/sample - loss: 5.9235 - mae: 0.5456 - mean_pred: 0.8450 - mae_t1: 0.0364 - val_loss: 4.9265 - val_mae: 0.4538 - val_mean_pred: 0.9449 - val_mae_t1: 0.0303\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.7503 - mae: 0.5296 - mean_pred: 0.7807 - mae_t1: 0.0353 - val_loss: 5.9530 - val_mae: 0.5483 - val_mean_pred: 0.5330 - val_mae_t1: 0.0366\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.9321 - mae: 0.5464 - mean_pred: 0.4822 - mae_t1: 0.0364 - val_loss: 5.5153 - val_mae: 0.5080 - val_mean_pred: 0.9341 - val_mae_t1: 0.0339\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.8347 - mae: 0.5374 - mean_pred: 0.9588 - mae_t1: 0.0358 - val_loss: 5.7978 - val_mae: 0.5340 - val_mean_pred: 1.0863 - val_mae_t1: 0.0356\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.9610 - mae: 0.5490 - mean_pred: 0.9435 - mae_t1: 0.0366 - val_loss: 5.5682 - val_mae: 0.5129 - val_mean_pred: 0.7203 - val_mae_t1: 0.0342\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.1201 - mae: 0.4716 - mean_pred: 0.7018 - mae_t1: 0.0314 - val_loss: 5.7079 - val_mae: 0.5257 - val_mean_pred: 0.7603 - val_mae_t1: 0.0350\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.0586 - mae: 0.4659 - mean_pred: 0.6969 - mae_t1: 0.0311 - val_loss: 5.9821 - val_mae: 0.5510 - val_mean_pred: 0.6464 - val_mae_t1: 0.0367\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.9098 - mae: 0.4522 - mean_pred: 0.6540 - mae_t1: 0.0301 - val_loss: 6.0937 - val_mae: 0.5613 - val_mean_pred: 0.8977 - val_mae_t1: 0.0374\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 5.2999 - mae: 0.4881 - mean_pred: 0.8908 - mae_t1: 0.0325 - val_loss: 6.1027 - val_mae: 0.5621 - val_mean_pred: 0.8217 - val_mae_t1: 0.0375\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.6279 - mae: 0.5184 - mean_pred: 0.7320 - mae_t1: 0.0346 - val_loss: 8.2966 - val_mae: 0.7642 - val_mean_pred: 0.5779 - val_mae_t1: 0.0509\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 7.0300 - mae: 0.6475 - mean_pred: 0.5536 - mae_t1: 0.0432 - val_loss: 6.9865 - val_mae: 0.6435 - val_mean_pred: 0.6431 - val_mae_t1: 0.0429\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.7298 - mae: 0.5277 - mean_pred: 0.6902 - mae_t1: 0.0352 - val_loss: 7.0240 - val_mae: 0.6469 - val_mean_pred: 1.0914 - val_mae_t1: 0.0431\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.4023 - mae: 0.5897 - mean_pred: 1.0530 - mae_t1: 0.0393 - val_loss: 6.3834 - val_mae: 0.5879 - val_mean_pred: 1.0323 - val_mae_t1: 0.0392\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.1678 - mae: 0.4760 - mean_pred: 0.9020 - mae_t1: 0.0317 - val_loss: 6.4975 - val_mae: 0.5985 - val_mean_pred: 0.7187 - val_mae_t1: 0.0399\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1637 - mae: 0.4756 - mean_pred: 0.6305 - mae_t1: 0.0317 - val_loss: 6.5450 - val_mae: 0.6028 - val_mean_pred: 0.7447 - val_mae_t1: 0.0402\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.9633 - mae: 0.4571 - mean_pred: 0.7012 - mae_t1: 0.0305 - val_loss: 6.4869 - val_mae: 0.5975 - val_mean_pred: 1.0299 - val_mae_t1: 0.0398\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.5997 - mae: 0.5158 - mean_pred: 0.9864 - mae_t1: 0.0344 - val_loss: 6.2074 - val_mae: 0.5717 - val_mean_pred: 0.9937 - val_mae_t1: 0.0381\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0932 - mae: 0.4691 - mean_pred: 0.8787 - mae_t1: 0.0313 - val_loss: 6.4387 - val_mae: 0.5930 - val_mean_pred: 0.6158 - val_mae_t1: 0.0395\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.5065 - mae: 0.5072 - mean_pred: 0.5388 - mae_t1: 0.0338 - val_loss: 6.7498 - val_mae: 0.6217 - val_mean_pred: 0.6113 - val_mae_t1: 0.0414\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.4168 - mae: 0.4989 - mean_pred: 0.5885 - mae_t1: 0.0333 - val_loss: 6.2919 - val_mae: 0.5795 - val_mean_pred: 1.0861 - val_mae_t1: 0.0386\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.1332 - mae: 0.5649 - mean_pred: 1.0839 - mae_t1: 0.0377 - val_loss: 6.2587 - val_mae: 0.5765 - val_mean_pred: 1.1218 - val_mae_t1: 0.0384\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 5.3281 - mae: 0.4908 - mean_pred: 0.9455 - mae_t1: 0.0327 - val_loss: 6.2698 - val_mae: 0.5775 - val_mean_pred: 0.5418 - val_mae_t1: 0.0385\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 5.9934 - mae: 0.5520 - mean_pred: 0.4821 - mae_t1: 0.0368 - val_loss: 5.9335 - val_mae: 0.5465 - val_mean_pred: 0.6865 - val_mae_t1: 0.0364\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.0616 - mae: 0.4662 - mean_pred: 0.7107 - mae_t1: 0.0311 - val_loss: 7.1175 - val_mae: 0.6556 - val_mean_pred: 1.2119 - val_mae_t1: 0.0437\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 6.0267 - mae: 0.5551 - mean_pred: 1.0856 - mae_t1: 0.0370 - val_loss: 5.9050 - val_mae: 0.5439 - val_mean_pred: 1.0177 - val_mae_t1: 0.0363\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.7905 - mae: 0.4412 - mean_pred: 0.8314 - mae_t1: 0.0294 - val_loss: 6.5005 - val_mae: 0.5987 - val_mean_pred: 0.5721 - val_mae_t1: 0.0399\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.9963 - mae: 0.5523 - mean_pred: 0.4218 - mae_t1: 0.0368 - val_loss: 7.1558 - val_mae: 0.6591 - val_mean_pred: 0.4374 - val_mae_t1: 0.0439\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.2243 - mae: 0.5733 - mean_pred: 0.3794 - mae_t1: 0.0382 - val_loss: 6.3716 - val_mae: 0.5869 - val_mean_pred: 0.6864 - val_mae_t1: 0.0391\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 5.1924 - mae: 0.4782 - mean_pred: 0.6227 - mae_t1: 0.0319 - val_loss: 6.1387 - val_mae: 0.5654 - val_mean_pred: 0.8634 - val_mae_t1: 0.0377\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.7847 - mae: 0.4407 - mean_pred: 0.7333 - mae_t1: 0.0294 - val_loss: 6.2670 - val_mae: 0.5772 - val_mean_pred: 0.7278 - val_mae_t1: 0.0385\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.7952 - mae: 0.4417 - mean_pred: 0.6352 - mae_t1: 0.0294 - val_loss: 6.6520 - val_mae: 0.6127 - val_mean_pred: 0.6134 - val_mae_t1: 0.0408\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.3940 - mae: 0.4968 - mean_pred: 0.6055 - mae_t1: 0.0331 - val_loss: 6.0930 - val_mae: 0.5612 - val_mean_pred: 0.6142 - val_mae_t1: 0.0374\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.0902 - mae: 0.4688 - mean_pred: 0.5964 - mae_t1: 0.0313 - val_loss: 5.7353 - val_mae: 0.5283 - val_mean_pred: 0.9204 - val_mae_t1: 0.0352\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.5015 - mae: 0.5067 - mean_pred: 0.9152 - mae_t1: 0.0338 - val_loss: 5.6324 - val_mae: 0.5188 - val_mean_pred: 1.0711 - val_mae_t1: 0.0346\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 5.5190 - mae: 0.5083 - mean_pred: 0.9274 - mae_t1: 0.0339 - val_loss: 5.1064 - val_mae: 0.4703 - val_mean_pred: 0.6733 - val_mae_t1: 0.0314\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1873 - mae: 0.4778 - mean_pred: 0.5793 - mae_t1: 0.0319 - val_loss: 5.3022 - val_mae: 0.4884 - val_mean_pred: 0.6404 - val_mae_t1: 0.0326\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.4403 - mae: 0.5011 - mean_pred: 0.5950 - mae_t1: 0.0334 - val_loss: 5.1473 - val_mae: 0.4741 - val_mean_pred: 0.7664 - val_mae_t1: 0.0316\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.7619 - mae: 0.4386 - mean_pred: 0.6854 - mae_t1: 0.0292 - val_loss: 5.6898 - val_mae: 0.5241 - val_mean_pred: 0.8425 - val_mae_t1: 0.0349\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.8260 - mae: 0.4445 - mean_pred: 0.7586 - mae_t1: 0.0296 - val_loss: 5.3846 - val_mae: 0.4960 - val_mean_pred: 0.9381 - val_mae_t1: 0.0331\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 4.3343 - mae: 0.3992 - mean_pred: 0.8458 - mae_t1: 0.0266 - val_loss: 7.1231 - val_mae: 0.6561 - val_mean_pred: 1.0684 - val_mae_t1: 0.0437\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 6.9345 - mae: 0.6387 - mean_pred: 0.9914 - mae_t1: 0.0426 - val_loss: 6.5918 - val_mae: 0.6071 - val_mean_pred: 1.0342 - val_mae_t1: 0.0405\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.2256 - mae: 0.4813 - mean_pred: 0.8859 - mae_t1: 0.0321 - val_loss: 7.4144 - val_mae: 0.6829 - val_mean_pred: 0.8415 - val_mae_t1: 0.0455\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 7.2540 - mae: 0.6681 - mean_pred: 0.7852 - mae_t1: 0.0445 - val_loss: 7.6325 - val_mae: 0.7030 - val_mean_pred: 0.8613 - val_mae_t1: 0.0469\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.1495 - mae: 0.5664 - mean_pred: 0.7772 - mae_t1: 0.0378 - val_loss: 6.0184 - val_mae: 0.5543 - val_mean_pred: 0.8365 - val_mae_t1: 0.0370\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0633 - mae: 0.4664 - mean_pred: 0.7428 - mae_t1: 0.0311 - val_loss: 5.8485 - val_mae: 0.5387 - val_mean_pred: 0.7191 - val_mae_t1: 0.0359\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.6599 - mae: 0.4292 - mean_pred: 0.6401 - mae_t1: 0.0286 - val_loss: 5.4606 - val_mae: 0.5030 - val_mean_pred: 0.7217 - val_mae_t1: 0.0335\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 4.6247 - mae: 0.4260 - mean_pred: 0.6303 - mae_t1: 0.0284 - val_loss: 5.4593 - val_mae: 0.5028 - val_mean_pred: 0.8004 - val_mae_t1: 0.0335\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 4.4083 - mae: 0.4060 - mean_pred: 0.7089 - mae_t1: 0.0271 - val_loss: 5.7809 - val_mae: 0.5325 - val_mean_pred: 0.8868 - val_mae_t1: 0.0355\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.8002 - mae: 0.4421 - mean_pred: 0.7788 - mae_t1: 0.0295 - val_loss: 5.8273 - val_mae: 0.5367 - val_mean_pred: 0.8563 - val_mae_t1: 0.0358\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.6978 - mae: 0.4327 - mean_pred: 0.7317 - mae_t1: 0.0288 - val_loss: 6.2809 - val_mae: 0.5785 - val_mean_pred: 0.9831 - val_mae_t1: 0.0386\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 4.9021 - mae: 0.4515 - mean_pred: 0.8503 - mae_t1: 0.0301 - val_loss: 5.7233 - val_mae: 0.5271 - val_mean_pred: 0.9941 - val_mae_t1: 0.0351\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.5400 - mae: 0.4182 - mean_pred: 0.7985 - mae_t1: 0.0279 - val_loss: 5.1212 - val_mae: 0.4717 - val_mean_pred: 0.7702 - val_mae_t1: 0.0314\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 4.5033 - mae: 0.4148 - mean_pred: 0.6896 - mae_t1: 0.0277 - val_loss: 5.5150 - val_mae: 0.5080 - val_mean_pred: 0.9749 - val_mae_t1: 0.0339\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.0266 - mae: 0.4630 - mean_pred: 0.8902 - mae_t1: 0.0309 - val_loss: 5.3964 - val_mae: 0.4970 - val_mean_pred: 0.9390 - val_mae_t1: 0.0331\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.6118 - mae: 0.4248 - mean_pred: 0.7682 - mae_t1: 0.0283 - val_loss: 5.7064 - val_mae: 0.5256 - val_mean_pred: 0.7048 - val_mae_t1: 0.0350\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 4.7861 - mae: 0.4408 - mean_pred: 0.6488 - mae_t1: 0.0294 - val_loss: 5.9933 - val_mae: 0.5520 - val_mean_pred: 0.8840 - val_mae_t1: 0.0368\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.0530 - mae: 0.4654 - mean_pred: 0.8337 - mae_t1: 0.0310 - val_loss: 5.9330 - val_mae: 0.5465 - val_mean_pred: 1.0663 - val_mae_t1: 0.0364\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.9231 - mae: 0.4534 - mean_pred: 0.9619 - mae_t1: 0.0302 - val_loss: 5.2922 - val_mae: 0.4874 - val_mean_pred: 0.9145 - val_mae_t1: 0.0325\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.1619 - mae: 0.3833 - mean_pred: 0.7483 - mae_t1: 0.0256 - val_loss: 5.7438 - val_mae: 0.5290 - val_mean_pred: 0.7018 - val_mae_t1: 0.0353\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.4790 - mae: 0.4125 - mean_pred: 0.6387 - mae_t1: 0.0275 - val_loss: 5.6541 - val_mae: 0.5208 - val_mean_pred: 0.8154 - val_mae_t1: 0.0347\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 4.6538 - mae: 0.4286 - mean_pred: 0.7521 - mae_t1: 0.0286 - val_loss: 5.1465 - val_mae: 0.4740 - val_mean_pred: 0.8800 - val_mae_t1: 0.0316\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.9714 - mae: 0.3658 - mean_pred: 0.7790 - mae_t1: 0.0244 - val_loss: 6.5322 - val_mae: 0.6016 - val_mean_pred: 0.9115 - val_mae_t1: 0.0401\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.2975 - mae: 0.4879 - mean_pred: 0.7930 - mae_t1: 0.0325 - val_loss: 5.7966 - val_mae: 0.5339 - val_mean_pred: 0.8903 - val_mae_t1: 0.0356\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.2332 - mae: 0.3899 - mean_pred: 0.7658 - mae_t1: 0.0260 - val_loss: 4.9974 - val_mae: 0.4603 - val_mean_pred: 0.8230 - val_mae_t1: 0.0307\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 4.1559 - mae: 0.3828 - mean_pred: 0.7165 - mae_t1: 0.0255 - val_loss: 5.3539 - val_mae: 0.4931 - val_mean_pred: 0.7662 - val_mae_t1: 0.0329\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.4708 - mae: 0.4118 - mean_pred: 0.7180 - mae_t1: 0.0275 - val_loss: 5.4798 - val_mae: 0.5047 - val_mean_pred: 0.8535 - val_mae_t1: 0.0336\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.4364 - mae: 0.4086 - mean_pred: 0.7668 - mae_t1: 0.0272 - val_loss: 5.0878 - val_mae: 0.4686 - val_mean_pred: 0.8818 - val_mae_t1: 0.0312\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 3.6842 - mae: 0.3393 - mean_pred: 0.7647 - mae_t1: 0.0226 - val_loss: 5.0536 - val_mae: 0.4655 - val_mean_pred: 0.9517 - val_mae_t1: 0.0310\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 3.6775 - mae: 0.3387 - mean_pred: 0.8059 - mae_t1: 0.0226 - val_loss: 5.2403 - val_mae: 0.4827 - val_mean_pred: 0.9402 - val_mae_t1: 0.0322\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 3.9708 - mae: 0.3657 - mean_pred: 0.7841 - mae_t1: 0.0244 - val_loss: 5.3325 - val_mae: 0.4912 - val_mean_pred: 0.8916 - val_mae_t1: 0.0327\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.8089 - mae: 0.3508 - mean_pred: 0.7404 - mae_t1: 0.0234 - val_loss: 5.3084 - val_mae: 0.4889 - val_mean_pred: 0.7411 - val_mae_t1: 0.0326\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.6444 - mae: 0.4278 - mean_pred: 0.6791 - mae_t1: 0.0285 - val_loss: 5.3279 - val_mae: 0.4907 - val_mean_pred: 0.7096 - val_mae_t1: 0.0327\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.1424 - mae: 0.3815 - mean_pred: 0.6783 - mae_t1: 0.0254 - val_loss: 5.6112 - val_mae: 0.5168 - val_mean_pred: 0.9890 - val_mae_t1: 0.0345\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.2638 - mae: 0.3927 - mean_pred: 0.8772 - mae_t1: 0.0262 - val_loss: 5.7368 - val_mae: 0.5284 - val_mean_pred: 1.0860 - val_mae_t1: 0.0352\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 129us/sample - loss: 4.1866 - mae: 0.3856 - mean_pred: 0.9211 - mae_t1: 0.0257 - val_loss: 4.8739 - val_mae: 0.4489 - val_mean_pred: 0.8251 - val_mae_t1: 0.0299\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.8021 - mae: 0.3502 - mean_pred: 0.6747 - mae_t1: 0.0233 - val_loss: 5.6823 - val_mae: 0.5234 - val_mean_pred: 0.6858 - val_mae_t1: 0.0349\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.2958 - mae: 0.3957 - mean_pred: 0.6199 - mae_t1: 0.0264 - val_loss: 5.1785 - val_mae: 0.4770 - val_mean_pred: 0.8673 - val_mae_t1: 0.0318\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.1160 - mae: 0.3791 - mean_pred: 0.7721 - mae_t1: 0.0253 - val_loss: 5.2839 - val_mae: 0.4867 - val_mean_pred: 0.9525 - val_mae_t1: 0.0324\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.9151 - mae: 0.3606 - mean_pred: 0.8302 - mae_t1: 0.0240 - val_loss: 5.7899 - val_mae: 0.5333 - val_mean_pred: 0.9794 - val_mae_t1: 0.0356\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.3779 - mae: 0.4032 - mean_pred: 0.8138 - mae_t1: 0.0269 - val_loss: 5.3238 - val_mae: 0.4904 - val_mean_pred: 0.9662 - val_mae_t1: 0.0327\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.0478 - mae: 0.3728 - mean_pred: 0.7969 - mae_t1: 0.0249 - val_loss: 5.5098 - val_mae: 0.5075 - val_mean_pred: 0.9447 - val_mae_t1: 0.0338\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.2628 - mae: 0.3926 - mean_pred: 0.7714 - mae_t1: 0.0262 - val_loss: 5.0790 - val_mae: 0.4678 - val_mean_pred: 0.8663 - val_mae_t1: 0.0312\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 3.7296 - mae: 0.3435 - mean_pred: 0.7381 - mae_t1: 0.0229 - val_loss: 4.8508 - val_mae: 0.4468 - val_mean_pred: 0.8916 - val_mae_t1: 0.0298\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 3.7939 - mae: 0.3494 - mean_pred: 0.8099 - mae_t1: 0.0233 - val_loss: 4.8181 - val_mae: 0.4438 - val_mean_pred: 0.8963 - val_mae_t1: 0.0296\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.7410 - mae: 0.3446 - mean_pred: 0.8226 - mae_t1: 0.0230 - val_loss: 4.9781 - val_mae: 0.4585 - val_mean_pred: 0.8644 - val_mae_t1: 0.0306\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.6181 - mae: 0.3332 - mean_pred: 0.7973 - mae_t1: 0.0222 - val_loss: 5.2070 - val_mae: 0.4796 - val_mean_pred: 0.8500 - val_mae_t1: 0.0320\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.9201 - mae: 0.3611 - mean_pred: 0.7457 - mae_t1: 0.0241 - val_loss: 5.3917 - val_mae: 0.4966 - val_mean_pred: 0.8228 - val_mae_t1: 0.0331\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 4.5763 - mae: 0.4215 - mean_pred: 0.7545 - mae_t1: 0.0281 - val_loss: 5.5125 - val_mae: 0.5077 - val_mean_pred: 0.8302 - val_mae_t1: 0.0338\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 4.4324 - mae: 0.4083 - mean_pred: 0.7484 - mae_t1: 0.0272 - val_loss: 5.0259 - val_mae: 0.4629 - val_mean_pred: 0.8611 - val_mae_t1: 0.0309\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.6094 - mae: 0.3324 - mean_pred: 0.7392 - mae_t1: 0.0222 - val_loss: 5.1088 - val_mae: 0.4705 - val_mean_pred: 1.0106 - val_mae_t1: 0.0314\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.6487 - mae: 0.3361 - mean_pred: 0.8721 - mae_t1: 0.0224 - val_loss: 5.0304 - val_mae: 0.4633 - val_mean_pred: 0.9865 - val_mae_t1: 0.0309\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.6228 - mae: 0.3337 - mean_pred: 0.8252 - mae_t1: 0.0222 - val_loss: 5.6288 - val_mae: 0.5184 - val_mean_pred: 0.7495 - val_mae_t1: 0.0346\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.5061 - mae: 0.4150 - mean_pred: 0.6256 - mae_t1: 0.0277 - val_loss: 5.7937 - val_mae: 0.5336 - val_mean_pred: 0.6180 - val_mae_t1: 0.0356\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 4.7415 - mae: 0.4367 - mean_pred: 0.6037 - mae_t1: 0.0291 - val_loss: 5.0310 - val_mae: 0.4634 - val_mean_pred: 0.8656 - val_mae_t1: 0.0309\n",
      "Earliness...\n",
      "0.0014996528625488281\n",
      "____________________________________________________________\n",
      "Test MAE:      0.32096122061299964  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▄▃▄▃▃▃▃▃▃▃▄▄▂▃▃▂▂▅▃▂▂▂▂▂▂▃▂▁▁▂▁▂▁▁▁▂▁▂</td></tr><tr><td>mae</td><td>█▄▄▃▄▃▃▃▃▃▃▃▄▄▂▃▃▂▂▅▃▂▂▂▂▂▂▃▂▁▁▂▁▂▁▁▁▂▁▂</td></tr><tr><td>mae_t1</td><td>█▄▄▃▄▃▃▃▃▃▃▃▄▄▂▃▃▂▂▅▃▂▂▂▂▂▂▃▂▁▁▂▁▂▁▁▁▂▁▂</td></tr><tr><td>mean_pred</td><td>▁▅▃▆▇▅▅▅▄▇▄▇█▂▅▄▇▅▆▆▅▅▅▅▆▇▅▆▅▅▅▅▅▆▆▅▆▅▆▄</td></tr><tr><td>val_loss</td><td>▂▄▂▃▂▃█▅▄▄▄▄▃▄▄▃▂▃▆▇▃▃▄▂▃▂▂▃▂▁▂▃▃▂▂▁▂▂▁▁</td></tr><tr><td>val_mae</td><td>▂▄▂▃▂▃█▅▄▄▄▄▃▄▄▃▂▃▆▇▃▃▄▂▃▂▂▃▂▁▂▃▃▂▂▁▂▂▁▁</td></tr><tr><td>val_mae_t1</td><td>▂▄▂▃▂▃█▅▄▄▄▄▃▄▄▃▂▃▆▇▃▃▄▂▃▂▂▃▂▁▂▃▃▂▂▁▂▂▁▁</td></tr><tr><td>val_mean_pred</td><td>▂▂▄▁▃▂▂█▄▇█▁▇▃▃▆▃▅█▅▃▅▇▇▃▆▅▅▅▆▄▇▃▆▆▅▅▅▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.38791</td></tr><tr><td>AE_2</td><td>0.25213</td></tr><tr><td>AE_3</td><td>0.2628</td></tr><tr><td>MAE</td><td>0.32096</td></tr><tr><td>best_epoch</td><td>89</td></tr><tr><td>best_val_loss</td><td>4.81815</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>4.7415</td></tr><tr><td>mae</td><td>0.43672</td></tr><tr><td>mae_t1</td><td>0.02911</td></tr><tr><td>mean_pred</td><td>0.60367</td></tr><tr><td>val_loss</td><td>5.03098</td></tr><tr><td>val_mae</td><td>0.46338</td></tr><tr><td>val_mae_t1</td><td>0.03089</td></tr><tr><td>val_mean_pred</td><td>0.86562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">light-fire-78</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3l94z2tt\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3l94z2tt</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155205-3l94z2tt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155231-26ctunk0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/26ctunk0\" target=\"_blank\">spring-leaf-79</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 849us/sample - loss: 8.5722 - mae: 0.6251 - mean_pred: 0.5787 - mae_t1: 0.0417 - val_loss: 7.8389 - val_mae: 0.5716 - val_mean_pred: 1.1681 - val_mae_t1: 0.0381\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 309us/sample - loss: 8.2593 - mae: 0.6022 - mean_pred: 0.8878 - mae_t1: 0.0401 - val_loss: 9.4552 - val_mae: 0.6894 - val_mean_pred: 0.5700 - val_mae_t1: 0.0460\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 249us/sample - loss: 10.4667 - mae: 0.7632 - mean_pred: 0.6481 - mae_t1: 0.0509 - val_loss: 8.5497 - val_mae: 0.6234 - val_mean_pred: 0.4961 - val_mae_t1: 0.0416\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 7.6046 - mae: 0.5545 - mean_pred: 0.6448 - mae_t1: 0.0370 - val_loss: 9.3644 - val_mae: 0.6828 - val_mean_pred: 1.2520 - val_mae_t1: 0.0455\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 9.1824 - mae: 0.6696 - mean_pred: 1.0391 - mae_t1: 0.0446 - val_loss: 9.1309 - val_mae: 0.6658 - val_mean_pred: 0.4487 - val_mae_t1: 0.0444\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 7.9095 - mae: 0.5767 - mean_pred: 0.6728 - mae_t1: 0.0384 - val_loss: 7.2017 - val_mae: 0.5251 - val_mean_pred: 1.0231 - val_mae_t1: 0.0350\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 7.2890 - mae: 0.5315 - mean_pred: 0.7376 - mae_t1: 0.0354 - val_loss: 9.0446 - val_mae: 0.6595 - val_mean_pred: 0.4792 - val_mae_t1: 0.0440\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 7.3050 - mae: 0.5327 - mean_pred: 0.5790 - mae_t1: 0.0355 - val_loss: 9.1466 - val_mae: 0.6669 - val_mean_pred: 1.1699 - val_mae_t1: 0.0445\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 6.6890 - mae: 0.4877 - mean_pred: 0.8482 - mae_t1: 0.0325 - val_loss: 8.4429 - val_mae: 0.6156 - val_mean_pred: 0.5766 - val_mae_t1: 0.0410\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 219us/sample - loss: 6.5656 - mae: 0.4787 - mean_pred: 0.5997 - mae_t1: 0.0319 - val_loss: 7.1147 - val_mae: 0.5188 - val_mean_pred: 0.9884 - val_mae_t1: 0.0346\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 6.0998 - mae: 0.4448 - mean_pred: 0.8399 - mae_t1: 0.0297 - val_loss: 7.4422 - val_mae: 0.5427 - val_mean_pred: 0.6728 - val_mae_t1: 0.0362\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 6.5910 - mae: 0.4806 - mean_pred: 0.5905 - mae_t1: 0.0320 - val_loss: 7.0429 - val_mae: 0.5135 - val_mean_pred: 0.8631 - val_mae_t1: 0.0342\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 5.8496 - mae: 0.4265 - mean_pred: 0.7765 - mae_t1: 0.0284 - val_loss: 6.4115 - val_mae: 0.4675 - val_mean_pred: 0.8461 - val_mae_t1: 0.0312\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 5.9769 - mae: 0.4358 - mean_pred: 0.7345 - mae_t1: 0.0291 - val_loss: 6.5731 - val_mae: 0.4793 - val_mean_pred: 0.7306 - val_mae_t1: 0.0320\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 6.0559 - mae: 0.4416 - mean_pred: 0.6765 - mae_t1: 0.0294 - val_loss: 7.3776 - val_mae: 0.5379 - val_mean_pred: 0.9675 - val_mae_t1: 0.0359\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 5.7574 - mae: 0.4198 - mean_pred: 0.8493 - mae_t1: 0.0280 - val_loss: 8.0577 - val_mae: 0.5875 - val_mean_pred: 0.7439 - val_mae_t1: 0.0392\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 8.6466 - mae: 0.6305 - mean_pred: 0.6248 - mae_t1: 0.0420 - val_loss: 8.4791 - val_mae: 0.6183 - val_mean_pred: 0.5681 - val_mae_t1: 0.0412\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 6.7128 - mae: 0.4895 - mean_pred: 0.7142 - mae_t1: 0.0326 - val_loss: 8.4882 - val_mae: 0.6189 - val_mean_pred: 1.1909 - val_mae_t1: 0.0413\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 5.7908 - mae: 0.4222 - mean_pred: 0.8184 - mae_t1: 0.0281 - val_loss: 8.5757 - val_mae: 0.6253 - val_mean_pred: 0.5327 - val_mae_t1: 0.0417\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 6.9627 - mae: 0.5077 - mean_pred: 0.4864 - mae_t1: 0.0338 - val_loss: 6.8733 - val_mae: 0.5012 - val_mean_pred: 0.8345 - val_mae_t1: 0.0334\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.9849 - mae: 0.4364 - mean_pred: 0.9021 - mae_t1: 0.0291 - val_loss: 7.4980 - val_mae: 0.5467 - val_mean_pred: 0.9829 - val_mae_t1: 0.0364\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 6.3659 - mae: 0.4642 - mean_pred: 0.7989 - mae_t1: 0.0309 - val_loss: 7.2157 - val_mae: 0.5261 - val_mean_pred: 0.7348 - val_mae_t1: 0.0351\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 5.6411 - mae: 0.4113 - mean_pred: 0.7155 - mae_t1: 0.0274 - val_loss: 6.4192 - val_mae: 0.4681 - val_mean_pred: 0.9342 - val_mae_t1: 0.0312\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 5.2548 - mae: 0.3832 - mean_pred: 0.8240 - mae_t1: 0.0255 - val_loss: 6.7808 - val_mae: 0.4944 - val_mean_pred: 0.8880 - val_mae_t1: 0.0330\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.3506 - mae: 0.3901 - mean_pred: 0.8265 - mae_t1: 0.0260 - val_loss: 6.7093 - val_mae: 0.4892 - val_mean_pred: 0.9428 - val_mae_t1: 0.0326\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 5.1154 - mae: 0.3730 - mean_pred: 0.8000 - mae_t1: 0.0249 - val_loss: 7.4255 - val_mae: 0.5414 - val_mean_pred: 0.9893 - val_mae_t1: 0.0361\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 5.2705 - mae: 0.3843 - mean_pred: 0.8900 - mae_t1: 0.0256 - val_loss: 6.7890 - val_mae: 0.4950 - val_mean_pred: 0.7605 - val_mae_t1: 0.0330\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 5.1099 - mae: 0.3726 - mean_pred: 0.6786 - mae_t1: 0.0248 - val_loss: 6.4769 - val_mae: 0.4723 - val_mean_pred: 1.0113 - val_mae_t1: 0.0315\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 4.8402 - mae: 0.3529 - mean_pred: 0.8408 - mae_t1: 0.0235 - val_loss: 6.0974 - val_mae: 0.4446 - val_mean_pred: 0.8505 - val_mae_t1: 0.0296\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 4.8462 - mae: 0.3534 - mean_pred: 0.8139 - mae_t1: 0.0236 - val_loss: 6.7248 - val_mae: 0.4904 - val_mean_pred: 0.9191 - val_mae_t1: 0.0327\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 4.7457 - mae: 0.3460 - mean_pred: 0.7927 - mae_t1: 0.0231 - val_loss: 7.6053 - val_mae: 0.5546 - val_mean_pred: 0.8329 - val_mae_t1: 0.0370\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 5.3461 - mae: 0.3898 - mean_pred: 0.7406 - mae_t1: 0.0260 - val_loss: 7.3317 - val_mae: 0.5346 - val_mean_pred: 0.9374 - val_mae_t1: 0.0356\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 5.2162 - mae: 0.3803 - mean_pred: 0.8192 - mae_t1: 0.0254 - val_loss: 8.1874 - val_mae: 0.5970 - val_mean_pred: 0.6663 - val_mae_t1: 0.0398\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 6.2275 - mae: 0.4541 - mean_pred: 0.5854 - mae_t1: 0.0303 - val_loss: 6.7324 - val_mae: 0.4909 - val_mean_pred: 0.8948 - val_mae_t1: 0.0327\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 231us/sample - loss: 5.0695 - mae: 0.3697 - mean_pred: 0.8569 - mae_t1: 0.0246 - val_loss: 6.0905 - val_mae: 0.4441 - val_mean_pred: 0.8707 - val_mae_t1: 0.0296\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.8890 - mae: 0.3565 - mean_pred: 0.7191 - mae_t1: 0.0238 - val_loss: 6.4499 - val_mae: 0.4703 - val_mean_pred: 0.8154 - val_mae_t1: 0.0314\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.7883 - mae: 0.3491 - mean_pred: 0.7849 - mae_t1: 0.0233 - val_loss: 6.1953 - val_mae: 0.4517 - val_mean_pred: 0.9267 - val_mae_t1: 0.0301\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.5505 - mae: 0.3318 - mean_pred: 0.7593 - mae_t1: 0.0221 - val_loss: 6.3877 - val_mae: 0.4658 - val_mean_pred: 0.9447 - val_mae_t1: 0.0311\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 5.4188 - mae: 0.3951 - mean_pred: 0.9131 - mae_t1: 0.0263 - val_loss: 6.4063 - val_mae: 0.4671 - val_mean_pred: 0.7864 - val_mae_t1: 0.0311\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 5.0753 - mae: 0.3701 - mean_pred: 0.6266 - mae_t1: 0.0247 - val_loss: 6.6575 - val_mae: 0.4854 - val_mean_pred: 0.8303 - val_mae_t1: 0.0324\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 5.1233 - mae: 0.3736 - mean_pred: 0.8179 - mae_t1: 0.0249 - val_loss: 6.7371 - val_mae: 0.4912 - val_mean_pred: 0.9144 - val_mae_t1: 0.0327\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 5.1456 - mae: 0.3752 - mean_pred: 0.7788 - mae_t1: 0.0250 - val_loss: 6.1044 - val_mae: 0.4451 - val_mean_pred: 0.7263 - val_mae_t1: 0.0297\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 5.3876 - mae: 0.3928 - mean_pred: 0.6610 - mae_t1: 0.0262 - val_loss: 6.6428 - val_mae: 0.4844 - val_mean_pred: 1.0467 - val_mae_t1: 0.0323\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 6.7228 - mae: 0.4902 - mean_pred: 1.0795 - mae_t1: 0.0327 - val_loss: 6.2838 - val_mae: 0.4582 - val_mean_pred: 0.7374 - val_mae_t1: 0.0305\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 5.7913 - mae: 0.4223 - mean_pred: 0.5701 - mae_t1: 0.0282 - val_loss: 7.3219 - val_mae: 0.5339 - val_mean_pred: 0.6419 - val_mae_t1: 0.0356\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 5.3534 - mae: 0.3904 - mean_pred: 0.7695 - mae_t1: 0.0260 - val_loss: 7.9136 - val_mae: 0.5770 - val_mean_pred: 1.1010 - val_mae_t1: 0.0385\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 5.3020 - mae: 0.3866 - mean_pred: 0.8446 - mae_t1: 0.0258 - val_loss: 7.1802 - val_mae: 0.5236 - val_mean_pred: 0.7039 - val_mae_t1: 0.0349\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 237us/sample - loss: 5.0837 - mae: 0.3707 - mean_pred: 0.6581 - mae_t1: 0.0247 - val_loss: 5.8131 - val_mae: 0.4239 - val_mean_pred: 0.8119 - val_mae_t1: 0.0283\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.7727 - mae: 0.3480 - mean_pred: 0.7291 - mae_t1: 0.0232 - val_loss: 5.8969 - val_mae: 0.4300 - val_mean_pred: 0.8505 - val_mae_t1: 0.0287\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.5802 - mae: 0.3340 - mean_pred: 0.8568 - mae_t1: 0.0223 - val_loss: 6.1750 - val_mae: 0.4503 - val_mean_pred: 0.7851 - val_mae_t1: 0.0300\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 5.0674 - mae: 0.3695 - mean_pred: 0.6482 - mae_t1: 0.0246 - val_loss: 6.0139 - val_mae: 0.4385 - val_mean_pred: 0.8376 - val_mae_t1: 0.0292\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 4.9619 - mae: 0.3618 - mean_pred: 0.9015 - mae_t1: 0.0241 - val_loss: 6.7246 - val_mae: 0.4903 - val_mean_pred: 0.8856 - val_mae_t1: 0.0327\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 5.5285 - mae: 0.4031 - mean_pred: 0.7360 - mae_t1: 0.0269 - val_loss: 9.3439 - val_mae: 0.6813 - val_mean_pred: 0.6506 - val_mae_t1: 0.0454\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 6.5219 - mae: 0.4756 - mean_pred: 0.6851 - mae_t1: 0.0317 - val_loss: 7.6326 - val_mae: 0.5565 - val_mean_pred: 0.8955 - val_mae_t1: 0.0371\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 7.9217 - mae: 0.5776 - mean_pred: 0.8928 - mae_t1: 0.0385 - val_loss: 7.3001 - val_mae: 0.5323 - val_mean_pred: 0.8398 - val_mae_t1: 0.0355\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 6.8412 - mae: 0.4988 - mean_pred: 0.7568 - mae_t1: 0.0333 - val_loss: 6.3915 - val_mae: 0.4660 - val_mean_pred: 0.7995 - val_mae_t1: 0.0311\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 6.3544 - mae: 0.4633 - mean_pred: 0.8007 - mae_t1: 0.0309 - val_loss: 6.1548 - val_mae: 0.4488 - val_mean_pred: 0.8629 - val_mae_t1: 0.0299\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 4.8276 - mae: 0.3520 - mean_pred: 0.7961 - mae_t1: 0.0235 - val_loss: 6.5257 - val_mae: 0.4758 - val_mean_pred: 0.8983 - val_mae_t1: 0.0317\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 4.1549 - mae: 0.3030 - mean_pred: 0.7812 - mae_t1: 0.0202 - val_loss: 6.2479 - val_mae: 0.4556 - val_mean_pred: 0.7900 - val_mae_t1: 0.0304\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.1430 - mae: 0.3021 - mean_pred: 0.7423 - mae_t1: 0.0201 - val_loss: 6.2156 - val_mae: 0.4532 - val_mean_pred: 0.8702 - val_mae_t1: 0.0302\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 4.2925 - mae: 0.3130 - mean_pred: 0.8089 - mae_t1: 0.0209 - val_loss: 6.2956 - val_mae: 0.4591 - val_mean_pred: 0.7688 - val_mae_t1: 0.0306\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 4.3544 - mae: 0.3175 - mean_pred: 0.7504 - mae_t1: 0.0212 - val_loss: 6.4522 - val_mae: 0.4705 - val_mean_pred: 0.8196 - val_mae_t1: 0.0314\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.5227 - mae: 0.3298 - mean_pred: 0.7826 - mae_t1: 0.0220 - val_loss: 6.4792 - val_mae: 0.4724 - val_mean_pred: 0.8953 - val_mae_t1: 0.0315\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.1682 - mae: 0.3039 - mean_pred: 0.8272 - mae_t1: 0.0203 - val_loss: 6.8795 - val_mae: 0.5016 - val_mean_pred: 0.8355 - val_mae_t1: 0.0334\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 5.2905 - mae: 0.3858 - mean_pred: 0.6974 - mae_t1: 0.0257 - val_loss: 6.9982 - val_mae: 0.5103 - val_mean_pred: 0.7455 - val_mae_t1: 0.0340\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.2912 - mae: 0.3129 - mean_pred: 0.8033 - mae_t1: 0.0209 - val_loss: 6.2063 - val_mae: 0.4525 - val_mean_pred: 0.9651 - val_mae_t1: 0.0302\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 4.4254 - mae: 0.3227 - mean_pred: 0.8051 - mae_t1: 0.0215 - val_loss: 6.5060 - val_mae: 0.4744 - val_mean_pred: 0.6790 - val_mae_t1: 0.0316\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 4.2114 - mae: 0.3071 - mean_pred: 0.7484 - mae_t1: 0.0205 - val_loss: 5.9324 - val_mae: 0.4326 - val_mean_pred: 0.9414 - val_mae_t1: 0.0288\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 4.2173 - mae: 0.3075 - mean_pred: 0.8821 - mae_t1: 0.0205 - val_loss: 6.5873 - val_mae: 0.4803 - val_mean_pred: 0.6899 - val_mae_t1: 0.0320\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 5.1947 - mae: 0.3788 - mean_pred: 0.6163 - mae_t1: 0.0253 - val_loss: 6.7472 - val_mae: 0.4920 - val_mean_pred: 0.6479 - val_mae_t1: 0.0328\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 4.6775 - mae: 0.3411 - mean_pred: 0.7285 - mae_t1: 0.0227 - val_loss: 7.2289 - val_mae: 0.5271 - val_mean_pred: 1.0216 - val_mae_t1: 0.0351\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.8078 - mae: 0.3506 - mean_pred: 0.9037 - mae_t1: 0.0234 - val_loss: 6.2951 - val_mae: 0.4590 - val_mean_pred: 0.8666 - val_mae_t1: 0.0306\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 4.3269 - mae: 0.3155 - mean_pred: 0.7567 - mae_t1: 0.0210 - val_loss: 6.2714 - val_mae: 0.4573 - val_mean_pred: 0.8615 - val_mae_t1: 0.0305\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 4.2908 - mae: 0.3129 - mean_pred: 0.8056 - mae_t1: 0.0209 - val_loss: 6.8936 - val_mae: 0.5027 - val_mean_pred: 0.9454 - val_mae_t1: 0.0335\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.6140 - mae: 0.3364 - mean_pred: 0.8097 - mae_t1: 0.0224 - val_loss: 6.5391 - val_mae: 0.4768 - val_mean_pred: 0.8816 - val_mae_t1: 0.0318\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 4.2450 - mae: 0.3095 - mean_pred: 0.8114 - mae_t1: 0.0206 - val_loss: 7.2566 - val_mae: 0.5291 - val_mean_pred: 0.7361 - val_mae_t1: 0.0353\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 4.8713 - mae: 0.3552 - mean_pred: 0.6906 - mae_t1: 0.0237 - val_loss: 7.3440 - val_mae: 0.5355 - val_mean_pred: 0.7580 - val_mae_t1: 0.0357\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 4.8813 - mae: 0.3559 - mean_pred: 0.8243 - mae_t1: 0.0237 - val_loss: 7.4879 - val_mae: 0.5460 - val_mean_pred: 0.9567 - val_mae_t1: 0.0364\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 5.0158 - mae: 0.3657 - mean_pred: 0.8283 - mae_t1: 0.0244 - val_loss: 7.5691 - val_mae: 0.5519 - val_mean_pred: 0.7306 - val_mae_t1: 0.0368\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 4.6542 - mae: 0.3394 - mean_pred: 0.7274 - mae_t1: 0.0226 - val_loss: 6.7223 - val_mae: 0.4902 - val_mean_pred: 0.9144 - val_mae_t1: 0.0327\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.1530 - mae: 0.3028 - mean_pred: 0.8307 - mae_t1: 0.0202 - val_loss: 7.6345 - val_mae: 0.5567 - val_mean_pred: 0.7712 - val_mae_t1: 0.0371\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 5.0279 - mae: 0.3666 - mean_pred: 0.6833 - mae_t1: 0.0244 - val_loss: 7.2021 - val_mae: 0.5252 - val_mean_pred: 0.7956 - val_mae_t1: 0.0350\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.8210 - mae: 0.3515 - mean_pred: 0.7899 - mae_t1: 0.0234 - val_loss: 7.1817 - val_mae: 0.5237 - val_mean_pred: 0.9198 - val_mae_t1: 0.0349\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 4.6485 - mae: 0.3390 - mean_pred: 0.8409 - mae_t1: 0.0226 - val_loss: 6.9234 - val_mae: 0.5048 - val_mean_pred: 0.7549 - val_mae_t1: 0.0337\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 4.8001 - mae: 0.3500 - mean_pred: 0.6653 - mae_t1: 0.0233 - val_loss: 7.0289 - val_mae: 0.5125 - val_mean_pred: 0.6900 - val_mae_t1: 0.0342\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 4.6522 - mae: 0.3392 - mean_pred: 0.7355 - mae_t1: 0.0226 - val_loss: 8.5481 - val_mae: 0.6233 - val_mean_pred: 0.9582 - val_mae_t1: 0.0416\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 6.6541 - mae: 0.4852 - mean_pred: 0.8948 - mae_t1: 0.0323 - val_loss: 7.1197 - val_mae: 0.5191 - val_mean_pred: 0.8558 - val_mae_t1: 0.0346\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 5.8706 - mae: 0.4281 - mean_pred: 0.8252 - mae_t1: 0.0285 - val_loss: 6.7300 - val_mae: 0.4907 - val_mean_pred: 0.7842 - val_mae_t1: 0.0327\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 4.1158 - mae: 0.3001 - mean_pred: 0.7469 - mae_t1: 0.0200 - val_loss: 6.9913 - val_mae: 0.5098 - val_mean_pred: 0.8747 - val_mae_t1: 0.0340\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 5.1650 - mae: 0.3766 - mean_pred: 0.8424 - mae_t1: 0.0251 - val_loss: 6.7846 - val_mae: 0.4947 - val_mean_pred: 0.8677 - val_mae_t1: 0.0330\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 4.3199 - mae: 0.3150 - mean_pred: 0.7766 - mae_t1: 0.0210 - val_loss: 6.5196 - val_mae: 0.4754 - val_mean_pred: 0.8831 - val_mae_t1: 0.0317\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 3.9717 - mae: 0.2896 - mean_pred: 0.8115 - mae_t1: 0.0193 - val_loss: 7.1290 - val_mae: 0.5198 - val_mean_pred: 0.8438 - val_mae_t1: 0.0347\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.4810 - mae: 0.3267 - mean_pred: 0.6861 - mae_t1: 0.0218 - val_loss: 6.8498 - val_mae: 0.4995 - val_mean_pred: 0.7957 - val_mae_t1: 0.0333\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 4.0257 - mae: 0.2935 - mean_pred: 0.7707 - mae_t1: 0.0196 - val_loss: 7.0008 - val_mae: 0.5105 - val_mean_pred: 0.9837 - val_mae_t1: 0.0340\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 4.0797 - mae: 0.2975 - mean_pred: 0.8977 - mae_t1: 0.0198 - val_loss: 6.3671 - val_mae: 0.4643 - val_mean_pred: 0.8966 - val_mae_t1: 0.0310\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.6843 - mae: 0.2686 - mean_pred: 0.8222 - mae_t1: 0.0179 - val_loss: 6.4237 - val_mae: 0.4684 - val_mean_pred: 0.9046 - val_mae_t1: 0.0312\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 4.0548 - mae: 0.2957 - mean_pred: 0.8682 - mae_t1: 0.0197 - val_loss: 7.1308 - val_mae: 0.5200 - val_mean_pred: 0.9261 - val_mae_t1: 0.0347\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 4.2524 - mae: 0.3101 - mean_pred: 0.8335 - mae_t1: 0.0207 - val_loss: 5.6952 - val_mae: 0.4153 - val_mean_pred: 0.7767 - val_mae_t1: 0.0277\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 4.2483 - mae: 0.3098 - mean_pred: 0.7048 - mae_t1: 0.0207 - val_loss: 8.2446 - val_mae: 0.6012 - val_mean_pred: 1.0110 - val_mae_t1: 0.0401\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 5.8732 - mae: 0.4283 - mean_pred: 0.9974 - mae_t1: 0.0286 - val_loss: 6.1009 - val_mae: 0.4449 - val_mean_pred: 0.8481 - val_mae_t1: 0.0297\n",
      "Earliness...\n",
      "0.0014996528625488281\n",
      "____________________________________________________________\n",
      "Test MAE:      0.34884404842384154  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▆█▅▅▃▃▃▄▃▃▂▂▂▃▂▃▂▄▂▂▂▄▄▁▁▁▁▁▂▁▂▂▂▂▄▁▁▁▁▃</td></tr><tr><td>mae</td><td>▆█▅▅▃▃▃▄▃▃▂▂▂▃▂▃▂▄▂▂▂▄▄▁▁▁▁▁▂▁▂▂▂▂▄▁▁▁▁▃</td></tr><tr><td>mae_t1</td><td>▆█▅▅▃▃▃▄▃▃▂▂▂▃▂▃▂▄▂▂▂▄▄▁▁▁▁▁▂▁▂▂▂▂▄▁▁▁▁▃</td></tr><tr><td>mean_pred</td><td>▁▂▂▁▅▄▅▃▆▃▄▂▄▁▃▆▄█▄▃▂▂▃▄▄▄▄▅▆▄▃▄▂▅▅▃▄▄▅▇</td></tr><tr><td>val_loss</td><td>▅▇▄█▄▂▆▇▄▂▄▂▅▃▂▂▃▂▅▁▁▅▂▂▂▃▂▂▂▃▄▅▄▃▄▃▄▃▄▁</td></tr><tr><td>val_mae</td><td>▅▇▄█▄▂▆▇▄▂▄▂▅▃▂▂▃▂▅▁▁▅▂▂▂▃▂▂▂▃▄▅▄▃▄▃▄▃▄▁</td></tr><tr><td>val_mae_t1</td><td>▅▇▄█▄▂▆▇▄▂▄▂▅▃▂▂▃▂▅▁▁▅▂▂▂▃▂▂▂▃▄▅▄▃▄▃▄▃▄▁</td></tr><tr><td>val_mean_pred</td><td>█▁▆█▃▅▃█▆▅▆▆▄▅▄▄▅▃▇▅▄▅▄▄▄▄▃▃▅▆▄▃▄▄▅▅▅▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.37783</td></tr><tr><td>AE_2</td><td>0.32305</td></tr><tr><td>AE_3</td><td>0.3471</td></tr><tr><td>MAE</td><td>0.34884</td></tr><tr><td>best_epoch</td><td>97</td></tr><tr><td>best_val_loss</td><td>5.69522</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.87315</td></tr><tr><td>mae</td><td>0.42825</td></tr><tr><td>mae_t1</td><td>0.02855</td></tr><tr><td>mean_pred</td><td>0.99744</td></tr><tr><td>val_loss</td><td>6.10085</td></tr><tr><td>val_mae</td><td>0.44485</td></tr><tr><td>val_mae_t1</td><td>0.02966</td></tr><tr><td>val_mean_pred</td><td>0.84814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">spring-leaf-79</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/26ctunk0\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/26ctunk0</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155231-26ctunk0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155301-2fetgjju</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fetgjju\" target=\"_blank\">cosmic-bush-80</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 720us/sample - loss: 10.6154 - mae: 0.7740 - mean_pred: 0.2498 - mae_t1: 0.0516 - val_loss: 8.0644 - val_mae: 0.5880 - val_mean_pred: 0.8779 - val_mae_t1: 0.0392\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 9.4732 - mae: 0.6908 - mean_pred: 0.8430 - mae_t1: 0.0461 - val_loss: 7.0010 - val_mae: 0.5105 - val_mean_pred: 0.9609 - val_mae_t1: 0.0340\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 146us/sample - loss: 8.7564 - mae: 0.6385 - mean_pred: 0.9382 - mae_t1: 0.0426 - val_loss: 9.0218 - val_mae: 0.6578 - val_mean_pred: 0.6567 - val_mae_t1: 0.0439\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 138us/sample - loss: 9.8345 - mae: 0.7171 - mean_pred: 0.5291 - mae_t1: 0.0478 - val_loss: 8.9974 - val_mae: 0.6561 - val_mean_pred: 0.3521 - val_mae_t1: 0.0437\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 120us/sample - loss: 8.8450 - mae: 0.6449 - mean_pred: 0.3711 - mae_t1: 0.0430 - val_loss: 9.1559 - val_mae: 0.6676 - val_mean_pred: 0.9135 - val_mae_t1: 0.0445\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 10.1835 - mae: 0.7425 - mean_pred: 0.9578 - mae_t1: 0.0495 - val_loss: 7.2898 - val_mae: 0.5315 - val_mean_pred: 1.0190 - val_mae_t1: 0.0354\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 8.7516 - mae: 0.6381 - mean_pred: 0.8927 - mae_t1: 0.0425 - val_loss: 11.7664 - val_mae: 0.8580 - val_mean_pred: 0.1469 - val_mae_t1: 0.0572\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 11.8453 - mae: 0.8637 - mean_pred: 0.1719 - mae_t1: 0.0576 - val_loss: 15.7144 - val_mae: 1.1458 - val_mean_pred: 0.3547 - val_mae_t1: 0.0764\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 13.8830 - mae: 1.0123 - mean_pred: 0.4527 - mae_t1: 0.0675 - val_loss: 11.8524 - val_mae: 0.8642 - val_mean_pred: 0.4952 - val_mae_t1: 0.0576\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 10.2086 - mae: 0.7444 - mean_pred: 0.5381 - mae_t1: 0.0496 - val_loss: 10.1191 - val_mae: 0.7379 - val_mean_pred: 1.2913 - val_mae_t1: 0.0492\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 11.0089 - mae: 0.8027 - mean_pred: 1.3009 - mae_t1: 0.0535 - val_loss: 8.8086 - val_mae: 0.6423 - val_mean_pred: 1.1592 - val_mae_t1: 0.0428\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 8.0273 - mae: 0.5853 - mean_pred: 0.8720 - mae_t1: 0.0390 - val_loss: 9.5276 - val_mae: 0.6947 - val_mean_pred: 0.4553 - val_mae_t1: 0.0463\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 8.7137 - mae: 0.6354 - mean_pred: 0.5120 - mae_t1: 0.0424 - val_loss: 7.8180 - val_mae: 0.5701 - val_mean_pred: 0.7207 - val_mae_t1: 0.0380\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 7.6815 - mae: 0.5601 - mean_pred: 0.7671 - mae_t1: 0.0373 - val_loss: 6.8002 - val_mae: 0.4958 - val_mean_pred: 0.7768 - val_mae_t1: 0.0331\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 7.0606 - mae: 0.5148 - mean_pred: 0.7265 - mae_t1: 0.0343 - val_loss: 7.6526 - val_mae: 0.5580 - val_mean_pred: 0.5357 - val_mae_t1: 0.0372\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 7.3130 - mae: 0.5332 - mean_pred: 0.5392 - mae_t1: 0.0355 - val_loss: 6.6914 - val_mae: 0.4879 - val_mean_pred: 0.9250 - val_mae_t1: 0.0325\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 7.4678 - mae: 0.5445 - mean_pred: 0.9728 - mae_t1: 0.0363 - val_loss: 8.9718 - val_mae: 0.6542 - val_mean_pred: 1.2567 - val_mae_t1: 0.0436\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 8.0177 - mae: 0.5846 - mean_pred: 1.0563 - mae_t1: 0.0390 - val_loss: 7.3076 - val_mae: 0.5328 - val_mean_pred: 0.6318 - val_mae_t1: 0.0355\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 7.5177 - mae: 0.5482 - mean_pred: 0.4743 - mae_t1: 0.0365 - val_loss: 8.1515 - val_mae: 0.5944 - val_mean_pred: 0.5982 - val_mae_t1: 0.0396\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 7.6212 - mae: 0.5557 - mean_pred: 0.5743 - mae_t1: 0.0370 - val_loss: 7.0902 - val_mae: 0.5170 - val_mean_pred: 1.0083 - val_mae_t1: 0.0345\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 7.3098 - mae: 0.5330 - mean_pred: 0.9062 - mae_t1: 0.0355 - val_loss: 7.6610 - val_mae: 0.5586 - val_mean_pred: 0.7572 - val_mae_t1: 0.0372\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 7.4936 - mae: 0.5464 - mean_pred: 0.5650 - mae_t1: 0.0364 - val_loss: 8.3759 - val_mae: 0.6107 - val_mean_pred: 0.5234 - val_mae_t1: 0.0407\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 8.4386 - mae: 0.6153 - mean_pred: 0.5255 - mae_t1: 0.0410 - val_loss: 8.6020 - val_mae: 0.6272 - val_mean_pred: 0.6604 - val_mae_t1: 0.0418\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 7.7835 - mae: 0.5675 - mean_pred: 0.5754 - mae_t1: 0.0378 - val_loss: 7.4271 - val_mae: 0.5416 - val_mean_pred: 0.8165 - val_mae_t1: 0.0361\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.3688 - mae: 0.4644 - mean_pred: 0.7597 - mae_t1: 0.0310 - val_loss: 7.8707 - val_mae: 0.5739 - val_mean_pred: 1.0406 - val_mae_t1: 0.0383\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 6.4338 - mae: 0.4691 - mean_pred: 0.8699 - mae_t1: 0.0313 - val_loss: 7.4832 - val_mae: 0.5457 - val_mean_pred: 0.8500 - val_mae_t1: 0.0364\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 6.2503 - mae: 0.4558 - mean_pred: 0.7395 - mae_t1: 0.0304 - val_loss: 7.2878 - val_mae: 0.5314 - val_mean_pred: 0.8135 - val_mae_t1: 0.0354\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.0880 - mae: 0.4439 - mean_pred: 0.6956 - mae_t1: 0.0296 - val_loss: 7.7898 - val_mae: 0.5680 - val_mean_pred: 0.8309 - val_mae_t1: 0.0379\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 6.6902 - mae: 0.4878 - mean_pred: 0.7333 - mae_t1: 0.0325 - val_loss: 7.3634 - val_mae: 0.5369 - val_mean_pred: 0.8860 - val_mae_t1: 0.0358\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.8771 - mae: 0.5015 - mean_pred: 0.8004 - mae_t1: 0.0334 - val_loss: 8.3606 - val_mae: 0.6096 - val_mean_pred: 1.0209 - val_mae_t1: 0.0406\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.9489 - mae: 0.5067 - mean_pred: 0.8866 - mae_t1: 0.0338 - val_loss: 7.0719 - val_mae: 0.5157 - val_mean_pred: 0.8185 - val_mae_t1: 0.0344\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 7.5146 - mae: 0.5479 - mean_pred: 0.6991 - mae_t1: 0.0365 - val_loss: 8.4559 - val_mae: 0.6166 - val_mean_pred: 0.5002 - val_mae_t1: 0.0411\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 7.9438 - mae: 0.5792 - mean_pred: 0.4355 - mae_t1: 0.0386 - val_loss: 8.4988 - val_mae: 0.6197 - val_mean_pred: 0.6380 - val_mae_t1: 0.0413\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 7.8664 - mae: 0.5736 - mean_pred: 0.6345 - mae_t1: 0.0382 - val_loss: 7.9930 - val_mae: 0.5828 - val_mean_pred: 1.1222 - val_mae_t1: 0.0389\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.2533 - mae: 0.5289 - mean_pred: 0.9979 - mae_t1: 0.0353 - val_loss: 9.3712 - val_mae: 0.6833 - val_mean_pred: 1.1437 - val_mae_t1: 0.0456\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 7.6580 - mae: 0.5584 - mean_pred: 0.9837 - mae_t1: 0.0372 - val_loss: 6.9544 - val_mae: 0.5071 - val_mean_pred: 0.7529 - val_mae_t1: 0.0338\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.4156 - mae: 0.4678 - mean_pred: 0.6053 - mae_t1: 0.0312 - val_loss: 8.3424 - val_mae: 0.6083 - val_mean_pred: 0.5742 - val_mae_t1: 0.0406\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.9115 - mae: 0.5040 - mean_pred: 0.5746 - mae_t1: 0.0336 - val_loss: 7.1012 - val_mae: 0.5178 - val_mean_pred: 0.8497 - val_mae_t1: 0.0345\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 6.0371 - mae: 0.4402 - mean_pred: 0.7894 - mae_t1: 0.0293 - val_loss: 6.6382 - val_mae: 0.4840 - val_mean_pred: 0.8772 - val_mae_t1: 0.0323\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.6095 - mae: 0.4090 - mean_pred: 0.8037 - mae_t1: 0.0273 - val_loss: 6.8552 - val_mae: 0.4999 - val_mean_pred: 0.9155 - val_mae_t1: 0.0333\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.2424 - mae: 0.4552 - mean_pred: 0.8264 - mae_t1: 0.0303 - val_loss: 6.9257 - val_mae: 0.5050 - val_mean_pred: 0.8938 - val_mae_t1: 0.0337\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 5.8355 - mae: 0.4255 - mean_pred: 0.7764 - mae_t1: 0.0284 - val_loss: 7.0570 - val_mae: 0.5146 - val_mean_pred: 0.7337 - val_mae_t1: 0.0343\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.2712 - mae: 0.4573 - mean_pred: 0.6411 - mae_t1: 0.0305 - val_loss: 7.2385 - val_mae: 0.5278 - val_mean_pred: 0.6046 - val_mae_t1: 0.0352\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.5468 - mae: 0.4774 - mean_pred: 0.5671 - mae_t1: 0.0318 - val_loss: 7.0479 - val_mae: 0.5139 - val_mean_pred: 0.8421 - val_mae_t1: 0.0343\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 6.1658 - mae: 0.4496 - mean_pred: 0.7990 - mae_t1: 0.0300 - val_loss: 8.7725 - val_mae: 0.6397 - val_mean_pred: 1.2010 - val_mae_t1: 0.0426\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 7.9500 - mae: 0.5797 - mean_pred: 1.0767 - mae_t1: 0.0386 - val_loss: 8.8433 - val_mae: 0.6448 - val_mean_pred: 1.1024 - val_mae_t1: 0.0430\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.3568 - mae: 0.5364 - mean_pred: 0.8753 - mae_t1: 0.0358 - val_loss: 8.8753 - val_mae: 0.6472 - val_mean_pred: 0.7690 - val_mae_t1: 0.0431\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 9.2108 - mae: 0.6716 - mean_pred: 0.6854 - mae_t1: 0.0448 - val_loss: 10.2214 - val_mae: 0.7453 - val_mean_pred: 0.9229 - val_mae_t1: 0.0497\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 8.2635 - mae: 0.6025 - mean_pred: 0.8201 - mae_t1: 0.0402 - val_loss: 9.1555 - val_mae: 0.6676 - val_mean_pred: 1.1469 - val_mae_t1: 0.0445\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 7.6413 - mae: 0.5572 - mean_pred: 0.9930 - mae_t1: 0.0371 - val_loss: 8.5837 - val_mae: 0.6259 - val_mean_pred: 1.0251 - val_mae_t1: 0.0417\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.7829 - mae: 0.4946 - mean_pred: 0.8762 - mae_t1: 0.0330 - val_loss: 9.3580 - val_mae: 0.6824 - val_mean_pred: 0.8560 - val_mae_t1: 0.0455\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 7.2477 - mae: 0.5285 - mean_pred: 0.7743 - mae_t1: 0.0352 - val_loss: 7.7312 - val_mae: 0.5637 - val_mean_pred: 0.8530 - val_mae_t1: 0.0376\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 6.5412 - mae: 0.4770 - mean_pred: 0.8081 - mae_t1: 0.0318 - val_loss: 7.9840 - val_mae: 0.5822 - val_mean_pred: 0.8481 - val_mae_t1: 0.0388\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.1695 - mae: 0.4499 - mean_pred: 0.7423 - mae_t1: 0.0300 - val_loss: 8.0451 - val_mae: 0.5866 - val_mean_pred: 0.7314 - val_mae_t1: 0.0391\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 6.9496 - mae: 0.5067 - mean_pred: 0.6118 - mae_t1: 0.0338 - val_loss: 7.3751 - val_mae: 0.5378 - val_mean_pred: 0.8129 - val_mae_t1: 0.0359\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.3606 - mae: 0.4638 - mean_pred: 0.7559 - mae_t1: 0.0309 - val_loss: 8.1264 - val_mae: 0.5926 - val_mean_pred: 1.1553 - val_mae_t1: 0.0395\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 6.5677 - mae: 0.4789 - mean_pred: 0.9980 - mae_t1: 0.0319 - val_loss: 6.7597 - val_mae: 0.4929 - val_mean_pred: 0.9640 - val_mae_t1: 0.0329\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.7843 - mae: 0.4218 - mean_pred: 0.7699 - mae_t1: 0.0281 - val_loss: 7.4029 - val_mae: 0.5398 - val_mean_pred: 0.7036 - val_mae_t1: 0.0360\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.9030 - mae: 0.4304 - mean_pred: 0.6221 - mae_t1: 0.0287 - val_loss: 7.1509 - val_mae: 0.5214 - val_mean_pred: 0.8993 - val_mae_t1: 0.0348\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.3913 - mae: 0.3931 - mean_pred: 0.8228 - mae_t1: 0.0262 - val_loss: 7.9893 - val_mae: 0.5826 - val_mean_pred: 0.9854 - val_mae_t1: 0.0388\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.9000 - mae: 0.4302 - mean_pred: 0.8265 - mae_t1: 0.0287 - val_loss: 8.3015 - val_mae: 0.6053 - val_mean_pred: 0.9032 - val_mae_t1: 0.0404\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 6.5518 - mae: 0.4777 - mean_pred: 0.8412 - mae_t1: 0.0318 - val_loss: 7.4956 - val_mae: 0.5466 - val_mean_pred: 0.9477 - val_mae_t1: 0.0364\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.0513 - mae: 0.4412 - mean_pred: 0.8106 - mae_t1: 0.0294 - val_loss: 8.1277 - val_mae: 0.5926 - val_mean_pred: 0.9110 - val_mae_t1: 0.0395\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 6.4254 - mae: 0.4685 - mean_pred: 0.8005 - mae_t1: 0.0312 - val_loss: 6.5049 - val_mae: 0.4743 - val_mean_pred: 0.9252 - val_mae_t1: 0.0316\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.6737 - mae: 0.4137 - mean_pred: 0.8642 - mae_t1: 0.0276 - val_loss: 7.5354 - val_mae: 0.5495 - val_mean_pred: 1.0210 - val_mae_t1: 0.0366\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 6.7826 - mae: 0.4946 - mean_pred: 0.9603 - mae_t1: 0.0330 - val_loss: 7.0196 - val_mae: 0.5118 - val_mean_pred: 0.9661 - val_mae_t1: 0.0341\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.5107 - mae: 0.4018 - mean_pred: 0.8609 - mae_t1: 0.0268 - val_loss: 7.5846 - val_mae: 0.5530 - val_mean_pred: 0.6806 - val_mae_t1: 0.0369\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 5.8156 - mae: 0.4241 - mean_pred: 0.5997 - mae_t1: 0.0283 - val_loss: 8.3981 - val_mae: 0.6124 - val_mean_pred: 0.5962 - val_mae_t1: 0.0408\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 5.8618 - mae: 0.4274 - mean_pred: 0.5995 - mae_t1: 0.0285 - val_loss: 7.4619 - val_mae: 0.5441 - val_mean_pred: 0.9946 - val_mae_t1: 0.0363\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.8658 - mae: 0.4277 - mean_pred: 0.9502 - mae_t1: 0.0285 - val_loss: 9.8578 - val_mae: 0.7188 - val_mean_pred: 1.2942 - val_mae_t1: 0.0479\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 7.2188 - mae: 0.5264 - mean_pred: 1.1317 - mae_t1: 0.0351 - val_loss: 6.8368 - val_mae: 0.4985 - val_mean_pred: 0.9076 - val_mae_t1: 0.0332\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.0343 - mae: 0.3671 - mean_pred: 0.7679 - mae_t1: 0.0245 - val_loss: 8.0139 - val_mae: 0.5843 - val_mean_pred: 0.6078 - val_mae_t1: 0.0390\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 6.0356 - mae: 0.4401 - mean_pred: 0.5743 - mae_t1: 0.0293 - val_loss: 6.7003 - val_mae: 0.4886 - val_mean_pred: 0.7416 - val_mae_t1: 0.0326\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.9902 - mae: 0.3639 - mean_pred: 0.7164 - mae_t1: 0.0243 - val_loss: 6.6055 - val_mae: 0.4817 - val_mean_pred: 0.9680 - val_mae_t1: 0.0321\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1486 - mae: 0.3754 - mean_pred: 0.8408 - mae_t1: 0.0250 - val_loss: 6.8235 - val_mae: 0.4975 - val_mean_pred: 0.9131 - val_mae_t1: 0.0332\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.8631 - mae: 0.3546 - mean_pred: 0.7463 - mae_t1: 0.0236 - val_loss: 7.4203 - val_mae: 0.5411 - val_mean_pred: 0.9147 - val_mae_t1: 0.0361\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.3340 - mae: 0.3889 - mean_pred: 0.7678 - mae_t1: 0.0259 - val_loss: 8.1552 - val_mae: 0.5947 - val_mean_pred: 1.1108 - val_mae_t1: 0.0396\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 6.1757 - mae: 0.4503 - mean_pred: 0.9220 - mae_t1: 0.0300 - val_loss: 9.8012 - val_mae: 0.7147 - val_mean_pred: 1.2263 - val_mae_t1: 0.0476\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 6.7786 - mae: 0.4943 - mean_pred: 0.9863 - mae_t1: 0.0330 - val_loss: 7.5597 - val_mae: 0.5512 - val_mean_pred: 1.1279 - val_mae_t1: 0.0367\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 5.5929 - mae: 0.4078 - mean_pred: 0.9570 - mae_t1: 0.0272 - val_loss: 5.6943 - val_mae: 0.4152 - val_mean_pred: 0.9246 - val_mae_t1: 0.0277\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.1918 - mae: 0.3786 - mean_pred: 0.8061 - mae_t1: 0.0252 - val_loss: 6.9345 - val_mae: 0.5056 - val_mean_pred: 0.6872 - val_mae_t1: 0.0337\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 5.7121 - mae: 0.4165 - mean_pred: 0.6230 - mae_t1: 0.0278 - val_loss: 7.6268 - val_mae: 0.5561 - val_mean_pred: 0.6699 - val_mae_t1: 0.0371\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 5.4711 - mae: 0.3989 - mean_pred: 0.6558 - mae_t1: 0.0266 - val_loss: 6.1446 - val_mae: 0.4480 - val_mean_pred: 0.9192 - val_mae_t1: 0.0299\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 5.1303 - mae: 0.3741 - mean_pred: 0.9084 - mae_t1: 0.0249 - val_loss: 7.5865 - val_mae: 0.5532 - val_mean_pred: 1.1210 - val_mae_t1: 0.0369\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 6.3255 - mae: 0.4612 - mean_pred: 1.0116 - mae_t1: 0.0307 - val_loss: 7.6417 - val_mae: 0.5572 - val_mean_pred: 0.9661 - val_mae_t1: 0.0371\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 5.6514 - mae: 0.4121 - mean_pred: 0.8160 - mae_t1: 0.0275 - val_loss: 7.6192 - val_mae: 0.5556 - val_mean_pred: 0.7387 - val_mae_t1: 0.0370\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.8202 - mae: 0.4244 - mean_pred: 0.6706 - mae_t1: 0.0283 - val_loss: 7.5198 - val_mae: 0.5483 - val_mean_pred: 0.7543 - val_mae_t1: 0.0366\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.6663 - mae: 0.4132 - mean_pred: 0.6960 - mae_t1: 0.0275 - val_loss: 7.1117 - val_mae: 0.5186 - val_mean_pred: 0.7973 - val_mae_t1: 0.0346\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.2791 - mae: 0.3849 - mean_pred: 0.6978 - mae_t1: 0.0257 - val_loss: 7.5109 - val_mae: 0.5477 - val_mean_pred: 0.8643 - val_mae_t1: 0.0365\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.7730 - mae: 0.4209 - mean_pred: 0.7484 - mae_t1: 0.0281 - val_loss: 6.9087 - val_mae: 0.5038 - val_mean_pred: 0.9389 - val_mae_t1: 0.0336\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 5.0602 - mae: 0.3690 - mean_pred: 0.8127 - mae_t1: 0.0246 - val_loss: 6.7156 - val_mae: 0.4897 - val_mean_pred: 1.0091 - val_mae_t1: 0.0326\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.8980 - mae: 0.3571 - mean_pred: 0.8688 - mae_t1: 0.0238 - val_loss: 6.4156 - val_mae: 0.4678 - val_mean_pred: 0.9292 - val_mae_t1: 0.0312\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.7259 - mae: 0.3446 - mean_pred: 0.7514 - mae_t1: 0.0230 - val_loss: 6.5663 - val_mae: 0.4788 - val_mean_pred: 0.8926 - val_mae_t1: 0.0319\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0390 - mae: 0.3674 - mean_pred: 0.7966 - mae_t1: 0.0245 - val_loss: 8.7749 - val_mae: 0.6398 - val_mean_pred: 1.1774 - val_mae_t1: 0.0427\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 6.3484 - mae: 0.4629 - mean_pred: 1.0180 - mae_t1: 0.0309 - val_loss: 7.4483 - val_mae: 0.5431 - val_mean_pred: 1.0880 - val_mae_t1: 0.0362\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 5.8218 - mae: 0.4245 - mean_pred: 0.8917 - mae_t1: 0.0283 - val_loss: 7.1643 - val_mae: 0.5224 - val_mean_pred: 0.7922 - val_mae_t1: 0.0348\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 5.5467 - mae: 0.4044 - mean_pred: 0.6828 - mae_t1: 0.0270 - val_loss: 7.3950 - val_mae: 0.5392 - val_mean_pred: 0.7352 - val_mae_t1: 0.0359\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.2363 - mae: 0.3818 - mean_pred: 0.6342 - mae_t1: 0.0255 - val_loss: 6.6387 - val_mae: 0.4841 - val_mean_pred: 0.8997 - val_mae_t1: 0.0323\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.7059 - mae: 0.3431 - mean_pred: 0.8270 - mae_t1: 0.0229 - val_loss: 6.9851 - val_mae: 0.5093 - val_mean_pred: 1.0140 - val_mae_t1: 0.0340\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.0814 - mae: 0.3705 - mean_pred: 0.8766 - mae_t1: 0.0247 - val_loss: 6.1512 - val_mae: 0.4485 - val_mean_pred: 0.7564 - val_mae_t1: 0.0299\n",
      "Earliness...\n",
      "0.0020024776458740234\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3500955300953886  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▇▅▆█▇▅▃▄▃▅▃▂▃▄▄▂▂▃▄▄▃▂▂▂▂▃▂▂▁▁▁▃▂▁▂▁▁▁▂▁</td></tr><tr><td>mae</td><td>▇▅▆█▇▅▃▄▃▅▃▂▃▄▄▂▂▃▄▄▃▂▂▂▂▃▂▂▁▁▁▃▂▁▂▁▁▁▂▁</td></tr><tr><td>mae_t1</td><td>▇▅▆█▇▅▃▄▃▅▃▂▃▄▄▂▂▃▄▄▃▂▂▂▂▃▂▂▁▁▁▃▂▁▂▁▁▁▂▁</td></tr><tr><td>mean_pred</td><td>▁▆▆▁█▃▃▆▆▃▅▄▅▄▆▅▅▃▇▅▅▅▅▄▅▅▅▄▅▄▅▆▄▆▄▄▅▅▄▅</td></tr><tr><td>val_loss</td><td>▂▃▂█▃▂▁▂▂▃▂▂▂▂▂▁▂▂▃▃▃▂▂▂▃▁▂▂▂▁▂▂▂▂▂▂▁▃▂▁</td></tr><tr><td>val_mae</td><td>▂▃▂█▃▂▁▂▂▃▂▂▂▂▂▁▂▂▃▃▃▂▂▂▃▁▂▂▂▁▂▂▂▂▂▂▁▃▂▁</td></tr><tr><td>val_mae_t1</td><td>▂▃▂█▃▂▁▂▂▃▂▂▂▂▂▁▂▂▃▃▃▂▂▂▃▁▂▂▂▁▂▂▂▂▂▂▁▃▂▁</td></tr><tr><td>val_mean_pred</td><td>▅▄▇▁█▄▆▃▄▄▅▅▅█▄▅▆▅▇█▅▄█▆▆▆▄▆▃▆▇█▄█▄▅▆█▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.39433</td></tr><tr><td>AE_2</td><td>0.3077</td></tr><tr><td>AE_3</td><td>0.34517</td></tr><tr><td>MAE</td><td>0.3501</td></tr><tr><td>best_epoch</td><td>79</td></tr><tr><td>best_val_loss</td><td>5.6943</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.08137</td></tr><tr><td>mae</td><td>0.37052</td></tr><tr><td>mae_t1</td><td>0.0247</td></tr><tr><td>mean_pred</td><td>0.87662</td></tr><tr><td>val_loss</td><td>6.15124</td></tr><tr><td>val_mae</td><td>0.44853</td></tr><tr><td>val_mae_t1</td><td>0.0299</td></tr><tr><td>val_mean_pred</td><td>0.75639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cosmic-bush-80</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fetgjju\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2fetgjju</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155301-2fetgjju\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155326-8olslxv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/8olslxv4\" target=\"_blank\">smooth-surf-81</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 852us/sample - loss: 1.2015 - mae: 0.7134 - mean_pred: 0.2494 - mae_t1: 0.0476 - val_loss: 0.9378 - val_mae: 0.5568 - val_mean_pred: 0.5099 - val_mae_t1: 0.0371\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 0.9805 - mae: 0.5822 - mean_pred: 0.6031 - mae_t1: 0.0388 - val_loss: 0.8217 - val_mae: 0.4879 - val_mean_pred: 0.8038 - val_mae_t1: 0.0325\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 0.9198 - mae: 0.5461 - mean_pred: 0.7246 - mae_t1: 0.0364 - val_loss: 0.8035 - val_mae: 0.4771 - val_mean_pred: 0.6738 - val_mae_t1: 0.0318\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.8750 - mae: 0.5195 - mean_pred: 0.5939 - mae_t1: 0.0346 - val_loss: 0.8405 - val_mae: 0.4991 - val_mean_pred: 0.6256 - val_mae_t1: 0.0333\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.8372 - mae: 0.4971 - mean_pred: 0.6025 - mae_t1: 0.0331 - val_loss: 0.7910 - val_mae: 0.4697 - val_mean_pred: 0.7595 - val_mae_t1: 0.0313\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.7818 - mae: 0.4642 - mean_pred: 0.7225 - mae_t1: 0.0309 - val_loss: 0.7882 - val_mae: 0.4680 - val_mean_pred: 0.8463 - val_mae_t1: 0.0312\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7671 - mae: 0.4555 - mean_pred: 0.7863 - mae_t1: 0.0304 - val_loss: 0.7973 - val_mae: 0.4734 - val_mean_pred: 0.8400 - val_mae_t1: 0.0316\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.7481 - mae: 0.4442 - mean_pred: 0.7507 - mae_t1: 0.0296 - val_loss: 0.8302 - val_mae: 0.4929 - val_mean_pred: 0.8084 - val_mae_t1: 0.0329\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.7398 - mae: 0.4392 - mean_pred: 0.7651 - mae_t1: 0.0293 - val_loss: 0.8087 - val_mae: 0.4802 - val_mean_pred: 0.8744 - val_mae_t1: 0.0320\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.7355 - mae: 0.4367 - mean_pred: 0.8514 - mae_t1: 0.0291 - val_loss: 0.8205 - val_mae: 0.4872 - val_mean_pred: 0.9248 - val_mae_t1: 0.0325\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.7422 - mae: 0.4407 - mean_pred: 0.8880 - mae_t1: 0.0294 - val_loss: 0.8084 - val_mae: 0.4800 - val_mean_pred: 0.8321 - val_mae_t1: 0.0320\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 0.7025 - mae: 0.4171 - mean_pred: 0.7633 - mae_t1: 0.0278 - val_loss: 0.8067 - val_mae: 0.4790 - val_mean_pred: 0.7908 - val_mae_t1: 0.0319\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 0.6877 - mae: 0.4083 - mean_pred: 0.7876 - mae_t1: 0.0272 - val_loss: 0.7535 - val_mae: 0.4474 - val_mean_pred: 0.8771 - val_mae_t1: 0.0298\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6767 - mae: 0.4018 - mean_pred: 0.8118 - mae_t1: 0.0268 - val_loss: 0.7839 - val_mae: 0.4654 - val_mean_pred: 0.8061 - val_mae_t1: 0.0310\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6683 - mae: 0.3968 - mean_pred: 0.7572 - mae_t1: 0.0265 - val_loss: 0.7893 - val_mae: 0.4686 - val_mean_pred: 0.7929 - val_mae_t1: 0.0312\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 0.6682 - mae: 0.3968 - mean_pred: 0.7513 - mae_t1: 0.0265 - val_loss: 0.7514 - val_mae: 0.4461 - val_mean_pred: 0.8398 - val_mae_t1: 0.0297\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6593 - mae: 0.3915 - mean_pred: 0.8096 - mae_t1: 0.0261 - val_loss: 0.7786 - val_mae: 0.4623 - val_mean_pred: 0.8266 - val_mae_t1: 0.0308\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6683 - mae: 0.3968 - mean_pred: 0.7648 - mae_t1: 0.0265 - val_loss: 0.8423 - val_mae: 0.5001 - val_mean_pred: 0.8082 - val_mae_t1: 0.0333\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.6705 - mae: 0.3981 - mean_pred: 0.7742 - mae_t1: 0.0265 - val_loss: 0.7846 - val_mae: 0.4659 - val_mean_pred: 0.8540 - val_mae_t1: 0.0311\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6397 - mae: 0.3798 - mean_pred: 0.7736 - mae_t1: 0.0253 - val_loss: 0.8181 - val_mae: 0.4858 - val_mean_pred: 0.7926 - val_mae_t1: 0.0324\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6569 - mae: 0.3900 - mean_pred: 0.6961 - mae_t1: 0.0260 - val_loss: 0.8126 - val_mae: 0.4825 - val_mean_pred: 0.8015 - val_mae_t1: 0.0322\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.6360 - mae: 0.3776 - mean_pred: 0.7484 - mae_t1: 0.0252 - val_loss: 0.7589 - val_mae: 0.4506 - val_mean_pred: 0.8894 - val_mae_t1: 0.0300\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.6340 - mae: 0.3765 - mean_pred: 0.7950 - mae_t1: 0.0251 - val_loss: 0.7498 - val_mae: 0.4452 - val_mean_pred: 0.8504 - val_mae_t1: 0.0297\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 0.6319 - mae: 0.3752 - mean_pred: 0.8232 - mae_t1: 0.0250 - val_loss: 0.7252 - val_mae: 0.4306 - val_mean_pred: 0.9027 - val_mae_t1: 0.0287\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6330 - mae: 0.3758 - mean_pred: 0.8360 - mae_t1: 0.0251 - val_loss: 0.7853 - val_mae: 0.4663 - val_mean_pred: 0.7618 - val_mae_t1: 0.0311\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.6435 - mae: 0.3821 - mean_pred: 0.6861 - mae_t1: 0.0255 - val_loss: 0.7780 - val_mae: 0.4620 - val_mean_pred: 0.8279 - val_mae_t1: 0.0308\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.6191 - mae: 0.3676 - mean_pred: 0.8245 - mae_t1: 0.0245 - val_loss: 0.7745 - val_mae: 0.4599 - val_mean_pred: 0.9107 - val_mae_t1: 0.0307\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6398 - mae: 0.3799 - mean_pred: 0.7946 - mae_t1: 0.0253 - val_loss: 0.7682 - val_mae: 0.4561 - val_mean_pred: 0.8390 - val_mae_t1: 0.0304\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6210 - mae: 0.3687 - mean_pred: 0.7913 - mae_t1: 0.0246 - val_loss: 0.7571 - val_mae: 0.4495 - val_mean_pred: 0.8793 - val_mae_t1: 0.0300\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6187 - mae: 0.3673 - mean_pred: 0.8233 - mae_t1: 0.0245 - val_loss: 0.7814 - val_mae: 0.4640 - val_mean_pred: 0.8960 - val_mae_t1: 0.0309\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6187 - mae: 0.3673 - mean_pred: 0.8424 - mae_t1: 0.0245 - val_loss: 0.7634 - val_mae: 0.4532 - val_mean_pred: 0.8546 - val_mae_t1: 0.0302\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6005 - mae: 0.3565 - mean_pred: 0.7697 - mae_t1: 0.0238 - val_loss: 0.8068 - val_mae: 0.4790 - val_mean_pred: 0.7757 - val_mae_t1: 0.0319\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.6269 - mae: 0.3722 - mean_pred: 0.6971 - mae_t1: 0.0248 - val_loss: 0.7419 - val_mae: 0.4405 - val_mean_pred: 0.8864 - val_mae_t1: 0.0294\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.7041 - mae: 0.4181 - mean_pred: 0.9035 - mae_t1: 0.0279 - val_loss: 0.7927 - val_mae: 0.4707 - val_mean_pred: 0.9985 - val_mae_t1: 0.0314\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.6411 - mae: 0.3807 - mean_pred: 0.8516 - mae_t1: 0.0254 - val_loss: 0.8453 - val_mae: 0.5019 - val_mean_pred: 0.7506 - val_mae_t1: 0.0335\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.6624 - mae: 0.3933 - mean_pred: 0.6737 - mae_t1: 0.0262 - val_loss: 0.7949 - val_mae: 0.4719 - val_mean_pred: 0.7637 - val_mae_t1: 0.0315\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 0.5976 - mae: 0.3548 - mean_pred: 0.7429 - mae_t1: 0.0237 - val_loss: 0.7222 - val_mae: 0.4288 - val_mean_pred: 0.9169 - val_mae_t1: 0.0286\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6032 - mae: 0.3581 - mean_pred: 0.8565 - mae_t1: 0.0239 - val_loss: 0.7532 - val_mae: 0.4472 - val_mean_pred: 0.8027 - val_mae_t1: 0.0298\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.6121 - mae: 0.3634 - mean_pred: 0.7025 - mae_t1: 0.0242 - val_loss: 0.7732 - val_mae: 0.4591 - val_mean_pred: 0.7388 - val_mae_t1: 0.0306\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.6208 - mae: 0.3686 - mean_pred: 0.6986 - mae_t1: 0.0246 - val_loss: 0.7318 - val_mae: 0.4345 - val_mean_pred: 0.8610 - val_mae_t1: 0.0290\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.5982 - mae: 0.3552 - mean_pred: 0.8527 - mae_t1: 0.0237 - val_loss: 0.7867 - val_mae: 0.4671 - val_mean_pred: 0.8947 - val_mae_t1: 0.0311\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.6032 - mae: 0.3582 - mean_pred: 0.7560 - mae_t1: 0.0239 - val_loss: 0.8240 - val_mae: 0.4893 - val_mean_pred: 0.7323 - val_mae_t1: 0.0326\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 0.6180 - mae: 0.3669 - mean_pred: 0.6581 - mae_t1: 0.0245 - val_loss: 0.7989 - val_mae: 0.4744 - val_mean_pred: 0.8002 - val_mae_t1: 0.0316\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.6224 - mae: 0.3695 - mean_pred: 0.7310 - mae_t1: 0.0246 - val_loss: 0.7739 - val_mae: 0.4595 - val_mean_pred: 0.8335 - val_mae_t1: 0.0306\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.6092 - mae: 0.3617 - mean_pred: 0.7035 - mae_t1: 0.0241 - val_loss: 0.7865 - val_mae: 0.4670 - val_mean_pred: 0.8352 - val_mae_t1: 0.0311\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.5884 - mae: 0.3494 - mean_pred: 0.7564 - mae_t1: 0.0233 - val_loss: 0.7451 - val_mae: 0.4424 - val_mean_pred: 0.9066 - val_mae_t1: 0.0295\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.6001 - mae: 0.3563 - mean_pred: 0.8325 - mae_t1: 0.0238 - val_loss: 0.7724 - val_mae: 0.4586 - val_mean_pred: 0.8416 - val_mae_t1: 0.0306\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.6011 - mae: 0.3569 - mean_pred: 0.7363 - mae_t1: 0.0238 - val_loss: 0.7895 - val_mae: 0.4688 - val_mean_pred: 0.7719 - val_mae_t1: 0.0313\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5916 - mae: 0.3513 - mean_pred: 0.7327 - mae_t1: 0.0234 - val_loss: 0.7550 - val_mae: 0.4483 - val_mean_pred: 0.8305 - val_mae_t1: 0.0299\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.5675 - mae: 0.3370 - mean_pred: 0.7709 - mae_t1: 0.0225 - val_loss: 0.7952 - val_mae: 0.4721 - val_mean_pred: 0.8634 - val_mae_t1: 0.0315\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5790 - mae: 0.3438 - mean_pred: 0.7890 - mae_t1: 0.0229 - val_loss: 0.7880 - val_mae: 0.4679 - val_mean_pred: 0.9462 - val_mae_t1: 0.0312\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.5778 - mae: 0.3431 - mean_pred: 0.8631 - mae_t1: 0.0229 - val_loss: 0.7424 - val_mae: 0.4408 - val_mean_pred: 0.8818 - val_mae_t1: 0.0294\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5495 - mae: 0.3263 - mean_pred: 0.7454 - mae_t1: 0.0218 - val_loss: 0.7855 - val_mae: 0.4664 - val_mean_pred: 0.7563 - val_mae_t1: 0.0311\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5493 - mae: 0.3262 - mean_pred: 0.7173 - mae_t1: 0.0217 - val_loss: 0.7475 - val_mae: 0.4438 - val_mean_pred: 0.9344 - val_mae_t1: 0.0296\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5924 - mae: 0.3518 - mean_pred: 0.8824 - mae_t1: 0.0235 - val_loss: 0.7532 - val_mae: 0.4472 - val_mean_pred: 0.9046 - val_mae_t1: 0.0298\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5465 - mae: 0.3245 - mean_pred: 0.7706 - mae_t1: 0.0216 - val_loss: 0.8557 - val_mae: 0.5081 - val_mean_pred: 0.6945 - val_mae_t1: 0.0339\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.6311 - mae: 0.3747 - mean_pred: 0.6158 - mae_t1: 0.0250 - val_loss: 0.8251 - val_mae: 0.4899 - val_mean_pred: 0.7418 - val_mae_t1: 0.0327\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5414 - mae: 0.3215 - mean_pred: 0.7587 - mae_t1: 0.0214 - val_loss: 0.7845 - val_mae: 0.4658 - val_mean_pred: 0.9993 - val_mae_t1: 0.0311\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 0.6451 - mae: 0.3830 - mean_pred: 0.9259 - mae_t1: 0.0255 - val_loss: 0.7209 - val_mae: 0.4280 - val_mean_pred: 0.8365 - val_mae_t1: 0.0285\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.5907 - mae: 0.3507 - mean_pred: 0.7035 - mae_t1: 0.0234 - val_loss: 0.8713 - val_mae: 0.5173 - val_mean_pred: 0.6904 - val_mae_t1: 0.0345\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.5938 - mae: 0.3526 - mean_pred: 0.6709 - mae_t1: 0.0235 - val_loss: 0.7588 - val_mae: 0.4505 - val_mean_pred: 0.8615 - val_mae_t1: 0.0300\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5837 - mae: 0.3465 - mean_pred: 0.7848 - mae_t1: 0.0231 - val_loss: 0.7701 - val_mae: 0.4573 - val_mean_pred: 0.8499 - val_mae_t1: 0.0305\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5527 - mae: 0.3282 - mean_pred: 0.7555 - mae_t1: 0.0219 - val_loss: 0.8060 - val_mae: 0.4786 - val_mean_pred: 0.8923 - val_mae_t1: 0.0319\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.5549 - mae: 0.3295 - mean_pred: 0.8152 - mae_t1: 0.0220 - val_loss: 0.7313 - val_mae: 0.4342 - val_mean_pred: 0.9102 - val_mae_t1: 0.0289\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5717 - mae: 0.3394 - mean_pred: 0.7823 - mae_t1: 0.0226 - val_loss: 0.7755 - val_mae: 0.4604 - val_mean_pred: 0.7459 - val_mae_t1: 0.0307\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5620 - mae: 0.3337 - mean_pred: 0.7142 - mae_t1: 0.0222 - val_loss: 0.7560 - val_mae: 0.4489 - val_mean_pred: 0.8603 - val_mae_t1: 0.0299\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 0.5548 - mae: 0.3294 - mean_pred: 0.8040 - mae_t1: 0.0220 - val_loss: 0.7154 - val_mae: 0.4248 - val_mean_pred: 0.9093 - val_mae_t1: 0.0283\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 0.5507 - mae: 0.3270 - mean_pred: 0.7934 - mae_t1: 0.0218 - val_loss: 0.7448 - val_mae: 0.4422 - val_mean_pred: 0.7880 - val_mae_t1: 0.0295\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5497 - mae: 0.3264 - mean_pred: 0.6787 - mae_t1: 0.0218 - val_loss: 0.7920 - val_mae: 0.4702 - val_mean_pred: 0.7657 - val_mae_t1: 0.0313\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.5434 - mae: 0.3226 - mean_pred: 0.7007 - mae_t1: 0.0215 - val_loss: 0.7360 - val_mae: 0.4370 - val_mean_pred: 0.8830 - val_mae_t1: 0.0291\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5144 - mae: 0.3054 - mean_pred: 0.7947 - mae_t1: 0.0204 - val_loss: 0.7337 - val_mae: 0.4356 - val_mean_pred: 0.9013 - val_mae_t1: 0.0290\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.5275 - mae: 0.3132 - mean_pred: 0.8438 - mae_t1: 0.0209 - val_loss: 0.7274 - val_mae: 0.4319 - val_mean_pred: 0.9061 - val_mae_t1: 0.0288\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.5131 - mae: 0.3047 - mean_pred: 0.8284 - mae_t1: 0.0203 - val_loss: 0.7226 - val_mae: 0.4290 - val_mean_pred: 0.8534 - val_mae_t1: 0.0286\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 0.5004 - mae: 0.2971 - mean_pred: 0.7852 - mae_t1: 0.0198 - val_loss: 0.7291 - val_mae: 0.4329 - val_mean_pred: 0.9263 - val_mae_t1: 0.0289\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 0.5090 - mae: 0.3022 - mean_pred: 0.8614 - mae_t1: 0.0201 - val_loss: 0.7098 - val_mae: 0.4214 - val_mean_pred: 0.8934 - val_mae_t1: 0.0281\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.5083 - mae: 0.3018 - mean_pred: 0.7716 - mae_t1: 0.0201 - val_loss: 0.7155 - val_mae: 0.4248 - val_mean_pred: 0.8433 - val_mae_t1: 0.0283\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.5146 - mae: 0.3055 - mean_pred: 0.8177 - mae_t1: 0.0204 - val_loss: 0.7356 - val_mae: 0.4368 - val_mean_pred: 0.9743 - val_mae_t1: 0.0291\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5031 - mae: 0.2987 - mean_pred: 0.8328 - mae_t1: 0.0199 - val_loss: 0.7362 - val_mae: 0.4371 - val_mean_pred: 0.7764 - val_mae_t1: 0.0291\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.5541 - mae: 0.3290 - mean_pred: 0.6680 - mae_t1: 0.0219 - val_loss: 0.7305 - val_mae: 0.4337 - val_mean_pred: 0.7367 - val_mae_t1: 0.0289\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.5479 - mae: 0.3253 - mean_pred: 0.6912 - mae_t1: 0.0217 - val_loss: 0.7101 - val_mae: 0.4216 - val_mean_pred: 0.8418 - val_mae_t1: 0.0281\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.5138 - mae: 0.3051 - mean_pred: 0.7603 - mae_t1: 0.0203 - val_loss: 0.7250 - val_mae: 0.4305 - val_mean_pred: 0.9202 - val_mae_t1: 0.0287\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.5121 - mae: 0.3041 - mean_pred: 0.8106 - mae_t1: 0.0203 - val_loss: 0.7200 - val_mae: 0.4275 - val_mean_pred: 0.8555 - val_mae_t1: 0.0285\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 0.5212 - mae: 0.3095 - mean_pred: 0.7291 - mae_t1: 0.0206 - val_loss: 0.7322 - val_mae: 0.4348 - val_mean_pred: 0.8530 - val_mae_t1: 0.0290\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.5085 - mae: 0.3019 - mean_pred: 0.8079 - mae_t1: 0.0201 - val_loss: 0.7738 - val_mae: 0.4595 - val_mean_pred: 0.9792 - val_mae_t1: 0.0306\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.5146 - mae: 0.3056 - mean_pred: 0.8307 - mae_t1: 0.0204 - val_loss: 0.7482 - val_mae: 0.4443 - val_mean_pred: 0.8492 - val_mae_t1: 0.0296\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4933 - mae: 0.2929 - mean_pred: 0.7645 - mae_t1: 0.0195 - val_loss: 0.7449 - val_mae: 0.4423 - val_mean_pred: 0.8689 - val_mae_t1: 0.0295\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 0.4814 - mae: 0.2859 - mean_pred: 0.8021 - mae_t1: 0.0191 - val_loss: 0.7312 - val_mae: 0.4341 - val_mean_pred: 0.9310 - val_mae_t1: 0.0289\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 167us/sample - loss: 0.4795 - mae: 0.2847 - mean_pred: 0.8573 - mae_t1: 0.0190 - val_loss: 0.7149 - val_mae: 0.4244 - val_mean_pred: 0.8735 - val_mae_t1: 0.0283\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 0.4781 - mae: 0.2839 - mean_pred: 0.7737 - mae_t1: 0.0189 - val_loss: 0.6842 - val_mae: 0.4063 - val_mean_pred: 0.8875 - val_mae_t1: 0.0271\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.4810 - mae: 0.2856 - mean_pred: 0.8565 - mae_t1: 0.0190 - val_loss: 0.7172 - val_mae: 0.4258 - val_mean_pred: 0.9644 - val_mae_t1: 0.0284\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4794 - mae: 0.2846 - mean_pred: 0.8458 - mae_t1: 0.0190 - val_loss: 0.7115 - val_mae: 0.4225 - val_mean_pred: 0.8175 - val_mae_t1: 0.0282\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4893 - mae: 0.2905 - mean_pred: 0.7512 - mae_t1: 0.0194 - val_loss: 0.7047 - val_mae: 0.4184 - val_mean_pred: 0.9058 - val_mae_t1: 0.0279\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 0.5201 - mae: 0.3088 - mean_pred: 0.8751 - mae_t1: 0.0206 - val_loss: 0.7935 - val_mae: 0.4712 - val_mean_pred: 0.9545 - val_mae_t1: 0.0314\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.4981 - mae: 0.2958 - mean_pred: 0.8088 - mae_t1: 0.0197 - val_loss: 0.7210 - val_mae: 0.4281 - val_mean_pred: 0.8635 - val_mae_t1: 0.0285\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4691 - mae: 0.2785 - mean_pred: 0.7745 - mae_t1: 0.0186 - val_loss: 0.7211 - val_mae: 0.4281 - val_mean_pred: 0.8823 - val_mae_t1: 0.0285\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4527 - mae: 0.2688 - mean_pred: 0.7843 - mae_t1: 0.0179 - val_loss: 0.7171 - val_mae: 0.4257 - val_mean_pred: 0.9282 - val_mae_t1: 0.0284\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 0.4604 - mae: 0.2734 - mean_pred: 0.8421 - mae_t1: 0.0182 - val_loss: 0.7119 - val_mae: 0.4227 - val_mean_pred: 0.9000 - val_mae_t1: 0.0282\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4621 - mae: 0.2744 - mean_pred: 0.7583 - mae_t1: 0.0183 - val_loss: 0.7010 - val_mae: 0.4162 - val_mean_pred: 0.8337 - val_mae_t1: 0.0277\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4643 - mae: 0.2757 - mean_pred: 0.7895 - mae_t1: 0.0184 - val_loss: 0.7127 - val_mae: 0.4232 - val_mean_pred: 0.8933 - val_mae_t1: 0.0282\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4359 - mae: 0.2588 - mean_pred: 0.7847 - mae_t1: 0.0173 - val_loss: 0.7230 - val_mae: 0.4293 - val_mean_pred: 0.8781 - val_mae_t1: 0.0286\n",
      "Earliness...\n",
      "0.002500295639038086\n",
      "____________________________________________________________\n",
      "Test MAE:      0.34007287447215506  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁</td></tr><tr><td>mean_pred</td><td>▁▆▆▆█▇▆▆▆▇▆▇▇█▅▆▇▆▆▆▇▆▆█▅▇▇▅▇▇▇▅▇▇▇▆▆▇▇▇</td></tr><tr><td>val_loss</td><td>█▄▄▅▄▃▃▅▅▃▄▃▃▄▄▃▄▃▃▃▄▃▆▂▃▂▂▄▂▂▂▂▂▃▂▁▂▂▂▂</td></tr><tr><td>val_mae</td><td>█▄▄▅▄▃▃▅▅▃▄▃▃▄▄▃▄▃▃▃▄▃▆▂▃▂▂▄▂▂▂▂▂▃▂▁▂▂▂▂</td></tr><tr><td>val_mae_t1</td><td>█▄▄▅▄▃▃▅▅▃▄▃▃▄▄▃▄▃▃▃▄▃▆▂▃▂▂▄▂▂▂▂▂▃▂▁▂▂▂▂</td></tr><tr><td>val_mean_pred</td><td>▁▃▆▅▆▆▆▅▅▆▆▆▆█▅▄▇▆▇▆▇▇▄▆▆▇▇▅▇▇█▄▆█▇▆▇▆▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.43777</td></tr><tr><td>AE_2</td><td>0.30751</td></tr><tr><td>AE_3</td><td>0.31045</td></tr><tr><td>MAE</td><td>0.34007</td></tr><tr><td>best_epoch</td><td>88</td></tr><tr><td>best_val_loss</td><td>0.68423</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.4359</td></tr><tr><td>mae</td><td>0.25881</td></tr><tr><td>mae_t1</td><td>0.01725</td></tr><tr><td>mean_pred</td><td>0.78474</td></tr><tr><td>val_loss</td><td>0.72302</td></tr><tr><td>val_mae</td><td>0.4293</td></tr><tr><td>val_mae_t1</td><td>0.02862</td></tr><tr><td>val_mean_pred</td><td>0.87811</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">smooth-surf-81</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/8olslxv4\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/8olslxv4</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155326-8olslxv4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155351-1fawp9x5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fawp9x5\" target=\"_blank\">tough-planet-82</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 684us/sample - loss: 1.4430 - mae: 0.8568 - mean_pred: 0.1061 - mae_t1: 0.0571 - val_loss: 1.2778 - val_mae: 0.7587 - val_mean_pred: 0.1686 - val_mae_t1: 0.0506\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.2705 - mae: 0.7544 - mean_pred: 0.1764 - mae_t1: 0.0503 - val_loss: 0.9317 - val_mae: 0.5532 - val_mean_pred: 0.4178 - val_mae_t1: 0.0369\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 144us/sample - loss: 1.0796 - mae: 0.6410 - mean_pred: 0.4424 - mae_t1: 0.0427 - val_loss: 0.9689 - val_mae: 0.5753 - val_mean_pred: 0.7450 - val_mae_t1: 0.0384\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 143us/sample - loss: 1.1166 - mae: 0.6630 - mean_pred: 0.7147 - mae_t1: 0.0442 - val_loss: 0.9983 - val_mae: 0.5927 - val_mean_pred: 0.8439 - val_mae_t1: 0.0395\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 157us/sample - loss: 1.0822 - mae: 0.6425 - mean_pred: 0.7793 - mae_t1: 0.0428 - val_loss: 0.8429 - val_mae: 0.5005 - val_mean_pred: 0.7532 - val_mae_t1: 0.0334\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 0.9491 - mae: 0.5635 - mean_pred: 0.6669 - mae_t1: 0.0376 - val_loss: 0.8843 - val_mae: 0.5250 - val_mean_pred: 0.6086 - val_mae_t1: 0.0350\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 0.9741 - mae: 0.5784 - mean_pred: 0.5538 - mae_t1: 0.0386 - val_loss: 0.9423 - val_mae: 0.5595 - val_mean_pred: 0.6193 - val_mae_t1: 0.0373\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 0.9951 - mae: 0.5909 - mean_pred: 0.5970 - mae_t1: 0.0394 - val_loss: 0.8563 - val_mae: 0.5084 - val_mean_pred: 0.7715 - val_mae_t1: 0.0339\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 144us/sample - loss: 0.9415 - mae: 0.5590 - mean_pred: 0.7651 - mae_t1: 0.0373 - val_loss: 0.8157 - val_mae: 0.4843 - val_mean_pred: 0.9411 - val_mae_t1: 0.0323\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 0.9138 - mae: 0.5426 - mean_pred: 0.8922 - mae_t1: 0.0362 - val_loss: 0.7917 - val_mae: 0.4701 - val_mean_pred: 0.8979 - val_mae_t1: 0.0313\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 0.8543 - mae: 0.5073 - mean_pred: 0.8119 - mae_t1: 0.0338 - val_loss: 0.8154 - val_mae: 0.4842 - val_mean_pred: 0.7517 - val_mae_t1: 0.0323\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 0.8375 - mae: 0.4973 - mean_pred: 0.6895 - mae_t1: 0.0332 - val_loss: 0.8568 - val_mae: 0.5087 - val_mean_pred: 0.6723 - val_mae_t1: 0.0339\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 0.8473 - mae: 0.5031 - mean_pred: 0.6163 - mae_t1: 0.0335 - val_loss: 0.9021 - val_mae: 0.5356 - val_mean_pred: 0.6392 - val_mae_t1: 0.0357\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 0.8504 - mae: 0.5049 - mean_pred: 0.5937 - mae_t1: 0.0337 - val_loss: 0.9027 - val_mae: 0.5360 - val_mean_pred: 0.6736 - val_mae_t1: 0.0357\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.8280 - mae: 0.4916 - mean_pred: 0.6226 - mae_t1: 0.0328 - val_loss: 0.8793 - val_mae: 0.5221 - val_mean_pred: 0.6818 - val_mae_t1: 0.0348\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.8098 - mae: 0.4808 - mean_pred: 0.6141 - mae_t1: 0.0321 - val_loss: 0.8600 - val_mae: 0.5106 - val_mean_pred: 0.7062 - val_mae_t1: 0.0340\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 0.7826 - mae: 0.4647 - mean_pred: 0.6630 - mae_t1: 0.0310 - val_loss: 0.8142 - val_mae: 0.4834 - val_mean_pred: 0.8436 - val_mae_t1: 0.0322\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7562 - mae: 0.4490 - mean_pred: 0.7806 - mae_t1: 0.0299 - val_loss: 0.8208 - val_mae: 0.4873 - val_mean_pred: 0.9241 - val_mae_t1: 0.0325\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7677 - mae: 0.4558 - mean_pred: 0.8442 - mae_t1: 0.0304 - val_loss: 0.8324 - val_mae: 0.4942 - val_mean_pred: 0.9222 - val_mae_t1: 0.0329\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.7589 - mae: 0.4506 - mean_pred: 0.8384 - mae_t1: 0.0300 - val_loss: 0.8239 - val_mae: 0.4892 - val_mean_pred: 0.8598 - val_mae_t1: 0.0326\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.7412 - mae: 0.4401 - mean_pred: 0.7746 - mae_t1: 0.0293 - val_loss: 0.8355 - val_mae: 0.4961 - val_mean_pred: 0.7851 - val_mae_t1: 0.0331\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.7416 - mae: 0.4403 - mean_pred: 0.7232 - mae_t1: 0.0294 - val_loss: 0.8171 - val_mae: 0.4852 - val_mean_pred: 0.7996 - val_mae_t1: 0.0323\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7518 - mae: 0.4464 - mean_pred: 0.7422 - mae_t1: 0.0298 - val_loss: 0.8021 - val_mae: 0.4763 - val_mean_pred: 0.8330 - val_mae_t1: 0.0318\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 0.7400 - mae: 0.4394 - mean_pred: 0.7731 - mae_t1: 0.0293 - val_loss: 0.8116 - val_mae: 0.4819 - val_mean_pred: 0.8003 - val_mae_t1: 0.0321\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7253 - mae: 0.4307 - mean_pred: 0.7185 - mae_t1: 0.0287 - val_loss: 0.8422 - val_mae: 0.5000 - val_mean_pred: 0.7637 - val_mae_t1: 0.0333\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7268 - mae: 0.4315 - mean_pred: 0.7040 - mae_t1: 0.0288 - val_loss: 0.8087 - val_mae: 0.4801 - val_mean_pred: 0.8555 - val_mae_t1: 0.0320\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7005 - mae: 0.4159 - mean_pred: 0.7854 - mae_t1: 0.0277 - val_loss: 0.8225 - val_mae: 0.4884 - val_mean_pred: 1.0031 - val_mae_t1: 0.0326\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7500 - mae: 0.4453 - mean_pred: 0.9320 - mae_t1: 0.0297 - val_loss: 0.8549 - val_mae: 0.5076 - val_mean_pred: 1.0454 - val_mae_t1: 0.0338\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7415 - mae: 0.4403 - mean_pred: 0.9269 - mae_t1: 0.0294 - val_loss: 0.8452 - val_mae: 0.5019 - val_mean_pred: 0.8970 - val_mae_t1: 0.0335\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7441 - mae: 0.4418 - mean_pred: 0.8037 - mae_t1: 0.0295 - val_loss: 0.8371 - val_mae: 0.4970 - val_mean_pred: 0.8670 - val_mae_t1: 0.0331\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.7184 - mae: 0.4265 - mean_pred: 0.8086 - mae_t1: 0.0284 - val_loss: 0.8055 - val_mae: 0.4782 - val_mean_pred: 0.9134 - val_mae_t1: 0.0319\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6988 - mae: 0.4149 - mean_pred: 0.8419 - mae_t1: 0.0277 - val_loss: 0.7936 - val_mae: 0.4712 - val_mean_pred: 0.8805 - val_mae_t1: 0.0314\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.6931 - mae: 0.4115 - mean_pred: 0.8078 - mae_t1: 0.0274 - val_loss: 0.7943 - val_mae: 0.4716 - val_mean_pred: 0.8602 - val_mae_t1: 0.0314\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.6926 - mae: 0.4112 - mean_pred: 0.7963 - mae_t1: 0.0274 - val_loss: 0.8025 - val_mae: 0.4765 - val_mean_pred: 0.8847 - val_mae_t1: 0.0318\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.6849 - mae: 0.4067 - mean_pred: 0.8143 - mae_t1: 0.0271 - val_loss: 0.8121 - val_mae: 0.4822 - val_mean_pred: 0.9420 - val_mae_t1: 0.0321\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6886 - mae: 0.4089 - mean_pred: 0.8700 - mae_t1: 0.0273 - val_loss: 0.8332 - val_mae: 0.4947 - val_mean_pred: 0.9626 - val_mae_t1: 0.0330\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7235 - mae: 0.4296 - mean_pred: 0.8739 - mae_t1: 0.0286 - val_loss: 0.8264 - val_mae: 0.4907 - val_mean_pred: 0.8677 - val_mae_t1: 0.0327\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.7128 - mae: 0.4232 - mean_pred: 0.7914 - mae_t1: 0.0282 - val_loss: 0.8064 - val_mae: 0.4788 - val_mean_pred: 0.7991 - val_mae_t1: 0.0319\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 0.6746 - mae: 0.4005 - mean_pred: 0.7607 - mae_t1: 0.0267 - val_loss: 0.7730 - val_mae: 0.4590 - val_mean_pred: 0.8680 - val_mae_t1: 0.0306\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7040 - mae: 0.4180 - mean_pred: 0.8430 - mae_t1: 0.0279 - val_loss: 0.7930 - val_mae: 0.4708 - val_mean_pred: 0.9903 - val_mae_t1: 0.0314\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 0.7531 - mae: 0.4472 - mean_pred: 0.9441 - mae_t1: 0.0298 - val_loss: 0.7568 - val_mae: 0.4494 - val_mean_pred: 0.9715 - val_mae_t1: 0.0300\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 0.6934 - mae: 0.4117 - mean_pred: 0.8670 - mae_t1: 0.0274 - val_loss: 0.8008 - val_mae: 0.4755 - val_mean_pred: 0.7950 - val_mae_t1: 0.0317\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.6875 - mae: 0.4082 - mean_pred: 0.7063 - mae_t1: 0.0272 - val_loss: 0.8430 - val_mae: 0.5005 - val_mean_pred: 0.7544 - val_mae_t1: 0.0334\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7072 - mae: 0.4199 - mean_pred: 0.6888 - mae_t1: 0.0280 - val_loss: 0.8751 - val_mae: 0.5196 - val_mean_pred: 0.8674 - val_mae_t1: 0.0346\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.7448 - mae: 0.4422 - mean_pred: 0.7862 - mae_t1: 0.0295 - val_loss: 0.8994 - val_mae: 0.5340 - val_mean_pred: 0.9655 - val_mae_t1: 0.0356\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7427 - mae: 0.4410 - mean_pred: 0.8588 - mae_t1: 0.0294 - val_loss: 0.8327 - val_mae: 0.4944 - val_mean_pred: 0.9311 - val_mae_t1: 0.0330\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.6880 - mae: 0.4085 - mean_pred: 0.7934 - mae_t1: 0.0272 - val_loss: 0.9043 - val_mae: 0.5369 - val_mean_pred: 0.7757 - val_mae_t1: 0.0358\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7616 - mae: 0.4522 - mean_pred: 0.6656 - mae_t1: 0.0301 - val_loss: 0.9360 - val_mae: 0.5557 - val_mean_pred: 0.6895 - val_mae_t1: 0.0370\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7613 - mae: 0.4520 - mean_pred: 0.6185 - mae_t1: 0.0301 - val_loss: 0.8427 - val_mae: 0.5004 - val_mean_pred: 0.7336 - val_mae_t1: 0.0334\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6686 - mae: 0.3970 - mean_pred: 0.7029 - mae_t1: 0.0265 - val_loss: 0.7698 - val_mae: 0.4570 - val_mean_pred: 0.8991 - val_mae_t1: 0.0305\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.6862 - mae: 0.4074 - mean_pred: 0.8658 - mae_t1: 0.0272 - val_loss: 0.7810 - val_mae: 0.4637 - val_mean_pred: 0.9339 - val_mae_t1: 0.0309\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.6916 - mae: 0.4106 - mean_pred: 0.8628 - mae_t1: 0.0274 - val_loss: 0.7686 - val_mae: 0.4563 - val_mean_pred: 0.8050 - val_mae_t1: 0.0304\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6577 - mae: 0.3905 - mean_pred: 0.7436 - mae_t1: 0.0260 - val_loss: 0.8080 - val_mae: 0.4798 - val_mean_pred: 0.7339 - val_mae_t1: 0.0320\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6828 - mae: 0.4054 - mean_pred: 0.6893 - mae_t1: 0.0270 - val_loss: 0.8376 - val_mae: 0.4973 - val_mean_pred: 0.7237 - val_mae_t1: 0.0332\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.7039 - mae: 0.4179 - mean_pred: 0.6642 - mae_t1: 0.0279 - val_loss: 0.8358 - val_mae: 0.4963 - val_mean_pred: 0.7314 - val_mae_t1: 0.0331\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.6784 - mae: 0.4028 - mean_pred: 0.6841 - mae_t1: 0.0269 - val_loss: 0.8096 - val_mae: 0.4807 - val_mean_pred: 0.8331 - val_mae_t1: 0.0320\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.6432 - mae: 0.3819 - mean_pred: 0.7721 - mae_t1: 0.0255 - val_loss: 0.8029 - val_mae: 0.4767 - val_mean_pred: 0.8966 - val_mae_t1: 0.0318\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.6476 - mae: 0.3845 - mean_pred: 0.8101 - mae_t1: 0.0256 - val_loss: 0.7955 - val_mae: 0.4723 - val_mean_pred: 0.8873 - val_mae_t1: 0.0315\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6479 - mae: 0.3847 - mean_pred: 0.7952 - mae_t1: 0.0256 - val_loss: 0.7931 - val_mae: 0.4709 - val_mean_pred: 0.8421 - val_mae_t1: 0.0314\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.6441 - mae: 0.3824 - mean_pred: 0.7473 - mae_t1: 0.0255 - val_loss: 0.7972 - val_mae: 0.4733 - val_mean_pred: 0.8039 - val_mae_t1: 0.0316\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.6428 - mae: 0.3817 - mean_pred: 0.7248 - mae_t1: 0.0254 - val_loss: 0.7653 - val_mae: 0.4544 - val_mean_pred: 0.8588 - val_mae_t1: 0.0303\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 148us/sample - loss: 0.6431 - mae: 0.3818 - mean_pred: 0.7890 - mae_t1: 0.0255 - val_loss: 0.7543 - val_mae: 0.4479 - val_mean_pred: 0.9505 - val_mae_t1: 0.0299\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.6561 - mae: 0.3895 - mean_pred: 0.8672 - mae_t1: 0.0260 - val_loss: 0.7742 - val_mae: 0.4597 - val_mean_pred: 0.9364 - val_mae_t1: 0.0306\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 0.6573 - mae: 0.3902 - mean_pred: 0.8386 - mae_t1: 0.0260 - val_loss: 0.7531 - val_mae: 0.4471 - val_mean_pred: 0.8437 - val_mae_t1: 0.0298\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 0.6330 - mae: 0.3759 - mean_pred: 0.7662 - mae_t1: 0.0251 - val_loss: 0.7406 - val_mae: 0.4398 - val_mean_pred: 0.8119 - val_mae_t1: 0.0293\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 0.6207 - mae: 0.3685 - mean_pred: 0.7514 - mae_t1: 0.0246 - val_loss: 0.7284 - val_mae: 0.4325 - val_mean_pred: 0.8395 - val_mae_t1: 0.0288\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 0.6292 - mae: 0.3736 - mean_pred: 0.7919 - mae_t1: 0.0249 - val_loss: 0.7184 - val_mae: 0.4266 - val_mean_pred: 0.8657 - val_mae_t1: 0.0284\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 0.6171 - mae: 0.3664 - mean_pred: 0.8007 - mae_t1: 0.0244 - val_loss: 0.7569 - val_mae: 0.4494 - val_mean_pred: 0.8143 - val_mae_t1: 0.0300\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6463 - mae: 0.3837 - mean_pred: 0.7628 - mae_t1: 0.0256 - val_loss: 0.7818 - val_mae: 0.4642 - val_mean_pred: 0.7924 - val_mae_t1: 0.0309\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6478 - mae: 0.3847 - mean_pred: 0.7510 - mae_t1: 0.0256 - val_loss: 0.7642 - val_mae: 0.4538 - val_mean_pred: 0.8119 - val_mae_t1: 0.0303\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6233 - mae: 0.3701 - mean_pred: 0.7696 - mae_t1: 0.0247 - val_loss: 0.7417 - val_mae: 0.4404 - val_mean_pred: 0.8687 - val_mae_t1: 0.0294\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6116 - mae: 0.3631 - mean_pred: 0.8220 - mae_t1: 0.0242 - val_loss: 0.7479 - val_mae: 0.4441 - val_mean_pred: 0.8989 - val_mae_t1: 0.0296\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6334 - mae: 0.3761 - mean_pred: 0.8284 - mae_t1: 0.0251 - val_loss: 0.7609 - val_mae: 0.4518 - val_mean_pred: 0.8397 - val_mae_t1: 0.0301\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6150 - mae: 0.3652 - mean_pred: 0.7582 - mae_t1: 0.0243 - val_loss: 0.8015 - val_mae: 0.4759 - val_mean_pred: 0.7853 - val_mae_t1: 0.0317\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6290 - mae: 0.3735 - mean_pred: 0.7115 - mae_t1: 0.0249 - val_loss: 0.7936 - val_mae: 0.4712 - val_mean_pred: 0.8078 - val_mae_t1: 0.0314\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.6130 - mae: 0.3640 - mean_pred: 0.7416 - mae_t1: 0.0243 - val_loss: 0.7462 - val_mae: 0.4431 - val_mean_pred: 0.9270 - val_mae_t1: 0.0295\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6358 - mae: 0.3775 - mean_pred: 0.8834 - mae_t1: 0.0252 - val_loss: 0.8025 - val_mae: 0.4765 - val_mean_pred: 1.0620 - val_mae_t1: 0.0318\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.7213 - mae: 0.4283 - mean_pred: 0.9861 - mae_t1: 0.0286 - val_loss: 0.7241 - val_mae: 0.4299 - val_mean_pred: 0.9697 - val_mae_t1: 0.0287\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 0.6159 - mae: 0.3657 - mean_pred: 0.8811 - mae_t1: 0.0244 - val_loss: 0.7832 - val_mae: 0.4650 - val_mean_pred: 0.7478 - val_mae_t1: 0.0310\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6509 - mae: 0.3865 - mean_pred: 0.6968 - mae_t1: 0.0258 - val_loss: 0.8430 - val_mae: 0.5005 - val_mean_pred: 0.6532 - val_mae_t1: 0.0334\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 0.6771 - mae: 0.4020 - mean_pred: 0.6538 - mae_t1: 0.0268 - val_loss: 0.7509 - val_mae: 0.4459 - val_mean_pred: 0.7465 - val_mae_t1: 0.0297\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 0.6289 - mae: 0.3734 - mean_pred: 0.7560 - mae_t1: 0.0249 - val_loss: 0.7143 - val_mae: 0.4241 - val_mean_pred: 0.8689 - val_mae_t1: 0.0283\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6234 - mae: 0.3701 - mean_pred: 0.8479 - mae_t1: 0.0247 - val_loss: 0.7277 - val_mae: 0.4321 - val_mean_pred: 0.9116 - val_mae_t1: 0.0288\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6303 - mae: 0.3742 - mean_pred: 0.8683 - mae_t1: 0.0249 - val_loss: 0.7308 - val_mae: 0.4339 - val_mean_pred: 0.8874 - val_mae_t1: 0.0289\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.6026 - mae: 0.3578 - mean_pred: 0.8286 - mae_t1: 0.0239 - val_loss: 0.7480 - val_mae: 0.4441 - val_mean_pred: 0.8109 - val_mae_t1: 0.0296\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5966 - mae: 0.3542 - mean_pred: 0.7352 - mae_t1: 0.0236 - val_loss: 0.7989 - val_mae: 0.4744 - val_mean_pred: 0.7287 - val_mae_t1: 0.0316\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 0.6369 - mae: 0.3781 - mean_pred: 0.6599 - mae_t1: 0.0252 - val_loss: 0.8403 - val_mae: 0.4989 - val_mean_pred: 0.7067 - val_mae_t1: 0.0333\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 0.6930 - mae: 0.4114 - mean_pred: 0.6385 - mae_t1: 0.0274 - val_loss: 0.8642 - val_mae: 0.5131 - val_mean_pred: 0.7153 - val_mae_t1: 0.0342\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.7062 - mae: 0.4193 - mean_pred: 0.6388 - mae_t1: 0.0280 - val_loss: 0.8427 - val_mae: 0.5003 - val_mean_pred: 0.7121 - val_mae_t1: 0.0334\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6835 - mae: 0.4059 - mean_pred: 0.6201 - mae_t1: 0.0271 - val_loss: 0.8465 - val_mae: 0.5026 - val_mean_pred: 0.6949 - val_mae_t1: 0.0335\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.6845 - mae: 0.4064 - mean_pred: 0.6047 - mae_t1: 0.0271 - val_loss: 0.8333 - val_mae: 0.4948 - val_mean_pred: 0.7354 - val_mae_t1: 0.0330\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.6608 - mae: 0.3923 - mean_pred: 0.6415 - mae_t1: 0.0262 - val_loss: 0.7568 - val_mae: 0.4493 - val_mean_pred: 0.8133 - val_mae_t1: 0.0300\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6112 - mae: 0.3629 - mean_pred: 0.7316 - mae_t1: 0.0242 - val_loss: 0.7396 - val_mae: 0.4391 - val_mean_pred: 0.9364 - val_mae_t1: 0.0293\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6566 - mae: 0.3899 - mean_pred: 0.8664 - mae_t1: 0.0260 - val_loss: 0.7382 - val_mae: 0.4383 - val_mean_pred: 0.9963 - val_mae_t1: 0.0292\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 137us/sample - loss: 0.6471 - mae: 0.3842 - mean_pred: 0.8950 - mae_t1: 0.0256 - val_loss: 0.6978 - val_mae: 0.4143 - val_mean_pred: 0.8915 - val_mae_t1: 0.0276\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5878 - mae: 0.3490 - mean_pred: 0.7984 - mae_t1: 0.0233 - val_loss: 0.7202 - val_mae: 0.4276 - val_mean_pred: 0.8128 - val_mae_t1: 0.0285\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5952 - mae: 0.3534 - mean_pred: 0.7580 - mae_t1: 0.0236 - val_loss: 0.7011 - val_mae: 0.4163 - val_mean_pred: 0.8317 - val_mae_t1: 0.0278\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.5920 - mae: 0.3515 - mean_pred: 0.7678 - mae_t1: 0.0234 - val_loss: 0.7154 - val_mae: 0.4248 - val_mean_pred: 0.8552 - val_mae_t1: 0.0283\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6189 - mae: 0.3675 - mean_pred: 0.7980 - mae_t1: 0.0245 - val_loss: 0.7393 - val_mae: 0.4390 - val_mean_pred: 0.9123 - val_mae_t1: 0.0293\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6313 - mae: 0.3748 - mean_pred: 0.8491 - mae_t1: 0.0250 - val_loss: 0.7324 - val_mae: 0.4349 - val_mean_pred: 0.9538 - val_mae_t1: 0.0290\n",
      "Earliness...\n",
      "0.002003908157348633\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3773057121514807  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁</td></tr><tr><td>mean_pred</td><td>▁▄▆▅▇▅▅▇▇▆▆█▇▇▇▆█▆▇▅▇▆▆▇▆▇▇▆▇▆▇▇▆▇▆▅▅▇▆▇</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▃▃▂▃▂▂▃▂▂▃▂▂▃▃▃▂▃▂▂▂▂▁▂▂▂▂▂▁▁▃▃▂▁▁▁</td></tr><tr><td>val_mae</td><td>█▄▃▃▂▃▃▂▃▂▂▃▂▂▃▂▂▃▃▃▂▃▂▂▂▂▁▂▂▂▂▂▁▁▃▃▂▁▁▁</td></tr><tr><td>val_mae_t1</td><td>█▄▃▃▂▃▃▂▃▂▂▃▂▂▃▂▂▃▃▃▂▃▂▂▂▂▁▂▂▂▂▂▁▁▃▃▂▁▁▁</td></tr><tr><td>val_mean_pred</td><td>▁▆▄▆▆▅▅▇▆▆▆█▇▇▇▆▇▆▇▅▇▅▆▆▆▆▆▆▇▆█▆▆▇▅▅▆▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.51183</td></tr><tr><td>AE_2</td><td>0.36075</td></tr><tr><td>AE_3</td><td>0.27101</td></tr><tr><td>MAE</td><td>0.37731</td></tr><tr><td>best_epoch</td><td>94</td></tr><tr><td>best_val_loss</td><td>0.69785</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.63126</td></tr><tr><td>mae</td><td>0.37481</td></tr><tr><td>mae_t1</td><td>0.02499</td></tr><tr><td>mean_pred</td><td>0.8491</td></tr><tr><td>val_loss</td><td>0.73244</td></tr><tr><td>val_mae</td><td>0.43489</td></tr><tr><td>val_mae_t1</td><td>0.02899</td></tr><tr><td>val_mean_pred</td><td>0.95379</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">tough-planet-82</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fawp9x5\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1fawp9x5</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155351-1fawp9x5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155415-yo6vnx7p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/yo6vnx7p\" target=\"_blank\">treasured-puddle-83</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 854us/sample - loss: 3.2503 - mae: 0.8577 - mean_pred: 0.2126 - mae_t1: 0.0572 - val_loss: 1.9946 - val_mae: 0.5264 - val_mean_pred: 0.6546 - val_mae_t1: 0.0351\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 300us/sample - loss: 2.5260 - mae: 0.6666 - mean_pred: 0.7599 - mae_t1: 0.0444 - val_loss: 1.8146 - val_mae: 0.4789 - val_mean_pred: 0.8156 - val_mae_t1: 0.0319\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 249us/sample - loss: 2.2303 - mae: 0.5886 - mean_pred: 0.6773 - mae_t1: 0.0392 - val_loss: 2.3106 - val_mae: 0.6097 - val_mean_pred: 0.5015 - val_mae_t1: 0.0406\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 2.4192 - mae: 0.6384 - mean_pred: 0.4743 - mae_t1: 0.0426 - val_loss: 1.9169 - val_mae: 0.5058 - val_mean_pred: 0.6560 - val_mae_t1: 0.0337\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.0627 - mae: 0.5443 - mean_pred: 0.8135 - mae_t1: 0.0363 - val_loss: 1.9448 - val_mae: 0.5132 - val_mean_pred: 1.0328 - val_mae_t1: 0.0342\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.9661 - mae: 0.5188 - mean_pred: 0.8569 - mae_t1: 0.0346 - val_loss: 1.8428 - val_mae: 0.4863 - val_mean_pred: 0.6859 - val_mae_t1: 0.0324\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.8465 - mae: 0.4873 - mean_pred: 0.6395 - mae_t1: 0.0325 - val_loss: 1.8843 - val_mae: 0.4972 - val_mean_pred: 0.7182 - val_mae_t1: 0.0331\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.7868 - mae: 0.4715 - mean_pred: 0.6654 - mae_t1: 0.0314 - val_loss: 1.9720 - val_mae: 0.5204 - val_mean_pred: 0.7210 - val_mae_t1: 0.0347\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7522 - mae: 0.4624 - mean_pred: 0.6878 - mae_t1: 0.0308 - val_loss: 1.8742 - val_mae: 0.4946 - val_mean_pred: 0.8056 - val_mae_t1: 0.0330\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6963 - mae: 0.4476 - mean_pred: 0.7906 - mae_t1: 0.0298 - val_loss: 1.8532 - val_mae: 0.4890 - val_mean_pred: 0.8701 - val_mae_t1: 0.0326\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.6564 - mae: 0.4371 - mean_pred: 0.7955 - mae_t1: 0.0291 - val_loss: 1.7730 - val_mae: 0.4679 - val_mean_pred: 0.7964 - val_mae_t1: 0.0312\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 1.5824 - mae: 0.4176 - mean_pred: 0.7509 - mae_t1: 0.0278 - val_loss: 1.6758 - val_mae: 0.4422 - val_mean_pred: 0.8771 - val_mae_t1: 0.0295\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.6143 - mae: 0.4260 - mean_pred: 0.8275 - mae_t1: 0.0284 - val_loss: 1.6543 - val_mae: 0.4365 - val_mean_pred: 0.9203 - val_mae_t1: 0.0291\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.6008 - mae: 0.4224 - mean_pred: 0.8506 - mae_t1: 0.0282 - val_loss: 1.7279 - val_mae: 0.4560 - val_mean_pred: 0.7771 - val_mae_t1: 0.0304\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.6816 - mae: 0.4438 - mean_pred: 0.6667 - mae_t1: 0.0296 - val_loss: 1.7767 - val_mae: 0.4688 - val_mean_pred: 0.7171 - val_mae_t1: 0.0313\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5826 - mae: 0.4176 - mean_pred: 0.7406 - mae_t1: 0.0278 - val_loss: 1.7045 - val_mae: 0.4498 - val_mean_pred: 0.9505 - val_mae_t1: 0.0300\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5885 - mae: 0.4192 - mean_pred: 0.8662 - mae_t1: 0.0279 - val_loss: 1.7732 - val_mae: 0.4679 - val_mean_pred: 0.8134 - val_mae_t1: 0.0312\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.5969 - mae: 0.4214 - mean_pred: 0.7163 - mae_t1: 0.0281 - val_loss: 1.9074 - val_mae: 0.5033 - val_mean_pred: 0.7533 - val_mae_t1: 0.0336\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 230us/sample - loss: 1.5082 - mae: 0.3980 - mean_pred: 0.7137 - mae_t1: 0.0265 - val_loss: 1.6190 - val_mae: 0.4272 - val_mean_pred: 0.9298 - val_mae_t1: 0.0285\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.4707 - mae: 0.3881 - mean_pred: 0.8627 - mae_t1: 0.0259 - val_loss: 1.6862 - val_mae: 0.4450 - val_mean_pred: 0.7859 - val_mae_t1: 0.0297\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.5634 - mae: 0.4126 - mean_pred: 0.6598 - mae_t1: 0.0275 - val_loss: 1.8904 - val_mae: 0.4988 - val_mean_pred: 0.6722 - val_mae_t1: 0.0333\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.5001 - mae: 0.3958 - mean_pred: 0.6809 - mae_t1: 0.0264 - val_loss: 1.7184 - val_mae: 0.4535 - val_mean_pred: 0.8867 - val_mae_t1: 0.0302\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.4111 - mae: 0.3724 - mean_pred: 0.7517 - mae_t1: 0.0248 - val_loss: 1.7797 - val_mae: 0.4696 - val_mean_pred: 0.8207 - val_mae_t1: 0.0313\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.4230 - mae: 0.3755 - mean_pred: 0.7403 - mae_t1: 0.0250 - val_loss: 1.7949 - val_mae: 0.4736 - val_mean_pred: 0.8372 - val_mae_t1: 0.0316\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.4186 - mae: 0.3743 - mean_pred: 0.7489 - mae_t1: 0.0250 - val_loss: 1.7694 - val_mae: 0.4669 - val_mean_pred: 0.9425 - val_mae_t1: 0.0311\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.4865 - mae: 0.3923 - mean_pred: 0.9111 - mae_t1: 0.0262 - val_loss: 1.7701 - val_mae: 0.4671 - val_mean_pred: 0.9428 - val_mae_t1: 0.0311\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4638 - mae: 0.3863 - mean_pred: 0.8391 - mae_t1: 0.0258 - val_loss: 1.7464 - val_mae: 0.4609 - val_mean_pred: 0.9034 - val_mae_t1: 0.0307\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.3715 - mae: 0.3619 - mean_pred: 0.8243 - mae_t1: 0.0241 - val_loss: 1.7995 - val_mae: 0.4749 - val_mean_pred: 0.8176 - val_mae_t1: 0.0317\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.4217 - mae: 0.3752 - mean_pred: 0.6886 - mae_t1: 0.0250 - val_loss: 1.8895 - val_mae: 0.4986 - val_mean_pred: 0.7657 - val_mae_t1: 0.0332\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3265 - mae: 0.3501 - mean_pred: 0.7569 - mae_t1: 0.0233 - val_loss: 1.7565 - val_mae: 0.4635 - val_mean_pred: 0.9336 - val_mae_t1: 0.0309\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3581 - mae: 0.3584 - mean_pred: 0.7642 - mae_t1: 0.0239 - val_loss: 1.7760 - val_mae: 0.4687 - val_mean_pred: 0.8390 - val_mae_t1: 0.0312\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 1.2876 - mae: 0.3398 - mean_pred: 0.7837 - mae_t1: 0.0227 - val_loss: 1.6911 - val_mae: 0.4463 - val_mean_pred: 0.9180 - val_mae_t1: 0.0298\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.2559 - mae: 0.3314 - mean_pred: 0.7997 - mae_t1: 0.0221 - val_loss: 1.6997 - val_mae: 0.4485 - val_mean_pred: 0.8679 - val_mae_t1: 0.0299\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.2527 - mae: 0.3306 - mean_pred: 0.7567 - mae_t1: 0.0220 - val_loss: 1.7248 - val_mae: 0.4552 - val_mean_pred: 0.9614 - val_mae_t1: 0.0303\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3514 - mae: 0.3566 - mean_pred: 0.8827 - mae_t1: 0.0238 - val_loss: 1.7752 - val_mae: 0.4685 - val_mean_pred: 0.9394 - val_mae_t1: 0.0312\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.2992 - mae: 0.3428 - mean_pred: 0.7761 - mae_t1: 0.0229 - val_loss: 1.6394 - val_mae: 0.4326 - val_mean_pred: 0.8910 - val_mae_t1: 0.0288\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.2755 - mae: 0.3366 - mean_pred: 0.8455 - mae_t1: 0.0224 - val_loss: 1.6408 - val_mae: 0.4330 - val_mean_pred: 0.8937 - val_mae_t1: 0.0289\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.2332 - mae: 0.3254 - mean_pred: 0.7970 - mae_t1: 0.0217 - val_loss: 1.6766 - val_mae: 0.4424 - val_mean_pred: 0.9297 - val_mae_t1: 0.0295\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.2329 - mae: 0.3253 - mean_pred: 0.8483 - mae_t1: 0.0217 - val_loss: 1.7621 - val_mae: 0.4650 - val_mean_pred: 0.9882 - val_mae_t1: 0.0310\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.2492 - mae: 0.3297 - mean_pred: 0.8478 - mae_t1: 0.0220 - val_loss: 1.7186 - val_mae: 0.4535 - val_mean_pred: 0.9215 - val_mae_t1: 0.0302\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.2225 - mae: 0.3226 - mean_pred: 0.8433 - mae_t1: 0.0215 - val_loss: 1.7167 - val_mae: 0.4530 - val_mean_pred: 0.9701 - val_mae_t1: 0.0302\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 218us/sample - loss: 1.2613 - mae: 0.3328 - mean_pred: 0.8372 - mae_t1: 0.0222 - val_loss: 1.6065 - val_mae: 0.4239 - val_mean_pred: 0.8436 - val_mae_t1: 0.0283\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.3742 - mae: 0.3626 - mean_pred: 0.7615 - mae_t1: 0.0242 - val_loss: 1.7282 - val_mae: 0.4561 - val_mean_pred: 0.7725 - val_mae_t1: 0.0304\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.3025 - mae: 0.3437 - mean_pred: 0.7064 - mae_t1: 0.0229 - val_loss: 1.7105 - val_mae: 0.4514 - val_mean_pred: 0.9566 - val_mae_t1: 0.0301\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.2642 - mae: 0.3336 - mean_pred: 0.8711 - mae_t1: 0.0222 - val_loss: 1.6604 - val_mae: 0.4382 - val_mean_pred: 0.8730 - val_mae_t1: 0.0292\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.1945 - mae: 0.3152 - mean_pred: 0.7284 - mae_t1: 0.0210 - val_loss: 1.6719 - val_mae: 0.4412 - val_mean_pred: 0.9314 - val_mae_t1: 0.0294\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.2598 - mae: 0.3325 - mean_pred: 0.8898 - mae_t1: 0.0222 - val_loss: 1.6814 - val_mae: 0.4437 - val_mean_pred: 0.9462 - val_mae_t1: 0.0296\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.3320 - mae: 0.3515 - mean_pred: 0.7541 - mae_t1: 0.0234 - val_loss: 1.7746 - val_mae: 0.4683 - val_mean_pred: 0.7673 - val_mae_t1: 0.0312\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.2724 - mae: 0.3358 - mean_pred: 0.7508 - mae_t1: 0.0224 - val_loss: 1.6806 - val_mae: 0.4435 - val_mean_pred: 1.0217 - val_mae_t1: 0.0296\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.3882 - mae: 0.3663 - mean_pred: 0.9315 - mae_t1: 0.0244 - val_loss: 1.6961 - val_mae: 0.4476 - val_mean_pred: 0.8079 - val_mae_t1: 0.0298\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.3132 - mae: 0.3465 - mean_pred: 0.6889 - mae_t1: 0.0231 - val_loss: 1.7471 - val_mae: 0.4610 - val_mean_pred: 0.7740 - val_mae_t1: 0.0307\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.1808 - mae: 0.3116 - mean_pred: 0.7345 - mae_t1: 0.0208 - val_loss: 1.7077 - val_mae: 0.4506 - val_mean_pred: 0.9797 - val_mae_t1: 0.0300\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4389 - mae: 0.3797 - mean_pred: 0.9166 - mae_t1: 0.0253 - val_loss: 1.6179 - val_mae: 0.4269 - val_mean_pred: 0.9531 - val_mae_t1: 0.0285\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.1550 - mae: 0.3048 - mean_pred: 0.7970 - mae_t1: 0.0203 - val_loss: 1.7321 - val_mae: 0.4571 - val_mean_pred: 0.7512 - val_mae_t1: 0.0305\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.3095 - mae: 0.3456 - mean_pred: 0.6806 - mae_t1: 0.0230 - val_loss: 1.6118 - val_mae: 0.4253 - val_mean_pred: 0.8853 - val_mae_t1: 0.0284\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 1.3216 - mae: 0.3488 - mean_pred: 0.8760 - mae_t1: 0.0233 - val_loss: 1.5877 - val_mae: 0.4190 - val_mean_pred: 0.9266 - val_mae_t1: 0.0279\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.2350 - mae: 0.3259 - mean_pred: 0.7340 - mae_t1: 0.0217 - val_loss: 1.7682 - val_mae: 0.4666 - val_mean_pred: 0.7943 - val_mae_t1: 0.0311\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.2354 - mae: 0.3260 - mean_pred: 0.7485 - mae_t1: 0.0217 - val_loss: 1.7385 - val_mae: 0.4588 - val_mean_pred: 0.9826 - val_mae_t1: 0.0306\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.1604 - mae: 0.3062 - mean_pred: 0.8158 - mae_t1: 0.0204 - val_loss: 1.6075 - val_mae: 0.4242 - val_mean_pred: 0.8965 - val_mae_t1: 0.0283\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.1000 - mae: 0.2903 - mean_pred: 0.8063 - mae_t1: 0.0194 - val_loss: 1.6396 - val_mae: 0.4327 - val_mean_pred: 0.9286 - val_mae_t1: 0.0288\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.1768 - mae: 0.3105 - mean_pred: 0.8401 - mae_t1: 0.0207 - val_loss: 1.6685 - val_mae: 0.4403 - val_mean_pred: 0.8823 - val_mae_t1: 0.0294\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.1913 - mae: 0.3144 - mean_pred: 0.7801 - mae_t1: 0.0210 - val_loss: 1.6557 - val_mae: 0.4369 - val_mean_pred: 0.8616 - val_mae_t1: 0.0291\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 1.1454 - mae: 0.3023 - mean_pred: 0.8176 - mae_t1: 0.0202 - val_loss: 1.7145 - val_mae: 0.4524 - val_mean_pred: 0.9711 - val_mae_t1: 0.0302\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 1.1630 - mae: 0.3069 - mean_pred: 0.8314 - mae_t1: 0.0205 - val_loss: 1.5609 - val_mae: 0.4119 - val_mean_pred: 0.8611 - val_mae_t1: 0.0275\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.1823 - mae: 0.3120 - mean_pred: 0.7778 - mae_t1: 0.0208 - val_loss: 1.5664 - val_mae: 0.4133 - val_mean_pred: 0.9104 - val_mae_t1: 0.0276\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.1607 - mae: 0.3063 - mean_pred: 0.8604 - mae_t1: 0.0204 - val_loss: 1.5647 - val_mae: 0.4129 - val_mean_pred: 0.9300 - val_mae_t1: 0.0275\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.1370 - mae: 0.3000 - mean_pred: 0.7919 - mae_t1: 0.0200 - val_loss: 1.6197 - val_mae: 0.4274 - val_mean_pred: 0.7692 - val_mae_t1: 0.0285\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 1.2189 - mae: 0.3217 - mean_pred: 0.7162 - mae_t1: 0.0214 - val_loss: 1.6168 - val_mae: 0.4266 - val_mean_pred: 0.9110 - val_mae_t1: 0.0284\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.0581 - mae: 0.2792 - mean_pred: 0.8417 - mae_t1: 0.0186 - val_loss: 1.6323 - val_mae: 0.4308 - val_mean_pred: 0.9016 - val_mae_t1: 0.0287\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 1.0732 - mae: 0.2832 - mean_pred: 0.7738 - mae_t1: 0.0189 - val_loss: 1.6450 - val_mae: 0.4341 - val_mean_pred: 0.9192 - val_mae_t1: 0.0289\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.0866 - mae: 0.2867 - mean_pred: 0.8416 - mae_t1: 0.0191 - val_loss: 1.6521 - val_mae: 0.4360 - val_mean_pred: 0.9505 - val_mae_t1: 0.0291\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.0597 - mae: 0.2796 - mean_pred: 0.8002 - mae_t1: 0.0186 - val_loss: 1.5818 - val_mae: 0.4174 - val_mean_pred: 0.8219 - val_mae_t1: 0.0278\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 215us/sample - loss: 1.0677 - mae: 0.2817 - mean_pred: 0.7650 - mae_t1: 0.0188 - val_loss: 1.4972 - val_mae: 0.3951 - val_mean_pred: 0.9089 - val_mae_t1: 0.0263\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.0675 - mae: 0.2817 - mean_pred: 0.8562 - mae_t1: 0.0188 - val_loss: 1.5883 - val_mae: 0.4191 - val_mean_pred: 0.8581 - val_mae_t1: 0.0279\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.1083 - mae: 0.2925 - mean_pred: 0.7699 - mae_t1: 0.0195 - val_loss: 1.5792 - val_mae: 0.4167 - val_mean_pred: 0.8656 - val_mae_t1: 0.0278\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.0499 - mae: 0.2771 - mean_pred: 0.8335 - mae_t1: 0.0185 - val_loss: 1.5317 - val_mae: 0.4042 - val_mean_pred: 0.9558 - val_mae_t1: 0.0269\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.0597 - mae: 0.2796 - mean_pred: 0.8546 - mae_t1: 0.0186 - val_loss: 1.5950 - val_mae: 0.4209 - val_mean_pred: 0.8202 - val_mae_t1: 0.0281\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.0820 - mae: 0.2855 - mean_pred: 0.7321 - mae_t1: 0.0190 - val_loss: 1.6446 - val_mae: 0.4340 - val_mean_pred: 0.8915 - val_mae_t1: 0.0289\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.0485 - mae: 0.2767 - mean_pred: 0.8445 - mae_t1: 0.0184 - val_loss: 1.6898 - val_mae: 0.4459 - val_mean_pred: 0.9291 - val_mae_t1: 0.0297\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.0715 - mae: 0.2828 - mean_pred: 0.7766 - mae_t1: 0.0189 - val_loss: 1.6420 - val_mae: 0.4333 - val_mean_pred: 0.7953 - val_mae_t1: 0.0289\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 1.1782 - mae: 0.3109 - mean_pred: 0.7595 - mae_t1: 0.0207 - val_loss: 1.6249 - val_mae: 0.4288 - val_mean_pred: 0.9230 - val_mae_t1: 0.0286\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.2173 - mae: 0.3212 - mean_pred: 0.9070 - mae_t1: 0.0214 - val_loss: 1.7049 - val_mae: 0.4499 - val_mean_pred: 0.9634 - val_mae_t1: 0.0300\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.0816 - mae: 0.2854 - mean_pred: 0.7700 - mae_t1: 0.0190 - val_loss: 1.7018 - val_mae: 0.4491 - val_mean_pred: 0.7183 - val_mae_t1: 0.0299\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.2625 - mae: 0.3332 - mean_pred: 0.6715 - mae_t1: 0.0222 - val_loss: 1.6367 - val_mae: 0.4319 - val_mean_pred: 0.7866 - val_mae_t1: 0.0288\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 1.0868 - mae: 0.2868 - mean_pred: 0.7738 - mae_t1: 0.0191 - val_loss: 1.7218 - val_mae: 0.4544 - val_mean_pred: 1.0406 - val_mae_t1: 0.0303\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.1503 - mae: 0.3036 - mean_pred: 0.9150 - mae_t1: 0.0202 - val_loss: 1.6077 - val_mae: 0.4243 - val_mean_pred: 0.8493 - val_mae_t1: 0.0283\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.1001 - mae: 0.2903 - mean_pred: 0.7348 - mae_t1: 0.0194 - val_loss: 1.6913 - val_mae: 0.4463 - val_mean_pred: 0.8725 - val_mae_t1: 0.0298\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.0875 - mae: 0.2870 - mean_pred: 0.8494 - mae_t1: 0.0191 - val_loss: 1.7833 - val_mae: 0.4706 - val_mean_pred: 0.9906 - val_mae_t1: 0.0314\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.0857 - mae: 0.2865 - mean_pred: 0.8275 - mae_t1: 0.0191 - val_loss: 1.7170 - val_mae: 0.4531 - val_mean_pred: 0.8285 - val_mae_t1: 0.0302\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.0462 - mae: 0.2761 - mean_pred: 0.7423 - mae_t1: 0.0184 - val_loss: 1.6873 - val_mae: 0.4452 - val_mean_pred: 0.9706 - val_mae_t1: 0.0297\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 1.0553 - mae: 0.2785 - mean_pred: 0.8622 - mae_t1: 0.0186 - val_loss: 1.6715 - val_mae: 0.4411 - val_mean_pred: 0.9100 - val_mae_t1: 0.0294\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.0494 - mae: 0.2769 - mean_pred: 0.7331 - mae_t1: 0.0185 - val_loss: 1.6117 - val_mae: 0.4253 - val_mean_pred: 0.8448 - val_mae_t1: 0.0284\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.0192 - mae: 0.2690 - mean_pred: 0.7894 - mae_t1: 0.0179 - val_loss: 1.6521 - val_mae: 0.4360 - val_mean_pred: 1.0033 - val_mae_t1: 0.0291\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.1169 - mae: 0.2947 - mean_pred: 0.8822 - mae_t1: 0.0196 - val_loss: 1.6565 - val_mae: 0.4371 - val_mean_pred: 0.9702 - val_mae_t1: 0.0291\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.0037 - mae: 0.2649 - mean_pred: 0.8600 - mae_t1: 0.0177 - val_loss: 1.6396 - val_mae: 0.4327 - val_mean_pred: 0.9147 - val_mae_t1: 0.0288\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.0008 - mae: 0.2641 - mean_pred: 0.7951 - mae_t1: 0.0176 - val_loss: 1.6770 - val_mae: 0.4425 - val_mean_pred: 0.8263 - val_mae_t1: 0.0295\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.1421 - mae: 0.3014 - mean_pred: 0.7367 - mae_t1: 0.0201 - val_loss: 1.7098 - val_mae: 0.4512 - val_mean_pred: 0.7786 - val_mae_t1: 0.0301\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.0873 - mae: 0.2869 - mean_pred: 0.7258 - mae_t1: 0.0191 - val_loss: 1.6631 - val_mae: 0.4389 - val_mean_pred: 0.9427 - val_mae_t1: 0.0293\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 1.0063 - mae: 0.2656 - mean_pred: 0.8469 - mae_t1: 0.0177 - val_loss: 1.6221 - val_mae: 0.4281 - val_mean_pred: 0.8766 - val_mae_t1: 0.0285\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.1001 - mae: 0.2903 - mean_pred: 0.7295 - mae_t1: 0.0194 - val_loss: 1.6676 - val_mae: 0.4401 - val_mean_pred: 0.8378 - val_mae_t1: 0.0293\n",
      "Earliness...\n",
      "0.002000093460083008\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3270985246401776  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▆▇▆▇▇▆▆▅▆█▇▇▆▇▇▇▆▆▆▆▇█▇▇▇▇▇▇▇▇▇█▆▆▇▆█▆▆</td></tr><tr><td>val_loss</td><td>▅█▄▅▃▂▂▄▄▃▃▃▃▃▂▃▂▂▂▂▃▃▁▁▂▁▂▂▁▁▁▂▂▂▂▂▁▂▂▂</td></tr><tr><td>val_mae</td><td>▅█▄▅▃▂▂▄▄▃▃▃▃▃▂▃▂▂▂▂▃▃▁▁▂▁▂▂▁▁▁▂▂▂▂▂▁▂▂▂</td></tr><tr><td>val_mae_t1</td><td>▅█▄▅▃▂▂▄▄▃▃▃▃▃▂▃▂▂▂▂▃▃▁▁▂▁▂▂▁▁▁▂▂▂▂▂▁▂▂▂</td></tr><tr><td>val_mean_pred</td><td>▃▁▃▄▅▇▇▄▃▅▇▅▆▇▆█▇▇▇█▅▄▇▆▆▆▅▆▅▆▅▇▇▅▆▅▆▇▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.36745</td></tr><tr><td>AE_2</td><td>0.33066</td></tr><tr><td>AE_3</td><td>0.30963</td></tr><tr><td>MAE</td><td>0.3271</td></tr><tr><td>best_epoch</td><td>72</td></tr><tr><td>best_val_loss</td><td>1.49719</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.10006</td></tr><tr><td>mae</td><td>0.29029</td></tr><tr><td>mae_t1</td><td>0.01935</td></tr><tr><td>mean_pred</td><td>0.72946</td></tr><tr><td>val_loss</td><td>1.6676</td></tr><tr><td>val_mae</td><td>0.44006</td></tr><tr><td>val_mae_t1</td><td>0.02934</td></tr><tr><td>val_mean_pred</td><td>0.83784</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">treasured-puddle-83</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/yo6vnx7p\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/yo6vnx7p</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155415-yo6vnx7p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155442-3mglyyon</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mglyyon\" target=\"_blank\">atomic-gorge-84</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 716us/sample - loss: 2.9146 - mae: 0.7691 - mean_pred: 0.1948 - mae_t1: 0.0513 - val_loss: 2.1504 - val_mae: 0.5675 - val_mean_pred: 0.4645 - val_mae_t1: 0.0378\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 2.3651 - mae: 0.6241 - mean_pred: 0.5230 - mae_t1: 0.0416 - val_loss: 1.8464 - val_mae: 0.4872 - val_mean_pred: 0.7839 - val_mae_t1: 0.0325\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.2459 - mae: 0.5927 - mean_pred: 0.7532 - mae_t1: 0.0395 - val_loss: 1.8311 - val_mae: 0.4832 - val_mean_pred: 0.7956 - val_mae_t1: 0.0322\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 156us/sample - loss: 2.1556 - mae: 0.5689 - mean_pred: 0.7676 - mae_t1: 0.0379 - val_loss: 1.8243 - val_mae: 0.4814 - val_mean_pred: 0.8021 - val_mae_t1: 0.0321\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 2.0698 - mae: 0.5462 - mean_pred: 0.7360 - mae_t1: 0.0364 - val_loss: 1.8461 - val_mae: 0.4872 - val_mean_pred: 0.7524 - val_mae_t1: 0.0325\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 1.9565 - mae: 0.5163 - mean_pred: 0.6976 - mae_t1: 0.0344 - val_loss: 1.8511 - val_mae: 0.4885 - val_mean_pred: 0.8572 - val_mae_t1: 0.0326\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 1.8832 - mae: 0.4970 - mean_pred: 0.7897 - mae_t1: 0.0331 - val_loss: 1.9933 - val_mae: 0.5260 - val_mean_pred: 0.8643 - val_mae_t1: 0.0351\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 1.8746 - mae: 0.4947 - mean_pred: 0.7257 - mae_t1: 0.0330 - val_loss: 1.9876 - val_mae: 0.5245 - val_mean_pred: 0.7652 - val_mae_t1: 0.0350\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.8285 - mae: 0.4825 - mean_pred: 0.6959 - mae_t1: 0.0322 - val_loss: 1.8820 - val_mae: 0.4966 - val_mean_pred: 0.8120 - val_mae_t1: 0.0331\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 141us/sample - loss: 1.8220 - mae: 0.4808 - mean_pred: 0.7125 - mae_t1: 0.0321 - val_loss: 1.8214 - val_mae: 0.4807 - val_mean_pred: 0.8041 - val_mae_t1: 0.0320\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 139us/sample - loss: 1.7232 - mae: 0.4547 - mean_pred: 0.7646 - mae_t1: 0.0303 - val_loss: 1.7208 - val_mae: 0.4541 - val_mean_pred: 0.9089 - val_mae_t1: 0.0303\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.7411 - mae: 0.4595 - mean_pred: 0.8345 - mae_t1: 0.0306 - val_loss: 1.8531 - val_mae: 0.4890 - val_mean_pred: 0.6937 - val_mae_t1: 0.0326\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.8589 - mae: 0.4905 - mean_pred: 0.6174 - mae_t1: 0.0327 - val_loss: 2.3235 - val_mae: 0.6132 - val_mean_pred: 0.4597 - val_mae_t1: 0.0409\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 2.1916 - mae: 0.5783 - mean_pred: 0.4527 - mae_t1: 0.0386 - val_loss: 2.1219 - val_mae: 0.5600 - val_mean_pred: 0.5534 - val_mae_t1: 0.0373\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.8242 - mae: 0.4814 - mean_pred: 0.5689 - mae_t1: 0.0321 - val_loss: 1.8311 - val_mae: 0.4832 - val_mean_pred: 0.9915 - val_mae_t1: 0.0322\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.9766 - mae: 0.5216 - mean_pred: 0.9710 - mae_t1: 0.0348 - val_loss: 2.0343 - val_mae: 0.5368 - val_mean_pred: 1.1419 - val_mae_t1: 0.0358\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.1245 - mae: 0.5606 - mean_pred: 1.0346 - mae_t1: 0.0374 - val_loss: 1.8314 - val_mae: 0.4833 - val_mean_pred: 0.8361 - val_mae_t1: 0.0322\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.8661 - mae: 0.4924 - mean_pred: 0.7172 - mae_t1: 0.0328 - val_loss: 2.1482 - val_mae: 0.5669 - val_mean_pred: 0.5295 - val_mae_t1: 0.0378\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.1052 - mae: 0.5555 - mean_pred: 0.4439 - mae_t1: 0.0370 - val_loss: 2.3361 - val_mae: 0.6165 - val_mean_pred: 0.3830 - val_mae_t1: 0.0411\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.2485 - mae: 0.5933 - mean_pred: 0.3473 - mae_t1: 0.0396 - val_loss: 2.3272 - val_mae: 0.6141 - val_mean_pred: 0.4334 - val_mae_t1: 0.0409\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.2355 - mae: 0.5899 - mean_pred: 0.4192 - mae_t1: 0.0393 - val_loss: 2.1800 - val_mae: 0.5753 - val_mean_pred: 0.5564 - val_mae_t1: 0.0384\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.0848 - mae: 0.5501 - mean_pred: 0.5182 - mae_t1: 0.0367 - val_loss: 2.0268 - val_mae: 0.5348 - val_mean_pred: 0.6205 - val_mae_t1: 0.0357\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.8885 - mae: 0.4984 - mean_pred: 0.5530 - mae_t1: 0.0332 - val_loss: 1.8931 - val_mae: 0.4996 - val_mean_pred: 0.6619 - val_mae_t1: 0.0333\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.7695 - mae: 0.4670 - mean_pred: 0.6010 - mae_t1: 0.0311 - val_loss: 1.9317 - val_mae: 0.5098 - val_mean_pred: 0.8006 - val_mae_t1: 0.0340\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.8553 - mae: 0.4896 - mean_pred: 0.7317 - mae_t1: 0.0326 - val_loss: 1.9333 - val_mae: 0.5102 - val_mean_pred: 0.9984 - val_mae_t1: 0.0340\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.9636 - mae: 0.5182 - mean_pred: 0.9413 - mae_t1: 0.0345 - val_loss: 1.9423 - val_mae: 0.5126 - val_mean_pred: 1.0770 - val_mae_t1: 0.0342\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.8337 - mae: 0.4839 - mean_pred: 0.9373 - mae_t1: 0.0323 - val_loss: 1.7819 - val_mae: 0.4702 - val_mean_pred: 0.8657 - val_mae_t1: 0.0313\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.6160 - mae: 0.4264 - mean_pred: 0.7489 - mae_t1: 0.0284 - val_loss: 1.7960 - val_mae: 0.4739 - val_mean_pred: 0.7789 - val_mae_t1: 0.0316\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.5677 - mae: 0.4137 - mean_pred: 0.7022 - mae_t1: 0.0276 - val_loss: 1.7211 - val_mae: 0.4542 - val_mean_pred: 0.8267 - val_mae_t1: 0.0303\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 1.5234 - mae: 0.4020 - mean_pred: 0.7726 - mae_t1: 0.0268 - val_loss: 1.6760 - val_mae: 0.4423 - val_mean_pred: 0.9130 - val_mae_t1: 0.0295\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.6156 - mae: 0.4263 - mean_pred: 0.8735 - mae_t1: 0.0284 - val_loss: 1.6802 - val_mae: 0.4434 - val_mean_pred: 0.9626 - val_mae_t1: 0.0296\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.6583 - mae: 0.4376 - mean_pred: 0.9276 - mae_t1: 0.0292 - val_loss: 1.6822 - val_mae: 0.4439 - val_mean_pred: 0.8956 - val_mae_t1: 0.0296\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.4930 - mae: 0.3940 - mean_pred: 0.8344 - mae_t1: 0.0263 - val_loss: 1.9128 - val_mae: 0.5048 - val_mean_pred: 0.7148 - val_mae_t1: 0.0337\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.6458 - mae: 0.4343 - mean_pred: 0.6573 - mae_t1: 0.0290 - val_loss: 2.0765 - val_mae: 0.5480 - val_mean_pred: 0.6876 - val_mae_t1: 0.0365\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.7051 - mae: 0.4499 - mean_pred: 0.6527 - mae_t1: 0.0300 - val_loss: 1.8780 - val_mae: 0.4956 - val_mean_pred: 0.8231 - val_mae_t1: 0.0330\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.4888 - mae: 0.3929 - mean_pred: 0.7560 - mae_t1: 0.0262 - val_loss: 2.0012 - val_mae: 0.5281 - val_mean_pred: 0.8970 - val_mae_t1: 0.0352\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 1.6466 - mae: 0.4345 - mean_pred: 0.8043 - mae_t1: 0.0290 - val_loss: 1.9662 - val_mae: 0.5189 - val_mean_pred: 0.8684 - val_mae_t1: 0.0346\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.5584 - mae: 0.4112 - mean_pred: 0.7491 - mae_t1: 0.0274 - val_loss: 1.9613 - val_mae: 0.5176 - val_mean_pred: 0.7374 - val_mae_t1: 0.0345\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.6687 - mae: 0.4404 - mean_pred: 0.6131 - mae_t1: 0.0294 - val_loss: 2.2019 - val_mae: 0.5811 - val_mean_pred: 0.6372 - val_mae_t1: 0.0387\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.8712 - mae: 0.4938 - mean_pred: 0.5397 - mae_t1: 0.0329 - val_loss: 1.9581 - val_mae: 0.5167 - val_mean_pred: 0.6708 - val_mae_t1: 0.0344\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.5830 - mae: 0.4177 - mean_pred: 0.6082 - mae_t1: 0.0278 - val_loss: 1.7790 - val_mae: 0.4695 - val_mean_pred: 0.8723 - val_mae_t1: 0.0313\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.6678 - mae: 0.4401 - mean_pred: 0.8200 - mae_t1: 0.0293 - val_loss: 1.9422 - val_mae: 0.5125 - val_mean_pred: 1.0618 - val_mae_t1: 0.0342\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.8445 - mae: 0.4867 - mean_pred: 0.9913 - mae_t1: 0.0324 - val_loss: 1.8924 - val_mae: 0.4994 - val_mean_pred: 1.0608 - val_mae_t1: 0.0333\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.6618 - mae: 0.4385 - mean_pred: 0.9534 - mae_t1: 0.0292 - val_loss: 1.9067 - val_mae: 0.5032 - val_mean_pred: 0.8401 - val_mae_t1: 0.0335\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.6574 - mae: 0.4374 - mean_pred: 0.7422 - mae_t1: 0.0292 - val_loss: 1.9889 - val_mae: 0.5249 - val_mean_pred: 0.6641 - val_mae_t1: 0.0350\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.6572 - mae: 0.4373 - mean_pred: 0.6141 - mae_t1: 0.0292 - val_loss: 1.8154 - val_mae: 0.4791 - val_mean_pred: 0.7071 - val_mae_t1: 0.0319\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.6145 - mae: 0.4261 - mean_pred: 0.7108 - mae_t1: 0.0284 - val_loss: 1.8525 - val_mae: 0.4889 - val_mean_pred: 0.8942 - val_mae_t1: 0.0326\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 137us/sample - loss: 1.6642 - mae: 0.4392 - mean_pred: 0.8613 - mae_t1: 0.0293 - val_loss: 1.6680 - val_mae: 0.4402 - val_mean_pred: 0.9395 - val_mae_t1: 0.0293\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 1.4377 - mae: 0.3794 - mean_pred: 0.8502 - mae_t1: 0.0253 - val_loss: 1.7027 - val_mae: 0.4493 - val_mean_pred: 0.8347 - val_mae_t1: 0.0300\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 1.4717 - mae: 0.3884 - mean_pred: 0.7361 - mae_t1: 0.0259 - val_loss: 1.8366 - val_mae: 0.4847 - val_mean_pred: 0.7449 - val_mae_t1: 0.0323\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.5608 - mae: 0.4119 - mean_pred: 0.6604 - mae_t1: 0.0275 - val_loss: 1.7622 - val_mae: 0.4650 - val_mean_pred: 0.7531 - val_mae_t1: 0.0310\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.4502 - mae: 0.3827 - mean_pred: 0.6792 - mae_t1: 0.0255 - val_loss: 1.7371 - val_mae: 0.4584 - val_mean_pred: 0.8325 - val_mae_t1: 0.0306\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.4092 - mae: 0.3719 - mean_pred: 0.7431 - mae_t1: 0.0248 - val_loss: 1.7611 - val_mae: 0.4647 - val_mean_pred: 0.8448 - val_mae_t1: 0.0310\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.3868 - mae: 0.3660 - mean_pred: 0.7245 - mae_t1: 0.0244 - val_loss: 1.8459 - val_mae: 0.4871 - val_mean_pred: 0.8405 - val_mae_t1: 0.0325\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 1.4552 - mae: 0.3840 - mean_pred: 0.7348 - mae_t1: 0.0256 - val_loss: 1.7934 - val_mae: 0.4733 - val_mean_pred: 0.9482 - val_mae_t1: 0.0316\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.4164 - mae: 0.3738 - mean_pred: 0.8449 - mae_t1: 0.0249 - val_loss: 1.7451 - val_mae: 0.4605 - val_mean_pred: 1.0082 - val_mae_t1: 0.0307\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.4140 - mae: 0.3731 - mean_pred: 0.8912 - mae_t1: 0.0249 - val_loss: 1.7140 - val_mae: 0.4523 - val_mean_pred: 0.9069 - val_mae_t1: 0.0302\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.3203 - mae: 0.3484 - mean_pred: 0.7987 - mae_t1: 0.0232 - val_loss: 1.8912 - val_mae: 0.4991 - val_mean_pred: 0.8292 - val_mae_t1: 0.0333\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.5049 - mae: 0.3971 - mean_pred: 0.7652 - mae_t1: 0.0265 - val_loss: 1.9750 - val_mae: 0.5212 - val_mean_pred: 0.8932 - val_mae_t1: 0.0347\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.5105 - mae: 0.3986 - mean_pred: 0.8451 - mae_t1: 0.0266 - val_loss: 1.8776 - val_mae: 0.4955 - val_mean_pred: 0.9377 - val_mae_t1: 0.0330\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.3819 - mae: 0.3647 - mean_pred: 0.8575 - mae_t1: 0.0243 - val_loss: 1.8506 - val_mae: 0.4884 - val_mean_pred: 0.9167 - val_mae_t1: 0.0326\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3856 - mae: 0.3656 - mean_pred: 0.8479 - mae_t1: 0.0244 - val_loss: 1.8297 - val_mae: 0.4828 - val_mean_pred: 0.9483 - val_mae_t1: 0.0322\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 1.3545 - mae: 0.3574 - mean_pred: 0.8523 - mae_t1: 0.0238 - val_loss: 1.7899 - val_mae: 0.4723 - val_mean_pred: 0.9231 - val_mae_t1: 0.0315\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3394 - mae: 0.3535 - mean_pred: 0.8400 - mae_t1: 0.0236 - val_loss: 1.7523 - val_mae: 0.4624 - val_mean_pred: 0.9422 - val_mae_t1: 0.0308\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.3485 - mae: 0.3559 - mean_pred: 0.8617 - mae_t1: 0.0237 - val_loss: 1.7535 - val_mae: 0.4627 - val_mean_pred: 0.9616 - val_mae_t1: 0.0308\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.3887 - mae: 0.3665 - mean_pred: 0.8874 - mae_t1: 0.0244 - val_loss: 1.7649 - val_mae: 0.4657 - val_mean_pred: 0.9977 - val_mae_t1: 0.0310\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.4442 - mae: 0.3811 - mean_pred: 0.9254 - mae_t1: 0.0254 - val_loss: 1.7409 - val_mae: 0.4594 - val_mean_pred: 0.9464 - val_mae_t1: 0.0306\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.3414 - mae: 0.3540 - mean_pred: 0.8353 - mae_t1: 0.0236 - val_loss: 1.7104 - val_mae: 0.4514 - val_mean_pred: 0.8319 - val_mae_t1: 0.0301\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3091 - mae: 0.3455 - mean_pred: 0.7720 - mae_t1: 0.0230 - val_loss: 1.6770 - val_mae: 0.4425 - val_mean_pred: 0.8816 - val_mae_t1: 0.0295\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 1.3308 - mae: 0.3512 - mean_pred: 0.8134 - mae_t1: 0.0234 - val_loss: 1.7026 - val_mae: 0.4493 - val_mean_pred: 0.9545 - val_mae_t1: 0.0300\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3278 - mae: 0.3504 - mean_pred: 0.8675 - mae_t1: 0.0234 - val_loss: 1.8187 - val_mae: 0.4799 - val_mean_pred: 1.0075 - val_mae_t1: 0.0320\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.3659 - mae: 0.3604 - mean_pred: 0.8900 - mae_t1: 0.0240 - val_loss: 1.7639 - val_mae: 0.4655 - val_mean_pred: 0.9538 - val_mae_t1: 0.0310\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.2526 - mae: 0.3306 - mean_pred: 0.8030 - mae_t1: 0.0220 - val_loss: 1.7802 - val_mae: 0.4698 - val_mean_pred: 0.8088 - val_mae_t1: 0.0313\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.4026 - mae: 0.3701 - mean_pred: 0.6939 - mae_t1: 0.0247 - val_loss: 1.8609 - val_mae: 0.4911 - val_mean_pred: 0.7093 - val_mae_t1: 0.0327\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.4818 - mae: 0.3910 - mean_pred: 0.6187 - mae_t1: 0.0261 - val_loss: 1.8328 - val_mae: 0.4837 - val_mean_pred: 0.7252 - val_mae_t1: 0.0322\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.4085 - mae: 0.3717 - mean_pred: 0.6403 - mae_t1: 0.0248 - val_loss: 1.6933 - val_mae: 0.4468 - val_mean_pred: 0.8467 - val_mae_t1: 0.0298\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 1.2963 - mae: 0.3421 - mean_pred: 0.7617 - mae_t1: 0.0228 - val_loss: 1.7387 - val_mae: 0.4588 - val_mean_pred: 0.9987 - val_mae_t1: 0.0306\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.3444 - mae: 0.3548 - mean_pred: 0.8894 - mae_t1: 0.0237 - val_loss: 1.7361 - val_mae: 0.4581 - val_mean_pred: 0.9794 - val_mae_t1: 0.0305\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.2734 - mae: 0.3360 - mean_pred: 0.8343 - mae_t1: 0.0224 - val_loss: 1.8203 - val_mae: 0.4804 - val_mean_pred: 0.8389 - val_mae_t1: 0.0320\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.3910 - mae: 0.3671 - mean_pred: 0.7250 - mae_t1: 0.0245 - val_loss: 1.9001 - val_mae: 0.5014 - val_mean_pred: 0.8570 - val_mae_t1: 0.0334\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.4222 - mae: 0.3753 - mean_pred: 0.7762 - mae_t1: 0.0250 - val_loss: 1.7568 - val_mae: 0.4636 - val_mean_pred: 0.9616 - val_mae_t1: 0.0309\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.3182 - mae: 0.3479 - mean_pred: 0.8783 - mae_t1: 0.0232 - val_loss: 1.8247 - val_mae: 0.4815 - val_mean_pred: 0.9756 - val_mae_t1: 0.0321\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 1.4093 - mae: 0.3719 - mean_pred: 0.8917 - mae_t1: 0.0248 - val_loss: 1.7900 - val_mae: 0.4724 - val_mean_pred: 0.8945 - val_mae_t1: 0.0315\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.3381 - mae: 0.3531 - mean_pred: 0.7992 - mae_t1: 0.0235 - val_loss: 1.8975 - val_mae: 0.5007 - val_mean_pred: 0.8603 - val_mae_t1: 0.0334\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.4431 - mae: 0.3808 - mean_pred: 0.7669 - mae_t1: 0.0254 - val_loss: 1.8565 - val_mae: 0.4899 - val_mean_pred: 0.8985 - val_mae_t1: 0.0327\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3383 - mae: 0.3532 - mean_pred: 0.8054 - mae_t1: 0.0235 - val_loss: 1.7561 - val_mae: 0.4634 - val_mean_pred: 0.9279 - val_mae_t1: 0.0309\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.2942 - mae: 0.3415 - mean_pred: 0.8065 - mae_t1: 0.0228 - val_loss: 1.7010 - val_mae: 0.4489 - val_mean_pred: 0.8953 - val_mae_t1: 0.0299\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.2617 - mae: 0.3329 - mean_pred: 0.7813 - mae_t1: 0.0222 - val_loss: 1.7541 - val_mae: 0.4629 - val_mean_pred: 0.9444 - val_mae_t1: 0.0309\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.2757 - mae: 0.3367 - mean_pred: 0.8197 - mae_t1: 0.0224 - val_loss: 1.7604 - val_mae: 0.4646 - val_mean_pred: 0.9871 - val_mae_t1: 0.0310\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 1.3099 - mae: 0.3457 - mean_pred: 0.8700 - mae_t1: 0.0230 - val_loss: 1.7450 - val_mae: 0.4605 - val_mean_pred: 0.9731 - val_mae_t1: 0.0307\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.2622 - mae: 0.3331 - mean_pred: 0.8308 - mae_t1: 0.0222 - val_loss: 1.6921 - val_mae: 0.4465 - val_mean_pred: 0.8912 - val_mae_t1: 0.0298\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 1.2530 - mae: 0.3307 - mean_pred: 0.7687 - mae_t1: 0.0220 - val_loss: 1.6893 - val_mae: 0.4458 - val_mean_pred: 0.8500 - val_mae_t1: 0.0297\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 1.3023 - mae: 0.3437 - mean_pred: 0.7366 - mae_t1: 0.0229 - val_loss: 1.7265 - val_mae: 0.4556 - val_mean_pred: 0.8684 - val_mae_t1: 0.0304\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 1.2515 - mae: 0.3303 - mean_pred: 0.7718 - mae_t1: 0.0220 - val_loss: 1.7583 - val_mae: 0.4640 - val_mean_pred: 0.9811 - val_mae_t1: 0.0309\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.2236 - mae: 0.3229 - mean_pred: 0.8741 - mae_t1: 0.0215 - val_loss: 1.7883 - val_mae: 0.4719 - val_mean_pred: 0.9692 - val_mae_t1: 0.0315\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 119us/sample - loss: 1.2375 - mae: 0.3266 - mean_pred: 0.8513 - mae_t1: 0.0218 - val_loss: 1.7731 - val_mae: 0.4679 - val_mean_pred: 0.8165 - val_mae_t1: 0.0312\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 1.2733 - mae: 0.3360 - mean_pred: 0.7045 - mae_t1: 0.0224 - val_loss: 1.8785 - val_mae: 0.4957 - val_mean_pred: 0.7477 - val_mae_t1: 0.0330\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 1.2996 - mae: 0.3429 - mean_pred: 0.6834 - mae_t1: 0.0229 - val_loss: 1.7627 - val_mae: 0.4652 - val_mean_pred: 0.8605 - val_mae_t1: 0.0310\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 1.2577 - mae: 0.3319 - mean_pred: 0.8121 - mae_t1: 0.0221 - val_loss: 1.8136 - val_mae: 0.4786 - val_mean_pred: 0.9506 - val_mae_t1: 0.0319\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 1.3669 - mae: 0.3607 - mean_pred: 0.8690 - mae_t1: 0.0240 - val_loss: 1.7478 - val_mae: 0.4612 - val_mean_pred: 0.8459 - val_mae_t1: 0.0307\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3165431790045134  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▄▄▄▅▄▄▃▃▃▂▃▂▃▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▄▄▄▅▄▄▃▃▃▂▃▂▃▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▄▄▄▅▄▄▃▃▃▂▃▂▃▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▆▆▆▆▅█▆▃▄█▆▇▅▆▅▅█▅▇▅▆▇▆▇▇█▆▇▆▆▇▇▆▇▇▆▆▆▇</td></tr><tr><td>val_loss</td><td>▆▃▃▄▁█▅▆▆▃▄▂▁▅▅▇▂▃▂▁▂▃▂▄▃▂▂▁▂▃▂▃▃▃▁▂▁▂▃▂</td></tr><tr><td>val_mae</td><td>▆▃▃▄▁█▅▆▆▃▄▂▁▅▅▇▂▃▂▁▂▃▂▄▃▂▂▁▂▃▂▃▃▃▁▂▁▂▃▂</td></tr><tr><td>val_mae_t1</td><td>▆▃▃▄▁█▅▆▆▃▄▂▁▅▅▇▂▃▂▁▂▃▂▄▃▂▂▁▂▃▂▃▃▃▁▂▁▂▃▂</td></tr><tr><td>val_mean_pred</td><td>▁▄▅▄▆▁█▂▂▃▇▄▆▃▅▃▅▅▄▅▄▅▇▅▆▆▆▅▆▄▇▅▆▅▅▆▅▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.40538</td></tr><tr><td>AE_2</td><td>0.28114</td></tr><tr><td>AE_3</td><td>0.32862</td></tr><tr><td>MAE</td><td>0.31654</td></tr><tr><td>best_epoch</td><td>47</td></tr><tr><td>best_val_loss</td><td>1.66798</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.36693</td></tr><tr><td>mae</td><td>0.36072</td></tr><tr><td>mae_t1</td><td>0.02405</td></tr><tr><td>mean_pred</td><td>0.86898</td></tr><tr><td>val_loss</td><td>1.74776</td></tr><tr><td>val_mae</td><td>0.46121</td></tr><tr><td>val_mae_t1</td><td>0.03075</td></tr><tr><td>val_mean_pred</td><td>0.8459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">atomic-gorge-84</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mglyyon\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3mglyyon</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155442-3mglyyon\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155504-1we0iueq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1we0iueq\" target=\"_blank\">misunderstood-snowball-85</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 854us/sample - loss: 4.3477 - mae: 0.7376 - mean_pred: 0.3620 - mae_t1: 0.0492 - val_loss: 2.9409 - val_mae: 0.4989 - val_mean_pred: 0.9015 - val_mae_t1: 0.0333\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 286us/sample - loss: 3.4190 - mae: 0.5800 - mean_pred: 0.7612 - mae_t1: 0.0387 - val_loss: 2.9193 - val_mae: 0.4952 - val_mean_pred: 0.6897 - val_mae_t1: 0.0330\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 265us/sample - loss: 3.1498 - mae: 0.5343 - mean_pred: 0.7146 - mae_t1: 0.0356 - val_loss: 2.8716 - val_mae: 0.4871 - val_mean_pred: 0.8662 - val_mae_t1: 0.0325\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 218us/sample - loss: 2.9805 - mae: 0.5056 - mean_pred: 0.7127 - mae_t1: 0.0337 - val_loss: 3.1097 - val_mae: 0.5275 - val_mean_pred: 0.9458 - val_mae_t1: 0.0352\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.8985 - mae: 0.4917 - mean_pred: 0.9231 - mae_t1: 0.0328 - val_loss: 2.8878 - val_mae: 0.4899 - val_mean_pred: 0.9296 - val_mae_t1: 0.0327\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.7359 - mae: 0.4641 - mean_pred: 0.8684 - mae_t1: 0.0309 - val_loss: 2.8984 - val_mae: 0.4917 - val_mean_pred: 0.8226 - val_mae_t1: 0.0328\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.5757 - mae: 0.4369 - mean_pred: 0.7774 - mae_t1: 0.0291 - val_loss: 2.8989 - val_mae: 0.4918 - val_mean_pred: 0.9965 - val_mae_t1: 0.0328\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6986 - mae: 0.4578 - mean_pred: 0.9498 - mae_t1: 0.0305 - val_loss: 3.1113 - val_mae: 0.5278 - val_mean_pred: 0.6996 - val_mae_t1: 0.0352\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.8666 - mae: 0.4863 - mean_pred: 0.5814 - mae_t1: 0.0324 - val_loss: 3.0730 - val_mae: 0.5213 - val_mean_pred: 0.8972 - val_mae_t1: 0.0348\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 2.7450 - mae: 0.4657 - mean_pred: 0.9635 - mae_t1: 0.0310 - val_loss: 2.8497 - val_mae: 0.4834 - val_mean_pred: 0.8620 - val_mae_t1: 0.0322\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.6510 - mae: 0.4497 - mean_pred: 0.5995 - mae_t1: 0.0300 - val_loss: 3.2232 - val_mae: 0.5468 - val_mean_pred: 0.5781 - val_mae_t1: 0.0365\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 2.6772 - mae: 0.4542 - mean_pred: 0.6178 - mae_t1: 0.0303 - val_loss: 2.6565 - val_mae: 0.4507 - val_mean_pred: 0.8767 - val_mae_t1: 0.0300\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 2.4702 - mae: 0.4191 - mean_pred: 0.7464 - mae_t1: 0.0279 - val_loss: 2.6650 - val_mae: 0.4521 - val_mean_pred: 0.8559 - val_mae_t1: 0.0301\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3169 - mae: 0.3931 - mean_pred: 0.8078 - mae_t1: 0.0262 - val_loss: 2.6976 - val_mae: 0.4576 - val_mean_pred: 0.8718 - val_mae_t1: 0.0305\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.2394 - mae: 0.3799 - mean_pred: 0.8142 - mae_t1: 0.0253 - val_loss: 2.9251 - val_mae: 0.4962 - val_mean_pred: 0.8772 - val_mae_t1: 0.0331\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5510 - mae: 0.4328 - mean_pred: 0.8658 - mae_t1: 0.0289 - val_loss: 2.8043 - val_mae: 0.4757 - val_mean_pred: 0.9541 - val_mae_t1: 0.0317\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2823 - mae: 0.3872 - mean_pred: 0.8980 - mae_t1: 0.0258 - val_loss: 2.8623 - val_mae: 0.4856 - val_mean_pred: 0.8049 - val_mae_t1: 0.0324\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 2.1707 - mae: 0.3682 - mean_pred: 0.7729 - mae_t1: 0.0245 - val_loss: 2.5545 - val_mae: 0.4333 - val_mean_pred: 0.9407 - val_mae_t1: 0.0289\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 2.2033 - mae: 0.3738 - mean_pred: 0.8301 - mae_t1: 0.0249 - val_loss: 2.6660 - val_mae: 0.4523 - val_mean_pred: 0.8881 - val_mae_t1: 0.0302\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2288 - mae: 0.3781 - mean_pred: 0.8145 - mae_t1: 0.0252 - val_loss: 2.6647 - val_mae: 0.4521 - val_mean_pred: 0.9002 - val_mae_t1: 0.0301\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2855 - mae: 0.3877 - mean_pred: 0.7375 - mae_t1: 0.0258 - val_loss: 2.7659 - val_mae: 0.4692 - val_mean_pred: 0.9417 - val_mae_t1: 0.0313\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3042 - mae: 0.3909 - mean_pred: 0.8945 - mae_t1: 0.0261 - val_loss: 2.8639 - val_mae: 0.4858 - val_mean_pred: 1.0194 - val_mae_t1: 0.0324\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1672 - mae: 0.3676 - mean_pred: 0.8511 - mae_t1: 0.0245 - val_loss: 2.8023 - val_mae: 0.4754 - val_mean_pred: 0.8681 - val_mae_t1: 0.0317\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.1544 - mae: 0.3655 - mean_pred: 0.7751 - mae_t1: 0.0244 - val_loss: 2.7227 - val_mae: 0.4619 - val_mean_pred: 0.9326 - val_mae_t1: 0.0308\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.1195 - mae: 0.3596 - mean_pred: 0.8556 - mae_t1: 0.0240 - val_loss: 2.5504 - val_mae: 0.4327 - val_mean_pred: 0.9448 - val_mae_t1: 0.0288\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1082 - mae: 0.3576 - mean_pred: 0.8915 - mae_t1: 0.0238 - val_loss: 2.7645 - val_mae: 0.4690 - val_mean_pred: 0.7327 - val_mae_t1: 0.0313\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3220 - mae: 0.3939 - mean_pred: 0.6347 - mae_t1: 0.0263 - val_loss: 3.0634 - val_mae: 0.5197 - val_mean_pred: 0.8478 - val_mae_t1: 0.0346\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.6844 - mae: 0.4554 - mean_pred: 0.8930 - mae_t1: 0.0304 - val_loss: 3.4030 - val_mae: 0.5773 - val_mean_pred: 1.1644 - val_mae_t1: 0.0385\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0517 - mae: 0.5177 - mean_pred: 0.9265 - mae_t1: 0.0345 - val_loss: 2.8555 - val_mae: 0.4844 - val_mean_pred: 0.8075 - val_mae_t1: 0.0323\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3360 - mae: 0.3963 - mean_pred: 0.6930 - mae_t1: 0.0264 - val_loss: 2.9052 - val_mae: 0.4928 - val_mean_pred: 0.8309 - val_mae_t1: 0.0329\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.2621 - mae: 0.3837 - mean_pred: 0.7364 - mae_t1: 0.0256 - val_loss: 3.2117 - val_mae: 0.5448 - val_mean_pred: 0.9332 - val_mae_t1: 0.0363\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.3563 - mae: 0.3997 - mean_pred: 0.8445 - mae_t1: 0.0266 - val_loss: 2.8643 - val_mae: 0.4859 - val_mean_pred: 0.8702 - val_mae_t1: 0.0324\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.3505 - mae: 0.3988 - mean_pred: 0.7103 - mae_t1: 0.0266 - val_loss: 3.0571 - val_mae: 0.5186 - val_mean_pred: 0.6824 - val_mae_t1: 0.0346\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1340 - mae: 0.3620 - mean_pred: 0.6936 - mae_t1: 0.0241 - val_loss: 3.1422 - val_mae: 0.5331 - val_mean_pred: 1.0999 - val_mae_t1: 0.0355\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4272 - mae: 0.4118 - mean_pred: 0.9800 - mae_t1: 0.0275 - val_loss: 2.7330 - val_mae: 0.4636 - val_mean_pred: 0.8861 - val_mae_t1: 0.0309\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.2365 - mae: 0.3794 - mean_pred: 0.7215 - mae_t1: 0.0253 - val_loss: 2.6173 - val_mae: 0.4440 - val_mean_pred: 0.7032 - val_mae_t1: 0.0296\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 2.0405 - mae: 0.3462 - mean_pred: 0.7191 - mae_t1: 0.0231 - val_loss: 2.8672 - val_mae: 0.4864 - val_mean_pred: 1.0560 - val_mae_t1: 0.0324\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4548 - mae: 0.4164 - mean_pred: 0.9480 - mae_t1: 0.0278 - val_loss: 2.6741 - val_mae: 0.4536 - val_mean_pred: 0.8923 - val_mae_t1: 0.0302\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.0610 - mae: 0.3496 - mean_pred: 0.7323 - mae_t1: 0.0233 - val_loss: 2.7600 - val_mae: 0.4682 - val_mean_pred: 0.7260 - val_mae_t1: 0.0312\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0774 - mae: 0.3524 - mean_pred: 0.6730 - mae_t1: 0.0235 - val_loss: 2.7984 - val_mae: 0.4747 - val_mean_pred: 0.9551 - val_mae_t1: 0.0316\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1752 - mae: 0.3690 - mean_pred: 0.9099 - mae_t1: 0.0246 - val_loss: 2.8653 - val_mae: 0.4861 - val_mean_pred: 1.0220 - val_mae_t1: 0.0324\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.0884 - mae: 0.3543 - mean_pred: 0.8443 - mae_t1: 0.0236 - val_loss: 2.8002 - val_mae: 0.4750 - val_mean_pred: 0.6899 - val_mae_t1: 0.0317\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2201 - mae: 0.3766 - mean_pred: 0.6364 - mae_t1: 0.0251 - val_loss: 2.6373 - val_mae: 0.4474 - val_mean_pred: 0.8721 - val_mae_t1: 0.0298\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8735 - mae: 0.3178 - mean_pred: 0.8364 - mae_t1: 0.0212 - val_loss: 2.8175 - val_mae: 0.4780 - val_mean_pred: 1.0098 - val_mae_t1: 0.0319\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.8761 - mae: 0.3183 - mean_pred: 0.8039 - mae_t1: 0.0212 - val_loss: 2.6781 - val_mae: 0.4543 - val_mean_pred: 0.8012 - val_mae_t1: 0.0303\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.0654 - mae: 0.3504 - mean_pred: 0.7030 - mae_t1: 0.0234 - val_loss: 2.6839 - val_mae: 0.4553 - val_mean_pred: 0.8108 - val_mae_t1: 0.0304\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8436 - mae: 0.3128 - mean_pred: 0.7847 - mae_t1: 0.0209 - val_loss: 2.9330 - val_mae: 0.4976 - val_mean_pred: 1.0395 - val_mae_t1: 0.0332\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.8643 - mae: 0.3163 - mean_pred: 0.8869 - mae_t1: 0.0211 - val_loss: 2.6055 - val_mae: 0.4420 - val_mean_pred: 0.8413 - val_mae_t1: 0.0295\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.8367 - mae: 0.3116 - mean_pred: 0.7031 - mae_t1: 0.0208 - val_loss: 2.5012 - val_mae: 0.4243 - val_mean_pred: 0.8228 - val_mae_t1: 0.0283\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8199 - mae: 0.3087 - mean_pred: 0.7886 - mae_t1: 0.0206 - val_loss: 2.6649 - val_mae: 0.4521 - val_mean_pred: 0.9753 - val_mae_t1: 0.0301\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7865 - mae: 0.3031 - mean_pred: 0.8554 - mae_t1: 0.0202 - val_loss: 2.6401 - val_mae: 0.4479 - val_mean_pred: 0.8489 - val_mae_t1: 0.0299\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9169 - mae: 0.3252 - mean_pred: 0.7232 - mae_t1: 0.0217 - val_loss: 2.8131 - val_mae: 0.4772 - val_mean_pred: 0.8251 - val_mae_t1: 0.0318\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8496 - mae: 0.3138 - mean_pred: 0.8289 - mae_t1: 0.0209 - val_loss: 2.8339 - val_mae: 0.4808 - val_mean_pred: 0.9872 - val_mae_t1: 0.0321\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7640 - mae: 0.2992 - mean_pred: 0.8237 - mae_t1: 0.0199 - val_loss: 2.7224 - val_mae: 0.4618 - val_mean_pred: 0.8308 - val_mae_t1: 0.0308\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.7309 - mae: 0.2936 - mean_pred: 0.7227 - mae_t1: 0.0196 - val_loss: 2.6667 - val_mae: 0.4524 - val_mean_pred: 0.8400 - val_mae_t1: 0.0302\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.7559 - mae: 0.2979 - mean_pred: 0.7325 - mae_t1: 0.0199 - val_loss: 2.5364 - val_mae: 0.4303 - val_mean_pred: 0.8272 - val_mae_t1: 0.0287\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.7848 - mae: 0.3028 - mean_pred: 0.7607 - mae_t1: 0.0202 - val_loss: 2.7928 - val_mae: 0.4738 - val_mean_pred: 0.8752 - val_mae_t1: 0.0316\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.0007 - mae: 0.3394 - mean_pred: 0.7665 - mae_t1: 0.0226 - val_loss: 2.6424 - val_mae: 0.4483 - val_mean_pred: 0.9105 - val_mae_t1: 0.0299\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9468 - mae: 0.3303 - mean_pred: 0.8561 - mae_t1: 0.0220 - val_loss: 2.8118 - val_mae: 0.4770 - val_mean_pred: 0.9150 - val_mae_t1: 0.0318\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.1693 - mae: 0.3680 - mean_pred: 0.7982 - mae_t1: 0.0245 - val_loss: 2.6322 - val_mae: 0.4465 - val_mean_pred: 0.9198 - val_mae_t1: 0.0298\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.0763 - mae: 0.3522 - mean_pred: 0.8654 - mae_t1: 0.0235 - val_loss: 2.7994 - val_mae: 0.4749 - val_mean_pred: 0.9173 - val_mae_t1: 0.0317\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.8108 - mae: 0.3072 - mean_pred: 0.7862 - mae_t1: 0.0205 - val_loss: 2.9522 - val_mae: 0.5008 - val_mean_pred: 0.8757 - val_mae_t1: 0.0334\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.7914 - mae: 0.3039 - mean_pred: 0.8121 - mae_t1: 0.0203 - val_loss: 2.9261 - val_mae: 0.4964 - val_mean_pred: 1.0073 - val_mae_t1: 0.0331\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 1.9960 - mae: 0.3386 - mean_pred: 0.9062 - mae_t1: 0.0226 - val_loss: 2.8360 - val_mae: 0.4811 - val_mean_pred: 0.9117 - val_mae_t1: 0.0321\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0390 - mae: 0.3459 - mean_pred: 0.7910 - mae_t1: 0.0231 - val_loss: 2.8556 - val_mae: 0.4844 - val_mean_pred: 0.8981 - val_mae_t1: 0.0323\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9009 - mae: 0.3225 - mean_pred: 0.8392 - mae_t1: 0.0215 - val_loss: 2.7701 - val_mae: 0.4699 - val_mean_pred: 0.9322 - val_mae_t1: 0.0313\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.6101 - mae: 0.2731 - mean_pred: 0.7948 - mae_t1: 0.0182 - val_loss: 2.5385 - val_mae: 0.4306 - val_mean_pred: 0.8081 - val_mae_t1: 0.0287\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7226 - mae: 0.2922 - mean_pred: 0.7356 - mae_t1: 0.0195 - val_loss: 2.5193 - val_mae: 0.4274 - val_mean_pred: 0.9059 - val_mae_t1: 0.0285\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 1.7645 - mae: 0.2993 - mean_pred: 0.8368 - mae_t1: 0.0200 - val_loss: 2.7252 - val_mae: 0.4623 - val_mean_pred: 0.9711 - val_mae_t1: 0.0308\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6022 - mae: 0.2718 - mean_pred: 0.8354 - mae_t1: 0.0181 - val_loss: 2.6598 - val_mae: 0.4512 - val_mean_pred: 0.8337 - val_mae_t1: 0.0301\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.6243 - mae: 0.2756 - mean_pred: 0.7518 - mae_t1: 0.0184 - val_loss: 2.5671 - val_mae: 0.4355 - val_mean_pred: 0.8959 - val_mae_t1: 0.0290\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5303 - mae: 0.2596 - mean_pred: 0.8306 - mae_t1: 0.0173 - val_loss: 2.5853 - val_mae: 0.4386 - val_mean_pred: 0.8788 - val_mae_t1: 0.0292\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.5689 - mae: 0.2662 - mean_pred: 0.7455 - mae_t1: 0.0177 - val_loss: 2.5804 - val_mae: 0.4377 - val_mean_pred: 0.8343 - val_mae_t1: 0.0292\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 1.5645 - mae: 0.2654 - mean_pred: 0.7680 - mae_t1: 0.0177 - val_loss: 2.7656 - val_mae: 0.4692 - val_mean_pred: 1.0325 - val_mae_t1: 0.0313\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 214us/sample - loss: 1.7034 - mae: 0.2890 - mean_pred: 0.9166 - mae_t1: 0.0193 - val_loss: 2.4958 - val_mae: 0.4234 - val_mean_pred: 0.8727 - val_mae_t1: 0.0282\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6996 - mae: 0.2883 - mean_pred: 0.7186 - mae_t1: 0.0192 - val_loss: 2.6418 - val_mae: 0.4482 - val_mean_pred: 0.7916 - val_mae_t1: 0.0299\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6144 - mae: 0.2739 - mean_pred: 0.7722 - mae_t1: 0.0183 - val_loss: 2.6801 - val_mae: 0.4547 - val_mean_pred: 0.9712 - val_mae_t1: 0.0303\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6309 - mae: 0.2767 - mean_pred: 0.8406 - mae_t1: 0.0184 - val_loss: 2.5555 - val_mae: 0.4335 - val_mean_pred: 0.9003 - val_mae_t1: 0.0289\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5767 - mae: 0.2675 - mean_pred: 0.8071 - mae_t1: 0.0178 - val_loss: 2.6229 - val_mae: 0.4450 - val_mean_pred: 0.8821 - val_mae_t1: 0.0297\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5330 - mae: 0.2601 - mean_pred: 0.7681 - mae_t1: 0.0173 - val_loss: 2.5012 - val_mae: 0.4243 - val_mean_pred: 0.9152 - val_mae_t1: 0.0283\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.4860 - mae: 0.2521 - mean_pred: 0.7786 - mae_t1: 0.0168 - val_loss: 2.5961 - val_mae: 0.4404 - val_mean_pred: 0.9606 - val_mae_t1: 0.0294\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.4668 - mae: 0.2488 - mean_pred: 0.8142 - mae_t1: 0.0166 - val_loss: 2.5310 - val_mae: 0.4294 - val_mean_pred: 0.9130 - val_mae_t1: 0.0286\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.4353 - mae: 0.2435 - mean_pred: 0.7856 - mae_t1: 0.0162 - val_loss: 2.5433 - val_mae: 0.4314 - val_mean_pred: 0.9021 - val_mae_t1: 0.0288\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4146 - mae: 0.2400 - mean_pred: 0.7795 - mae_t1: 0.0160 - val_loss: 2.5862 - val_mae: 0.4387 - val_mean_pred: 0.8944 - val_mae_t1: 0.0292\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.4464 - mae: 0.2454 - mean_pred: 0.8041 - mae_t1: 0.0164 - val_loss: 2.5974 - val_mae: 0.4406 - val_mean_pred: 0.9060 - val_mae_t1: 0.0294\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.5766 - mae: 0.2675 - mean_pred: 0.7686 - mae_t1: 0.0178 - val_loss: 2.8033 - val_mae: 0.4756 - val_mean_pred: 0.9093 - val_mae_t1: 0.0317\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7196 - mae: 0.2917 - mean_pred: 0.8442 - mae_t1: 0.0194 - val_loss: 2.9010 - val_mae: 0.4921 - val_mean_pred: 0.9756 - val_mae_t1: 0.0328\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.7419 - mae: 0.2955 - mean_pred: 0.8003 - mae_t1: 0.0197 - val_loss: 2.7608 - val_mae: 0.4683 - val_mean_pred: 0.7931 - val_mae_t1: 0.0312\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6938 - mae: 0.2873 - mean_pred: 0.7213 - mae_t1: 0.0192 - val_loss: 2.7279 - val_mae: 0.4628 - val_mean_pred: 1.0023 - val_mae_t1: 0.0309\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.6600 - mae: 0.2816 - mean_pred: 0.9240 - mae_t1: 0.0188 - val_loss: 2.5547 - val_mae: 0.4334 - val_mean_pred: 0.8977 - val_mae_t1: 0.0289\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.5381 - mae: 0.2609 - mean_pred: 0.7477 - mae_t1: 0.0174 - val_loss: 2.7629 - val_mae: 0.4687 - val_mean_pred: 0.7673 - val_mae_t1: 0.0312\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5832 - mae: 0.2686 - mean_pred: 0.7187 - mae_t1: 0.0179 - val_loss: 2.8638 - val_mae: 0.4858 - val_mean_pred: 0.9650 - val_mae_t1: 0.0324\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6235 - mae: 0.2754 - mean_pred: 0.8763 - mae_t1: 0.0184 - val_loss: 2.7253 - val_mae: 0.4623 - val_mean_pred: 0.9269 - val_mae_t1: 0.0308\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4753 - mae: 0.2503 - mean_pred: 0.7857 - mae_t1: 0.0167 - val_loss: 2.6365 - val_mae: 0.4473 - val_mean_pred: 0.8370 - val_mae_t1: 0.0298\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.6224 - mae: 0.2752 - mean_pred: 0.7770 - mae_t1: 0.0183 - val_loss: 2.5808 - val_mae: 0.4378 - val_mean_pred: 0.9516 - val_mae_t1: 0.0292\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.5725 - mae: 0.2668 - mean_pred: 0.8738 - mae_t1: 0.0178 - val_loss: 2.6531 - val_mae: 0.4501 - val_mean_pred: 0.9611 - val_mae_t1: 0.0300\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 1.5592 - mae: 0.2645 - mean_pred: 0.8412 - mae_t1: 0.0176 - val_loss: 2.8419 - val_mae: 0.4821 - val_mean_pred: 0.9008 - val_mae_t1: 0.0321\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6946 - mae: 0.2875 - mean_pred: 0.7915 - mae_t1: 0.0192 - val_loss: 2.9675 - val_mae: 0.5034 - val_mean_pred: 0.9742 - val_mae_t1: 0.0336\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6060 - mae: 0.2725 - mean_pred: 0.8583 - mae_t1: 0.0182 - val_loss: 2.7570 - val_mae: 0.4677 - val_mean_pred: 0.8910 - val_mae_t1: 0.0312\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 1.5388 - mae: 0.2610 - mean_pred: 0.7580 - mae_t1: 0.0174 - val_loss: 2.6777 - val_mae: 0.4543 - val_mean_pred: 0.8611 - val_mae_t1: 0.0303\n",
      "Earliness...\n",
      "0.001999378204345703\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3107233316298813  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▄▃▃▃▃▄▃▃▃▃▃▂▃▂▂▂▂▂▃▂▁▂▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▄▄▄▃▃▃▃▄▃▃▃▃▃▂▃▂▂▂▂▂▃▂▁▂▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▄▄▄▃▃▃▃▄▃▃▃▃▃▂▃▂▂▂▂▂▃▂▁▂▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▅▇█▄▆▇▆▅▇▇▇▅▅▅▅█▇▅▅▇▆▅▇▇▇▆▇▇▆▆▆▆▆▇▅▅▆▇▆</td></tr><tr><td>val_loss</td><td>▄▄▄▆▇▂▃▁▃▃▃█▇▆▂▃▄▃▂▁▂▃▁▃▃▄▁▃▂▃▂▂▁▂▄▃▄▂▄▂</td></tr><tr><td>val_mae</td><td>▄▄▄▆▇▂▃▁▃▃▃█▇▆▂▃▄▃▂▁▂▃▁▃▃▄▁▃▂▃▂▂▁▂▄▃▄▂▄▂</td></tr><tr><td>val_mae_t1</td><td>▄▄▄▆▇▂▃▁▃▃▃█▇▆▂▃▄▃▂▁▂▃▁▃▃▄▁▃▂▃▂▂▁▂▄▃▄▂▄▂</td></tr><tr><td>val_mean_pred</td><td>▅▄▄▂▁▄▅▅▅▄▃█▅▇▂▃▆▆▄▄▄▄▄▅▅▅▄▆▅▆▆▅▅▅▆▆▆▄▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33537</td></tr><tr><td>AE_2</td><td>0.30359</td></tr><tr><td>AE_3</td><td>0.30006</td></tr><tr><td>MAE</td><td>0.31072</td></tr><tr><td>best_epoch</td><td>74</td></tr><tr><td>best_val_loss</td><td>2.49577</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.53877</td></tr><tr><td>mae</td><td>0.26104</td></tr><tr><td>mae_t1</td><td>0.0174</td></tr><tr><td>mean_pred</td><td>0.75803</td></tr><tr><td>val_loss</td><td>2.67769</td></tr><tr><td>val_mae</td><td>0.45425</td></tr><tr><td>val_mae_t1</td><td>0.03028</td></tr><tr><td>val_mean_pred</td><td>0.86105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">misunderstood-snowball-85</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1we0iueq\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1we0iueq</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155504-1we0iueq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155531-2sgkur3c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sgkur3c\" target=\"_blank\">pleasant-dawn-86</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 708us/sample - loss: 4.8727 - mae: 0.8266 - mean_pred: 0.2169 - mae_t1: 0.0551 - val_loss: 3.4451 - val_mae: 0.5844 - val_mean_pred: 0.5059 - val_mae_t1: 0.0390\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.7265 - mae: 0.6322 - mean_pred: 0.5908 - mae_t1: 0.0421 - val_loss: 3.8063 - val_mae: 0.6457 - val_mean_pred: 1.1781 - val_mae_t1: 0.0430\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 4.8734 - mae: 0.8267 - mean_pred: 1.2490 - mae_t1: 0.0551 - val_loss: 3.8332 - val_mae: 0.6503 - val_mean_pred: 1.2417 - val_mae_t1: 0.0434\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 3.8724 - mae: 0.6569 - mean_pred: 1.0669 - mae_t1: 0.0438 - val_loss: 3.1823 - val_mae: 0.5399 - val_mean_pred: 0.5813 - val_mae_t1: 0.0360\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.4419 - mae: 0.5839 - mean_pred: 0.4379 - mae_t1: 0.0389 - val_loss: 4.2394 - val_mae: 0.7192 - val_mean_pred: 0.3350 - val_mae_t1: 0.0479\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 4.1659 - mae: 0.7067 - mean_pred: 0.3237 - mae_t1: 0.0471 - val_loss: 4.5761 - val_mae: 0.7763 - val_mean_pred: 0.4833 - val_mae_t1: 0.0518\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 4.3499 - mae: 0.7379 - mean_pred: 0.4844 - mae_t1: 0.0492 - val_loss: 3.9898 - val_mae: 0.6768 - val_mean_pred: 0.6378 - val_mae_t1: 0.0451\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.9568 - mae: 0.6712 - mean_pred: 0.5931 - mae_t1: 0.0447 - val_loss: 3.2386 - val_mae: 0.5494 - val_mean_pred: 0.8476 - val_mae_t1: 0.0366\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 134us/sample - loss: 3.2050 - mae: 0.5437 - mean_pred: 0.7385 - mae_t1: 0.0362 - val_loss: 2.8706 - val_mae: 0.4870 - val_mean_pred: 0.7761 - val_mae_t1: 0.0325\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.3193 - mae: 0.5631 - mean_pred: 0.7756 - mae_t1: 0.0375 - val_loss: 3.0431 - val_mae: 0.5162 - val_mean_pred: 0.8644 - val_mae_t1: 0.0344\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.4226 - mae: 0.5806 - mean_pred: 0.8641 - mae_t1: 0.0387 - val_loss: 2.9496 - val_mae: 0.5004 - val_mean_pred: 0.7651 - val_mae_t1: 0.0334\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.1060 - mae: 0.5269 - mean_pred: 0.7354 - mae_t1: 0.0351 - val_loss: 3.0881 - val_mae: 0.5239 - val_mean_pred: 0.7413 - val_mae_t1: 0.0349\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 3.0413 - mae: 0.5159 - mean_pred: 0.7517 - mae_t1: 0.0344 - val_loss: 2.9316 - val_mae: 0.4973 - val_mean_pred: 0.8700 - val_mae_t1: 0.0332\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 2.8574 - mae: 0.4847 - mean_pred: 0.8680 - mae_t1: 0.0323 - val_loss: 2.8463 - val_mae: 0.4828 - val_mean_pred: 0.9498 - val_mae_t1: 0.0322\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 2.9125 - mae: 0.4941 - mean_pred: 0.9490 - mae_t1: 0.0329 - val_loss: 2.8034 - val_mae: 0.4756 - val_mean_pred: 0.8630 - val_mae_t1: 0.0317\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 2.6396 - mae: 0.4478 - mean_pred: 0.7921 - mae_t1: 0.0299 - val_loss: 3.0686 - val_mae: 0.5206 - val_mean_pred: 0.7178 - val_mae_t1: 0.0347\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.6430 - mae: 0.4484 - mean_pred: 0.6935 - mae_t1: 0.0299 - val_loss: 2.9625 - val_mae: 0.5026 - val_mean_pred: 0.8154 - val_mae_t1: 0.0335\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.5534 - mae: 0.4332 - mean_pred: 0.7673 - mae_t1: 0.0289 - val_loss: 2.8953 - val_mae: 0.4912 - val_mean_pred: 0.9362 - val_mae_t1: 0.0327\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.5924 - mae: 0.4398 - mean_pred: 0.9009 - mae_t1: 0.0293 - val_loss: 2.8268 - val_mae: 0.4795 - val_mean_pred: 0.8957 - val_mae_t1: 0.0320\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 2.4906 - mae: 0.4225 - mean_pred: 0.7962 - mae_t1: 0.0282 - val_loss: 2.9622 - val_mae: 0.5025 - val_mean_pred: 0.8031 - val_mae_t1: 0.0335\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 2.5322 - mae: 0.4296 - mean_pred: 0.7692 - mae_t1: 0.0286 - val_loss: 3.2298 - val_mae: 0.5479 - val_mean_pred: 0.9418 - val_mae_t1: 0.0365\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 2.6266 - mae: 0.4456 - mean_pred: 0.8678 - mae_t1: 0.0297 - val_loss: 2.9965 - val_mae: 0.5083 - val_mean_pred: 0.8546 - val_mae_t1: 0.0339\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.6467 - mae: 0.4490 - mean_pred: 0.7404 - mae_t1: 0.0299 - val_loss: 3.0981 - val_mae: 0.5256 - val_mean_pred: 0.7658 - val_mae_t1: 0.0350\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.5007 - mae: 0.4242 - mean_pred: 0.6921 - mae_t1: 0.0283 - val_loss: 3.3848 - val_mae: 0.5742 - val_mean_pred: 0.9269 - val_mae_t1: 0.0383\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.6726 - mae: 0.4534 - mean_pred: 0.8415 - mae_t1: 0.0302 - val_loss: 3.0973 - val_mae: 0.5254 - val_mean_pred: 0.9584 - val_mae_t1: 0.0350\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.4055 - mae: 0.4081 - mean_pred: 0.8304 - mae_t1: 0.0272 - val_loss: 3.0966 - val_mae: 0.5253 - val_mean_pred: 0.8327 - val_mae_t1: 0.0350\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.4099 - mae: 0.4088 - mean_pred: 0.7444 - mae_t1: 0.0273 - val_loss: 2.9965 - val_mae: 0.5083 - val_mean_pred: 0.8382 - val_mae_t1: 0.0339\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.3510 - mae: 0.3988 - mean_pred: 0.7803 - mae_t1: 0.0266 - val_loss: 2.9228 - val_mae: 0.4958 - val_mean_pred: 0.9213 - val_mae_t1: 0.0331\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 144us/sample - loss: 2.4659 - mae: 0.4183 - mean_pred: 0.8922 - mae_t1: 0.0279 - val_loss: 2.7823 - val_mae: 0.4720 - val_mean_pred: 0.8966 - val_mae_t1: 0.0315\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.2356 - mae: 0.3793 - mean_pred: 0.8268 - mae_t1: 0.0253 - val_loss: 3.0854 - val_mae: 0.5234 - val_mean_pred: 0.6417 - val_mae_t1: 0.0349\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 2.5352 - mae: 0.4301 - mean_pred: 0.6040 - mae_t1: 0.0287 - val_loss: 3.2676 - val_mae: 0.5543 - val_mean_pred: 0.5349 - val_mae_t1: 0.0370\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 2.7198 - mae: 0.4614 - mean_pred: 0.5221 - mae_t1: 0.0308 - val_loss: 3.1272 - val_mae: 0.5305 - val_mean_pred: 0.5597 - val_mae_t1: 0.0354\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.7098 - mae: 0.4597 - mean_pred: 0.5293 - mae_t1: 0.0306 - val_loss: 2.8674 - val_mae: 0.4864 - val_mean_pred: 0.7014 - val_mae_t1: 0.0324\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.5426 - mae: 0.4313 - mean_pred: 0.6697 - mae_t1: 0.0288 - val_loss: 3.0183 - val_mae: 0.5120 - val_mean_pred: 1.0274 - val_mae_t1: 0.0341\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 2.7542 - mae: 0.4672 - mean_pred: 0.9656 - mae_t1: 0.0311 - val_loss: 3.4554 - val_mae: 0.5862 - val_mean_pred: 1.1733 - val_mae_t1: 0.0391\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 2.9662 - mae: 0.5032 - mean_pred: 1.0411 - mae_t1: 0.0335 - val_loss: 2.7433 - val_mae: 0.4654 - val_mean_pred: 0.8541 - val_mae_t1: 0.0310\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.5192 - mae: 0.4274 - mean_pred: 0.7237 - mae_t1: 0.0285 - val_loss: 2.8849 - val_mae: 0.4894 - val_mean_pred: 0.6605 - val_mae_t1: 0.0326\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.6035 - mae: 0.4417 - mean_pred: 0.6302 - mae_t1: 0.0294 - val_loss: 3.1051 - val_mae: 0.5268 - val_mean_pred: 0.8100 - val_mae_t1: 0.0351\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 2.7203 - mae: 0.4615 - mean_pred: 0.7661 - mae_t1: 0.0308 - val_loss: 2.6705 - val_mae: 0.4530 - val_mean_pred: 0.8898 - val_mae_t1: 0.0302\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.3982 - mae: 0.4068 - mean_pred: 0.8324 - mae_t1: 0.0271 - val_loss: 3.0432 - val_mae: 0.5163 - val_mean_pred: 0.9611 - val_mae_t1: 0.0344\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.7745 - mae: 0.4707 - mean_pred: 0.9008 - mae_t1: 0.0314 - val_loss: 2.8663 - val_mae: 0.4862 - val_mean_pred: 0.9390 - val_mae_t1: 0.0324\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.2972 - mae: 0.3897 - mean_pred: 0.8272 - mae_t1: 0.0260 - val_loss: 3.0962 - val_mae: 0.5253 - val_mean_pred: 0.8380 - val_mae_t1: 0.0350\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.3203 - mae: 0.3936 - mean_pred: 0.7517 - mae_t1: 0.0262 - val_loss: 3.1050 - val_mae: 0.5267 - val_mean_pred: 0.8173 - val_mae_t1: 0.0351\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 2.2858 - mae: 0.3878 - mean_pred: 0.7161 - mae_t1: 0.0259 - val_loss: 3.0992 - val_mae: 0.5258 - val_mean_pred: 0.7886 - val_mae_t1: 0.0351\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.2636 - mae: 0.3840 - mean_pred: 0.7233 - mae_t1: 0.0256 - val_loss: 2.8744 - val_mae: 0.4876 - val_mean_pred: 0.8108 - val_mae_t1: 0.0325\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.1994 - mae: 0.3731 - mean_pred: 0.7563 - mae_t1: 0.0249 - val_loss: 2.8200 - val_mae: 0.4784 - val_mean_pred: 0.8355 - val_mae_t1: 0.0319\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.3440 - mae: 0.3976 - mean_pred: 0.7628 - mae_t1: 0.0265 - val_loss: 2.7476 - val_mae: 0.4661 - val_mean_pred: 0.8844 - val_mae_t1: 0.0311\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 2.2597 - mae: 0.3833 - mean_pred: 0.8186 - mae_t1: 0.0256 - val_loss: 2.6457 - val_mae: 0.4488 - val_mean_pred: 0.8876 - val_mae_t1: 0.0299\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.1065 - mae: 0.3573 - mean_pred: 0.7703 - mae_t1: 0.0238 - val_loss: 2.7118 - val_mae: 0.4600 - val_mean_pred: 0.8102 - val_mae_t1: 0.0307\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.1979 - mae: 0.3729 - mean_pred: 0.7333 - mae_t1: 0.0249 - val_loss: 2.6771 - val_mae: 0.4542 - val_mean_pred: 0.8966 - val_mae_t1: 0.0303\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.1693 - mae: 0.3680 - mean_pred: 0.8354 - mae_t1: 0.0245 - val_loss: 2.9229 - val_mae: 0.4959 - val_mean_pred: 0.9747 - val_mae_t1: 0.0331\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.2811 - mae: 0.3870 - mean_pred: 0.8970 - mae_t1: 0.0258 - val_loss: 2.7378 - val_mae: 0.4644 - val_mean_pred: 0.9261 - val_mae_t1: 0.0310\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.1043 - mae: 0.3570 - mean_pred: 0.8360 - mae_t1: 0.0238 - val_loss: 2.7003 - val_mae: 0.4581 - val_mean_pred: 0.8431 - val_mae_t1: 0.0305\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.0656 - mae: 0.3504 - mean_pred: 0.7809 - mae_t1: 0.0234 - val_loss: 2.6939 - val_mae: 0.4570 - val_mean_pred: 0.8762 - val_mae_t1: 0.0305\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.0303 - mae: 0.3444 - mean_pred: 0.8258 - mae_t1: 0.0230 - val_loss: 2.7152 - val_mae: 0.4606 - val_mean_pred: 0.9171 - val_mae_t1: 0.0307\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 2.0028 - mae: 0.3398 - mean_pred: 0.8397 - mae_t1: 0.0227 - val_loss: 2.6377 - val_mae: 0.4475 - val_mean_pred: 0.8796 - val_mae_t1: 0.0298\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.0166 - mae: 0.3421 - mean_pred: 0.7902 - mae_t1: 0.0228 - val_loss: 2.8006 - val_mae: 0.4751 - val_mean_pred: 0.9875 - val_mae_t1: 0.0317\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.3255 - mae: 0.3945 - mean_pred: 0.9343 - mae_t1: 0.0263 - val_loss: 2.8167 - val_mae: 0.4778 - val_mean_pred: 1.0227 - val_mae_t1: 0.0319\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.0793 - mae: 0.3527 - mean_pred: 0.8687 - mae_t1: 0.0235 - val_loss: 2.8156 - val_mae: 0.4776 - val_mean_pred: 0.6934 - val_mae_t1: 0.0318\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.3707 - mae: 0.4022 - mean_pred: 0.5996 - mae_t1: 0.0268 - val_loss: 2.9833 - val_mae: 0.5061 - val_mean_pred: 0.7079 - val_mae_t1: 0.0337\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.3261 - mae: 0.3946 - mean_pred: 0.6411 - mae_t1: 0.0263 - val_loss: 2.6985 - val_mae: 0.4578 - val_mean_pred: 0.8723 - val_mae_t1: 0.0305\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 139us/sample - loss: 2.1268 - mae: 0.3608 - mean_pred: 0.7453 - mae_t1: 0.0241 - val_loss: 2.6161 - val_mae: 0.4438 - val_mean_pred: 0.9317 - val_mae_t1: 0.0296\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 2.1188 - mae: 0.3594 - mean_pred: 0.7928 - mae_t1: 0.0240 - val_loss: 2.5741 - val_mae: 0.4367 - val_mean_pred: 0.9670 - val_mae_t1: 0.0291\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 2.1268 - mae: 0.3608 - mean_pred: 0.8471 - mae_t1: 0.0241 - val_loss: 2.4793 - val_mae: 0.4206 - val_mean_pred: 0.9212 - val_mae_t1: 0.0280\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.0740 - mae: 0.3518 - mean_pred: 0.7837 - mae_t1: 0.0235 - val_loss: 2.5676 - val_mae: 0.4356 - val_mean_pred: 0.7283 - val_mae_t1: 0.0290\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.2743 - mae: 0.3858 - mean_pred: 0.6551 - mae_t1: 0.0257 - val_loss: 2.4842 - val_mae: 0.4214 - val_mean_pred: 0.7354 - val_mae_t1: 0.0281\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.1546 - mae: 0.3655 - mean_pred: 0.7170 - mae_t1: 0.0244 - val_loss: 2.5234 - val_mae: 0.4281 - val_mean_pred: 1.0140 - val_mae_t1: 0.0285\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 2.4393 - mae: 0.4138 - mean_pred: 0.9847 - mae_t1: 0.0276 - val_loss: 2.8838 - val_mae: 0.4892 - val_mean_pred: 1.1161 - val_mae_t1: 0.0326\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.3840 - mae: 0.4044 - mean_pred: 0.9960 - mae_t1: 0.0270 - val_loss: 2.6111 - val_mae: 0.4429 - val_mean_pred: 0.7897 - val_mae_t1: 0.0295\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.2909 - mae: 0.3886 - mean_pred: 0.6773 - mae_t1: 0.0259 - val_loss: 3.1046 - val_mae: 0.5267 - val_mean_pred: 0.6065 - val_mae_t1: 0.0351\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 2.4990 - mae: 0.4239 - mean_pred: 0.5955 - mae_t1: 0.0283 - val_loss: 2.8947 - val_mae: 0.4911 - val_mean_pred: 0.7185 - val_mae_t1: 0.0327\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.2151 - mae: 0.3758 - mean_pred: 0.7014 - mae_t1: 0.0251 - val_loss: 2.6100 - val_mae: 0.4428 - val_mean_pred: 0.8747 - val_mae_t1: 0.0295\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 1.9568 - mae: 0.3320 - mean_pred: 0.8261 - mae_t1: 0.0221 - val_loss: 2.6319 - val_mae: 0.4465 - val_mean_pred: 0.9664 - val_mae_t1: 0.0298\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 2.0485 - mae: 0.3475 - mean_pred: 0.8892 - mae_t1: 0.0232 - val_loss: 2.4668 - val_mae: 0.4185 - val_mean_pred: 0.8397 - val_mae_t1: 0.0279\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.9537 - mae: 0.3314 - mean_pred: 0.7560 - mae_t1: 0.0221 - val_loss: 2.5862 - val_mae: 0.4387 - val_mean_pred: 0.7223 - val_mae_t1: 0.0292\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.1332 - mae: 0.3619 - mean_pred: 0.6713 - mae_t1: 0.0241 - val_loss: 2.5305 - val_mae: 0.4293 - val_mean_pred: 0.7953 - val_mae_t1: 0.0286\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.0251 - mae: 0.3435 - mean_pred: 0.7363 - mae_t1: 0.0229 - val_loss: 2.5596 - val_mae: 0.4342 - val_mean_pred: 0.9400 - val_mae_t1: 0.0289\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.3480 - mae: 0.3983 - mean_pred: 0.8811 - mae_t1: 0.0266 - val_loss: 2.6947 - val_mae: 0.4571 - val_mean_pred: 1.0280 - val_mae_t1: 0.0305\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.1839 - mae: 0.3705 - mean_pred: 0.9172 - mae_t1: 0.0247 - val_loss: 2.7961 - val_mae: 0.4743 - val_mean_pred: 0.9464 - val_mae_t1: 0.0316\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.2276 - mae: 0.3779 - mean_pred: 0.8061 - mae_t1: 0.0252 - val_loss: 2.5665 - val_mae: 0.4354 - val_mean_pred: 0.7802 - val_mae_t1: 0.0290\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.1571 - mae: 0.3659 - mean_pred: 0.7026 - mae_t1: 0.0244 - val_loss: 2.8723 - val_mae: 0.4873 - val_mean_pred: 0.7262 - val_mae_t1: 0.0325\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 2.4903 - mae: 0.4225 - mean_pred: 0.6705 - mae_t1: 0.0282 - val_loss: 2.9499 - val_mae: 0.5004 - val_mean_pred: 0.7556 - val_mae_t1: 0.0334\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.3376 - mae: 0.3965 - mean_pred: 0.7266 - mae_t1: 0.0264 - val_loss: 2.9421 - val_mae: 0.4991 - val_mean_pred: 0.9887 - val_mae_t1: 0.0333\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.3008 - mae: 0.3903 - mean_pred: 0.9249 - mae_t1: 0.0260 - val_loss: 3.1538 - val_mae: 0.5350 - val_mean_pred: 1.1204 - val_mae_t1: 0.0357\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.5257 - mae: 0.4285 - mean_pred: 1.0133 - mae_t1: 0.0286 - val_loss: 2.9795 - val_mae: 0.5055 - val_mean_pred: 1.0332 - val_mae_t1: 0.0337\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.3193 - mae: 0.3935 - mean_pred: 0.9235 - mae_t1: 0.0262 - val_loss: 2.6804 - val_mae: 0.4547 - val_mean_pred: 0.8238 - val_mae_t1: 0.0303\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.0518 - mae: 0.3481 - mean_pred: 0.7488 - mae_t1: 0.0232 - val_loss: 2.8972 - val_mae: 0.4915 - val_mean_pred: 0.7142 - val_mae_t1: 0.0328\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.3312 - mae: 0.3955 - mean_pred: 0.7034 - mae_t1: 0.0264 - val_loss: 2.5692 - val_mae: 0.4358 - val_mean_pred: 0.8340 - val_mae_t1: 0.0291\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.9935 - mae: 0.3382 - mean_pred: 0.8113 - mae_t1: 0.0225 - val_loss: 2.9391 - val_mae: 0.4986 - val_mean_pred: 1.0573 - val_mae_t1: 0.0332\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.3747 - mae: 0.4028 - mean_pred: 0.9971 - mae_t1: 0.0269 - val_loss: 3.2173 - val_mae: 0.5458 - val_mean_pred: 1.1531 - val_mae_t1: 0.0364\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.5740 - mae: 0.4367 - mean_pred: 1.0510 - mae_t1: 0.0291 - val_loss: 2.6593 - val_mae: 0.4511 - val_mean_pred: 0.9840 - val_mae_t1: 0.0301\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.1904 - mae: 0.3716 - mean_pred: 0.8759 - mae_t1: 0.0248 - val_loss: 2.6035 - val_mae: 0.4417 - val_mean_pred: 0.7416 - val_mae_t1: 0.0294\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 2.0816 - mae: 0.3531 - mean_pred: 0.6782 - mae_t1: 0.0235 - val_loss: 2.8445 - val_mae: 0.4825 - val_mean_pred: 0.6936 - val_mae_t1: 0.0322\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.3212 - mae: 0.3938 - mean_pred: 0.6463 - mae_t1: 0.0263 - val_loss: 2.5585 - val_mae: 0.4340 - val_mean_pred: 0.7498 - val_mae_t1: 0.0289\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 1.8923 - mae: 0.3210 - mean_pred: 0.7057 - mae_t1: 0.0214 - val_loss: 2.4695 - val_mae: 0.4189 - val_mean_pred: 0.8503 - val_mae_t1: 0.0279\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.9191 - mae: 0.3256 - mean_pred: 0.8036 - mae_t1: 0.0217 - val_loss: 2.5356 - val_mae: 0.4301 - val_mean_pred: 0.9260 - val_mae_t1: 0.0287\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.0451 - mae: 0.3469 - mean_pred: 0.8510 - mae_t1: 0.0231 - val_loss: 2.9747 - val_mae: 0.5046 - val_mean_pred: 0.9837 - val_mae_t1: 0.0336\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.1542 - mae: 0.3654 - mean_pred: 0.8767 - mae_t1: 0.0244 - val_loss: 2.6892 - val_mae: 0.4562 - val_mean_pred: 0.9452 - val_mae_t1: 0.0304\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.2281 - mae: 0.3780 - mean_pred: 0.8392 - mae_t1: 0.0252 - val_loss: 3.1446 - val_mae: 0.5335 - val_mean_pred: 0.8695 - val_mae_t1: 0.0356\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.5834 - mae: 0.4383 - mean_pred: 0.7735 - mae_t1: 0.0292 - val_loss: 2.5371 - val_mae: 0.4304 - val_mean_pred: 0.7928 - val_mae_t1: 0.0287\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3033448375456512  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>██▆▆▄▄▃▂▂▃▂▂▂▂▃▃▃▂▂▁▁▁▁▁▂▁▁▂▂▁▁▁▂▂▁▁▁▂▁▂</td></tr><tr><td>mae</td><td>██▆▆▄▄▃▂▂▃▂▂▂▂▃▃▃▂▂▁▁▁▁▁▂▁▁▂▂▁▁▁▂▂▁▁▁▂▁▂</td></tr><tr><td>mae_t1</td><td>██▆▆▄▄▃▂▂▃▂▂▂▂▃▃▃▂▂▁▁▁▁▁▂▁▁▂▂▁▁▁▂▂▁▁▁▂▁▂</td></tr><tr><td>mean_pred</td><td>▁█▂▄▅▅▅▅▅▅▅▅▄▄▇▅▆▄▅▅▅▅▅▅▄▅▄▆▄▆▅▆▄▆▅▅▅▄▅▅</td></tr><tr><td>val_loss</td><td>▄▆█▄▃▃▃▂▄▃▃▃▄▃▂▂▂▃▂▂▃▂▂▂▂▁▁▁▁▁▁▂▃▃▂▃▁▁▃▁</td></tr><tr><td>val_mae</td><td>▄▆█▄▃▃▃▂▄▃▃▃▄▃▂▂▂▃▂▂▃▂▂▂▂▁▁▁▁▁▁▂▃▃▂▃▁▁▃▁</td></tr><tr><td>val_mae_t1</td><td>▄▆█▄▃▃▃▂▄▃▃▃▄▃▂▂▂▃▂▂▃▂▂▂▂▁▁▁▁▁▁▂▃▃▂▃▁▁▃▁</td></tr><tr><td>val_mean_pred</td><td>▁█▁▄▄▅▃▅▅▄▄▅▁▆▄▅▅▄▄▄▆▅▅▃▅▅▆▄▅▄▅▅▄▇▃▆▃▃▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.35028</td></tr><tr><td>AE_2</td><td>0.29357</td></tr><tr><td>AE_3</td><td>0.27532</td></tr><tr><td>MAE</td><td>0.30334</td></tr><tr><td>best_epoch</td><td>73</td></tr><tr><td>best_val_loss</td><td>2.46676</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.5834</td></tr><tr><td>mae</td><td>0.43826</td></tr><tr><td>mae_t1</td><td>0.02922</td></tr><tr><td>mean_pred</td><td>0.77351</td></tr><tr><td>val_loss</td><td>2.53707</td></tr><tr><td>val_mae</td><td>0.4304</td></tr><tr><td>val_mae_t1</td><td>0.02869</td></tr><tr><td>val_mean_pred</td><td>0.7928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">pleasant-dawn-86</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sgkur3c\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sgkur3c</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155531-2sgkur3c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155553-3r9c9ece</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3r9c9ece\" target=\"_blank\">treasured-oath-87</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 855us/sample - loss: 5.7553 - mae: 0.7194 - mean_pred: 0.4403 - mae_t1: 0.0480 - val_loss: 4.8174 - val_mae: 0.6022 - val_mean_pred: 1.0724 - val_mae_t1: 0.0401\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 305us/sample - loss: 4.8058 - mae: 0.6007 - mean_pred: 0.8485 - mae_t1: 0.0400 - val_loss: 5.3570 - val_mae: 0.6696 - val_mean_pred: 0.3681 - val_mae_t1: 0.0446\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 278us/sample - loss: 5.4301 - mae: 0.6788 - mean_pred: 0.3444 - mae_t1: 0.0453 - val_loss: 3.8444 - val_mae: 0.4806 - val_mean_pred: 0.7891 - val_mae_t1: 0.0320\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 244us/sample - loss: 4.9513 - mae: 0.6189 - mean_pred: 1.0184 - mae_t1: 0.0413 - val_loss: 3.4661 - val_mae: 0.4333 - val_mean_pred: 0.9305 - val_mae_t1: 0.0289\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 4.2113 - mae: 0.5264 - mean_pred: 0.6934 - mae_t1: 0.0351 - val_loss: 4.6238 - val_mae: 0.5780 - val_mean_pred: 0.4852 - val_mae_t1: 0.0385\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.3808 - mae: 0.5476 - mean_pred: 0.5064 - mae_t1: 0.0365 - val_loss: 4.6895 - val_mae: 0.5862 - val_mean_pred: 0.7899 - val_mae_t1: 0.0391\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 4.0801 - mae: 0.5100 - mean_pred: 0.6929 - mae_t1: 0.0340 - val_loss: 4.0306 - val_mae: 0.5038 - val_mean_pred: 0.8349 - val_mae_t1: 0.0336\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.7701 - mae: 0.4713 - mean_pred: 0.7541 - mae_t1: 0.0314 - val_loss: 4.5246 - val_mae: 0.5656 - val_mean_pred: 0.7225 - val_mae_t1: 0.0377\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 4.3582 - mae: 0.5448 - mean_pred: 0.4987 - mae_t1: 0.0363 - val_loss: 4.6050 - val_mae: 0.5756 - val_mean_pred: 0.6759 - val_mae_t1: 0.0384\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.6199 - mae: 0.4525 - mean_pred: 0.7530 - mae_t1: 0.0302 - val_loss: 4.1443 - val_mae: 0.5180 - val_mean_pred: 1.0023 - val_mae_t1: 0.0345\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.7306 - mae: 0.4663 - mean_pred: 0.7274 - mae_t1: 0.0311 - val_loss: 3.9055 - val_mae: 0.4882 - val_mean_pred: 0.7387 - val_mae_t1: 0.0325\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6402 - mae: 0.4550 - mean_pred: 0.7692 - mae_t1: 0.0303 - val_loss: 4.1109 - val_mae: 0.5139 - val_mean_pred: 1.0304 - val_mae_t1: 0.0343\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.7944 - mae: 0.4743 - mean_pred: 0.9448 - mae_t1: 0.0316 - val_loss: 3.8417 - val_mae: 0.4802 - val_mean_pred: 0.8349 - val_mae_t1: 0.0320\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.4666 - mae: 0.4333 - mean_pred: 0.7081 - mae_t1: 0.0289 - val_loss: 3.9684 - val_mae: 0.4961 - val_mean_pred: 0.7570 - val_mae_t1: 0.0331\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.3395 - mae: 0.4174 - mean_pred: 0.7401 - mae_t1: 0.0278 - val_loss: 3.9100 - val_mae: 0.4888 - val_mean_pred: 0.9382 - val_mae_t1: 0.0326\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.4989 - mae: 0.4374 - mean_pred: 0.8118 - mae_t1: 0.0292 - val_loss: 4.0788 - val_mae: 0.5099 - val_mean_pred: 0.7081 - val_mae_t1: 0.0340\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 3.4297 - mae: 0.4287 - mean_pred: 0.6552 - mae_t1: 0.0286 - val_loss: 3.9493 - val_mae: 0.4937 - val_mean_pred: 1.0141 - val_mae_t1: 0.0329\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7693 - mae: 0.4712 - mean_pred: 1.0127 - mae_t1: 0.0314 - val_loss: 3.4973 - val_mae: 0.4372 - val_mean_pred: 0.8055 - val_mae_t1: 0.0291\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.3326 - mae: 0.4166 - mean_pred: 0.6764 - mae_t1: 0.0278 - val_loss: 3.8548 - val_mae: 0.4819 - val_mean_pred: 0.6707 - val_mae_t1: 0.0321\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.3481 - mae: 0.4185 - mean_pred: 0.6940 - mae_t1: 0.0279 - val_loss: 3.7755 - val_mae: 0.4719 - val_mean_pred: 0.9747 - val_mae_t1: 0.0315\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4813 - mae: 0.4352 - mean_pred: 0.9374 - mae_t1: 0.0290 - val_loss: 3.6412 - val_mae: 0.4551 - val_mean_pred: 0.8995 - val_mae_t1: 0.0303\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 3.0787 - mae: 0.3848 - mean_pred: 0.7202 - mae_t1: 0.0257 - val_loss: 3.6510 - val_mae: 0.4564 - val_mean_pred: 0.7097 - val_mae_t1: 0.0304\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0002 - mae: 0.3750 - mean_pred: 0.7348 - mae_t1: 0.0250 - val_loss: 4.0563 - val_mae: 0.5070 - val_mean_pred: 1.0152 - val_mae_t1: 0.0338\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.1001 - mae: 0.3875 - mean_pred: 0.8628 - mae_t1: 0.0258 - val_loss: 3.5640 - val_mae: 0.4455 - val_mean_pred: 0.8215 - val_mae_t1: 0.0297\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0698 - mae: 0.3837 - mean_pred: 0.6893 - mae_t1: 0.0256 - val_loss: 3.5708 - val_mae: 0.4464 - val_mean_pred: 0.8256 - val_mae_t1: 0.0298\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0701 - mae: 0.3838 - mean_pred: 0.7986 - mae_t1: 0.0256 - val_loss: 4.0381 - val_mae: 0.5048 - val_mean_pred: 1.0857 - val_mae_t1: 0.0337\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.4363 - mae: 0.4295 - mean_pred: 0.9667 - mae_t1: 0.0286 - val_loss: 3.6111 - val_mae: 0.4514 - val_mean_pred: 0.7443 - val_mae_t1: 0.0301\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4580 - mae: 0.4323 - mean_pred: 0.5574 - mae_t1: 0.0288 - val_loss: 4.0290 - val_mae: 0.5036 - val_mean_pred: 0.7111 - val_mae_t1: 0.0336\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.2074 - mae: 0.4009 - mean_pred: 0.7604 - mae_t1: 0.0267 - val_loss: 3.8167 - val_mae: 0.4771 - val_mean_pred: 1.0008 - val_mae_t1: 0.0318\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.2001 - mae: 0.4000 - mean_pred: 0.7979 - mae_t1: 0.0267 - val_loss: 4.2271 - val_mae: 0.5284 - val_mean_pred: 0.6588 - val_mae_t1: 0.0352\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.7624 - mae: 0.4703 - mean_pred: 0.6184 - mae_t1: 0.0314 - val_loss: 3.5894 - val_mae: 0.4487 - val_mean_pred: 0.8667 - val_mae_t1: 0.0299\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0152 - mae: 0.3769 - mean_pred: 0.8728 - mae_t1: 0.0251 - val_loss: 3.7226 - val_mae: 0.4653 - val_mean_pred: 0.9293 - val_mae_t1: 0.0310\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.9992 - mae: 0.3749 - mean_pred: 0.7624 - mae_t1: 0.0250 - val_loss: 3.8562 - val_mae: 0.4820 - val_mean_pred: 0.7790 - val_mae_t1: 0.0321\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.8063 - mae: 0.3508 - mean_pred: 0.7526 - mae_t1: 0.0234 - val_loss: 3.6567 - val_mae: 0.4571 - val_mean_pred: 1.0022 - val_mae_t1: 0.0305\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7792 - mae: 0.3474 - mean_pred: 0.8593 - mae_t1: 0.0232 - val_loss: 3.6734 - val_mae: 0.4592 - val_mean_pred: 0.8612 - val_mae_t1: 0.0306\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.6523 - mae: 0.3315 - mean_pred: 0.7564 - mae_t1: 0.0221 - val_loss: 3.6025 - val_mae: 0.4503 - val_mean_pred: 0.9162 - val_mae_t1: 0.0300\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.5851 - mae: 0.3231 - mean_pred: 0.8392 - mae_t1: 0.0215 - val_loss: 3.6007 - val_mae: 0.4501 - val_mean_pred: 0.8920 - val_mae_t1: 0.0300\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 2.6381 - mae: 0.3298 - mean_pred: 0.7572 - mae_t1: 0.0220 - val_loss: 3.4438 - val_mae: 0.4305 - val_mean_pred: 0.8365 - val_mae_t1: 0.0287\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.7614 - mae: 0.3452 - mean_pred: 0.8215 - mae_t1: 0.0230 - val_loss: 3.5330 - val_mae: 0.4416 - val_mean_pred: 0.9861 - val_mae_t1: 0.0294\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.7843 - mae: 0.3480 - mean_pred: 0.8916 - mae_t1: 0.0232 - val_loss: 3.6901 - val_mae: 0.4613 - val_mean_pred: 0.8991 - val_mae_t1: 0.0308\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 3.0004 - mae: 0.3750 - mean_pred: 0.7974 - mae_t1: 0.0250 - val_loss: 3.5746 - val_mae: 0.4468 - val_mean_pred: 0.9645 - val_mae_t1: 0.0298\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 220us/sample - loss: 2.7262 - mae: 0.3408 - mean_pred: 0.8887 - mae_t1: 0.0227 - val_loss: 3.3452 - val_mae: 0.4182 - val_mean_pred: 0.9299 - val_mae_t1: 0.0279\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.6509 - mae: 0.3314 - mean_pred: 0.8031 - mae_t1: 0.0221 - val_loss: 3.4566 - val_mae: 0.4321 - val_mean_pred: 0.8553 - val_mae_t1: 0.0288\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.9576 - mae: 0.3697 - mean_pred: 0.8012 - mae_t1: 0.0246 - val_loss: 4.1297 - val_mae: 0.5162 - val_mean_pred: 1.0050 - val_mae_t1: 0.0344\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 3.5164 - mae: 0.4395 - mean_pred: 0.9116 - mae_t1: 0.0293 - val_loss: 3.5712 - val_mae: 0.4464 - val_mean_pred: 0.9178 - val_mae_t1: 0.0298\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4990 - mae: 0.4374 - mean_pred: 0.7331 - mae_t1: 0.0292 - val_loss: 3.6389 - val_mae: 0.4549 - val_mean_pred: 0.7814 - val_mae_t1: 0.0303\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.9184 - mae: 0.3648 - mean_pred: 0.7785 - mae_t1: 0.0243 - val_loss: 3.9301 - val_mae: 0.4913 - val_mean_pred: 1.0439 - val_mae_t1: 0.0328\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.8622 - mae: 0.3578 - mean_pred: 0.8763 - mae_t1: 0.0239 - val_loss: 3.8278 - val_mae: 0.4785 - val_mean_pred: 0.7264 - val_mae_t1: 0.0319\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1552 - mae: 0.3944 - mean_pred: 0.6094 - mae_t1: 0.0263 - val_loss: 3.6380 - val_mae: 0.4547 - val_mean_pred: 0.8044 - val_mae_t1: 0.0303\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.6901 - mae: 0.3363 - mean_pred: 0.8354 - mae_t1: 0.0224 - val_loss: 3.9986 - val_mae: 0.4998 - val_mean_pred: 1.0561 - val_mae_t1: 0.0333\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.6859 - mae: 0.3357 - mean_pred: 0.8145 - mae_t1: 0.0224 - val_loss: 3.7126 - val_mae: 0.4641 - val_mean_pred: 0.8116 - val_mae_t1: 0.0309\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5107 - mae: 0.3138 - mean_pred: 0.7807 - mae_t1: 0.0209 - val_loss: 3.7403 - val_mae: 0.4675 - val_mean_pred: 0.9996 - val_mae_t1: 0.0312\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3887 - mae: 0.2986 - mean_pred: 0.8241 - mae_t1: 0.0199 - val_loss: 3.4985 - val_mae: 0.4373 - val_mean_pred: 0.8233 - val_mae_t1: 0.0292\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.5727 - mae: 0.3216 - mean_pred: 0.7077 - mae_t1: 0.0214 - val_loss: 3.4204 - val_mae: 0.4275 - val_mean_pred: 0.9010 - val_mae_t1: 0.0285\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3147 - mae: 0.2893 - mean_pred: 0.8105 - mae_t1: 0.0193 - val_loss: 3.8058 - val_mae: 0.4757 - val_mean_pred: 0.9932 - val_mae_t1: 0.0317\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.4920 - mae: 0.3115 - mean_pred: 0.8715 - mae_t1: 0.0208 - val_loss: 3.5919 - val_mae: 0.4490 - val_mean_pred: 0.8582 - val_mae_t1: 0.0299\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5357 - mae: 0.3170 - mean_pred: 0.7245 - mae_t1: 0.0211 - val_loss: 3.8536 - val_mae: 0.4817 - val_mean_pred: 0.9347 - val_mae_t1: 0.0321\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3577 - mae: 0.2947 - mean_pred: 0.8700 - mae_t1: 0.0196 - val_loss: 3.4646 - val_mae: 0.4331 - val_mean_pred: 0.9399 - val_mae_t1: 0.0289\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3496 - mae: 0.2937 - mean_pred: 0.7665 - mae_t1: 0.0196 - val_loss: 3.7615 - val_mae: 0.4702 - val_mean_pred: 0.8336 - val_mae_t1: 0.0313\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.3965 - mae: 0.2996 - mean_pred: 0.7888 - mae_t1: 0.0200 - val_loss: 3.3695 - val_mae: 0.4212 - val_mean_pred: 0.9670 - val_mae_t1: 0.0281\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 2.1561 - mae: 0.2695 - mean_pred: 0.8265 - mae_t1: 0.0180 - val_loss: 3.2511 - val_mae: 0.4064 - val_mean_pred: 0.8947 - val_mae_t1: 0.0271\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.7334 - mae: 0.3417 - mean_pred: 0.7806 - mae_t1: 0.0228 - val_loss: 4.0563 - val_mae: 0.5070 - val_mean_pred: 0.8837 - val_mae_t1: 0.0338\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 3.2756 - mae: 0.4095 - mean_pred: 0.7160 - mae_t1: 0.0273 - val_loss: 3.3527 - val_mae: 0.4191 - val_mean_pred: 0.8822 - val_mae_t1: 0.0279\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7447 - mae: 0.3431 - mean_pred: 0.8305 - mae_t1: 0.0229 - val_loss: 3.5969 - val_mae: 0.4496 - val_mean_pred: 1.0010 - val_mae_t1: 0.0300\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 2.3875 - mae: 0.2984 - mean_pred: 0.8275 - mae_t1: 0.0199 - val_loss: 3.2000 - val_mae: 0.4000 - val_mean_pred: 0.8711 - val_mae_t1: 0.0267\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.5012 - mae: 0.3126 - mean_pred: 0.7562 - mae_t1: 0.0208 - val_loss: 3.4066 - val_mae: 0.4258 - val_mean_pred: 0.8733 - val_mae_t1: 0.0284\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 2.5214 - mae: 0.3152 - mean_pred: 0.7544 - mae_t1: 0.0210 - val_loss: 3.4837 - val_mae: 0.4355 - val_mean_pred: 0.9652 - val_mae_t1: 0.0290\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3676 - mae: 0.2960 - mean_pred: 0.8581 - mae_t1: 0.0197 - val_loss: 3.4312 - val_mae: 0.4289 - val_mean_pred: 0.8614 - val_mae_t1: 0.0286\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.4386 - mae: 0.3048 - mean_pred: 0.7084 - mae_t1: 0.0203 - val_loss: 3.3582 - val_mae: 0.4198 - val_mean_pred: 0.8106 - val_mae_t1: 0.0280\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3061 - mae: 0.2883 - mean_pred: 0.7522 - mae_t1: 0.0192 - val_loss: 3.5513 - val_mae: 0.4439 - val_mean_pred: 0.9771 - val_mae_t1: 0.0296\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 2.2394 - mae: 0.2799 - mean_pred: 0.7834 - mae_t1: 0.0187 - val_loss: 3.1836 - val_mae: 0.3979 - val_mean_pred: 0.8987 - val_mae_t1: 0.0265\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3102 - mae: 0.2888 - mean_pred: 0.8157 - mae_t1: 0.0193 - val_loss: 4.1622 - val_mae: 0.5203 - val_mean_pred: 0.9576 - val_mae_t1: 0.0347\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 2.7747 - mae: 0.3468 - mean_pred: 0.7786 - mae_t1: 0.0231 - val_loss: 3.1471 - val_mae: 0.3934 - val_mean_pred: 0.7974 - val_mae_t1: 0.0262\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.7610 - mae: 0.3451 - mean_pred: 0.7101 - mae_t1: 0.0230 - val_loss: 3.4525 - val_mae: 0.4316 - val_mean_pred: 0.8157 - val_mae_t1: 0.0288\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.4330 - mae: 0.3041 - mean_pred: 0.7296 - mae_t1: 0.0203 - val_loss: 3.6254 - val_mae: 0.4532 - val_mean_pred: 0.9901 - val_mae_t1: 0.0302\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 214us/sample - loss: 2.3589 - mae: 0.2949 - mean_pred: 0.8344 - mae_t1: 0.0197 - val_loss: 3.0367 - val_mae: 0.3796 - val_mean_pred: 0.8869 - val_mae_t1: 0.0253\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.6657 - mae: 0.3332 - mean_pred: 0.7582 - mae_t1: 0.0222 - val_loss: 3.7428 - val_mae: 0.4679 - val_mean_pred: 0.8611 - val_mae_t1: 0.0312\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.5679 - mae: 0.3210 - mean_pred: 0.8204 - mae_t1: 0.0214 - val_loss: 3.6964 - val_mae: 0.4620 - val_mean_pred: 1.0386 - val_mae_t1: 0.0308\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1996 - mae: 0.2750 - mean_pred: 0.8219 - mae_t1: 0.0183 - val_loss: 3.1387 - val_mae: 0.3923 - val_mean_pred: 0.8556 - val_mae_t1: 0.0262\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0934 - mae: 0.2617 - mean_pred: 0.7740 - mae_t1: 0.0174 - val_loss: 3.3646 - val_mae: 0.4206 - val_mean_pred: 1.0047 - val_mae_t1: 0.0280\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2083 - mae: 0.2760 - mean_pred: 0.8568 - mae_t1: 0.0184 - val_loss: 3.1151 - val_mae: 0.3894 - val_mean_pred: 0.9067 - val_mae_t1: 0.0260\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.2070 - mae: 0.2759 - mean_pred: 0.7991 - mae_t1: 0.0184 - val_loss: 3.1628 - val_mae: 0.3953 - val_mean_pred: 0.9488 - val_mae_t1: 0.0264\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.1215 - mae: 0.2652 - mean_pred: 0.8022 - mae_t1: 0.0177 - val_loss: 3.3211 - val_mae: 0.4151 - val_mean_pred: 0.9013 - val_mae_t1: 0.0277\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.1588 - mae: 0.2698 - mean_pred: 0.7899 - mae_t1: 0.0180 - val_loss: 3.4455 - val_mae: 0.4307 - val_mean_pred: 0.9722 - val_mae_t1: 0.0287\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.0961 - mae: 0.2620 - mean_pred: 0.8037 - mae_t1: 0.0175 - val_loss: 3.2627 - val_mae: 0.4078 - val_mean_pred: 0.8944 - val_mae_t1: 0.0272\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.0466 - mae: 0.2558 - mean_pred: 0.7814 - mae_t1: 0.0171 - val_loss: 3.4207 - val_mae: 0.4276 - val_mean_pred: 0.9359 - val_mae_t1: 0.0285\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1608 - mae: 0.2701 - mean_pred: 0.8426 - mae_t1: 0.0180 - val_loss: 3.4429 - val_mae: 0.4304 - val_mean_pred: 0.9086 - val_mae_t1: 0.0287\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0713 - mae: 0.2589 - mean_pred: 0.7897 - mae_t1: 0.0173 - val_loss: 3.8022 - val_mae: 0.4753 - val_mean_pred: 0.9348 - val_mae_t1: 0.0317\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.0761 - mae: 0.2595 - mean_pred: 0.8211 - mae_t1: 0.0173 - val_loss: 3.6870 - val_mae: 0.4609 - val_mean_pred: 0.9619 - val_mae_t1: 0.0307\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0472 - mae: 0.2559 - mean_pred: 0.8194 - mae_t1: 0.0171 - val_loss: 3.5383 - val_mae: 0.4423 - val_mean_pred: 0.9444 - val_mae_t1: 0.0295\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.0394 - mae: 0.2549 - mean_pred: 0.8484 - mae_t1: 0.0170 - val_loss: 3.3010 - val_mae: 0.4126 - val_mean_pred: 0.9016 - val_mae_t1: 0.0275\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.9966 - mae: 0.2496 - mean_pred: 0.7753 - mae_t1: 0.0166 - val_loss: 3.5876 - val_mae: 0.4485 - val_mean_pred: 0.9392 - val_mae_t1: 0.0299\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1789 - mae: 0.2724 - mean_pred: 0.8572 - mae_t1: 0.0182 - val_loss: 3.3185 - val_mae: 0.4148 - val_mean_pred: 0.9026 - val_mae_t1: 0.0277\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.0619 - mae: 0.2577 - mean_pred: 0.7415 - mae_t1: 0.0172 - val_loss: 3.2399 - val_mae: 0.4050 - val_mean_pred: 0.9035 - val_mae_t1: 0.0270\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.1593 - mae: 0.2699 - mean_pred: 0.8879 - mae_t1: 0.0180 - val_loss: 3.5458 - val_mae: 0.4432 - val_mean_pred: 0.9638 - val_mae_t1: 0.0295\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0802 - mae: 0.2600 - mean_pred: 0.7871 - mae_t1: 0.0173 - val_loss: 3.3594 - val_mae: 0.4199 - val_mean_pred: 0.7634 - val_mae_t1: 0.0280\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.1204 - mae: 0.3901 - mean_pred: 0.7156 - mae_t1: 0.0260 - val_loss: 4.2209 - val_mae: 0.5276 - val_mean_pred: 0.7897 - val_mae_t1: 0.0352\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.6212 - mae: 0.4526 - mean_pred: 0.7799 - mae_t1: 0.0302 - val_loss: 3.7274 - val_mae: 0.4659 - val_mean_pred: 1.0488 - val_mae_t1: 0.0311\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.4257 - mae: 0.4282 - mean_pred: 0.9666 - mae_t1: 0.0285 - val_loss: 3.6305 - val_mae: 0.4538 - val_mean_pred: 0.9400 - val_mae_t1: 0.0303\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.8775 - mae: 0.3597 - mean_pred: 0.8391 - mae_t1: 0.0240 - val_loss: 4.3526 - val_mae: 0.5441 - val_mean_pred: 0.8546 - val_mae_t1: 0.0363\n",
      "Earliness...\n",
      "0.002000093460083008\n",
      "____________________________________________________________\n",
      "Test MAE:      0.350509035731385  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▅▄▄▄▄▄▄▃▃▄▄▃▂▂▃▃▄▃▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▃▃</td></tr><tr><td>mae</td><td>█▇▅▄▄▄▄▄▄▃▃▄▄▃▂▂▃▃▄▃▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▃▃</td></tr><tr><td>mae_t1</td><td>█▇▅▄▄▄▄▄▄▃▃▄▄▃▂▂▃▃▄▃▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▃▃</td></tr><tr><td>mean_pred</td><td>▂▁▃▅▅▇▆█▇▅▆▃▄▅▅▆▆▆▅▄▆▅▇▅▆▆▅▅▆▅▅▆▆▆▆▆▆▅▅▆</td></tr><tr><td>val_loss</td><td>█▄▇▇▄▄▅▂▃▅▅▅▃▃▃▃▃▅▃▃▃▂▃▄▁▃▂▂▅▂▄▁▁▂▂▃▃▁▆▆</td></tr><tr><td>val_mae</td><td>█▄▇▇▄▄▅▂▃▅▅▅▃▃▃▃▃▅▃▃▃▂▃▄▁▃▂▂▅▂▄▁▁▂▂▃▃▁▆▆</td></tr><tr><td>val_mae_t1</td><td>█▄▇▇▄▄▅▂▃▅▅▅▃▃▃▃▃▅▃▃▃▂▃▄▁▃▂▂▅▂▄▁▁▂▂▃▃▁▆▆</td></tr><tr><td>val_mean_pred</td><td>█▃▃▁▂▃▁▃▅▇█▁▄▆▅▆▆▇▂▃▃▅▄▃▄▆▆▃▆▃▄▄▅▆▅▆▅▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.43959</td></tr><tr><td>AE_2</td><td>0.39032</td></tr><tr><td>AE_3</td><td>0.29527</td></tr><tr><td>MAE</td><td>0.35051</td></tr><tr><td>best_epoch</td><td>75</td></tr><tr><td>best_val_loss</td><td>3.03666</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.87751</td></tr><tr><td>mae</td><td>0.35969</td></tr><tr><td>mae_t1</td><td>0.02398</td></tr><tr><td>mean_pred</td><td>0.83907</td></tr><tr><td>val_loss</td><td>4.35263</td></tr><tr><td>val_mae</td><td>0.54408</td></tr><tr><td>val_mae_t1</td><td>0.03627</td></tr><tr><td>val_mean_pred</td><td>0.85462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">treasured-oath-87</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3r9c9ece\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3r9c9ece</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155553-3r9c9ece\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155620-224x82xf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/224x82xf\" target=\"_blank\">dark-deluge-88</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 722us/sample - loss: 6.2351 - mae: 0.7794 - mean_pred: 0.1849 - mae_t1: 0.0520 - val_loss: 4.0914 - val_mae: 0.5114 - val_mean_pred: 0.6846 - val_mae_t1: 0.0341\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 153us/sample - loss: 5.0127 - mae: 0.6266 - mean_pred: 0.6985 - mae_t1: 0.0418 - val_loss: 4.3461 - val_mae: 0.5433 - val_mean_pred: 0.5577 - val_mae_t1: 0.0362\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 150us/sample - loss: 5.0394 - mae: 0.6299 - mean_pred: 0.3950 - mae_t1: 0.0420 - val_loss: 5.0677 - val_mae: 0.6335 - val_mean_pred: 0.4053 - val_mae_t1: 0.0422\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 140us/sample - loss: 4.7374 - mae: 0.5922 - mean_pred: 0.4662 - mae_t1: 0.0395 - val_loss: 4.1649 - val_mae: 0.5206 - val_mean_pred: 1.0313 - val_mae_t1: 0.0347\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 154us/sample - loss: 5.0125 - mae: 0.6266 - mean_pred: 1.0475 - mae_t1: 0.0418 - val_loss: 3.7001 - val_mae: 0.4625 - val_mean_pred: 0.9273 - val_mae_t1: 0.0308\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 4.2387 - mae: 0.5298 - mean_pred: 0.7701 - mae_t1: 0.0353 - val_loss: 5.2087 - val_mae: 0.6511 - val_mean_pred: 0.3719 - val_mae_t1: 0.0434\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 5.1191 - mae: 0.6399 - mean_pred: 0.3266 - mae_t1: 0.0427 - val_loss: 5.0488 - val_mae: 0.6311 - val_mean_pred: 0.3626 - val_mae_t1: 0.0421\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 4.8018 - mae: 0.6002 - mean_pred: 0.3797 - mae_t1: 0.0400 - val_loss: 3.9261 - val_mae: 0.4908 - val_mean_pred: 0.6155 - val_mae_t1: 0.0327\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 143us/sample - loss: 4.1380 - mae: 0.5172 - mean_pred: 0.6255 - mae_t1: 0.0345 - val_loss: 3.5715 - val_mae: 0.4464 - val_mean_pred: 0.8512 - val_mae_t1: 0.0298\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 4.0038 - mae: 0.5005 - mean_pred: 0.8314 - mae_t1: 0.0334 - val_loss: 3.6957 - val_mae: 0.4620 - val_mean_pred: 0.8953 - val_mae_t1: 0.0308\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 3.8433 - mae: 0.4804 - mean_pred: 0.8110 - mae_t1: 0.0320 - val_loss: 4.0116 - val_mae: 0.5015 - val_mean_pred: 0.7166 - val_mae_t1: 0.0334\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 110us/sample - loss: 3.8829 - mae: 0.4854 - mean_pred: 0.6149 - mae_t1: 0.0324 - val_loss: 4.6043 - val_mae: 0.5755 - val_mean_pred: 0.5323 - val_mae_t1: 0.0384\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 4.2872 - mae: 0.5359 - mean_pred: 0.4792 - mae_t1: 0.0357 - val_loss: 4.7266 - val_mae: 0.5908 - val_mean_pred: 0.5738 - val_mae_t1: 0.0394\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.2976 - mae: 0.5372 - mean_pred: 0.5851 - mae_t1: 0.0358 - val_loss: 4.5655 - val_mae: 0.5707 - val_mean_pred: 0.7630 - val_mae_t1: 0.0380\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.9796 - mae: 0.4974 - mean_pred: 0.7241 - mae_t1: 0.0332 - val_loss: 4.0720 - val_mae: 0.5090 - val_mean_pred: 1.0052 - val_mae_t1: 0.0339\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.2746 - mae: 0.5343 - mean_pred: 0.9983 - mae_t1: 0.0356 - val_loss: 4.7282 - val_mae: 0.5910 - val_mean_pred: 1.1506 - val_mae_t1: 0.0394\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.3277 - mae: 0.5410 - mean_pred: 1.0185 - mae_t1: 0.0361 - val_loss: 4.1439 - val_mae: 0.5180 - val_mean_pred: 0.6623 - val_mae_t1: 0.0345\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.7680 - mae: 0.4710 - mean_pred: 0.5802 - mae_t1: 0.0314 - val_loss: 4.7340 - val_mae: 0.5918 - val_mean_pred: 0.4859 - val_mae_t1: 0.0395\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.2924 - mae: 0.5365 - mean_pred: 0.5157 - mae_t1: 0.0358 - val_loss: 3.6565 - val_mae: 0.4571 - val_mean_pred: 0.7425 - val_mae_t1: 0.0305\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 3.4558 - mae: 0.4320 - mean_pred: 0.7523 - mae_t1: 0.0288 - val_loss: 3.7943 - val_mae: 0.4743 - val_mean_pred: 1.0367 - val_mae_t1: 0.0316\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 135us/sample - loss: 3.9106 - mae: 0.4888 - mean_pred: 0.9623 - mae_t1: 0.0326 - val_loss: 3.4717 - val_mae: 0.4340 - val_mean_pred: 0.8186 - val_mae_t1: 0.0289\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.5701 - mae: 0.4463 - mean_pred: 0.6544 - mae_t1: 0.0298 - val_loss: 4.4275 - val_mae: 0.5534 - val_mean_pred: 0.6131 - val_mae_t1: 0.0369\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.6237 - mae: 0.5780 - mean_pred: 0.5702 - mae_t1: 0.0385 - val_loss: 4.4160 - val_mae: 0.5520 - val_mean_pred: 0.6718 - val_mae_t1: 0.0368\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.2254 - mae: 0.5282 - mean_pred: 0.5902 - mae_t1: 0.0352 - val_loss: 4.1352 - val_mae: 0.5169 - val_mean_pred: 0.7761 - val_mae_t1: 0.0345\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.9184 - mae: 0.4898 - mean_pred: 0.6997 - mae_t1: 0.0327 - val_loss: 4.0548 - val_mae: 0.5068 - val_mean_pred: 1.0902 - val_mae_t1: 0.0338\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.1072 - mae: 0.5134 - mean_pred: 1.0236 - mae_t1: 0.0342 - val_loss: 3.9537 - val_mae: 0.4942 - val_mean_pred: 1.0633 - val_mae_t1: 0.0329\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.9562 - mae: 0.4945 - mean_pred: 0.8992 - mae_t1: 0.0330 - val_loss: 4.1535 - val_mae: 0.5192 - val_mean_pred: 0.6219 - val_mae_t1: 0.0346\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.9604 - mae: 0.4951 - mean_pred: 0.5385 - mae_t1: 0.0330 - val_loss: 4.6086 - val_mae: 0.5761 - val_mean_pred: 0.5397 - val_mae_t1: 0.0384\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.0483 - mae: 0.5060 - mean_pred: 0.5148 - mae_t1: 0.0337 - val_loss: 4.0975 - val_mae: 0.5122 - val_mean_pred: 0.7827 - val_mae_t1: 0.0341\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.8741 - mae: 0.4843 - mean_pred: 0.7289 - mae_t1: 0.0323 - val_loss: 4.0346 - val_mae: 0.5043 - val_mean_pred: 0.8484 - val_mae_t1: 0.0336\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.4372 - mae: 0.4296 - mean_pred: 0.6970 - mae_t1: 0.0286 - val_loss: 4.0903 - val_mae: 0.5113 - val_mean_pred: 0.8229 - val_mae_t1: 0.0341\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.5879 - mae: 0.4485 - mean_pred: 0.7367 - mae_t1: 0.0299 - val_loss: 4.1648 - val_mae: 0.5206 - val_mean_pred: 1.0230 - val_mae_t1: 0.0347\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.5867 - mae: 0.4483 - mean_pred: 0.8987 - mae_t1: 0.0299 - val_loss: 4.2138 - val_mae: 0.5267 - val_mean_pred: 1.1052 - val_mae_t1: 0.0351\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.9280 - mae: 0.4910 - mean_pred: 0.9916 - mae_t1: 0.0327 - val_loss: 4.0345 - val_mae: 0.5043 - val_mean_pred: 1.0059 - val_mae_t1: 0.0336\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 3.6566 - mae: 0.4571 - mean_pred: 0.8604 - mae_t1: 0.0305 - val_loss: 4.3005 - val_mae: 0.5376 - val_mean_pred: 0.7251 - val_mae_t1: 0.0358\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.7263 - mae: 0.4658 - mean_pred: 0.6381 - mae_t1: 0.0311 - val_loss: 3.6389 - val_mae: 0.4549 - val_mean_pred: 0.7140 - val_mae_t1: 0.0303\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.4469 - mae: 0.4309 - mean_pred: 0.6857 - mae_t1: 0.0287 - val_loss: 3.8500 - val_mae: 0.4812 - val_mean_pred: 1.0394 - val_mae_t1: 0.0321\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.5428 - mae: 0.4429 - mean_pred: 0.9829 - mae_t1: 0.0295 - val_loss: 4.1005 - val_mae: 0.5126 - val_mean_pred: 1.0706 - val_mae_t1: 0.0342\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.3573 - mae: 0.4197 - mean_pred: 0.8612 - mae_t1: 0.0280 - val_loss: 4.4519 - val_mae: 0.5565 - val_mean_pred: 0.5632 - val_mae_t1: 0.0371\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.1695 - mae: 0.5212 - mean_pred: 0.4378 - mae_t1: 0.0347 - val_loss: 5.2734 - val_mae: 0.6592 - val_mean_pred: 0.3856 - val_mae_t1: 0.0439\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.0244 - mae: 0.6281 - mean_pred: 0.4039 - mae_t1: 0.0419 - val_loss: 5.3769 - val_mae: 0.6721 - val_mean_pred: 0.4390 - val_mae_t1: 0.0448\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.9860 - mae: 0.6232 - mean_pred: 0.4014 - mae_t1: 0.0415 - val_loss: 4.9365 - val_mae: 0.6171 - val_mean_pred: 0.4391 - val_mae_t1: 0.0411\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.3952 - mae: 0.5494 - mean_pred: 0.4020 - mae_t1: 0.0366 - val_loss: 4.4604 - val_mae: 0.5575 - val_mean_pred: 0.6790 - val_mae_t1: 0.0372\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.6698 - mae: 0.4587 - mean_pred: 0.6228 - mae_t1: 0.0306 - val_loss: 4.5282 - val_mae: 0.5660 - val_mean_pred: 1.0294 - val_mae_t1: 0.0377\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.8252 - mae: 0.4781 - mean_pred: 0.9276 - mae_t1: 0.0319 - val_loss: 4.8629 - val_mae: 0.6079 - val_mean_pred: 1.1426 - val_mae_t1: 0.0405\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.5888 - mae: 0.4486 - mean_pred: 0.9389 - mae_t1: 0.0299 - val_loss: 4.5529 - val_mae: 0.5691 - val_mean_pred: 0.8654 - val_mae_t1: 0.0379\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.3315 - mae: 0.4164 - mean_pred: 0.7023 - mae_t1: 0.0278 - val_loss: 4.5648 - val_mae: 0.5706 - val_mean_pred: 0.6673 - val_mae_t1: 0.0380\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.5662 - mae: 0.4458 - mean_pred: 0.5734 - mae_t1: 0.0297 - val_loss: 4.1071 - val_mae: 0.5134 - val_mean_pred: 0.6965 - val_mae_t1: 0.0342\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.2780 - mae: 0.4097 - mean_pred: 0.6469 - mae_t1: 0.0273 - val_loss: 3.6606 - val_mae: 0.4576 - val_mean_pred: 0.8658 - val_mae_t1: 0.0305\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.9744 - mae: 0.3718 - mean_pred: 0.7699 - mae_t1: 0.0248 - val_loss: 3.8015 - val_mae: 0.4752 - val_mean_pred: 0.9596 - val_mae_t1: 0.0317\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 3.0208 - mae: 0.3776 - mean_pred: 0.8440 - mae_t1: 0.0252 - val_loss: 3.4515 - val_mae: 0.4314 - val_mean_pred: 0.9517 - val_mae_t1: 0.0288\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 143us/sample - loss: 2.9089 - mae: 0.3636 - mean_pred: 0.8331 - mae_t1: 0.0242 - val_loss: 3.3908 - val_mae: 0.4238 - val_mean_pred: 0.8379 - val_mae_t1: 0.0283\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.9331 - mae: 0.3666 - mean_pred: 0.7671 - mae_t1: 0.0244 - val_loss: 3.7921 - val_mae: 0.4740 - val_mean_pred: 0.8715 - val_mae_t1: 0.0316\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.3385 - mae: 0.4173 - mean_pred: 0.8163 - mae_t1: 0.0278 - val_loss: 3.6819 - val_mae: 0.4602 - val_mean_pred: 0.8600 - val_mae_t1: 0.0307\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.0786 - mae: 0.3848 - mean_pred: 0.7650 - mae_t1: 0.0257 - val_loss: 3.5801 - val_mae: 0.4475 - val_mean_pred: 0.8004 - val_mae_t1: 0.0298\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 2.9608 - mae: 0.3701 - mean_pred: 0.7326 - mae_t1: 0.0247 - val_loss: 3.8580 - val_mae: 0.4822 - val_mean_pred: 0.8614 - val_mae_t1: 0.0321\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 3.0158 - mae: 0.3770 - mean_pred: 0.7701 - mae_t1: 0.0251 - val_loss: 4.1283 - val_mae: 0.5160 - val_mean_pred: 1.0001 - val_mae_t1: 0.0344\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.4424 - mae: 0.4303 - mean_pred: 0.8876 - mae_t1: 0.0287 - val_loss: 4.3271 - val_mae: 0.5409 - val_mean_pred: 0.9662 - val_mae_t1: 0.0361\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 3.4548 - mae: 0.4318 - mean_pred: 0.7743 - mae_t1: 0.0288 - val_loss: 4.4026 - val_mae: 0.5503 - val_mean_pred: 0.7379 - val_mae_t1: 0.0367\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.6010 - mae: 0.4501 - mean_pred: 0.6425 - mae_t1: 0.0300 - val_loss: 4.6538 - val_mae: 0.5817 - val_mean_pred: 0.6846 - val_mae_t1: 0.0388\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.8245 - mae: 0.4781 - mean_pred: 0.5791 - mae_t1: 0.0319 - val_loss: 4.6051 - val_mae: 0.5756 - val_mean_pred: 0.6309 - val_mae_t1: 0.0384\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.6556 - mae: 0.4570 - mean_pred: 0.5795 - mae_t1: 0.0305 - val_loss: 4.2663 - val_mae: 0.5333 - val_mean_pred: 0.8730 - val_mae_t1: 0.0356\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.2314 - mae: 0.4039 - mean_pred: 0.8320 - mae_t1: 0.0269 - val_loss: 4.4011 - val_mae: 0.5501 - val_mean_pred: 1.0195 - val_mae_t1: 0.0367\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.2384 - mae: 0.4048 - mean_pred: 0.8936 - mae_t1: 0.0270 - val_loss: 4.1912 - val_mae: 0.5239 - val_mean_pred: 0.8321 - val_mae_t1: 0.0349\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.2613 - mae: 0.4077 - mean_pred: 0.7202 - mae_t1: 0.0272 - val_loss: 4.1031 - val_mae: 0.5129 - val_mean_pred: 0.7125 - val_mae_t1: 0.0342\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.4205 - mae: 0.4276 - mean_pred: 0.6481 - mae_t1: 0.0285 - val_loss: 3.9967 - val_mae: 0.4996 - val_mean_pred: 0.7469 - val_mae_t1: 0.0333\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.1974 - mae: 0.3997 - mean_pred: 0.6816 - mae_t1: 0.0266 - val_loss: 3.9849 - val_mae: 0.4981 - val_mean_pred: 0.9800 - val_mae_t1: 0.0332\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.0840 - mae: 0.3855 - mean_pred: 0.8926 - mae_t1: 0.0257 - val_loss: 4.1203 - val_mae: 0.5150 - val_mean_pred: 1.0634 - val_mae_t1: 0.0343\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.1047 - mae: 0.3881 - mean_pred: 0.9281 - mae_t1: 0.0259 - val_loss: 3.7222 - val_mae: 0.4653 - val_mean_pred: 0.8236 - val_mae_t1: 0.0310\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.8720 - mae: 0.3590 - mean_pred: 0.7152 - mae_t1: 0.0239 - val_loss: 4.0890 - val_mae: 0.5111 - val_mean_pred: 0.7102 - val_mae_t1: 0.0341\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.0404 - mae: 0.3801 - mean_pred: 0.6498 - mae_t1: 0.0253 - val_loss: 3.7154 - val_mae: 0.4644 - val_mean_pred: 0.8392 - val_mae_t1: 0.0310\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.0013 - mae: 0.3752 - mean_pred: 0.7811 - mae_t1: 0.0250 - val_loss: 3.6140 - val_mae: 0.4518 - val_mean_pred: 0.9348 - val_mae_t1: 0.0301\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.9800 - mae: 0.3725 - mean_pred: 0.8296 - mae_t1: 0.0248 - val_loss: 3.9961 - val_mae: 0.4995 - val_mean_pred: 0.8709 - val_mae_t1: 0.0333\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.9535 - mae: 0.3692 - mean_pred: 0.7316 - mae_t1: 0.0246 - val_loss: 3.7049 - val_mae: 0.4631 - val_mean_pred: 0.8167 - val_mae_t1: 0.0309\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.9613 - mae: 0.3702 - mean_pred: 0.7152 - mae_t1: 0.0247 - val_loss: 4.1302 - val_mae: 0.5163 - val_mean_pred: 0.7683 - val_mae_t1: 0.0344\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.6359 - mae: 0.4545 - mean_pred: 0.6685 - mae_t1: 0.0303 - val_loss: 4.2929 - val_mae: 0.5366 - val_mean_pred: 0.6743 - val_mae_t1: 0.0358\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 3.3292 - mae: 0.4162 - mean_pred: 0.6110 - mae_t1: 0.0277 - val_loss: 4.1110 - val_mae: 0.5139 - val_mean_pred: 0.9080 - val_mae_t1: 0.0343\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 3.0196 - mae: 0.3775 - mean_pred: 0.7957 - mae_t1: 0.0252 - val_loss: 4.8666 - val_mae: 0.6083 - val_mean_pred: 1.1503 - val_mae_t1: 0.0406\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.2623 - mae: 0.4078 - mean_pred: 0.9777 - mae_t1: 0.0272 - val_loss: 4.1223 - val_mae: 0.5153 - val_mean_pred: 1.0095 - val_mae_t1: 0.0344\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 2.8144 - mae: 0.3518 - mean_pred: 0.8594 - mae_t1: 0.0235 - val_loss: 3.7446 - val_mae: 0.4681 - val_mean_pred: 0.7466 - val_mae_t1: 0.0312\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.0807 - mae: 0.3851 - mean_pred: 0.6719 - mae_t1: 0.0257 - val_loss: 3.9536 - val_mae: 0.4942 - val_mean_pred: 0.6836 - val_mae_t1: 0.0329\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 3.0145 - mae: 0.3768 - mean_pred: 0.6511 - mae_t1: 0.0251 - val_loss: 3.4763 - val_mae: 0.4345 - val_mean_pred: 0.8964 - val_mae_t1: 0.0290\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.9001 - mae: 0.3625 - mean_pred: 0.8770 - mae_t1: 0.0242 - val_loss: 4.1127 - val_mae: 0.5141 - val_mean_pred: 1.0857 - val_mae_t1: 0.0343\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.2619 - mae: 0.4077 - mean_pred: 0.9782 - mae_t1: 0.0272 - val_loss: 3.7781 - val_mae: 0.4723 - val_mean_pred: 0.8790 - val_mae_t1: 0.0315\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.9917 - mae: 0.3740 - mean_pred: 0.7553 - mae_t1: 0.0249 - val_loss: 3.7438 - val_mae: 0.4680 - val_mean_pred: 0.7106 - val_mae_t1: 0.0312\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.1292 - mae: 0.3911 - mean_pred: 0.6619 - mae_t1: 0.0261 - val_loss: 3.7437 - val_mae: 0.4680 - val_mean_pred: 0.8601 - val_mae_t1: 0.0312\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.4741 - mae: 0.4343 - mean_pred: 0.8239 - mae_t1: 0.0290 - val_loss: 3.7271 - val_mae: 0.4659 - val_mean_pred: 0.9817 - val_mae_t1: 0.0311\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 2.9580 - mae: 0.3698 - mean_pred: 0.8518 - mae_t1: 0.0247 - val_loss: 3.9899 - val_mae: 0.4987 - val_mean_pred: 0.9455 - val_mae_t1: 0.0332\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.0300 - mae: 0.3787 - mean_pred: 0.8122 - mae_t1: 0.0252 - val_loss: 4.1619 - val_mae: 0.5202 - val_mean_pred: 1.0129 - val_mae_t1: 0.0347\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.0146 - mae: 0.3768 - mean_pred: 0.8699 - mae_t1: 0.0251 - val_loss: 3.9765 - val_mae: 0.4971 - val_mean_pred: 1.0173 - val_mae_t1: 0.0331\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 2.9386 - mae: 0.3673 - mean_pred: 0.8598 - mae_t1: 0.0245 - val_loss: 3.6673 - val_mae: 0.4584 - val_mean_pred: 0.8942 - val_mae_t1: 0.0306\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 2.6727 - mae: 0.3341 - mean_pred: 0.7505 - mae_t1: 0.0223 - val_loss: 3.8339 - val_mae: 0.4792 - val_mean_pred: 0.8817 - val_mae_t1: 0.0319\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 2.7149 - mae: 0.3394 - mean_pred: 0.7730 - mae_t1: 0.0226 - val_loss: 3.7480 - val_mae: 0.4685 - val_mean_pred: 0.9233 - val_mae_t1: 0.0312\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.3484 - mae: 0.4185 - mean_pred: 0.8580 - mae_t1: 0.0279 - val_loss: 3.6545 - val_mae: 0.4568 - val_mean_pred: 0.8384 - val_mae_t1: 0.0305\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 3.1396 - mae: 0.3924 - mean_pred: 0.7727 - mae_t1: 0.0262 - val_loss: 4.2406 - val_mae: 0.5301 - val_mean_pred: 0.7711 - val_mae_t1: 0.0353\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.2732 - mae: 0.4091 - mean_pred: 0.6771 - mae_t1: 0.0273 - val_loss: 4.0483 - val_mae: 0.5060 - val_mean_pred: 0.8530 - val_mae_t1: 0.0337\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 2.8423 - mae: 0.3553 - mean_pred: 0.7634 - mae_t1: 0.0237 - val_loss: 4.5175 - val_mae: 0.5647 - val_mean_pred: 0.9989 - val_mae_t1: 0.0376\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.3163 - mae: 0.5395 - mean_pred: 0.9397 - mae_t1: 0.0360 - val_loss: 4.7359 - val_mae: 0.5920 - val_mean_pred: 1.0974 - val_mae_t1: 0.0395\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.6887 - mae: 0.4611 - mean_pred: 0.9891 - mae_t1: 0.0307 - val_loss: 5.0107 - val_mae: 0.6263 - val_mean_pred: 1.1008 - val_mae_t1: 0.0418\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.5275 - mae: 0.4409 - mean_pred: 0.8797 - mae_t1: 0.0294 - val_loss: 4.4285 - val_mae: 0.5536 - val_mean_pred: 0.8769 - val_mae_t1: 0.0369\n",
      "Earliness...\n",
      "0.001500844955444336\n",
      "____________________________________________________________\n",
      "Test MAE:      0.365439356381898  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▅▃▄▄▃▃▅▄▄▃▃▃▂▆▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▃▂▁▂▁▃</td></tr><tr><td>mae</td><td>█▆▄▅▃▄▄▃▃▅▄▄▃▃▃▂▆▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▃▂▁▂▁▃</td></tr><tr><td>mae_t1</td><td>█▆▄▅▃▄▄▃▃▅▄▄▃▃▃▂▆▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▃▂▁▂▁▃</td></tr><tr><td>mean_pred</td><td>▁▃▆▃▆▃█▄▇▄█▄▅█▅▇▃▅▇▅▇▆▆▆▄▇▅▇▆▆▅█▅█▆▆▆▇▆▇</td></tr><tr><td>val_loss</td><td>▃▇▇▃▃▆▆▆▁▅▃▅▃▃▂▅█▅▅▂▁▂▂▄▅▄▃▂▂▂▃▃▁▂▂▄▂▂▅▅</td></tr><tr><td>val_mae</td><td>▃▇▇▃▃▆▆▆▁▅▃▅▃▃▂▅█▅▅▂▁▂▂▄▅▄▃▂▂▂▃▃▁▂▂▄▂▂▅▅</td></tr><tr><td>val_mae_t1</td><td>▃▇▇▃▃▆▆▆▁▅▃▅▃▃▂▅█▅▅▂▁▂▂▄▅▄▃▂▂▂▃▃▁▂▂▄▂▂▅▅</td></tr><tr><td>val_mean_pred</td><td>▄▁▁▃▄▃█▂▅▄▇▃▅▇▄▃▂▇▅▅▆▅▅▄▃▅▆▅▆▅▆▇▆▆▆▇▆▅▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.43346</td></tr><tr><td>AE_2</td><td>0.30165</td></tr><tr><td>AE_3</td><td>0.37588</td></tr><tr><td>MAE</td><td>0.36544</td></tr><tr><td>best_epoch</td><td>51</td></tr><tr><td>best_val_loss</td><td>3.39079</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>3.52752</td></tr><tr><td>mae</td><td>0.44094</td></tr><tr><td>mae_t1</td><td>0.0294</td></tr><tr><td>mean_pred</td><td>0.87968</td></tr><tr><td>val_loss</td><td>4.42852</td></tr><tr><td>val_mae</td><td>0.55356</td></tr><tr><td>val_mae_t1</td><td>0.0369</td></tr><tr><td>val_mean_pred</td><td>0.87694</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dark-deluge-88</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/224x82xf\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/224x82xf</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155620-224x82xf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155643-3dqhq1db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3dqhq1db\" target=\"_blank\">earnest-water-89</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 861us/sample - loss: 7.6698 - mae: 0.7590 - mean_pred: 0.4738 - mae_t1: 0.0506 - val_loss: 6.6065 - val_mae: 0.6538 - val_mean_pred: 1.2654 - val_mae_t1: 0.0436\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 287us/sample - loss: 6.6554 - mae: 0.6586 - mean_pred: 0.9103 - mae_t1: 0.0439 - val_loss: 6.3203 - val_mae: 0.6254 - val_mean_pred: 0.4807 - val_mae_t1: 0.0417\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 280us/sample - loss: 5.7194 - mae: 0.5660 - mean_pred: 0.5373 - mae_t1: 0.0377 - val_loss: 5.2564 - val_mae: 0.5202 - val_mean_pred: 0.7588 - val_mae_t1: 0.0347\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 248us/sample - loss: 5.2933 - mae: 0.5238 - mean_pred: 0.5906 - mae_t1: 0.0349 - val_loss: 5.0041 - val_mae: 0.4952 - val_mean_pred: 0.7592 - val_mae_t1: 0.0330\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 5.4274 - mae: 0.5371 - mean_pred: 0.9165 - mae_t1: 0.0358 - val_loss: 5.3576 - val_mae: 0.5302 - val_mean_pred: 0.6892 - val_mae_t1: 0.0353\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 6.7652 - mae: 0.6695 - mean_pred: 0.5135 - mae_t1: 0.0446 - val_loss: 6.3132 - val_mae: 0.6247 - val_mean_pred: 0.4388 - val_mae_t1: 0.0416\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 5.4190 - mae: 0.5363 - mean_pred: 0.5935 - mae_t1: 0.0358 - val_loss: 5.3584 - val_mae: 0.5303 - val_mean_pred: 1.0393 - val_mae_t1: 0.0354\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 4.8533 - mae: 0.4803 - mean_pred: 0.8635 - mae_t1: 0.0320 - val_loss: 6.4774 - val_mae: 0.6410 - val_mean_pred: 0.4532 - val_mae_t1: 0.0427\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 232us/sample - loss: 5.9089 - mae: 0.5847 - mean_pred: 0.4892 - mae_t1: 0.0390 - val_loss: 4.9819 - val_mae: 0.4930 - val_mean_pred: 0.9295 - val_mae_t1: 0.0329\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 5.0704 - mae: 0.5018 - mean_pred: 0.9011 - mae_t1: 0.0335 - val_loss: 5.6403 - val_mae: 0.5582 - val_mean_pred: 0.7168 - val_mae_t1: 0.0372\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 5.0127 - mae: 0.4960 - mean_pred: 0.6001 - mae_t1: 0.0331 - val_loss: 6.7642 - val_mae: 0.6694 - val_mean_pred: 0.7650 - val_mae_t1: 0.0446\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 5.7494 - mae: 0.5689 - mean_pred: 0.6070 - mae_t1: 0.0379 - val_loss: 6.1912 - val_mae: 0.6127 - val_mean_pred: 0.6045 - val_mae_t1: 0.0408\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 4.7968 - mae: 0.4747 - mean_pred: 0.6497 - mae_t1: 0.0316 - val_loss: 5.6355 - val_mae: 0.5577 - val_mean_pred: 1.0664 - val_mae_t1: 0.0372\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.8314 - mae: 0.4781 - mean_pred: 0.8679 - mae_t1: 0.0319 - val_loss: 5.6548 - val_mae: 0.5596 - val_mean_pred: 0.7144 - val_mae_t1: 0.0373\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 4.9868 - mae: 0.4935 - mean_pred: 0.6000 - mae_t1: 0.0329 - val_loss: 5.7424 - val_mae: 0.5683 - val_mean_pred: 0.7247 - val_mae_t1: 0.0379\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 4.5243 - mae: 0.4477 - mean_pred: 0.7375 - mae_t1: 0.0298 - val_loss: 5.2520 - val_mae: 0.5197 - val_mean_pred: 0.9729 - val_mae_t1: 0.0346\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 4.3519 - mae: 0.4307 - mean_pred: 0.7680 - mae_t1: 0.0287 - val_loss: 5.0063 - val_mae: 0.4954 - val_mean_pred: 0.8115 - val_mae_t1: 0.0330\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 4.3487 - mae: 0.4303 - mean_pred: 0.7919 - mae_t1: 0.0287 - val_loss: 5.1266 - val_mae: 0.5073 - val_mean_pred: 0.9247 - val_mae_t1: 0.0338\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 4.6269 - mae: 0.4579 - mean_pred: 0.9283 - mae_t1: 0.0305 - val_loss: 5.5194 - val_mae: 0.5462 - val_mean_pred: 0.7252 - val_mae_t1: 0.0364\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 4.8891 - mae: 0.4838 - mean_pred: 0.5493 - mae_t1: 0.0323 - val_loss: 5.6033 - val_mae: 0.5545 - val_mean_pred: 0.6370 - val_mae_t1: 0.0370\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 4.0753 - mae: 0.4033 - mean_pred: 0.7388 - mae_t1: 0.0269 - val_loss: 5.0716 - val_mae: 0.5019 - val_mean_pred: 1.0030 - val_mae_t1: 0.0335\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.2151 - mae: 0.4171 - mean_pred: 0.7455 - mae_t1: 0.0278 - val_loss: 5.2606 - val_mae: 0.5206 - val_mean_pred: 0.7044 - val_mae_t1: 0.0347\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 224us/sample - loss: 4.1378 - mae: 0.4095 - mean_pred: 0.7056 - mae_t1: 0.0273 - val_loss: 4.7103 - val_mae: 0.4661 - val_mean_pred: 0.9483 - val_mae_t1: 0.0311\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 225us/sample - loss: 3.8930 - mae: 0.3852 - mean_pred: 0.8549 - mae_t1: 0.0257 - val_loss: 4.2178 - val_mae: 0.4174 - val_mean_pred: 0.8412 - val_mae_t1: 0.0278\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 4.0381 - mae: 0.3996 - mean_pred: 0.7750 - mae_t1: 0.0266 - val_loss: 5.0060 - val_mae: 0.4954 - val_mean_pred: 0.7516 - val_mae_t1: 0.0330\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 4.3863 - mae: 0.4341 - mean_pred: 0.7787 - mae_t1: 0.0289 - val_loss: 4.5184 - val_mae: 0.4471 - val_mean_pred: 0.9079 - val_mae_t1: 0.0298\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 4.5142 - mae: 0.4467 - mean_pred: 0.7685 - mae_t1: 0.0298 - val_loss: 6.1529 - val_mae: 0.6089 - val_mean_pred: 0.6802 - val_mae_t1: 0.0406\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 4.9206 - mae: 0.4869 - mean_pred: 0.7126 - mae_t1: 0.0325 - val_loss: 5.5117 - val_mae: 0.5454 - val_mean_pred: 0.9730 - val_mae_t1: 0.0364\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 4.3697 - mae: 0.4324 - mean_pred: 0.8111 - mae_t1: 0.0288 - val_loss: 5.5029 - val_mae: 0.5446 - val_mean_pred: 0.7614 - val_mae_t1: 0.0363\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 4.1781 - mae: 0.4135 - mean_pred: 0.7820 - mae_t1: 0.0276 - val_loss: 5.0389 - val_mae: 0.4986 - val_mean_pred: 1.0134 - val_mae_t1: 0.0332\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 3.9625 - mae: 0.3921 - mean_pred: 0.8873 - mae_t1: 0.0261 - val_loss: 4.9917 - val_mae: 0.4940 - val_mean_pred: 0.8073 - val_mae_t1: 0.0329\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 3.6806 - mae: 0.3642 - mean_pred: 0.8062 - mae_t1: 0.0243 - val_loss: 4.9333 - val_mae: 0.4882 - val_mean_pred: 0.9275 - val_mae_t1: 0.0325\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 3.5701 - mae: 0.3533 - mean_pred: 0.8788 - mae_t1: 0.0236 - val_loss: 4.8411 - val_mae: 0.4791 - val_mean_pred: 0.8042 - val_mae_t1: 0.0319\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 3.7877 - mae: 0.3748 - mean_pred: 0.6865 - mae_t1: 0.0250 - val_loss: 5.1941 - val_mae: 0.5140 - val_mean_pred: 0.7886 - val_mae_t1: 0.0343\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 4.1877 - mae: 0.4144 - mean_pred: 0.7482 - mae_t1: 0.0276 - val_loss: 4.7705 - val_mae: 0.4721 - val_mean_pred: 0.8723 - val_mae_t1: 0.0315\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 3.7810 - mae: 0.3742 - mean_pred: 0.7584 - mae_t1: 0.0249 - val_loss: 4.9312 - val_mae: 0.4880 - val_mean_pred: 0.9746 - val_mae_t1: 0.0325\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 3.6159 - mae: 0.3578 - mean_pred: 0.8535 - mae_t1: 0.0239 - val_loss: 5.1064 - val_mae: 0.5053 - val_mean_pred: 0.8295 - val_mae_t1: 0.0337\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 4.0507 - mae: 0.4008 - mean_pred: 0.7154 - mae_t1: 0.0267 - val_loss: 4.7055 - val_mae: 0.4657 - val_mean_pred: 0.9054 - val_mae_t1: 0.0310\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 3.9759 - mae: 0.3935 - mean_pred: 0.8253 - mae_t1: 0.0262 - val_loss: 4.2651 - val_mae: 0.4221 - val_mean_pred: 0.8630 - val_mae_t1: 0.0281\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 3.5861 - mae: 0.3549 - mean_pred: 0.7961 - mae_t1: 0.0237 - val_loss: 4.3638 - val_mae: 0.4318 - val_mean_pred: 0.7719 - val_mae_t1: 0.0288\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.8884 - mae: 0.3848 - mean_pred: 0.7220 - mae_t1: 0.0257 - val_loss: 4.6477 - val_mae: 0.4599 - val_mean_pred: 0.7619 - val_mae_t1: 0.0307\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 3.4015 - mae: 0.3366 - mean_pred: 0.7171 - mae_t1: 0.0224 - val_loss: 4.7235 - val_mae: 0.4674 - val_mean_pred: 0.9537 - val_mae_t1: 0.0312\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 3.5824 - mae: 0.3545 - mean_pred: 0.8960 - mae_t1: 0.0236 - val_loss: 4.5395 - val_mae: 0.4492 - val_mean_pred: 0.8283 - val_mae_t1: 0.0299\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 3.4546 - mae: 0.3419 - mean_pred: 0.6863 - mae_t1: 0.0228 - val_loss: 4.4196 - val_mae: 0.4374 - val_mean_pred: 0.8011 - val_mae_t1: 0.0292\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 3.4064 - mae: 0.3371 - mean_pred: 0.7800 - mae_t1: 0.0225 - val_loss: 4.5124 - val_mae: 0.4465 - val_mean_pred: 0.9584 - val_mae_t1: 0.0298\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 3.3514 - mae: 0.3316 - mean_pred: 0.8013 - mae_t1: 0.0221 - val_loss: 4.4338 - val_mae: 0.4388 - val_mean_pred: 0.8278 - val_mae_t1: 0.0293\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 3.3390 - mae: 0.3304 - mean_pred: 0.7885 - mae_t1: 0.0220 - val_loss: 4.9669 - val_mae: 0.4915 - val_mean_pred: 0.9001 - val_mae_t1: 0.0328\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 3.6306 - mae: 0.3593 - mean_pred: 0.8303 - mae_t1: 0.0240 - val_loss: 4.2550 - val_mae: 0.4211 - val_mean_pred: 0.8811 - val_mae_t1: 0.0281\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 3.6722 - mae: 0.3634 - mean_pred: 0.6910 - mae_t1: 0.0242 - val_loss: 4.2376 - val_mae: 0.4193 - val_mean_pred: 0.8555 - val_mae_t1: 0.0280\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 3.6943 - mae: 0.3656 - mean_pred: 0.8658 - mae_t1: 0.0244 - val_loss: 4.3639 - val_mae: 0.4318 - val_mean_pred: 0.9390 - val_mae_t1: 0.0288\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 3.5818 - mae: 0.3545 - mean_pred: 0.7198 - mae_t1: 0.0236 - val_loss: 4.7473 - val_mae: 0.4698 - val_mean_pred: 0.7550 - val_mae_t1: 0.0313\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 3.4922 - mae: 0.3456 - mean_pred: 0.8237 - mae_t1: 0.0230 - val_loss: 4.9753 - val_mae: 0.4924 - val_mean_pred: 1.0455 - val_mae_t1: 0.0328\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 3.9252 - mae: 0.3884 - mean_pred: 0.8661 - mae_t1: 0.0259 - val_loss: 5.0788 - val_mae: 0.5026 - val_mean_pred: 0.7237 - val_mae_t1: 0.0335\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 3.4539 - mae: 0.3418 - mean_pred: 0.6707 - mae_t1: 0.0228 - val_loss: 4.5879 - val_mae: 0.4540 - val_mean_pred: 0.9290 - val_mae_t1: 0.0303\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 3.5866 - mae: 0.3549 - mean_pred: 0.8384 - mae_t1: 0.0237 - val_loss: 4.7273 - val_mae: 0.4678 - val_mean_pred: 0.8416 - val_mae_t1: 0.0312\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 4.3051 - mae: 0.4260 - mean_pred: 0.7004 - mae_t1: 0.0284 - val_loss: 5.7359 - val_mae: 0.5676 - val_mean_pred: 0.7044 - val_mae_t1: 0.0378\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 4.6051 - mae: 0.4557 - mean_pred: 0.7216 - mae_t1: 0.0304 - val_loss: 4.9956 - val_mae: 0.4944 - val_mean_pred: 1.0445 - val_mae_t1: 0.0330\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 3.7977 - mae: 0.3758 - mean_pred: 0.9362 - mae_t1: 0.0251 - val_loss: 5.0919 - val_mae: 0.5039 - val_mean_pred: 0.9151 - val_mae_t1: 0.0336\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 4.0708 - mae: 0.4028 - mean_pred: 0.7984 - mae_t1: 0.0269 - val_loss: 4.8359 - val_mae: 0.4786 - val_mean_pred: 0.8669 - val_mae_t1: 0.0319\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 5.7145 - mae: 0.5655 - mean_pred: 0.7915 - mae_t1: 0.0377 - val_loss: 4.9816 - val_mae: 0.4930 - val_mean_pred: 0.7515 - val_mae_t1: 0.0329\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 4.9214 - mae: 0.4870 - mean_pred: 0.7099 - mae_t1: 0.0325 - val_loss: 5.2487 - val_mae: 0.5194 - val_mean_pred: 1.0100 - val_mae_t1: 0.0346\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 215us/sample - loss: 4.4582 - mae: 0.4412 - mean_pred: 0.9207 - mae_t1: 0.0294 - val_loss: 4.7326 - val_mae: 0.4683 - val_mean_pred: 0.9458 - val_mae_t1: 0.0312\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 4.0874 - mae: 0.4045 - mean_pred: 0.7101 - mae_t1: 0.0270 - val_loss: 5.1959 - val_mae: 0.5142 - val_mean_pred: 0.7902 - val_mae_t1: 0.0343\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 3.7392 - mae: 0.3700 - mean_pred: 0.7424 - mae_t1: 0.0247 - val_loss: 4.6291 - val_mae: 0.4581 - val_mean_pred: 0.9593 - val_mae_t1: 0.0305\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 3.5185 - mae: 0.3482 - mean_pred: 0.8256 - mae_t1: 0.0232 - val_loss: 5.2883 - val_mae: 0.5233 - val_mean_pred: 0.7643 - val_mae_t1: 0.0349\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 3.6371 - mae: 0.3599 - mean_pred: 0.7066 - mae_t1: 0.0240 - val_loss: 4.5875 - val_mae: 0.4540 - val_mean_pred: 0.8462 - val_mae_t1: 0.0303\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.2854 - mae: 0.3251 - mean_pred: 0.8139 - mae_t1: 0.0217 - val_loss: 4.3961 - val_mae: 0.4350 - val_mean_pred: 0.9112 - val_mae_t1: 0.0290\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 3.0580 - mae: 0.3026 - mean_pred: 0.8459 - mae_t1: 0.0202 - val_loss: 4.5573 - val_mae: 0.4510 - val_mean_pred: 0.8192 - val_mae_t1: 0.0301\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 3.0569 - mae: 0.3025 - mean_pred: 0.7464 - mae_t1: 0.0202 - val_loss: 4.3530 - val_mae: 0.4308 - val_mean_pred: 0.8994 - val_mae_t1: 0.0287\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.8921 - mae: 0.2862 - mean_pred: 0.7920 - mae_t1: 0.0191 - val_loss: 4.7386 - val_mae: 0.4689 - val_mean_pred: 0.8022 - val_mae_t1: 0.0313\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.1576 - mae: 0.3125 - mean_pred: 0.6927 - mae_t1: 0.0208 - val_loss: 4.7327 - val_mae: 0.4683 - val_mean_pred: 0.9436 - val_mae_t1: 0.0312\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 3.0942 - mae: 0.3062 - mean_pred: 0.8946 - mae_t1: 0.0204 - val_loss: 4.8777 - val_mae: 0.4827 - val_mean_pred: 0.8569 - val_mae_t1: 0.0322\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 3.1468 - mae: 0.3114 - mean_pred: 0.7155 - mae_t1: 0.0208 - val_loss: 4.1595 - val_mae: 0.4116 - val_mean_pred: 0.8364 - val_mae_t1: 0.0274\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 3.0396 - mae: 0.3008 - mean_pred: 0.8847 - mae_t1: 0.0201 - val_loss: 5.5679 - val_mae: 0.5510 - val_mean_pred: 0.9418 - val_mae_t1: 0.0367\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 231us/sample - loss: 4.0006 - mae: 0.3959 - mean_pred: 0.7679 - mae_t1: 0.0264 - val_loss: 3.8337 - val_mae: 0.3794 - val_mean_pred: 0.8200 - val_mae_t1: 0.0253\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 3.0121 - mae: 0.2981 - mean_pred: 0.8248 - mae_t1: 0.0199 - val_loss: 4.2725 - val_mae: 0.4228 - val_mean_pred: 0.9423 - val_mae_t1: 0.0282\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 3.2561 - mae: 0.3222 - mean_pred: 0.7651 - mae_t1: 0.0215 - val_loss: 4.8923 - val_mae: 0.4841 - val_mean_pred: 0.7121 - val_mae_t1: 0.0323\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 3.5749 - mae: 0.3538 - mean_pred: 0.6666 - mae_t1: 0.0236 - val_loss: 4.5032 - val_mae: 0.4456 - val_mean_pred: 0.8512 - val_mae_t1: 0.0297\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 3.3374 - mae: 0.3303 - mean_pred: 0.8750 - mae_t1: 0.0220 - val_loss: 4.4139 - val_mae: 0.4368 - val_mean_pred: 0.8595 - val_mae_t1: 0.0291\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.2511 - mae: 0.3217 - mean_pred: 0.7740 - mae_t1: 0.0214 - val_loss: 4.2173 - val_mae: 0.4173 - val_mean_pred: 0.7987 - val_mae_t1: 0.0278\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.7333 - mae: 0.2705 - mean_pred: 0.8056 - mae_t1: 0.0180 - val_loss: 4.4516 - val_mae: 0.4405 - val_mean_pred: 0.9421 - val_mae_t1: 0.0294\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.8989 - mae: 0.2869 - mean_pred: 0.7816 - mae_t1: 0.0191 - val_loss: 4.9599 - val_mae: 0.4908 - val_mean_pred: 0.9465 - val_mae_t1: 0.0327\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 3.4446 - mae: 0.3409 - mean_pred: 0.9173 - mae_t1: 0.0227 - val_loss: 4.9226 - val_mae: 0.4871 - val_mean_pred: 1.0091 - val_mae_t1: 0.0325\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 3.1067 - mae: 0.3074 - mean_pred: 0.8274 - mae_t1: 0.0205 - val_loss: 5.3186 - val_mae: 0.5263 - val_mean_pred: 0.8151 - val_mae_t1: 0.0351\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 3.4433 - mae: 0.3407 - mean_pred: 0.8031 - mae_t1: 0.0227 - val_loss: 4.1950 - val_mae: 0.4151 - val_mean_pred: 1.0136 - val_mae_t1: 0.0277\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 3.1548 - mae: 0.3122 - mean_pred: 0.9422 - mae_t1: 0.0208 - val_loss: 4.2263 - val_mae: 0.4182 - val_mean_pred: 0.8442 - val_mae_t1: 0.0279\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 2.7775 - mae: 0.2749 - mean_pred: 0.7435 - mae_t1: 0.0183 - val_loss: 4.0252 - val_mae: 0.3983 - val_mean_pred: 0.8449 - val_mae_t1: 0.0266\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 2.7188 - mae: 0.2690 - mean_pred: 0.8438 - mae_t1: 0.0179 - val_loss: 4.3478 - val_mae: 0.4303 - val_mean_pred: 0.9725 - val_mae_t1: 0.0287\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.8213 - mae: 0.2792 - mean_pred: 0.8484 - mae_t1: 0.0186 - val_loss: 4.4143 - val_mae: 0.4368 - val_mean_pred: 0.8559 - val_mae_t1: 0.0291\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 2.7166 - mae: 0.2688 - mean_pred: 0.7838 - mae_t1: 0.0179 - val_loss: 4.4988 - val_mae: 0.4452 - val_mean_pred: 0.8846 - val_mae_t1: 0.0297\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 2.7059 - mae: 0.2678 - mean_pred: 0.7837 - mae_t1: 0.0179 - val_loss: 4.4878 - val_mae: 0.4441 - val_mean_pred: 0.8444 - val_mae_t1: 0.0296\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.7955 - mae: 0.2766 - mean_pred: 0.7575 - mae_t1: 0.0184 - val_loss: 4.4067 - val_mae: 0.4361 - val_mean_pred: 0.8645 - val_mae_t1: 0.0291\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.5759 - mae: 0.2549 - mean_pred: 0.7812 - mae_t1: 0.0170 - val_loss: 4.4280 - val_mae: 0.4382 - val_mean_pred: 0.8678 - val_mae_t1: 0.0292\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 2.5439 - mae: 0.2517 - mean_pred: 0.7814 - mae_t1: 0.0168 - val_loss: 4.5152 - val_mae: 0.4468 - val_mean_pred: 0.7729 - val_mae_t1: 0.0298\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 2.9180 - mae: 0.2888 - mean_pred: 0.7051 - mae_t1: 0.0193 - val_loss: 4.5507 - val_mae: 0.4503 - val_mean_pred: 0.8726 - val_mae_t1: 0.0300\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 2.7462 - mae: 0.2718 - mean_pred: 0.8490 - mae_t1: 0.0181 - val_loss: 4.6069 - val_mae: 0.4559 - val_mean_pred: 0.9915 - val_mae_t1: 0.0304\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 2.5874 - mae: 0.2560 - mean_pred: 0.8433 - mae_t1: 0.0171 - val_loss: 4.4167 - val_mae: 0.4371 - val_mean_pred: 0.8533 - val_mae_t1: 0.0291\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 2.6476 - mae: 0.2620 - mean_pred: 0.8246 - mae_t1: 0.0175 - val_loss: 4.8801 - val_mae: 0.4829 - val_mean_pred: 0.9037 - val_mae_t1: 0.0322\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 2.9127 - mae: 0.2882 - mean_pred: 0.7659 - mae_t1: 0.0192 - val_loss: 4.4639 - val_mae: 0.4417 - val_mean_pred: 0.8264 - val_mae_t1: 0.0294\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 2.7836 - mae: 0.2755 - mean_pred: 0.7410 - mae_t1: 0.0184 - val_loss: 4.8412 - val_mae: 0.4791 - val_mean_pred: 0.8968 - val_mae_t1: 0.0319\n",
      "Earliness...\n",
      "0.0015001296997070312\n",
      "____________________________________________________________\n",
      "Test MAE:      0.2938206639578  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▇▄▄▄▄▃▃▃▄▄▃▃▃▃▃▂▂▃▂▂▃▃▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▇▄▄▄▄▃▃▃▄▄▃▃▃▃▃▂▂▃▂▂▃▃▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▇▄▄▄▄▃▃▃▄▄▃▃▃▃▃▂▂▃▂▂▃▃▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▂▂▇▃▄▅▆▅▅▆▅█▅▆▇▅▅▆▅▅▄▅▆▅▅▇▆██▆█▆▇▅▇▆▆▇▅</td></tr><tr><td>val_loss</td><td>█▄▇▇█▅▄▄▄▃▂▅▃▄▃▂▃▂▂▂▃▂▅▃▄▃▂▂▃▅▃▂▃▄▁▂▂▂▂▃</td></tr><tr><td>val_mae</td><td>█▄▇▇█▅▄▄▄▃▂▅▃▄▃▂▃▂▂▂▃▂▅▃▄▃▂▂▃▅▃▂▃▄▁▂▂▂▂▃</td></tr><tr><td>val_mae_t1</td><td>█▄▇▇█▅▄▄▄▃▂▅▃▄▃▂▃▂▂▂▃▂▅▃▄▃▂▂▃▅▃▂▃▄▁▂▂▂▂▃</td></tr><tr><td>val_mean_pred</td><td>█▄▁▁▄▆▆▅▆▅▅▆▄▄▆▅▄▄▄▅▄▅▃▅▆▅▅▅▅▅▃▅▅▄▄▅▅▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33567</td></tr><tr><td>AE_2</td><td>0.28724</td></tr><tr><td>AE_3</td><td>0.29968</td></tr><tr><td>MAE</td><td>0.29382</td></tr><tr><td>best_epoch</td><td>74</td></tr><tr><td>best_val_loss</td><td>3.83367</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.78356</td></tr><tr><td>mae</td><td>0.27546</td></tr><tr><td>mae_t1</td><td>0.01836</td></tr><tr><td>mean_pred</td><td>0.74096</td></tr><tr><td>val_loss</td><td>4.84116</td></tr><tr><td>val_mae</td><td>0.47907</td></tr><tr><td>val_mae_t1</td><td>0.03194</td></tr><tr><td>val_mean_pred</td><td>0.89685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">earnest-water-89</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3dqhq1db\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/3dqhq1db</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155643-3dqhq1db\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155711-317k23np</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/317k23np\" target=\"_blank\">glamorous-grass-90</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 720us/sample - loss: 8.0713 - mae: 0.7987 - mean_pred: 0.1890 - mae_t1: 0.0532 - val_loss: 5.1023 - val_mae: 0.5049 - val_mean_pred: 0.8847 - val_mae_t1: 0.0337\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 166us/sample - loss: 6.4203 - mae: 0.6353 - mean_pred: 0.9336 - mae_t1: 0.0424 - val_loss: 5.4014 - val_mae: 0.5345 - val_mean_pred: 0.7003 - val_mae_t1: 0.0356\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 162us/sample - loss: 6.2371 - mae: 0.6172 - mean_pred: 0.5851 - mae_t1: 0.0411 - val_loss: 6.2999 - val_mae: 0.6234 - val_mean_pred: 0.4246 - val_mae_t1: 0.0416\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 6.1166 - mae: 0.6053 - mean_pred: 0.4610 - mae_t1: 0.0404 - val_loss: 5.9607 - val_mae: 0.5899 - val_mean_pred: 0.9572 - val_mae_t1: 0.0393\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 118us/sample - loss: 6.4006 - mae: 0.6334 - mean_pred: 0.9271 - mae_t1: 0.0422 - val_loss: 6.1894 - val_mae: 0.6125 - val_mean_pred: 1.1624 - val_mae_t1: 0.0408\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 6.1866 - mae: 0.6122 - mean_pred: 1.0250 - mae_t1: 0.0408 - val_loss: 6.4942 - val_mae: 0.6427 - val_mean_pred: 0.6483 - val_mae_t1: 0.0428\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 6.2794 - mae: 0.6214 - mean_pred: 0.5019 - mae_t1: 0.0414 - val_loss: 7.3905 - val_mae: 0.7314 - val_mean_pred: 0.4941 - val_mae_t1: 0.0488\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 6.4592 - mae: 0.6392 - mean_pred: 0.4402 - mae_t1: 0.0426 - val_loss: 6.3692 - val_mae: 0.6303 - val_mean_pred: 0.6154 - val_mae_t1: 0.0420\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 5.6352 - mae: 0.5576 - mean_pred: 0.5340 - mae_t1: 0.0372 - val_loss: 5.5198 - val_mae: 0.5462 - val_mean_pred: 0.8463 - val_mae_t1: 0.0364\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 5.2804 - mae: 0.5225 - mean_pred: 0.7916 - mae_t1: 0.0348 - val_loss: 5.4336 - val_mae: 0.5377 - val_mean_pred: 0.9004 - val_mae_t1: 0.0358\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 5.0777 - mae: 0.5025 - mean_pred: 0.7467 - mae_t1: 0.0335 - val_loss: 5.6971 - val_mae: 0.5638 - val_mean_pred: 0.7125 - val_mae_t1: 0.0376\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 5.4592 - mae: 0.5402 - mean_pred: 0.6930 - mae_t1: 0.0360 - val_loss: 6.3363 - val_mae: 0.6270 - val_mean_pred: 1.0881 - val_mae_t1: 0.0418\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 6.7429 - mae: 0.6673 - mean_pred: 1.1022 - mae_t1: 0.0445 - val_loss: 5.4714 - val_mae: 0.5414 - val_mean_pred: 1.0006 - val_mae_t1: 0.0361\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 5.1565 - mae: 0.5103 - mean_pred: 0.8741 - mae_t1: 0.0340 - val_loss: 6.5204 - val_mae: 0.6453 - val_mean_pred: 0.4530 - val_mae_t1: 0.0430\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 6.1294 - mae: 0.6066 - mean_pred: 0.4031 - mae_t1: 0.0404 - val_loss: 6.3096 - val_mae: 0.6244 - val_mean_pred: 0.4839 - val_mae_t1: 0.0416\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 5.4756 - mae: 0.5419 - mean_pred: 0.5143 - mae_t1: 0.0361 - val_loss: 6.1412 - val_mae: 0.6077 - val_mean_pred: 1.1683 - val_mae_t1: 0.0405\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 6.1337 - mae: 0.6070 - mean_pred: 1.1111 - mae_t1: 0.0405 - val_loss: 6.9376 - val_mae: 0.6865 - val_mean_pred: 1.2766 - val_mae_t1: 0.0458\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 5.7226 - mae: 0.5663 - mean_pred: 1.0504 - mae_t1: 0.0378 - val_loss: 5.1943 - val_mae: 0.5140 - val_mean_pred: 0.8675 - val_mae_t1: 0.0343\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 4.8287 - mae: 0.4778 - mean_pred: 0.7669 - mae_t1: 0.0319 - val_loss: 5.1073 - val_mae: 0.5054 - val_mean_pred: 0.8279 - val_mae_t1: 0.0337\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.7790 - mae: 0.4729 - mean_pred: 0.7382 - mae_t1: 0.0315 - val_loss: 5.2079 - val_mae: 0.5154 - val_mean_pred: 0.7268 - val_mae_t1: 0.0344\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 5.2645 - mae: 0.5210 - mean_pred: 0.6590 - mae_t1: 0.0347 - val_loss: 5.8794 - val_mae: 0.5818 - val_mean_pred: 0.5777 - val_mae_t1: 0.0388\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 5.6783 - mae: 0.5619 - mean_pred: 0.5338 - mae_t1: 0.0375 - val_loss: 5.6841 - val_mae: 0.5625 - val_mean_pred: 0.5925 - val_mae_t1: 0.0375\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.8929 - mae: 0.4842 - mean_pred: 0.5826 - mae_t1: 0.0323 - val_loss: 5.8074 - val_mae: 0.5747 - val_mean_pred: 1.0102 - val_mae_t1: 0.0383\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.1024 - mae: 0.5049 - mean_pred: 0.9587 - mae_t1: 0.0337 - val_loss: 6.9273 - val_mae: 0.6855 - val_mean_pred: 1.2408 - val_mae_t1: 0.0457\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.6893 - mae: 0.5630 - mean_pred: 1.0735 - mae_t1: 0.0375 - val_loss: 5.7727 - val_mae: 0.5713 - val_mean_pred: 1.0369 - val_mae_t1: 0.0381\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 4.5953 - mae: 0.4547 - mean_pred: 0.8592 - mae_t1: 0.0303 - val_loss: 5.5208 - val_mae: 0.5463 - val_mean_pred: 0.8143 - val_mae_t1: 0.0364\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0777 - mae: 0.5025 - mean_pred: 0.7188 - mae_t1: 0.0335 - val_loss: 5.4618 - val_mae: 0.5405 - val_mean_pred: 0.6609 - val_mae_t1: 0.0360\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 5.1611 - mae: 0.5107 - mean_pred: 0.6391 - mae_t1: 0.0340 - val_loss: 5.2146 - val_mae: 0.5160 - val_mean_pred: 0.7148 - val_mae_t1: 0.0344\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.3951 - mae: 0.4349 - mean_pred: 0.7043 - mae_t1: 0.0290 - val_loss: 5.1689 - val_mae: 0.5115 - val_mean_pred: 0.9581 - val_mae_t1: 0.0341\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 4.4035 - mae: 0.4358 - mean_pred: 0.9043 - mae_t1: 0.0291 - val_loss: 4.9203 - val_mae: 0.4869 - val_mean_pred: 0.8634 - val_mae_t1: 0.0325\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.1759 - mae: 0.4132 - mean_pred: 0.7109 - mae_t1: 0.0275 - val_loss: 5.8247 - val_mae: 0.5764 - val_mean_pred: 0.6328 - val_mae_t1: 0.0384\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 4.7884 - mae: 0.4739 - mean_pred: 0.5644 - mae_t1: 0.0316 - val_loss: 5.7666 - val_mae: 0.5707 - val_mean_pred: 0.7378 - val_mae_t1: 0.0380\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 4.3758 - mae: 0.4330 - mean_pred: 0.6618 - mae_t1: 0.0289 - val_loss: 5.3593 - val_mae: 0.5303 - val_mean_pred: 1.0043 - val_mae_t1: 0.0354\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 4.3193 - mae: 0.4274 - mean_pred: 0.9190 - mae_t1: 0.0285 - val_loss: 5.4482 - val_mae: 0.5391 - val_mean_pred: 1.0678 - val_mae_t1: 0.0359\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 4.2237 - mae: 0.4180 - mean_pred: 0.9052 - mae_t1: 0.0279 - val_loss: 5.4458 - val_mae: 0.5389 - val_mean_pred: 0.6935 - val_mae_t1: 0.0359\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.9701 - mae: 0.4918 - mean_pred: 0.5987 - mae_t1: 0.0328 - val_loss: 5.7513 - val_mae: 0.5691 - val_mean_pred: 0.6049 - val_mae_t1: 0.0379\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 4.7980 - mae: 0.4748 - mean_pred: 0.6211 - mae_t1: 0.0317 - val_loss: 4.8419 - val_mae: 0.4791 - val_mean_pred: 0.9762 - val_mae_t1: 0.0319\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.3458 - mae: 0.4300 - mean_pred: 0.9218 - mae_t1: 0.0287 - val_loss: 4.8984 - val_mae: 0.4847 - val_mean_pred: 1.0541 - val_mae_t1: 0.0323\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.2968 - mae: 0.4252 - mean_pred: 0.9346 - mae_t1: 0.0283 - val_loss: 4.8879 - val_mae: 0.4837 - val_mean_pred: 0.8494 - val_mae_t1: 0.0322\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 3.9222 - mae: 0.3881 - mean_pred: 0.7607 - mae_t1: 0.0259 - val_loss: 4.8025 - val_mae: 0.4752 - val_mean_pred: 0.8005 - val_mae_t1: 0.0317\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 3.8009 - mae: 0.3761 - mean_pred: 0.7568 - mae_t1: 0.0251 - val_loss: 5.0627 - val_mae: 0.5010 - val_mean_pred: 0.9376 - val_mae_t1: 0.0334\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.1628 - mae: 0.4119 - mean_pred: 0.8861 - mae_t1: 0.0275 - val_loss: 5.1119 - val_mae: 0.5059 - val_mean_pred: 0.8785 - val_mae_t1: 0.0337\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.9909 - mae: 0.3949 - mean_pred: 0.7559 - mae_t1: 0.0263 - val_loss: 5.1385 - val_mae: 0.5085 - val_mean_pred: 0.7643 - val_mae_t1: 0.0339\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 136us/sample - loss: 3.8536 - mae: 0.3813 - mean_pred: 0.7061 - mae_t1: 0.0254 - val_loss: 4.7089 - val_mae: 0.4660 - val_mean_pred: 0.9508 - val_mae_t1: 0.0311\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 3.7978 - mae: 0.3758 - mean_pred: 0.8615 - mae_t1: 0.0251 - val_loss: 4.5559 - val_mae: 0.4508 - val_mean_pred: 0.9106 - val_mae_t1: 0.0301\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 3.7517 - mae: 0.3713 - mean_pred: 0.7660 - mae_t1: 0.0248 - val_loss: 4.4214 - val_mae: 0.4375 - val_mean_pred: 0.7373 - val_mae_t1: 0.0292\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 462us/sample - loss: 4.1303 - mae: 0.4087 - mean_pred: 0.6715 - mae_t1: 0.0272 - val_loss: 4.4788 - val_mae: 0.4432 - val_mean_pred: 0.7196 - val_mae_t1: 0.0295\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 106us/sample - loss: 4.0564 - mae: 0.4014 - mean_pred: 0.6716 - mae_t1: 0.0268 - val_loss: 5.1110 - val_mae: 0.5058 - val_mean_pred: 0.8562 - val_mae_t1: 0.0337\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 3.8996 - mae: 0.3859 - mean_pred: 0.7722 - mae_t1: 0.0257 - val_loss: 5.0241 - val_mae: 0.4972 - val_mean_pred: 1.0608 - val_mae_t1: 0.0331\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 4.7650 - mae: 0.4715 - mean_pred: 0.9737 - mae_t1: 0.0314 - val_loss: 4.8932 - val_mae: 0.4842 - val_mean_pred: 1.0344 - val_mae_t1: 0.0323\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.0243 - mae: 0.3982 - mean_pred: 0.8478 - mae_t1: 0.0265 - val_loss: 5.8863 - val_mae: 0.5825 - val_mean_pred: 0.8148 - val_mae_t1: 0.0388\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.3623 - mae: 0.4317 - mean_pred: 0.6909 - mae_t1: 0.0288 - val_loss: 5.3149 - val_mae: 0.5259 - val_mean_pred: 0.8325 - val_mae_t1: 0.0351\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.3638 - mae: 0.4318 - mean_pred: 0.7219 - mae_t1: 0.0288 - val_loss: 5.2762 - val_mae: 0.5221 - val_mean_pred: 0.8086 - val_mae_t1: 0.0348\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.9331 - mae: 0.3892 - mean_pred: 0.6781 - mae_t1: 0.0259 - val_loss: 5.6934 - val_mae: 0.5634 - val_mean_pred: 0.9022 - val_mae_t1: 0.0376\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.4453 - mae: 0.4399 - mean_pred: 0.8257 - mae_t1: 0.0293 - val_loss: 5.1400 - val_mae: 0.5086 - val_mean_pred: 1.0437 - val_mae_t1: 0.0339\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.6096 - mae: 0.4562 - mean_pred: 0.9672 - mae_t1: 0.0304 - val_loss: 5.3520 - val_mae: 0.5296 - val_mean_pred: 0.9205 - val_mae_t1: 0.0353\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.7786 - mae: 0.4729 - mean_pred: 0.8330 - mae_t1: 0.0315 - val_loss: 6.1716 - val_mae: 0.6107 - val_mean_pred: 0.6848 - val_mae_t1: 0.0407\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.3490 - mae: 0.5293 - mean_pred: 0.6788 - mae_t1: 0.0353 - val_loss: 7.3476 - val_mae: 0.7271 - val_mean_pred: 0.7374 - val_mae_t1: 0.0485\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.4458 - mae: 0.5389 - mean_pred: 0.6765 - mae_t1: 0.0359 - val_loss: 4.8067 - val_mae: 0.4757 - val_mean_pred: 0.7923 - val_mae_t1: 0.0317\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.4586 - mae: 0.4412 - mean_pred: 0.8017 - mae_t1: 0.0294 - val_loss: 5.2388 - val_mae: 0.5184 - val_mean_pred: 1.0816 - val_mae_t1: 0.0346\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 5.4388 - mae: 0.5382 - mean_pred: 1.0570 - mae_t1: 0.0359 - val_loss: 5.3891 - val_mae: 0.5333 - val_mean_pred: 1.0812 - val_mae_t1: 0.0356\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 4.2409 - mae: 0.4197 - mean_pred: 0.9245 - mae_t1: 0.0280 - val_loss: 5.8891 - val_mae: 0.5828 - val_mean_pred: 0.6767 - val_mae_t1: 0.0389\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.0752 - mae: 0.5022 - mean_pred: 0.5539 - mae_t1: 0.0335 - val_loss: 6.1352 - val_mae: 0.6071 - val_mean_pred: 0.5253 - val_mae_t1: 0.0405\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.9174 - mae: 0.4866 - mean_pred: 0.5060 - mae_t1: 0.0324 - val_loss: 4.8327 - val_mae: 0.4782 - val_mean_pred: 0.8460 - val_mae_t1: 0.0319\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.7760 - mae: 0.3737 - mean_pred: 0.8052 - mae_t1: 0.0249 - val_loss: 5.9741 - val_mae: 0.5912 - val_mean_pred: 1.1560 - val_mae_t1: 0.0394\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.5212 - mae: 0.4474 - mean_pred: 0.9931 - mae_t1: 0.0298 - val_loss: 4.7565 - val_mae: 0.4707 - val_mean_pred: 0.8552 - val_mae_t1: 0.0314\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.9115 - mae: 0.3871 - mean_pred: 0.6893 - mae_t1: 0.0258 - val_loss: 5.3904 - val_mae: 0.5334 - val_mean_pred: 0.6151 - val_mae_t1: 0.0356\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.5256 - mae: 0.4478 - mean_pred: 0.5344 - mae_t1: 0.0299 - val_loss: 4.9016 - val_mae: 0.4851 - val_mean_pred: 0.7402 - val_mae_t1: 0.0323\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 3.9187 - mae: 0.3878 - mean_pred: 0.6896 - mae_t1: 0.0259 - val_loss: 5.6358 - val_mae: 0.5577 - val_mean_pred: 1.1509 - val_mae_t1: 0.0372\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.8295 - mae: 0.4779 - mean_pred: 1.0303 - mae_t1: 0.0319 - val_loss: 5.7373 - val_mae: 0.5678 - val_mean_pred: 1.1652 - val_mae_t1: 0.0379\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.8305 - mae: 0.4780 - mean_pred: 0.9769 - mae_t1: 0.0319 - val_loss: 4.7667 - val_mae: 0.4717 - val_mean_pred: 0.7720 - val_mae_t1: 0.0314\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.4878 - mae: 0.4441 - mean_pred: 0.6289 - mae_t1: 0.0296 - val_loss: 6.5583 - val_mae: 0.6490 - val_mean_pred: 0.6025 - val_mae_t1: 0.0433\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.6465 - mae: 0.5588 - mean_pred: 0.5715 - mae_t1: 0.0373 - val_loss: 7.2650 - val_mae: 0.7189 - val_mean_pred: 0.8170 - val_mae_t1: 0.0479\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 5.5606 - mae: 0.5503 - mean_pred: 0.7547 - mae_t1: 0.0367 - val_loss: 4.6439 - val_mae: 0.4596 - val_mean_pred: 0.9198 - val_mae_t1: 0.0306\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 4.5759 - mae: 0.4528 - mean_pred: 0.8638 - mae_t1: 0.0302 - val_loss: 5.1073 - val_mae: 0.5054 - val_mean_pred: 0.9761 - val_mae_t1: 0.0337\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 5.1661 - mae: 0.5112 - mean_pred: 0.8994 - mae_t1: 0.0341 - val_loss: 5.1555 - val_mae: 0.5102 - val_mean_pred: 0.9716 - val_mae_t1: 0.0340\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.2681 - mae: 0.4224 - mean_pred: 0.8703 - mae_t1: 0.0282 - val_loss: 5.9512 - val_mae: 0.5889 - val_mean_pred: 1.0131 - val_mae_t1: 0.0393\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.2106 - mae: 0.4167 - mean_pred: 0.8867 - mae_t1: 0.0278 - val_loss: 4.7653 - val_mae: 0.4716 - val_mean_pred: 0.9599 - val_mae_t1: 0.0314\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 4.0137 - mae: 0.3972 - mean_pred: 0.8136 - mae_t1: 0.0265 - val_loss: 5.0718 - val_mae: 0.5019 - val_mean_pred: 0.8014 - val_mae_t1: 0.0335\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.3088 - mae: 0.4264 - mean_pred: 0.7059 - mae_t1: 0.0284 - val_loss: 5.1106 - val_mae: 0.5057 - val_mean_pred: 0.8388 - val_mae_t1: 0.0337\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.8130 - mae: 0.3773 - mean_pred: 0.7599 - mae_t1: 0.0252 - val_loss: 5.1092 - val_mae: 0.5056 - val_mean_pred: 0.9509 - val_mae_t1: 0.0337\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.7252 - mae: 0.3686 - mean_pred: 0.7959 - mae_t1: 0.0246 - val_loss: 5.0527 - val_mae: 0.5000 - val_mean_pred: 0.8608 - val_mae_t1: 0.0333\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.8625 - mae: 0.3822 - mean_pred: 0.7399 - mae_t1: 0.0255 - val_loss: 4.8142 - val_mae: 0.4764 - val_mean_pred: 0.9180 - val_mae_t1: 0.0318\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.5593 - mae: 0.3522 - mean_pred: 0.8188 - mae_t1: 0.0235 - val_loss: 4.8910 - val_mae: 0.4840 - val_mean_pred: 0.9782 - val_mae_t1: 0.0323\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.7320 - mae: 0.3693 - mean_pred: 0.8392 - mae_t1: 0.0246 - val_loss: 4.7770 - val_mae: 0.4727 - val_mean_pred: 0.8532 - val_mae_t1: 0.0315\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.7353 - mae: 0.3696 - mean_pred: 0.7447 - mae_t1: 0.0246 - val_loss: 4.6271 - val_mae: 0.4579 - val_mean_pred: 0.7329 - val_mae_t1: 0.0305\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.7729 - mae: 0.3734 - mean_pred: 0.6930 - mae_t1: 0.0249 - val_loss: 4.8677 - val_mae: 0.4817 - val_mean_pred: 0.7972 - val_mae_t1: 0.0321\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.9033 - mae: 0.3863 - mean_pred: 0.7296 - mae_t1: 0.0258 - val_loss: 5.0439 - val_mae: 0.4991 - val_mean_pred: 0.9323 - val_mae_t1: 0.0333\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 3.7992 - mae: 0.3760 - mean_pred: 0.8515 - mae_t1: 0.0251 - val_loss: 5.1055 - val_mae: 0.5052 - val_mean_pred: 0.9855 - val_mae_t1: 0.0337\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.9835 - mae: 0.3942 - mean_pred: 0.8601 - mae_t1: 0.0263 - val_loss: 4.8197 - val_mae: 0.4770 - val_mean_pred: 0.8910 - val_mae_t1: 0.0318\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 3.4855 - mae: 0.3449 - mean_pred: 0.7523 - mae_t1: 0.0230 - val_loss: 6.0984 - val_mae: 0.6035 - val_mean_pred: 0.9706 - val_mae_t1: 0.0402\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 5.1798 - mae: 0.5126 - mean_pred: 0.8479 - mae_t1: 0.0342 - val_loss: 4.8788 - val_mae: 0.4828 - val_mean_pred: 0.9821 - val_mae_t1: 0.0322\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 3.5612 - mae: 0.3524 - mean_pred: 0.8175 - mae_t1: 0.0235 - val_loss: 4.8513 - val_mae: 0.4801 - val_mean_pred: 0.8893 - val_mae_t1: 0.0320\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 4.2972 - mae: 0.4252 - mean_pred: 0.7855 - mae_t1: 0.0283 - val_loss: 4.5774 - val_mae: 0.4530 - val_mean_pred: 0.8361 - val_mae_t1: 0.0302\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.8494 - mae: 0.3809 - mean_pred: 0.7303 - mae_t1: 0.0254 - val_loss: 5.7790 - val_mae: 0.5719 - val_mean_pred: 0.9525 - val_mae_t1: 0.0381\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 4.5371 - mae: 0.4490 - mean_pred: 0.8276 - mae_t1: 0.0299 - val_loss: 4.9504 - val_mae: 0.4899 - val_mean_pred: 1.0118 - val_mae_t1: 0.0327\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 3.6159 - mae: 0.3578 - mean_pred: 0.8532 - mae_t1: 0.0239 - val_loss: 5.4528 - val_mae: 0.5396 - val_mean_pred: 0.9522 - val_mae_t1: 0.0360\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.3146 - mae: 0.4270 - mean_pred: 0.8165 - mae_t1: 0.0285 - val_loss: 4.7911 - val_mae: 0.4741 - val_mean_pred: 0.8477 - val_mae_t1: 0.0316\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.4888 - mae: 0.3452 - mean_pred: 0.7090 - mae_t1: 0.0230 - val_loss: 5.1919 - val_mae: 0.5138 - val_mean_pred: 0.8369 - val_mae_t1: 0.0343\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.5078 - mae: 0.3471 - mean_pred: 0.7354 - mae_t1: 0.0231 - val_loss: 4.7186 - val_mae: 0.4669 - val_mean_pred: 0.9424 - val_mae_t1: 0.0311\n",
      "Earliness...\n",
      "0.0019991397857666016\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3383419588620665  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▅▆▃▆▄▄▄▃▃▄▂▂▃▂▁▂▁▂▂▂▃▄▄▃▂▂▃▄▂▂▁▁▁▁▄▂▁▁</td></tr><tr><td>mae</td><td>█▅▅▆▃▆▄▄▄▃▃▄▂▂▃▂▁▂▁▂▂▂▃▄▄▃▂▂▃▄▂▂▁▁▁▁▄▂▁▁</td></tr><tr><td>mae_t1</td><td>█▅▅▆▃▆▄▄▄▃▃▄▂▂▃▂▁▂▁▂▂▂▃▄▄▃▂▂▃▄▂▂▁▁▁▁▄▂▁▁</td></tr><tr><td>mean_pred</td><td>▁▄▇▃▅█▃█▅▄▆▄▅▇▄▇▅▅▅▅▆▅▇▅█▃▅▅▄▅▆▆▆▆▅▆▆▆▆▅</td></tr><tr><td>val_loss</td><td>▃▇█▇▅▄▇▄▆▆▅▄▆▄▅▃▃▂▁▃▆▅▄▂▄▂▄▅█▂▆▃▃▃▂▃▂▂▄▂</td></tr><tr><td>val_mae</td><td>▃▇█▇▅▄▇▄▆▆▅▄▆▄▅▃▃▂▁▃▆▅▄▂▄▂▄▅█▂▆▃▃▃▂▃▂▂▄▂</td></tr><tr><td>val_mae_t1</td><td>▃▇█▇▅▄▇▄▆▆▅▄▆▄▅▃▃▂▁▃▆▅▄▂▄▂▄▅█▂▆▃▃▃▂▃▂▂▄▂</td></tr><tr><td>val_mean_pred</td><td>▅▁▃▃▄▆█▅▂▇▅▄▃▇▃▅▆▆▄▇▅▅▆▄▇▅▃█▃▆▇▅▅▆▅▆▆▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.35784</td></tr><tr><td>AE_2</td><td>0.27659</td></tr><tr><td>AE_3</td><td>0.36816</td></tr><tr><td>MAE</td><td>0.33834</td></tr><tr><td>best_epoch</td><td>45</td></tr><tr><td>best_val_loss</td><td>4.4214</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>3.50779</td></tr><tr><td>mae</td><td>0.34712</td></tr><tr><td>mae_t1</td><td>0.02314</td></tr><tr><td>mean_pred</td><td>0.7354</td></tr><tr><td>val_loss</td><td>4.71865</td></tr><tr><td>val_mae</td><td>0.46695</td></tr><tr><td>val_mae_t1</td><td>0.03113</td></tr><tr><td>val_mean_pred</td><td>0.94235</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glamorous-grass-90</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/317k23np\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/317k23np</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155711-317k23np\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155735-19qjflc3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/19qjflc3\" target=\"_blank\">wandering-deluge-91</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 876us/sample - loss: 1.0703 - mae: 0.8028 - mean_pred: 0.1659 - mae_t1: 0.0535 - val_loss: 0.8546 - val_mae: 0.6409 - val_mean_pred: 0.3722 - val_mae_t1: 0.0427\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 314us/sample - loss: 0.8383 - mae: 0.6287 - mean_pred: 0.4584 - mae_t1: 0.0419 - val_loss: 0.6858 - val_mae: 0.5143 - val_mean_pred: 0.8195 - val_mae_t1: 0.0343\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 244us/sample - loss: 0.7811 - mae: 0.5858 - mean_pred: 0.8525 - mae_t1: 0.0391 - val_loss: 0.7116 - val_mae: 0.5337 - val_mean_pred: 1.0335 - val_mae_t1: 0.0356\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 246us/sample - loss: 0.7565 - mae: 0.5674 - mean_pred: 0.8767 - mae_t1: 0.0378 - val_loss: 0.6556 - val_mae: 0.4917 - val_mean_pred: 0.8063 - val_mae_t1: 0.0328\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.7146 - mae: 0.5360 - mean_pred: 0.6358 - mae_t1: 0.0357 - val_loss: 0.6806 - val_mae: 0.5105 - val_mean_pred: 0.7055 - val_mae_t1: 0.0340\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 0.6742 - mae: 0.5056 - mean_pred: 0.6595 - mae_t1: 0.0337 - val_loss: 0.6645 - val_mae: 0.4984 - val_mean_pred: 0.8749 - val_mae_t1: 0.0332\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 218us/sample - loss: 0.6584 - mae: 0.4938 - mean_pred: 0.7715 - mae_t1: 0.0329 - val_loss: 0.6451 - val_mae: 0.4838 - val_mean_pred: 0.8082 - val_mae_t1: 0.0323\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.6438 - mae: 0.4829 - mean_pred: 0.6824 - mae_t1: 0.0322 - val_loss: 0.6559 - val_mae: 0.4919 - val_mean_pred: 0.7301 - val_mae_t1: 0.0328\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 0.6332 - mae: 0.4749 - mean_pred: 0.6712 - mae_t1: 0.0317 - val_loss: 0.6394 - val_mae: 0.4796 - val_mean_pred: 0.7982 - val_mae_t1: 0.0320\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 0.6175 - mae: 0.4631 - mean_pred: 0.7466 - mae_t1: 0.0309 - val_loss: 0.6306 - val_mae: 0.4730 - val_mean_pred: 0.8509 - val_mae_t1: 0.0315\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.6125 - mae: 0.4594 - mean_pred: 0.8018 - mae_t1: 0.0306 - val_loss: 0.6442 - val_mae: 0.4832 - val_mean_pred: 0.8624 - val_mae_t1: 0.0322\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 0.6088 - mae: 0.4566 - mean_pred: 0.8172 - mae_t1: 0.0304 - val_loss: 0.6859 - val_mae: 0.5144 - val_mean_pred: 0.8108 - val_mae_t1: 0.0343\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.6071 - mae: 0.4554 - mean_pred: 0.7189 - mae_t1: 0.0304 - val_loss: 0.7222 - val_mae: 0.5417 - val_mean_pred: 0.7101 - val_mae_t1: 0.0361\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 0.6004 - mae: 0.4503 - mean_pred: 0.6712 - mae_t1: 0.0300 - val_loss: 0.6996 - val_mae: 0.5247 - val_mean_pred: 0.8094 - val_mae_t1: 0.0350\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.5784 - mae: 0.4338 - mean_pred: 0.7956 - mae_t1: 0.0289 - val_loss: 0.6737 - val_mae: 0.5053 - val_mean_pred: 0.8823 - val_mae_t1: 0.0337\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.5637 - mae: 0.4228 - mean_pred: 0.7629 - mae_t1: 0.0282 - val_loss: 0.6928 - val_mae: 0.5196 - val_mean_pred: 0.7388 - val_mae_t1: 0.0346\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5707 - mae: 0.4280 - mean_pred: 0.6839 - mae_t1: 0.0285 - val_loss: 0.6846 - val_mae: 0.5135 - val_mean_pred: 0.7988 - val_mae_t1: 0.0342\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5580 - mae: 0.4185 - mean_pred: 0.7705 - mae_t1: 0.0279 - val_loss: 0.6515 - val_mae: 0.4886 - val_mean_pred: 0.8347 - val_mae_t1: 0.0326\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.5674 - mae: 0.4256 - mean_pred: 0.7508 - mae_t1: 0.0284 - val_loss: 0.6572 - val_mae: 0.4929 - val_mean_pred: 0.7943 - val_mae_t1: 0.0329\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5517 - mae: 0.4138 - mean_pred: 0.7282 - mae_t1: 0.0276 - val_loss: 0.6520 - val_mae: 0.4890 - val_mean_pred: 0.8084 - val_mae_t1: 0.0326\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5557 - mae: 0.4168 - mean_pred: 0.7362 - mae_t1: 0.0278 - val_loss: 0.6366 - val_mae: 0.4774 - val_mean_pred: 0.7776 - val_mae_t1: 0.0318\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 0.5339 - mae: 0.4004 - mean_pred: 0.7204 - mae_t1: 0.0267 - val_loss: 0.6055 - val_mae: 0.4541 - val_mean_pred: 0.8749 - val_mae_t1: 0.0303\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 0.5372 - mae: 0.4029 - mean_pred: 0.8274 - mae_t1: 0.0269 - val_loss: 0.5840 - val_mae: 0.4380 - val_mean_pred: 0.8991 - val_mae_t1: 0.0292\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.5192 - mae: 0.3894 - mean_pred: 0.7926 - mae_t1: 0.0260 - val_loss: 0.6206 - val_mae: 0.4654 - val_mean_pred: 0.7287 - val_mae_t1: 0.0310\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.5393 - mae: 0.4045 - mean_pred: 0.6723 - mae_t1: 0.0270 - val_loss: 0.6036 - val_mae: 0.4527 - val_mean_pred: 0.7790 - val_mae_t1: 0.0302\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.5197 - mae: 0.3898 - mean_pred: 0.7868 - mae_t1: 0.0260 - val_loss: 0.5892 - val_mae: 0.4419 - val_mean_pred: 0.9047 - val_mae_t1: 0.0295\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.5136 - mae: 0.3852 - mean_pred: 0.7983 - mae_t1: 0.0257 - val_loss: 0.6015 - val_mae: 0.4511 - val_mean_pred: 0.7898 - val_mae_t1: 0.0301\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 0.5148 - mae: 0.3861 - mean_pred: 0.7297 - mae_t1: 0.0257 - val_loss: 0.5818 - val_mae: 0.4364 - val_mean_pred: 0.8264 - val_mae_t1: 0.0291\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 0.5039 - mae: 0.3780 - mean_pred: 0.7844 - mae_t1: 0.0252 - val_loss: 0.5683 - val_mae: 0.4262 - val_mean_pred: 0.8930 - val_mae_t1: 0.0284\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5062 - mae: 0.3796 - mean_pred: 0.7953 - mae_t1: 0.0253 - val_loss: 0.5884 - val_mae: 0.4413 - val_mean_pred: 0.8691 - val_mae_t1: 0.0294\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.5034 - mae: 0.3776 - mean_pred: 0.8011 - mae_t1: 0.0252 - val_loss: 0.5879 - val_mae: 0.4409 - val_mean_pred: 0.8783 - val_mae_t1: 0.0294\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.5033 - mae: 0.3775 - mean_pred: 0.7728 - mae_t1: 0.0252 - val_loss: 0.6038 - val_mae: 0.4528 - val_mean_pred: 0.8208 - val_mae_t1: 0.0302\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4950 - mae: 0.3713 - mean_pred: 0.7443 - mae_t1: 0.0248 - val_loss: 0.6040 - val_mae: 0.4530 - val_mean_pred: 0.8416 - val_mae_t1: 0.0302\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4859 - mae: 0.3644 - mean_pred: 0.7775 - mae_t1: 0.0243 - val_loss: 0.6027 - val_mae: 0.4520 - val_mean_pred: 0.8544 - val_mae_t1: 0.0301\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 0.4879 - mae: 0.3659 - mean_pred: 0.7617 - mae_t1: 0.0244 - val_loss: 0.5893 - val_mae: 0.4419 - val_mean_pred: 0.8283 - val_mae_t1: 0.0295\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4971 - mae: 0.3728 - mean_pred: 0.7644 - mae_t1: 0.0249 - val_loss: 0.5904 - val_mae: 0.4428 - val_mean_pred: 0.8075 - val_mae_t1: 0.0295\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4818 - mae: 0.3613 - mean_pred: 0.7482 - mae_t1: 0.0241 - val_loss: 0.5818 - val_mae: 0.4364 - val_mean_pred: 0.8359 - val_mae_t1: 0.0291\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4760 - mae: 0.3570 - mean_pred: 0.7678 - mae_t1: 0.0238 - val_loss: 0.5785 - val_mae: 0.4338 - val_mean_pred: 0.8846 - val_mae_t1: 0.0289\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4731 - mae: 0.3548 - mean_pred: 0.8051 - mae_t1: 0.0237 - val_loss: 0.5999 - val_mae: 0.4499 - val_mean_pred: 0.8183 - val_mae_t1: 0.0300\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4783 - mae: 0.3587 - mean_pred: 0.7350 - mae_t1: 0.0239 - val_loss: 0.5986 - val_mae: 0.4490 - val_mean_pred: 0.7837 - val_mae_t1: 0.0299\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 0.4779 - mae: 0.3584 - mean_pred: 0.7496 - mae_t1: 0.0239 - val_loss: 0.5667 - val_mae: 0.4250 - val_mean_pred: 0.8763 - val_mae_t1: 0.0283\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.4696 - mae: 0.3522 - mean_pred: 0.8276 - mae_t1: 0.0235 - val_loss: 0.5754 - val_mae: 0.4315 - val_mean_pred: 0.8924 - val_mae_t1: 0.0288\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4671 - mae: 0.3503 - mean_pred: 0.8028 - mae_t1: 0.0234 - val_loss: 0.5919 - val_mae: 0.4439 - val_mean_pred: 0.8450 - val_mae_t1: 0.0296\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.4719 - mae: 0.3539 - mean_pred: 0.8064 - mae_t1: 0.0236 - val_loss: 0.5977 - val_mae: 0.4483 - val_mean_pred: 0.8410 - val_mae_t1: 0.0299\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4795 - mae: 0.3596 - mean_pred: 0.7243 - mae_t1: 0.0240 - val_loss: 0.5919 - val_mae: 0.4439 - val_mean_pred: 0.8466 - val_mae_t1: 0.0296\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.4682 - mae: 0.3512 - mean_pred: 0.8318 - mae_t1: 0.0234 - val_loss: 0.5818 - val_mae: 0.4364 - val_mean_pred: 0.9304 - val_mae_t1: 0.0291\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4531 - mae: 0.3398 - mean_pred: 0.7978 - mae_t1: 0.0227 - val_loss: 0.6125 - val_mae: 0.4594 - val_mean_pred: 0.7759 - val_mae_t1: 0.0306\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 0.4618 - mae: 0.3463 - mean_pred: 0.7140 - mae_t1: 0.0231 - val_loss: 0.5778 - val_mae: 0.4333 - val_mean_pred: 0.9082 - val_mae_t1: 0.0289\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.4836 - mae: 0.3627 - mean_pred: 0.8517 - mae_t1: 0.0242 - val_loss: 0.6383 - val_mae: 0.4787 - val_mean_pred: 1.0470 - val_mae_t1: 0.0319\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.5358 - mae: 0.4019 - mean_pred: 0.9414 - mae_t1: 0.0268 - val_loss: 0.6110 - val_mae: 0.4582 - val_mean_pred: 0.9121 - val_mae_t1: 0.0305\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4701 - mae: 0.3525 - mean_pred: 0.7469 - mae_t1: 0.0235 - val_loss: 0.6086 - val_mae: 0.4564 - val_mean_pred: 0.7554 - val_mae_t1: 0.0304\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 0.4670 - mae: 0.3503 - mean_pred: 0.7255 - mae_t1: 0.0234 - val_loss: 0.5664 - val_mae: 0.4248 - val_mean_pred: 0.8655 - val_mae_t1: 0.0283\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4440 - mae: 0.3330 - mean_pred: 0.7821 - mae_t1: 0.0222 - val_loss: 0.5915 - val_mae: 0.4436 - val_mean_pred: 0.8323 - val_mae_t1: 0.0296\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.4542 - mae: 0.3406 - mean_pred: 0.7592 - mae_t1: 0.0227 - val_loss: 0.5752 - val_mae: 0.4314 - val_mean_pred: 0.8482 - val_mae_t1: 0.0288\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 212us/sample - loss: 0.4388 - mae: 0.3291 - mean_pred: 0.7838 - mae_t1: 0.0219 - val_loss: 0.5574 - val_mae: 0.4181 - val_mean_pred: 0.8590 - val_mae_t1: 0.0279\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.4361 - mae: 0.3271 - mean_pred: 0.7629 - mae_t1: 0.0218 - val_loss: 0.5744 - val_mae: 0.4308 - val_mean_pred: 0.8722 - val_mae_t1: 0.0287\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 0.4460 - mae: 0.3345 - mean_pred: 0.8171 - mae_t1: 0.0223 - val_loss: 0.5761 - val_mae: 0.4321 - val_mean_pred: 0.8693 - val_mae_t1: 0.0288\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4675 - mae: 0.3507 - mean_pred: 0.6860 - mae_t1: 0.0234 - val_loss: 0.6120 - val_mae: 0.4590 - val_mean_pred: 0.7309 - val_mae_t1: 0.0306\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.5249 - mae: 0.3937 - mean_pred: 0.7129 - mae_t1: 0.0262 - val_loss: 0.5847 - val_mae: 0.4385 - val_mean_pred: 0.8421 - val_mae_t1: 0.0292\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4553 - mae: 0.3415 - mean_pred: 0.7191 - mae_t1: 0.0228 - val_loss: 0.6445 - val_mae: 0.4833 - val_mean_pred: 0.8190 - val_mae_t1: 0.0322\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4907 - mae: 0.3680 - mean_pred: 0.7432 - mae_t1: 0.0245 - val_loss: 0.5692 - val_mae: 0.4269 - val_mean_pred: 0.8632 - val_mae_t1: 0.0285\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 0.4448 - mae: 0.3336 - mean_pred: 0.7892 - mae_t1: 0.0222 - val_loss: 0.5570 - val_mae: 0.4178 - val_mean_pred: 0.8318 - val_mae_t1: 0.0279\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4332 - mae: 0.3249 - mean_pred: 0.7810 - mae_t1: 0.0217 - val_loss: 0.5771 - val_mae: 0.4328 - val_mean_pred: 0.9146 - val_mae_t1: 0.0289\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4586 - mae: 0.3439 - mean_pred: 0.8686 - mae_t1: 0.0229 - val_loss: 0.5814 - val_mae: 0.4361 - val_mean_pred: 0.9322 - val_mae_t1: 0.0291\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4415 - mae: 0.3311 - mean_pred: 0.7793 - mae_t1: 0.0221 - val_loss: 0.5782 - val_mae: 0.4337 - val_mean_pred: 0.8202 - val_mae_t1: 0.0289\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4316 - mae: 0.3237 - mean_pred: 0.7953 - mae_t1: 0.0216 - val_loss: 0.5730 - val_mae: 0.4298 - val_mean_pred: 0.9493 - val_mae_t1: 0.0287\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4252 - mae: 0.3189 - mean_pred: 0.8296 - mae_t1: 0.0213 - val_loss: 0.5847 - val_mae: 0.4385 - val_mean_pred: 0.8418 - val_mae_t1: 0.0292\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4321 - mae: 0.3241 - mean_pred: 0.7228 - mae_t1: 0.0216 - val_loss: 0.5859 - val_mae: 0.4394 - val_mean_pred: 0.8015 - val_mae_t1: 0.0293\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4368 - mae: 0.3276 - mean_pred: 0.7264 - mae_t1: 0.0218 - val_loss: 0.5704 - val_mae: 0.4278 - val_mean_pred: 0.8539 - val_mae_t1: 0.0285\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4198 - mae: 0.3149 - mean_pred: 0.7669 - mae_t1: 0.0210 - val_loss: 0.5651 - val_mae: 0.4239 - val_mean_pred: 0.9146 - val_mae_t1: 0.0283\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4322 - mae: 0.3241 - mean_pred: 0.8434 - mae_t1: 0.0216 - val_loss: 0.5779 - val_mae: 0.4334 - val_mean_pred: 0.9156 - val_mae_t1: 0.0289\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4248 - mae: 0.3186 - mean_pred: 0.8022 - mae_t1: 0.0212 - val_loss: 0.5949 - val_mae: 0.4461 - val_mean_pred: 0.9268 - val_mae_t1: 0.0297\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.4193 - mae: 0.3145 - mean_pred: 0.8372 - mae_t1: 0.0210 - val_loss: 0.5833 - val_mae: 0.4374 - val_mean_pred: 0.9425 - val_mae_t1: 0.0292\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4260 - mae: 0.3195 - mean_pred: 0.8056 - mae_t1: 0.0213 - val_loss: 0.6100 - val_mae: 0.4575 - val_mean_pred: 0.9372 - val_mae_t1: 0.0305\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4676 - mae: 0.3507 - mean_pred: 0.8732 - mae_t1: 0.0234 - val_loss: 0.6127 - val_mae: 0.4595 - val_mean_pred: 0.9790 - val_mae_t1: 0.0306\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.4238 - mae: 0.3178 - mean_pred: 0.8371 - mae_t1: 0.0212 - val_loss: 0.5626 - val_mae: 0.4219 - val_mean_pred: 0.8094 - val_mae_t1: 0.0281\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 0.4235 - mae: 0.3177 - mean_pred: 0.7179 - mae_t1: 0.0212 - val_loss: 0.5474 - val_mae: 0.4106 - val_mean_pred: 0.8544 - val_mae_t1: 0.0274\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4069 - mae: 0.3052 - mean_pred: 0.8385 - mae_t1: 0.0203 - val_loss: 0.6000 - val_mae: 0.4500 - val_mean_pred: 0.9810 - val_mae_t1: 0.0300\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4380 - mae: 0.3285 - mean_pred: 0.8493 - mae_t1: 0.0219 - val_loss: 0.5643 - val_mae: 0.4232 - val_mean_pred: 0.8438 - val_mae_t1: 0.0282\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 0.4112 - mae: 0.3084 - mean_pred: 0.7840 - mae_t1: 0.0206 - val_loss: 0.5398 - val_mae: 0.4048 - val_mean_pred: 0.8831 - val_mae_t1: 0.0270\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4070 - mae: 0.3053 - mean_pred: 0.8192 - mae_t1: 0.0204 - val_loss: 0.5751 - val_mae: 0.4313 - val_mean_pred: 0.9180 - val_mae_t1: 0.0288\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 0.4085 - mae: 0.3064 - mean_pred: 0.8280 - mae_t1: 0.0204 - val_loss: 0.5539 - val_mae: 0.4154 - val_mean_pred: 0.8669 - val_mae_t1: 0.0277\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 0.4103 - mae: 0.3077 - mean_pred: 0.7519 - mae_t1: 0.0205 - val_loss: 0.5807 - val_mae: 0.4355 - val_mean_pred: 0.8127 - val_mae_t1: 0.0290\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4040 - mae: 0.3030 - mean_pred: 0.7663 - mae_t1: 0.0202 - val_loss: 0.5742 - val_mae: 0.4307 - val_mean_pred: 0.9598 - val_mae_t1: 0.0287\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4191 - mae: 0.3144 - mean_pred: 0.8723 - mae_t1: 0.0210 - val_loss: 0.5468 - val_mae: 0.4101 - val_mean_pred: 0.8908 - val_mae_t1: 0.0273\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.3941 - mae: 0.2956 - mean_pred: 0.7677 - mae_t1: 0.0197 - val_loss: 0.5633 - val_mae: 0.4224 - val_mean_pred: 0.7715 - val_mae_t1: 0.0282\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 0.4108 - mae: 0.3081 - mean_pred: 0.7369 - mae_t1: 0.0205 - val_loss: 0.5539 - val_mae: 0.4154 - val_mean_pred: 0.8860 - val_mae_t1: 0.0277\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.4133 - mae: 0.3100 - mean_pred: 0.8470 - mae_t1: 0.0207 - val_loss: 0.5819 - val_mae: 0.4364 - val_mean_pred: 0.9611 - val_mae_t1: 0.0291\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.3997 - mae: 0.2998 - mean_pred: 0.8594 - mae_t1: 0.0200 - val_loss: 0.5736 - val_mae: 0.4302 - val_mean_pred: 0.8839 - val_mae_t1: 0.0287\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.3859 - mae: 0.2894 - mean_pred: 0.7955 - mae_t1: 0.0193 - val_loss: 0.5787 - val_mae: 0.4340 - val_mean_pred: 0.8671 - val_mae_t1: 0.0289\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.4022 - mae: 0.3017 - mean_pred: 0.8338 - mae_t1: 0.0201 - val_loss: 0.5672 - val_mae: 0.4254 - val_mean_pred: 0.8924 - val_mae_t1: 0.0284\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.3817 - mae: 0.2863 - mean_pred: 0.7853 - mae_t1: 0.0191 - val_loss: 0.5687 - val_mae: 0.4265 - val_mean_pred: 0.8105 - val_mae_t1: 0.0284\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 0.3964 - mae: 0.2973 - mean_pred: 0.7587 - mae_t1: 0.0198 - val_loss: 0.5476 - val_mae: 0.4107 - val_mean_pred: 0.8887 - val_mae_t1: 0.0274\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.3869 - mae: 0.2902 - mean_pred: 0.8259 - mae_t1: 0.0193 - val_loss: 0.5721 - val_mae: 0.4291 - val_mean_pred: 0.9550 - val_mae_t1: 0.0286\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 0.3902 - mae: 0.2927 - mean_pred: 0.8541 - mae_t1: 0.0195 - val_loss: 0.5471 - val_mae: 0.4103 - val_mean_pred: 0.8783 - val_mae_t1: 0.0274\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 0.3750 - mae: 0.2812 - mean_pred: 0.7788 - mae_t1: 0.0187 - val_loss: 0.5588 - val_mae: 0.4191 - val_mean_pred: 0.8370 - val_mae_t1: 0.0279\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 0.3915 - mae: 0.2936 - mean_pred: 0.7412 - mae_t1: 0.0196 - val_loss: 0.5839 - val_mae: 0.4379 - val_mean_pred: 0.9027 - val_mae_t1: 0.0292\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 0.3899 - mae: 0.2924 - mean_pred: 0.8155 - mae_t1: 0.0195 - val_loss: 0.6208 - val_mae: 0.4656 - val_mean_pred: 0.9608 - val_mae_t1: 0.0310\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 0.4014 - mae: 0.3010 - mean_pred: 0.8058 - mae_t1: 0.0201 - val_loss: 0.5816 - val_mae: 0.4362 - val_mean_pred: 0.9252 - val_mae_t1: 0.0291\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 0.4104 - mae: 0.3078 - mean_pred: 0.8397 - mae_t1: 0.0205 - val_loss: 0.5783 - val_mae: 0.4337 - val_mean_pred: 0.9501 - val_mae_t1: 0.0289\n",
      "Earliness...\n",
      "0.002000093460083008\n",
      "____________________________________________________________\n",
      "Test MAE:      0.37757067812348893  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁█▆▆▇▇▇▇▇█▇▇▇▇▇▇▇▇██▇▇▇▆▇██▇▇▇▆██▇▇█▇█▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▅▄▃▃▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▁▂▂▂▂▂▁▁▁▂▁▂▁▂▂▂</td></tr><tr><td>val_mae</td><td>█▅▄▃▃▅▄▃▃▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▁▂▂▂▂▂▁▁▁▂▁▂▁▂▂▂</td></tr><tr><td>val_mae_t1</td><td>█▅▄▃▃▅▄▃▃▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▁▂▂▂▂▂▁▁▁▂▁▂▁▂▂▂</td></tr><tr><td>val_mean_pred</td><td>▁█▆▅▆▅▅▆▅▆▇▆▆▆▆▆▆▆▇█▅▆▆▆▆▇▆▆▇▇▆▆▆▇▆▆▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.53194</td></tr><tr><td>AE_2</td><td>0.34211</td></tr><tr><td>AE_3</td><td>0.30474</td></tr><tr><td>MAE</td><td>0.37757</td></tr><tr><td>best_epoch</td><td>79</td></tr><tr><td>best_val_loss</td><td>0.53976</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.41042</td></tr><tr><td>mae</td><td>0.30782</td></tr><tr><td>mae_t1</td><td>0.02052</td></tr><tr><td>mean_pred</td><td>0.8397</td></tr><tr><td>val_loss</td><td>0.57831</td></tr><tr><td>val_mae</td><td>0.43374</td></tr><tr><td>val_mae_t1</td><td>0.02892</td></tr><tr><td>val_mean_pred</td><td>0.95015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wandering-deluge-91</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/19qjflc3\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/19qjflc3</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155735-19qjflc3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155806-22im8z60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/22im8z60\" target=\"_blank\">youthful-silence-92</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 712us/sample - loss: 1.1598 - mae: 0.8698 - mean_pred: 0.1603 - mae_t1: 0.0580 - val_loss: 1.0530 - val_mae: 0.7897 - val_mean_pred: 0.2192 - val_mae_t1: 0.0526\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 1.0494 - mae: 0.7871 - mean_pred: 0.2224 - mae_t1: 0.0525 - val_loss: 0.8575 - val_mae: 0.6431 - val_mean_pred: 0.3932 - val_mae_t1: 0.0429\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 155us/sample - loss: 0.9137 - mae: 0.6852 - mean_pred: 0.3696 - mae_t1: 0.0457 - val_loss: 0.7192 - val_mae: 0.5394 - val_mean_pred: 0.5978 - val_mae_t1: 0.0360\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 154us/sample - loss: 0.8266 - mae: 0.6200 - mean_pred: 0.5646 - mae_t1: 0.0413 - val_loss: 0.7146 - val_mae: 0.5359 - val_mean_pred: 0.8011 - val_mae_t1: 0.0357\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 135us/sample - loss: 0.8140 - mae: 0.6105 - mean_pred: 0.7472 - mae_t1: 0.0407 - val_loss: 0.7109 - val_mae: 0.5332 - val_mean_pred: 0.9377 - val_mae_t1: 0.0355\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 138us/sample - loss: 0.8110 - mae: 0.6083 - mean_pred: 0.8787 - mae_t1: 0.0406 - val_loss: 0.6848 - val_mae: 0.5136 - val_mean_pred: 1.0158 - val_mae_t1: 0.0342\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 0.8058 - mae: 0.6044 - mean_pred: 0.9527 - mae_t1: 0.0403 - val_loss: 0.6326 - val_mae: 0.4745 - val_mean_pred: 0.9916 - val_mae_t1: 0.0316\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 0.7887 - mae: 0.5916 - mean_pred: 0.9165 - mae_t1: 0.0394 - val_loss: 0.5935 - val_mae: 0.4451 - val_mean_pred: 0.8905 - val_mae_t1: 0.0297\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.7471 - mae: 0.5603 - mean_pred: 0.8128 - mae_t1: 0.0374 - val_loss: 0.6202 - val_mae: 0.4652 - val_mean_pred: 0.7599 - val_mae_t1: 0.0310\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.7304 - mae: 0.5478 - mean_pred: 0.6720 - mae_t1: 0.0365 - val_loss: 0.6503 - val_mae: 0.4877 - val_mean_pred: 0.6768 - val_mae_t1: 0.0325\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7116 - mae: 0.5337 - mean_pred: 0.6133 - mae_t1: 0.0356 - val_loss: 0.6516 - val_mae: 0.4887 - val_mean_pred: 0.6717 - val_mae_t1: 0.0326\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.7013 - mae: 0.5260 - mean_pred: 0.5952 - mae_t1: 0.0351 - val_loss: 0.6593 - val_mae: 0.4945 - val_mean_pred: 0.6660 - val_mae_t1: 0.0330\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.6893 - mae: 0.5170 - mean_pred: 0.5989 - mae_t1: 0.0345 - val_loss: 0.6802 - val_mae: 0.5102 - val_mean_pred: 0.7372 - val_mae_t1: 0.0340\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.6828 - mae: 0.5121 - mean_pred: 0.6674 - mae_t1: 0.0341 - val_loss: 0.7061 - val_mae: 0.5296 - val_mean_pred: 0.8424 - val_mae_t1: 0.0353\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.6850 - mae: 0.5138 - mean_pred: 0.7637 - mae_t1: 0.0343 - val_loss: 0.6906 - val_mae: 0.5179 - val_mean_pred: 0.9188 - val_mae_t1: 0.0345\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.6690 - mae: 0.5018 - mean_pred: 0.8238 - mae_t1: 0.0335 - val_loss: 0.6592 - val_mae: 0.4944 - val_mean_pred: 0.9068 - val_mae_t1: 0.0330\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 0.6445 - mae: 0.4834 - mean_pred: 0.8022 - mae_t1: 0.0322 - val_loss: 0.6506 - val_mae: 0.4879 - val_mean_pred: 0.8256 - val_mae_t1: 0.0325\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.6339 - mae: 0.4754 - mean_pred: 0.7298 - mae_t1: 0.0317 - val_loss: 0.6656 - val_mae: 0.4992 - val_mean_pred: 0.7682 - val_mae_t1: 0.0333\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.6341 - mae: 0.4756 - mean_pred: 0.6875 - mae_t1: 0.0317 - val_loss: 0.6673 - val_mae: 0.5004 - val_mean_pred: 0.7858 - val_mae_t1: 0.0334\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 0.6148 - mae: 0.4611 - mean_pred: 0.7174 - mae_t1: 0.0307 - val_loss: 0.6639 - val_mae: 0.4979 - val_mean_pred: 0.8214 - val_mae_t1: 0.0332\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.6028 - mae: 0.4521 - mean_pred: 0.7417 - mae_t1: 0.0301 - val_loss: 0.6693 - val_mae: 0.5020 - val_mean_pred: 0.8162 - val_mae_t1: 0.0335\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 0.6040 - mae: 0.4530 - mean_pred: 0.7508 - mae_t1: 0.0302 - val_loss: 0.6657 - val_mae: 0.4993 - val_mean_pred: 0.8454 - val_mae_t1: 0.0333\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 0.6048 - mae: 0.4536 - mean_pred: 0.7936 - mae_t1: 0.0302 - val_loss: 0.6386 - val_mae: 0.4789 - val_mean_pred: 0.8489 - val_mae_t1: 0.0319\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5908 - mae: 0.4431 - mean_pred: 0.7890 - mae_t1: 0.0295 - val_loss: 0.6425 - val_mae: 0.4819 - val_mean_pred: 0.7785 - val_mae_t1: 0.0321\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.6215 - mae: 0.4661 - mean_pred: 0.7267 - mae_t1: 0.0311 - val_loss: 0.6513 - val_mae: 0.4884 - val_mean_pred: 0.7394 - val_mae_t1: 0.0326\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 0.6141 - mae: 0.4605 - mean_pred: 0.7064 - mae_t1: 0.0307 - val_loss: 0.6148 - val_mae: 0.4611 - val_mean_pred: 0.8006 - val_mae_t1: 0.0307\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.5842 - mae: 0.4381 - mean_pred: 0.7705 - mae_t1: 0.0292 - val_loss: 0.6388 - val_mae: 0.4791 - val_mean_pred: 0.8860 - val_mae_t1: 0.0319\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.6138 - mae: 0.4603 - mean_pred: 0.8393 - mae_t1: 0.0307 - val_loss: 0.6288 - val_mae: 0.4716 - val_mean_pred: 0.8834 - val_mae_t1: 0.0314\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 0.5953 - mae: 0.4465 - mean_pred: 0.8137 - mae_t1: 0.0298 - val_loss: 0.6112 - val_mae: 0.4584 - val_mean_pred: 0.8013 - val_mae_t1: 0.0306\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.5813 - mae: 0.4360 - mean_pred: 0.7362 - mae_t1: 0.0291 - val_loss: 0.6288 - val_mae: 0.4716 - val_mean_pred: 0.7512 - val_mae_t1: 0.0314\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 0.5856 - mae: 0.4392 - mean_pred: 0.6945 - mae_t1: 0.0293 - val_loss: 0.6273 - val_mae: 0.4705 - val_mean_pred: 0.7486 - val_mae_t1: 0.0314\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.5749 - mae: 0.4311 - mean_pred: 0.7057 - mae_t1: 0.0287 - val_loss: 0.6000 - val_mae: 0.4500 - val_mean_pred: 0.8204 - val_mae_t1: 0.0300\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 130us/sample - loss: 0.5706 - mae: 0.4279 - mean_pred: 0.7871 - mae_t1: 0.0285 - val_loss: 0.5782 - val_mae: 0.4336 - val_mean_pred: 0.9001 - val_mae_t1: 0.0289\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 0.5724 - mae: 0.4293 - mean_pred: 0.8487 - mae_t1: 0.0286 - val_loss: 0.5672 - val_mae: 0.4254 - val_mean_pred: 0.9475 - val_mae_t1: 0.0284\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 0.5820 - mae: 0.4365 - mean_pred: 0.9053 - mae_t1: 0.0291 - val_loss: 0.5759 - val_mae: 0.4319 - val_mean_pred: 0.9569 - val_mae_t1: 0.0288\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.5658 - mae: 0.4243 - mean_pred: 0.8819 - mae_t1: 0.0283 - val_loss: 0.6226 - val_mae: 0.4669 - val_mean_pred: 0.8004 - val_mae_t1: 0.0311\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5642 - mae: 0.4232 - mean_pred: 0.7239 - mae_t1: 0.0282 - val_loss: 0.6869 - val_mae: 0.5152 - val_mean_pred: 0.6928 - val_mae_t1: 0.0343\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5812 - mae: 0.4359 - mean_pred: 0.6527 - mae_t1: 0.0291 - val_loss: 0.6626 - val_mae: 0.4969 - val_mean_pred: 0.7351 - val_mae_t1: 0.0331\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5510 - mae: 0.4133 - mean_pred: 0.7105 - mae_t1: 0.0276 - val_loss: 0.6162 - val_mae: 0.4621 - val_mean_pred: 0.8495 - val_mae_t1: 0.0308\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.5431 - mae: 0.4074 - mean_pred: 0.8300 - mae_t1: 0.0272 - val_loss: 0.6320 - val_mae: 0.4740 - val_mean_pred: 0.9855 - val_mae_t1: 0.0316\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.6005 - mae: 0.4504 - mean_pred: 0.9570 - mae_t1: 0.0300 - val_loss: 0.6512 - val_mae: 0.4884 - val_mean_pred: 1.0272 - val_mae_t1: 0.0326\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 0.6073 - mae: 0.4555 - mean_pred: 0.9658 - mae_t1: 0.0304 - val_loss: 0.6189 - val_mae: 0.4642 - val_mean_pred: 0.9355 - val_mae_t1: 0.0309\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5554 - mae: 0.4166 - mean_pred: 0.8661 - mae_t1: 0.0278 - val_loss: 0.6470 - val_mae: 0.4852 - val_mean_pred: 0.7760 - val_mae_t1: 0.0323\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.5561 - mae: 0.4171 - mean_pred: 0.7032 - mae_t1: 0.0278 - val_loss: 0.7133 - val_mae: 0.5350 - val_mean_pred: 0.6410 - val_mae_t1: 0.0357\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.5977 - mae: 0.4483 - mean_pred: 0.6078 - mae_t1: 0.0299 - val_loss: 0.7053 - val_mae: 0.5290 - val_mean_pred: 0.6819 - val_mae_t1: 0.0353\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.5655 - mae: 0.4241 - mean_pred: 0.6440 - mae_t1: 0.0283 - val_loss: 0.6684 - val_mae: 0.5013 - val_mean_pred: 0.7878 - val_mae_t1: 0.0334\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5205 - mae: 0.3904 - mean_pred: 0.7347 - mae_t1: 0.0260 - val_loss: 0.6506 - val_mae: 0.4880 - val_mean_pred: 0.9149 - val_mae_t1: 0.0325\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.5386 - mae: 0.4040 - mean_pred: 0.8418 - mae_t1: 0.0269 - val_loss: 0.6570 - val_mae: 0.4927 - val_mean_pred: 0.9663 - val_mae_t1: 0.0328\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.5525 - mae: 0.4143 - mean_pred: 0.8528 - mae_t1: 0.0276 - val_loss: 0.6626 - val_mae: 0.4970 - val_mean_pred: 0.8933 - val_mae_t1: 0.0331\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5384 - mae: 0.4038 - mean_pred: 0.7764 - mae_t1: 0.0269 - val_loss: 0.6600 - val_mae: 0.4950 - val_mean_pred: 0.8759 - val_mae_t1: 0.0330\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 0.5362 - mae: 0.4021 - mean_pred: 0.7746 - mae_t1: 0.0268 - val_loss: 0.6387 - val_mae: 0.4790 - val_mean_pred: 0.9513 - val_mae_t1: 0.0319\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5501 - mae: 0.4125 - mean_pred: 0.8484 - mae_t1: 0.0275 - val_loss: 0.6223 - val_mae: 0.4667 - val_mean_pred: 1.0040 - val_mae_t1: 0.0311\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.5501 - mae: 0.4126 - mean_pred: 0.8846 - mae_t1: 0.0275 - val_loss: 0.5961 - val_mae: 0.4471 - val_mean_pred: 0.9178 - val_mae_t1: 0.0298\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5185 - mae: 0.3889 - mean_pred: 0.7966 - mae_t1: 0.0259 - val_loss: 0.6213 - val_mae: 0.4660 - val_mean_pred: 0.7778 - val_mae_t1: 0.0311\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5342 - mae: 0.4006 - mean_pred: 0.6927 - mae_t1: 0.0267 - val_loss: 0.6321 - val_mae: 0.4741 - val_mean_pred: 0.7320 - val_mae_t1: 0.0316\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.5406 - mae: 0.4054 - mean_pred: 0.6839 - mae_t1: 0.0270 - val_loss: 0.5927 - val_mae: 0.4445 - val_mean_pred: 0.7916 - val_mae_t1: 0.0296\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 0.5193 - mae: 0.3895 - mean_pred: 0.7629 - mae_t1: 0.0260 - val_loss: 0.5505 - val_mae: 0.4129 - val_mean_pred: 0.9369 - val_mae_t1: 0.0275\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.5380 - mae: 0.4035 - mean_pred: 0.9063 - mae_t1: 0.0269 - val_loss: 0.5796 - val_mae: 0.4347 - val_mean_pred: 0.9995 - val_mae_t1: 0.0290\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.5622 - mae: 0.4217 - mean_pred: 0.9214 - mae_t1: 0.0281 - val_loss: 0.5645 - val_mae: 0.4234 - val_mean_pred: 0.9101 - val_mae_t1: 0.0282\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 0.5244 - mae: 0.3933 - mean_pred: 0.8467 - mae_t1: 0.0262 - val_loss: 0.5778 - val_mae: 0.4333 - val_mean_pred: 0.7974 - val_mae_t1: 0.0289\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.5081 - mae: 0.3811 - mean_pred: 0.7352 - mae_t1: 0.0254 - val_loss: 0.6633 - val_mae: 0.4975 - val_mean_pred: 0.6857 - val_mae_t1: 0.0332\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.5566 - mae: 0.4175 - mean_pred: 0.6495 - mae_t1: 0.0278 - val_loss: 0.6649 - val_mae: 0.4987 - val_mean_pred: 0.7072 - val_mae_t1: 0.0332\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.5392 - mae: 0.4044 - mean_pred: 0.6855 - mae_t1: 0.0270 - val_loss: 0.6038 - val_mae: 0.4529 - val_mean_pred: 0.8150 - val_mae_t1: 0.0302\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.4968 - mae: 0.3726 - mean_pred: 0.7678 - mae_t1: 0.0248 - val_loss: 0.6173 - val_mae: 0.4630 - val_mean_pred: 0.8589 - val_mae_t1: 0.0309\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.4992 - mae: 0.3744 - mean_pred: 0.7889 - mae_t1: 0.0250 - val_loss: 0.6282 - val_mae: 0.4711 - val_mean_pred: 0.8249 - val_mae_t1: 0.0314\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 0.5013 - mae: 0.3760 - mean_pred: 0.7465 - mae_t1: 0.0251 - val_loss: 0.6418 - val_mae: 0.4814 - val_mean_pred: 0.7696 - val_mae_t1: 0.0321\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 0.5096 - mae: 0.3822 - mean_pred: 0.6923 - mae_t1: 0.0255 - val_loss: 0.6295 - val_mae: 0.4721 - val_mean_pred: 0.7799 - val_mae_t1: 0.0315\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.4950 - mae: 0.3712 - mean_pred: 0.7225 - mae_t1: 0.0247 - val_loss: 0.5833 - val_mae: 0.4375 - val_mean_pred: 0.8743 - val_mae_t1: 0.0292\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 0.4866 - mae: 0.3650 - mean_pred: 0.8056 - mae_t1: 0.0243 - val_loss: 0.5603 - val_mae: 0.4202 - val_mean_pred: 0.9070 - val_mae_t1: 0.0280\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.5013 - mae: 0.3760 - mean_pred: 0.8319 - mae_t1: 0.0251 - val_loss: 0.5680 - val_mae: 0.4260 - val_mean_pred: 0.8393 - val_mae_t1: 0.0284\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 0.4930 - mae: 0.3698 - mean_pred: 0.7729 - mae_t1: 0.0247 - val_loss: 0.6053 - val_mae: 0.4540 - val_mean_pred: 0.7546 - val_mae_t1: 0.0303\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.5043 - mae: 0.3782 - mean_pred: 0.7037 - mae_t1: 0.0252 - val_loss: 0.6101 - val_mae: 0.4576 - val_mean_pred: 0.7622 - val_mae_t1: 0.0305\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.4943 - mae: 0.3707 - mean_pred: 0.7250 - mae_t1: 0.0247 - val_loss: 0.6025 - val_mae: 0.4519 - val_mean_pred: 0.8294 - val_mae_t1: 0.0301\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 0.4807 - mae: 0.3605 - mean_pred: 0.7682 - mae_t1: 0.0240 - val_loss: 0.6076 - val_mae: 0.4557 - val_mean_pred: 0.8181 - val_mae_t1: 0.0304\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.4837 - mae: 0.3628 - mean_pred: 0.7445 - mae_t1: 0.0242 - val_loss: 0.6031 - val_mae: 0.4523 - val_mean_pred: 0.8435 - val_mae_t1: 0.0302\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.4758 - mae: 0.3568 - mean_pred: 0.7966 - mae_t1: 0.0238 - val_loss: 0.5899 - val_mae: 0.4425 - val_mean_pred: 0.9248 - val_mae_t1: 0.0295\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.4884 - mae: 0.3663 - mean_pred: 0.8574 - mae_t1: 0.0244 - val_loss: 0.5944 - val_mae: 0.4458 - val_mean_pred: 0.8943 - val_mae_t1: 0.0297\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.4751 - mae: 0.3563 - mean_pred: 0.8206 - mae_t1: 0.0238 - val_loss: 0.6045 - val_mae: 0.4534 - val_mean_pred: 0.8310 - val_mae_t1: 0.0302\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.4658 - mae: 0.3493 - mean_pred: 0.7683 - mae_t1: 0.0233 - val_loss: 0.6188 - val_mae: 0.4641 - val_mean_pred: 0.7964 - val_mae_t1: 0.0309\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4718 - mae: 0.3539 - mean_pred: 0.7249 - mae_t1: 0.0236 - val_loss: 0.6373 - val_mae: 0.4780 - val_mean_pred: 0.7666 - val_mae_t1: 0.0319\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 0.4842 - mae: 0.3631 - mean_pred: 0.6982 - mae_t1: 0.0242 - val_loss: 0.6286 - val_mae: 0.4715 - val_mean_pred: 0.7986 - val_mae_t1: 0.0314\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4726 - mae: 0.3544 - mean_pred: 0.7405 - mae_t1: 0.0236 - val_loss: 0.5886 - val_mae: 0.4415 - val_mean_pred: 0.8967 - val_mae_t1: 0.0294\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.4630 - mae: 0.3472 - mean_pred: 0.8284 - mae_t1: 0.0231 - val_loss: 0.5680 - val_mae: 0.4260 - val_mean_pred: 0.9361 - val_mae_t1: 0.0284\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.4692 - mae: 0.3519 - mean_pred: 0.8497 - mae_t1: 0.0235 - val_loss: 0.5695 - val_mae: 0.4272 - val_mean_pred: 0.8785 - val_mae_t1: 0.0285\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.4550 - mae: 0.3412 - mean_pred: 0.7990 - mae_t1: 0.0227 - val_loss: 0.5738 - val_mae: 0.4304 - val_mean_pred: 0.8514 - val_mae_t1: 0.0287\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.4502 - mae: 0.3377 - mean_pred: 0.7866 - mae_t1: 0.0225 - val_loss: 0.5736 - val_mae: 0.4302 - val_mean_pred: 0.8753 - val_mae_t1: 0.0287\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 0.4658 - mae: 0.3493 - mean_pred: 0.8062 - mae_t1: 0.0233 - val_loss: 0.5703 - val_mae: 0.4277 - val_mean_pred: 0.8717 - val_mae_t1: 0.0285\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4562 - mae: 0.3421 - mean_pred: 0.7928 - mae_t1: 0.0228 - val_loss: 0.5662 - val_mae: 0.4247 - val_mean_pred: 0.8335 - val_mae_t1: 0.0283\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4675 - mae: 0.3507 - mean_pred: 0.7486 - mae_t1: 0.0234 - val_loss: 0.5829 - val_mae: 0.4372 - val_mean_pred: 0.8434 - val_mae_t1: 0.0291\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 0.4760 - mae: 0.3570 - mean_pred: 0.7919 - mae_t1: 0.0238 - val_loss: 0.5660 - val_mae: 0.4245 - val_mean_pred: 0.9444 - val_mae_t1: 0.0283\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 0.4642 - mae: 0.3481 - mean_pred: 0.8752 - mae_t1: 0.0232 - val_loss: 0.5546 - val_mae: 0.4159 - val_mean_pred: 0.9289 - val_mae_t1: 0.0277\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4562 - mae: 0.3421 - mean_pred: 0.8498 - mae_t1: 0.0228 - val_loss: 0.5520 - val_mae: 0.4140 - val_mean_pred: 0.8693 - val_mae_t1: 0.0276\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 94us/sample - loss: 0.4520 - mae: 0.3390 - mean_pred: 0.8067 - mae_t1: 0.0226 - val_loss: 0.5514 - val_mae: 0.4136 - val_mean_pred: 0.8810 - val_mae_t1: 0.0276\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 0.4509 - mae: 0.3382 - mean_pred: 0.8338 - mae_t1: 0.0225 - val_loss: 0.5639 - val_mae: 0.4229 - val_mean_pred: 0.9526 - val_mae_t1: 0.0282\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 0.4756 - mae: 0.3567 - mean_pred: 0.8883 - mae_t1: 0.0238 - val_loss: 0.5788 - val_mae: 0.4341 - val_mean_pred: 0.9596 - val_mae_t1: 0.0289\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 0.4642 - mae: 0.3481 - mean_pred: 0.8568 - mae_t1: 0.0232 - val_loss: 0.5984 - val_mae: 0.4488 - val_mean_pred: 0.9027 - val_mae_t1: 0.0299\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4460 - mae: 0.3345 - mean_pred: 0.8031 - mae_t1: 0.0223 - val_loss: 0.6205 - val_mae: 0.4654 - val_mean_pred: 0.8788 - val_mae_t1: 0.0310\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 0.4684 - mae: 0.3513 - mean_pred: 0.7692 - mae_t1: 0.0234 - val_loss: 0.6351 - val_mae: 0.4763 - val_mean_pred: 0.8404 - val_mae_t1: 0.0318\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 0.4589 - mae: 0.3442 - mean_pred: 0.7280 - mae_t1: 0.0229 - val_loss: 0.6493 - val_mae: 0.4870 - val_mean_pred: 0.8260 - val_mae_t1: 0.0325\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 0.4679 - mae: 0.3509 - mean_pred: 0.7161 - mae_t1: 0.0234 - val_loss: 0.6306 - val_mae: 0.4729 - val_mean_pred: 0.8602 - val_mae_t1: 0.0315\n",
      "Earliness...\n",
      "0.001987934112548828\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3483585513079608  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▃▇█▅▅▇▆▆▇▆▇▆▇▇▆█▆▅▇▆▇▆█▆▆▆▇▆▆▇▆▆▇▇▆▇▇▇▆</td></tr><tr><td>val_loss</td><td>█▃▃▂▂▃▂▃▃▂▂▂▂▁▂▂▂▃▃▃▂▂▂▁▃▂▂▁▂▂▂▂▂▁▁▁▁▁▂▂</td></tr><tr><td>val_mae</td><td>█▃▃▂▂▃▂▃▃▂▂▂▂▁▂▂▂▃▃▃▂▂▂▁▃▂▂▁▂▂▂▂▂▁▁▁▁▁▂▂</td></tr><tr><td>val_mae_t1</td><td>█▃▃▂▂▃▂▃▃▂▂▂▂▁▂▂▂▃▃▃▂▂▂▁▃▂▂▁▂▂▂▂▂▁▁▁▁▁▂▂</td></tr><tr><td>val_mean_pred</td><td>▁▄█▇▅▅▇▆▆▆▆▇▆▇▆▆█▅▆▇▇▆▆▇▅▇▆▇▆▆▇▆▇▇▇▆▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.47323</td></tr><tr><td>AE_2</td><td>0.32389</td></tr><tr><td>AE_3</td><td>0.2904</td></tr><tr><td>MAE</td><td>0.34836</td></tr><tr><td>best_epoch</td><td>56</td></tr><tr><td>best_val_loss</td><td>0.55052</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.46789</td></tr><tr><td>mae</td><td>0.35092</td></tr><tr><td>mae_t1</td><td>0.02339</td></tr><tr><td>mean_pred</td><td>0.71609</td></tr><tr><td>val_loss</td><td>0.6306</td></tr><tr><td>val_mae</td><td>0.47295</td></tr><tr><td>val_mae_t1</td><td>0.03153</td></tr><tr><td>val_mean_pred</td><td>0.86023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-silence-92</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/22im8z60\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/22im8z60</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155806-22im8z60\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155832-lif5r8ns</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/lif5r8ns\" target=\"_blank\">clear-oath-93</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 862us/sample - loss: 2.3206 - mae: 0.7735 - mean_pred: 0.2643 - mae_t1: 0.0516 - val_loss: 1.4753 - val_mae: 0.4918 - val_mean_pred: 0.7223 - val_mae_t1: 0.0328\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 298us/sample - loss: 1.7740 - mae: 0.5913 - mean_pred: 0.8084 - mae_t1: 0.0394 - val_loss: 1.4669 - val_mae: 0.4890 - val_mean_pred: 0.8307 - val_mae_t1: 0.0326\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 244us/sample - loss: 1.6591 - mae: 0.5530 - mean_pred: 0.6171 - mae_t1: 0.0369 - val_loss: 1.6679 - val_mae: 0.5560 - val_mean_pred: 0.6187 - val_mae_t1: 0.0371\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 1.6091 - mae: 0.5364 - mean_pred: 0.6372 - mae_t1: 0.0358 - val_loss: 1.6238 - val_mae: 0.5413 - val_mean_pred: 1.0265 - val_mae_t1: 0.0361\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 1.6559 - mae: 0.5520 - mean_pred: 0.9895 - mae_t1: 0.0368 - val_loss: 1.5579 - val_mae: 0.5193 - val_mean_pred: 0.9550 - val_mae_t1: 0.0346\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.5204 - mae: 0.5068 - mean_pred: 0.7469 - mae_t1: 0.0338 - val_loss: 1.5961 - val_mae: 0.5320 - val_mean_pred: 0.7078 - val_mae_t1: 0.0355\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.4268 - mae: 0.4756 - mean_pred: 0.6862 - mae_t1: 0.0317 - val_loss: 1.5180 - val_mae: 0.5060 - val_mean_pred: 0.8266 - val_mae_t1: 0.0337\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 228us/sample - loss: 1.3663 - mae: 0.4554 - mean_pred: 0.7210 - mae_t1: 0.0304 - val_loss: 1.4509 - val_mae: 0.4836 - val_mean_pred: 0.8160 - val_mae_t1: 0.0322\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 218us/sample - loss: 1.3130 - mae: 0.4377 - mean_pred: 0.7813 - mae_t1: 0.0292 - val_loss: 1.4487 - val_mae: 0.4829 - val_mean_pred: 0.9348 - val_mae_t1: 0.0322\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 1.2973 - mae: 0.4324 - mean_pred: 0.8101 - mae_t1: 0.0288 - val_loss: 1.4466 - val_mae: 0.4822 - val_mean_pred: 0.8431 - val_mae_t1: 0.0321\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 219us/sample - loss: 1.2576 - mae: 0.4192 - mean_pred: 0.7836 - mae_t1: 0.0279 - val_loss: 1.4247 - val_mae: 0.4749 - val_mean_pred: 0.8014 - val_mae_t1: 0.0317\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2897 - mae: 0.4299 - mean_pred: 0.7030 - mae_t1: 0.0287 - val_loss: 1.4837 - val_mae: 0.4946 - val_mean_pred: 0.7315 - val_mae_t1: 0.0330\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 220us/sample - loss: 1.2500 - mae: 0.4167 - mean_pred: 0.7425 - mae_t1: 0.0278 - val_loss: 1.3604 - val_mae: 0.4535 - val_mean_pred: 0.9372 - val_mae_t1: 0.0302\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 219us/sample - loss: 1.2791 - mae: 0.4264 - mean_pred: 0.8678 - mae_t1: 0.0284 - val_loss: 1.3382 - val_mae: 0.4461 - val_mean_pred: 0.8626 - val_mae_t1: 0.0297\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.2116 - mae: 0.4039 - mean_pred: 0.7565 - mae_t1: 0.0269 - val_loss: 1.3901 - val_mae: 0.4634 - val_mean_pred: 0.7697 - val_mae_t1: 0.0309\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.1843 - mae: 0.3948 - mean_pred: 0.7231 - mae_t1: 0.0263 - val_loss: 1.3527 - val_mae: 0.4509 - val_mean_pred: 0.9107 - val_mae_t1: 0.0301\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.2305 - mae: 0.4102 - mean_pred: 0.8953 - mae_t1: 0.0273 - val_loss: 1.3563 - val_mae: 0.4521 - val_mean_pred: 0.8931 - val_mae_t1: 0.0301\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.2033 - mae: 0.4011 - mean_pred: 0.7298 - mae_t1: 0.0267 - val_loss: 1.5377 - val_mae: 0.5126 - val_mean_pred: 0.6733 - val_mae_t1: 0.0342\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.3373 - mae: 0.4458 - mean_pred: 0.6357 - mae_t1: 0.0297 - val_loss: 1.3432 - val_mae: 0.4477 - val_mean_pred: 0.8309 - val_mae_t1: 0.0298\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 231us/sample - loss: 1.2047 - mae: 0.4016 - mean_pred: 0.8455 - mae_t1: 0.0268 - val_loss: 1.3267 - val_mae: 0.4422 - val_mean_pred: 0.9200 - val_mae_t1: 0.0295\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.1584 - mae: 0.3861 - mean_pred: 0.7486 - mae_t1: 0.0257 - val_loss: 1.3556 - val_mae: 0.4519 - val_mean_pred: 0.7879 - val_mae_t1: 0.0301\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.1345 - mae: 0.3782 - mean_pred: 0.7866 - mae_t1: 0.0252 - val_loss: 1.3663 - val_mae: 0.4554 - val_mean_pred: 0.9101 - val_mae_t1: 0.0304\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.1485 - mae: 0.3828 - mean_pred: 0.7697 - mae_t1: 0.0255 - val_loss: 1.4211 - val_mae: 0.4737 - val_mean_pred: 0.7152 - val_mae_t1: 0.0316\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.1899 - mae: 0.3966 - mean_pred: 0.6419 - mae_t1: 0.0264 - val_loss: 1.4374 - val_mae: 0.4791 - val_mean_pred: 0.7195 - val_mae_t1: 0.0319\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.2271 - mae: 0.4090 - mean_pred: 0.6466 - mae_t1: 0.0273 - val_loss: 1.4029 - val_mae: 0.4676 - val_mean_pred: 0.8370 - val_mae_t1: 0.0312\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.1276 - mae: 0.3759 - mean_pred: 0.7872 - mae_t1: 0.0251 - val_loss: 1.4735 - val_mae: 0.4912 - val_mean_pred: 1.0014 - val_mae_t1: 0.0327\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.1231 - mae: 0.3744 - mean_pred: 0.8359 - mae_t1: 0.0250 - val_loss: 1.4035 - val_mae: 0.4678 - val_mean_pred: 0.8737 - val_mae_t1: 0.0312\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.0849 - mae: 0.3616 - mean_pred: 0.7097 - mae_t1: 0.0241 - val_loss: 1.4187 - val_mae: 0.4729 - val_mean_pred: 0.8955 - val_mae_t1: 0.0315\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.0880 - mae: 0.3627 - mean_pred: 0.8107 - mae_t1: 0.0242 - val_loss: 1.4594 - val_mae: 0.4865 - val_mean_pred: 1.0252 - val_mae_t1: 0.0324\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.1343 - mae: 0.3781 - mean_pred: 0.8984 - mae_t1: 0.0252 - val_loss: 1.3334 - val_mae: 0.4445 - val_mean_pred: 0.8385 - val_mae_t1: 0.0296\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.0780 - mae: 0.3593 - mean_pred: 0.7254 - mae_t1: 0.0240 - val_loss: 1.4094 - val_mae: 0.4698 - val_mean_pred: 0.7708 - val_mae_t1: 0.0313\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.0819 - mae: 0.3606 - mean_pred: 0.7233 - mae_t1: 0.0240 - val_loss: 1.3757 - val_mae: 0.4586 - val_mean_pred: 0.9722 - val_mae_t1: 0.0306\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.1264 - mae: 0.3755 - mean_pred: 0.9191 - mae_t1: 0.0250 - val_loss: 1.3463 - val_mae: 0.4488 - val_mean_pred: 0.9391 - val_mae_t1: 0.0299\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.0465 - mae: 0.3488 - mean_pred: 0.7794 - mae_t1: 0.0233 - val_loss: 1.3788 - val_mae: 0.4596 - val_mean_pred: 0.7460 - val_mae_t1: 0.0306\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 222us/sample - loss: 1.2792 - mae: 0.4264 - mean_pred: 0.7086 - mae_t1: 0.0284 - val_loss: 1.2819 - val_mae: 0.4273 - val_mean_pred: 0.7557 - val_mae_t1: 0.0285\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.1124 - mae: 0.3708 - mean_pred: 0.7172 - mae_t1: 0.0247 - val_loss: 1.2910 - val_mae: 0.4303 - val_mean_pred: 0.8707 - val_mae_t1: 0.0287\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 221us/sample - loss: 1.0614 - mae: 0.3538 - mean_pred: 0.7634 - mae_t1: 0.0236 - val_loss: 1.2537 - val_mae: 0.4179 - val_mean_pred: 0.8662 - val_mae_t1: 0.0279\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 1.0296 - mae: 0.3432 - mean_pred: 0.8241 - mae_t1: 0.0229 - val_loss: 1.2949 - val_mae: 0.4316 - val_mean_pred: 0.9178 - val_mae_t1: 0.0288\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.0068 - mae: 0.3356 - mean_pred: 0.8461 - mae_t1: 0.0224 - val_loss: 1.2646 - val_mae: 0.4215 - val_mean_pred: 0.9245 - val_mae_t1: 0.0281\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 0.9580 - mae: 0.3193 - mean_pred: 0.8071 - mae_t1: 0.0213 - val_loss: 1.2677 - val_mae: 0.4226 - val_mean_pred: 0.8558 - val_mae_t1: 0.0282\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.0045 - mae: 0.3348 - mean_pred: 0.7285 - mae_t1: 0.0223 - val_loss: 1.2726 - val_mae: 0.4242 - val_mean_pred: 0.8659 - val_mae_t1: 0.0283\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 0.9947 - mae: 0.3316 - mean_pred: 0.7947 - mae_t1: 0.0221 - val_loss: 1.3499 - val_mae: 0.4500 - val_mean_pred: 0.9897 - val_mae_t1: 0.0300\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.0370 - mae: 0.3457 - mean_pred: 0.8407 - mae_t1: 0.0230 - val_loss: 1.2651 - val_mae: 0.4217 - val_mean_pred: 0.8310 - val_mae_t1: 0.0281\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.0629 - mae: 0.3543 - mean_pred: 0.7210 - mae_t1: 0.0236 - val_loss: 1.2941 - val_mae: 0.4314 - val_mean_pred: 0.8848 - val_mae_t1: 0.0288\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9633 - mae: 0.3211 - mean_pred: 0.8438 - mae_t1: 0.0214 - val_loss: 1.4734 - val_mae: 0.4911 - val_mean_pred: 1.0333 - val_mae_t1: 0.0327\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.0819 - mae: 0.3606 - mean_pred: 0.8980 - mae_t1: 0.0240 - val_loss: 1.3875 - val_mae: 0.4625 - val_mean_pred: 0.7729 - val_mae_t1: 0.0308\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.0476 - mae: 0.3492 - mean_pred: 0.6937 - mae_t1: 0.0233 - val_loss: 1.3874 - val_mae: 0.4625 - val_mean_pred: 0.8439 - val_mae_t1: 0.0308\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 1.0433 - mae: 0.3478 - mean_pred: 0.8541 - mae_t1: 0.0232 - val_loss: 1.3578 - val_mae: 0.4526 - val_mean_pred: 0.9668 - val_mae_t1: 0.0302\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.0286 - mae: 0.3429 - mean_pred: 0.7735 - mae_t1: 0.0229 - val_loss: 1.3067 - val_mae: 0.4356 - val_mean_pred: 0.8402 - val_mae_t1: 0.0290\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 0.9975 - mae: 0.3325 - mean_pred: 0.7950 - mae_t1: 0.0222 - val_loss: 1.3641 - val_mae: 0.4547 - val_mean_pred: 0.9871 - val_mae_t1: 0.0303\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.0355 - mae: 0.3452 - mean_pred: 0.8461 - mae_t1: 0.0230 - val_loss: 1.3533 - val_mae: 0.4511 - val_mean_pred: 0.8370 - val_mae_t1: 0.0301\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 1.0067 - mae: 0.3356 - mean_pred: 0.7379 - mae_t1: 0.0224 - val_loss: 1.2419 - val_mae: 0.4140 - val_mean_pred: 0.8649 - val_mae_t1: 0.0276\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9163 - mae: 0.3054 - mean_pred: 0.7894 - mae_t1: 0.0204 - val_loss: 1.3028 - val_mae: 0.4343 - val_mean_pred: 0.9674 - val_mae_t1: 0.0290\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 0.9899 - mae: 0.3300 - mean_pred: 0.8794 - mae_t1: 0.0220 - val_loss: 1.2529 - val_mae: 0.4176 - val_mean_pred: 0.8975 - val_mae_t1: 0.0278\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9815 - mae: 0.3272 - mean_pred: 0.7587 - mae_t1: 0.0218 - val_loss: 1.3526 - val_mae: 0.4509 - val_mean_pred: 0.7824 - val_mae_t1: 0.0301\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.9596 - mae: 0.3199 - mean_pred: 0.7640 - mae_t1: 0.0213 - val_loss: 1.3136 - val_mae: 0.4379 - val_mean_pred: 0.9415 - val_mae_t1: 0.0292\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.9194 - mae: 0.3065 - mean_pred: 0.7940 - mae_t1: 0.0204 - val_loss: 1.3431 - val_mae: 0.4477 - val_mean_pred: 0.8758 - val_mae_t1: 0.0298\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 1.0117 - mae: 0.3372 - mean_pred: 0.7548 - mae_t1: 0.0225 - val_loss: 1.2796 - val_mae: 0.4265 - val_mean_pred: 0.8817 - val_mae_t1: 0.0284\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.0256 - mae: 0.3419 - mean_pred: 0.7922 - mae_t1: 0.0228 - val_loss: 1.3498 - val_mae: 0.4499 - val_mean_pred: 0.9420 - val_mae_t1: 0.0300\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 0.9437 - mae: 0.3146 - mean_pred: 0.8010 - mae_t1: 0.0210 - val_loss: 1.2528 - val_mae: 0.4176 - val_mean_pred: 0.8641 - val_mae_t1: 0.0278\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 0.9242 - mae: 0.3081 - mean_pred: 0.7388 - mae_t1: 0.0205 - val_loss: 1.3267 - val_mae: 0.4422 - val_mean_pred: 0.8961 - val_mae_t1: 0.0295\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.0269 - mae: 0.3423 - mean_pred: 0.8272 - mae_t1: 0.0228 - val_loss: 1.3601 - val_mae: 0.4534 - val_mean_pred: 0.9939 - val_mae_t1: 0.0302\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 0.9258 - mae: 0.3086 - mean_pred: 0.8178 - mae_t1: 0.0206 - val_loss: 1.3194 - val_mae: 0.4398 - val_mean_pred: 0.8053 - val_mae_t1: 0.0293\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 0.9346 - mae: 0.3115 - mean_pred: 0.7328 - mae_t1: 0.0208 - val_loss: 1.2542 - val_mae: 0.4181 - val_mean_pred: 0.8862 - val_mae_t1: 0.0279\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 0.9356 - mae: 0.3119 - mean_pred: 0.8402 - mae_t1: 0.0208 - val_loss: 1.2910 - val_mae: 0.4303 - val_mean_pred: 0.9661 - val_mae_t1: 0.0287\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 0.9367 - mae: 0.3122 - mean_pred: 0.8112 - mae_t1: 0.0208 - val_loss: 1.3156 - val_mae: 0.4385 - val_mean_pred: 0.8118 - val_mae_t1: 0.0292\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.0188 - mae: 0.3396 - mean_pred: 0.6825 - mae_t1: 0.0226 - val_loss: 1.2986 - val_mae: 0.4329 - val_mean_pred: 0.8122 - val_mae_t1: 0.0289\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 0.9307 - mae: 0.3102 - mean_pred: 0.7708 - mae_t1: 0.0207 - val_loss: 1.3511 - val_mae: 0.4504 - val_mean_pred: 0.9967 - val_mae_t1: 0.0300\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.9266 - mae: 0.3089 - mean_pred: 0.8514 - mae_t1: 0.0206 - val_loss: 1.2942 - val_mae: 0.4314 - val_mean_pred: 0.9525 - val_mae_t1: 0.0288\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 0.8669 - mae: 0.2890 - mean_pred: 0.8104 - mae_t1: 0.0193 - val_loss: 1.2702 - val_mae: 0.4234 - val_mean_pred: 0.9295 - val_mae_t1: 0.0282\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 0.9040 - mae: 0.3013 - mean_pred: 0.8410 - mae_t1: 0.0201 - val_loss: 1.2756 - val_mae: 0.4252 - val_mean_pred: 0.9260 - val_mae_t1: 0.0283\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 0.9197 - mae: 0.3066 - mean_pred: 0.7570 - mae_t1: 0.0204 - val_loss: 1.3447 - val_mae: 0.4482 - val_mean_pred: 0.7371 - val_mae_t1: 0.0299\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 0.9591 - mae: 0.3197 - mean_pred: 0.6667 - mae_t1: 0.0213 - val_loss: 1.2543 - val_mae: 0.4181 - val_mean_pred: 0.8434 - val_mae_t1: 0.0279\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 0.8533 - mae: 0.2844 - mean_pred: 0.7519 - mae_t1: 0.0190 - val_loss: 1.2565 - val_mae: 0.4188 - val_mean_pred: 0.8873 - val_mae_t1: 0.0279\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.8456 - mae: 0.2819 - mean_pred: 0.7831 - mae_t1: 0.0188 - val_loss: 1.3005 - val_mae: 0.4335 - val_mean_pred: 0.9623 - val_mae_t1: 0.0289\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.8852 - mae: 0.2951 - mean_pred: 0.8707 - mae_t1: 0.0197 - val_loss: 1.2611 - val_mae: 0.4204 - val_mean_pred: 0.8963 - val_mae_t1: 0.0280\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 0.8933 - mae: 0.2978 - mean_pred: 0.7473 - mae_t1: 0.0199 - val_loss: 1.2489 - val_mae: 0.4163 - val_mean_pred: 0.7823 - val_mae_t1: 0.0278\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.8871 - mae: 0.2957 - mean_pred: 0.7403 - mae_t1: 0.0197 - val_loss: 1.2615 - val_mae: 0.4205 - val_mean_pred: 0.9521 - val_mae_t1: 0.0280\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 0.8881 - mae: 0.2960 - mean_pred: 0.8644 - mae_t1: 0.0197 - val_loss: 1.2411 - val_mae: 0.4137 - val_mean_pred: 0.9030 - val_mae_t1: 0.0276\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 0.8470 - mae: 0.2823 - mean_pred: 0.7747 - mae_t1: 0.0188 - val_loss: 1.2313 - val_mae: 0.4104 - val_mean_pred: 0.8583 - val_mae_t1: 0.0274\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 0.8249 - mae: 0.2750 - mean_pred: 0.8036 - mae_t1: 0.0183 - val_loss: 1.2700 - val_mae: 0.4233 - val_mean_pred: 0.9001 - val_mae_t1: 0.0282\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 0.8260 - mae: 0.2753 - mean_pred: 0.7404 - mae_t1: 0.0184 - val_loss: 1.2797 - val_mae: 0.4266 - val_mean_pred: 0.8049 - val_mae_t1: 0.0284\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 0.8472 - mae: 0.2824 - mean_pred: 0.7404 - mae_t1: 0.0188 - val_loss: 1.3152 - val_mae: 0.4384 - val_mean_pred: 0.9553 - val_mae_t1: 0.0292\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 0.8047 - mae: 0.2682 - mean_pred: 0.8182 - mae_t1: 0.0179 - val_loss: 1.2735 - val_mae: 0.4245 - val_mean_pred: 0.8756 - val_mae_t1: 0.0283\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 0.8556 - mae: 0.2852 - mean_pred: 0.7281 - mae_t1: 0.0190 - val_loss: 1.2667 - val_mae: 0.4222 - val_mean_pred: 0.8678 - val_mae_t1: 0.0281\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 0.8063 - mae: 0.2688 - mean_pred: 0.7979 - mae_t1: 0.0179 - val_loss: 1.3551 - val_mae: 0.4517 - val_mean_pred: 0.9424 - val_mae_t1: 0.0301\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 0.8041 - mae: 0.2680 - mean_pred: 0.8031 - mae_t1: 0.0179 - val_loss: 1.3230 - val_mae: 0.4410 - val_mean_pred: 0.8907 - val_mae_t1: 0.0294\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 0.8209 - mae: 0.2736 - mean_pred: 0.7903 - mae_t1: 0.0182 - val_loss: 1.3498 - val_mae: 0.4499 - val_mean_pred: 0.9693 - val_mae_t1: 0.0300\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 0.8669 - mae: 0.2890 - mean_pred: 0.8681 - mae_t1: 0.0193 - val_loss: 1.3118 - val_mae: 0.4373 - val_mean_pred: 0.9628 - val_mae_t1: 0.0292\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 228us/sample - loss: 0.8731 - mae: 0.2910 - mean_pred: 0.8245 - mae_t1: 0.0194 - val_loss: 1.2215 - val_mae: 0.4072 - val_mean_pred: 0.8057 - val_mae_t1: 0.0271\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.8589 - mae: 0.2863 - mean_pred: 0.7280 - mae_t1: 0.0191 - val_loss: 1.3329 - val_mae: 0.4443 - val_mean_pred: 0.9592 - val_mae_t1: 0.0296\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 0.8319 - mae: 0.2773 - mean_pred: 0.8641 - mae_t1: 0.0185 - val_loss: 1.2965 - val_mae: 0.4322 - val_mean_pred: 0.9163 - val_mae_t1: 0.0288\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 0.8598 - mae: 0.2866 - mean_pred: 0.7359 - mae_t1: 0.0191 - val_loss: 1.3366 - val_mae: 0.4455 - val_mean_pred: 0.8074 - val_mae_t1: 0.0297\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 0.8839 - mae: 0.2946 - mean_pred: 0.7603 - mae_t1: 0.0196 - val_loss: 1.4444 - val_mae: 0.4815 - val_mean_pred: 1.0557 - val_mae_t1: 0.0321\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.0487 - mae: 0.3496 - mean_pred: 0.9576 - mae_t1: 0.0233 - val_loss: 1.2499 - val_mae: 0.4166 - val_mean_pred: 0.8631 - val_mae_t1: 0.0278\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 0.9206 - mae: 0.3069 - mean_pred: 0.7118 - mae_t1: 0.0205 - val_loss: 1.4400 - val_mae: 0.4800 - val_mean_pred: 0.7709 - val_mae_t1: 0.0320\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 0.9249 - mae: 0.3083 - mean_pred: 0.7305 - mae_t1: 0.0206 - val_loss: 1.3642 - val_mae: 0.4547 - val_mean_pred: 0.9757 - val_mae_t1: 0.0303\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 0.9711 - mae: 0.3237 - mean_pred: 0.8773 - mae_t1: 0.0216 - val_loss: 1.2717 - val_mae: 0.4239 - val_mean_pred: 0.9133 - val_mae_t1: 0.0283\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 0.8480 - mae: 0.2827 - mean_pred: 0.7630 - mae_t1: 0.0188 - val_loss: 1.2976 - val_mae: 0.4325 - val_mean_pred: 0.8254 - val_mae_t1: 0.0288\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 0.8363 - mae: 0.2788 - mean_pred: 0.7771 - mae_t1: 0.0186 - val_loss: 1.2146 - val_mae: 0.4049 - val_mean_pred: 0.8840 - val_mae_t1: 0.0270\n",
      "Earliness...\n",
      "0.002063274383544922\n",
      "____________________________________________________________\n",
      "Test MAE:      0.34001279752854036  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>mean_pred</td><td>▁▅▆▆▇▆▆▆▆▇▇▆▆▇▆▇▆▆█▇▇█▇▇▆▆▆▇▆▆▆█▆▇▇██▆▆▇</td></tr><tr><td>val_loss</td><td>▅█▇▅▄▃▃▆▃▄▅▄▄▄▂▂▂▂▄▂▃▂▃▃▃▂▂▂▃▂▂▁▂▂▃▃▂▅▃▁</td></tr><tr><td>val_mae</td><td>▅█▇▅▄▃▃▆▃▄▅▄▄▄▂▂▂▂▄▂▃▂▃▃▃▂▂▂▃▂▂▁▂▂▃▃▂▅▃▁</td></tr><tr><td>val_mae_t1</td><td>▅█▇▅▄▃▃▆▃▄▅▄▄▄▂▂▂▂▄▂▃▂▃▃▃▂▂▂▃▂▂▁▂▂▃▃▂▅▃▁</td></tr><tr><td>val_mean_pred</td><td>▃▁▂▄▄▆▆▂▄▃▇▅▃▃▅▆▅▅▃▅▄▅▆▆▅▅▄▆▃▅▄▆▄▅▅▇▆█▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.38005</td></tr><tr><td>AE_2</td><td>0.32003</td></tr><tr><td>AE_3</td><td>0.32604</td></tr><tr><td>MAE</td><td>0.34001</td></tr><tr><td>best_epoch</td><td>99</td></tr><tr><td>best_val_loss</td><td>1.21456</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>0.83632</td></tr><tr><td>mae</td><td>0.27877</td></tr><tr><td>mae_t1</td><td>0.01858</td></tr><tr><td>mean_pred</td><td>0.77706</td></tr><tr><td>val_loss</td><td>1.21456</td></tr><tr><td>val_mae</td><td>0.40485</td></tr><tr><td>val_mae_t1</td><td>0.02699</td></tr><tr><td>val_mean_pred</td><td>0.88401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">clear-oath-93</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/lif5r8ns\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/lif5r8ns</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155832-lif5r8ns\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155900-334vbzg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/334vbzg0\" target=\"_blank\">daily-mountain-94</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 699us/sample - loss: 2.2933 - mae: 0.7644 - mean_pred: 0.1878 - mae_t1: 0.0510 - val_loss: 1.7954 - val_mae: 0.5985 - val_mean_pred: 0.3792 - val_mae_t1: 0.0399\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.9339 - mae: 0.6446 - mean_pred: 0.4269 - mae_t1: 0.0430 - val_loss: 1.5784 - val_mae: 0.5261 - val_mean_pred: 0.6469 - val_mae_t1: 0.0351\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 134us/sample - loss: 1.8122 - mae: 0.6041 - mean_pred: 0.6118 - mae_t1: 0.0403 - val_loss: 1.6181 - val_mae: 0.5394 - val_mean_pred: 0.5926 - val_mae_t1: 0.0360\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 131us/sample - loss: 1.7935 - mae: 0.5978 - mean_pred: 0.5089 - mae_t1: 0.0399 - val_loss: 1.6672 - val_mae: 0.5557 - val_mean_pred: 0.5005 - val_mae_t1: 0.0370\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 146us/sample - loss: 1.7529 - mae: 0.5843 - mean_pred: 0.4744 - mae_t1: 0.0390 - val_loss: 1.5563 - val_mae: 0.5188 - val_mean_pred: 0.6486 - val_mae_t1: 0.0346\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 137us/sample - loss: 1.6441 - mae: 0.5480 - mean_pred: 0.6172 - mae_t1: 0.0365 - val_loss: 1.5154 - val_mae: 0.5051 - val_mean_pred: 0.7930 - val_mae_t1: 0.0337\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 1.5857 - mae: 0.5286 - mean_pred: 0.7312 - mae_t1: 0.0352 - val_loss: 1.5789 - val_mae: 0.5263 - val_mean_pred: 0.7951 - val_mae_t1: 0.0351\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 1.5622 - mae: 0.5207 - mean_pred: 0.7013 - mae_t1: 0.0347 - val_loss: 1.6044 - val_mae: 0.5348 - val_mean_pred: 0.6645 - val_mae_t1: 0.0357\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 1.5494 - mae: 0.5165 - mean_pred: 0.5741 - mae_t1: 0.0344 - val_loss: 1.6493 - val_mae: 0.5498 - val_mean_pred: 0.6025 - val_mae_t1: 0.0367\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 124us/sample - loss: 1.5592 - mae: 0.5197 - mean_pred: 0.5574 - mae_t1: 0.0346 - val_loss: 1.4472 - val_mae: 0.4824 - val_mean_pred: 0.8150 - val_mae_t1: 0.0322\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.4853 - mae: 0.4951 - mean_pred: 0.8271 - mae_t1: 0.0330 - val_loss: 1.7134 - val_mae: 0.5711 - val_mean_pred: 1.1246 - val_mae_t1: 0.0381\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.8004 - mae: 0.6001 - mean_pred: 1.0682 - mae_t1: 0.0400 - val_loss: 1.4743 - val_mae: 0.4914 - val_mean_pred: 0.9551 - val_mae_t1: 0.0328\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4602 - mae: 0.4867 - mean_pred: 0.8366 - mae_t1: 0.0324 - val_loss: 1.5577 - val_mae: 0.5192 - val_mean_pred: 0.6425 - val_mae_t1: 0.0346\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.5131 - mae: 0.5044 - mean_pred: 0.5883 - mae_t1: 0.0336 - val_loss: 1.6473 - val_mae: 0.5491 - val_mean_pred: 0.5750 - val_mae_t1: 0.0366\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.5455 - mae: 0.5152 - mean_pred: 0.5469 - mae_t1: 0.0343 - val_loss: 1.5897 - val_mae: 0.5299 - val_mean_pred: 0.6483 - val_mae_t1: 0.0353\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4649 - mae: 0.4883 - mean_pred: 0.6318 - mae_t1: 0.0326 - val_loss: 1.5477 - val_mae: 0.5159 - val_mean_pred: 0.7997 - val_mae_t1: 0.0344\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.4194 - mae: 0.4731 - mean_pred: 0.7619 - mae_t1: 0.0315 - val_loss: 1.5296 - val_mae: 0.5099 - val_mean_pred: 0.9134 - val_mae_t1: 0.0340\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.4392 - mae: 0.4797 - mean_pred: 0.8548 - mae_t1: 0.0320 - val_loss: 1.5725 - val_mae: 0.5242 - val_mean_pred: 0.9076 - val_mae_t1: 0.0349\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.5177 - mae: 0.5059 - mean_pred: 0.8322 - mae_t1: 0.0337 - val_loss: 1.5540 - val_mae: 0.5180 - val_mean_pred: 0.8173 - val_mae_t1: 0.0345\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.4317 - mae: 0.4772 - mean_pred: 0.7581 - mae_t1: 0.0318 - val_loss: 1.4595 - val_mae: 0.4865 - val_mean_pred: 0.7952 - val_mae_t1: 0.0324\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 1.3477 - mae: 0.4492 - mean_pred: 0.7534 - mae_t1: 0.0299 - val_loss: 1.4259 - val_mae: 0.4753 - val_mean_pred: 0.8133 - val_mae_t1: 0.0317\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 1.3279 - mae: 0.4426 - mean_pred: 0.7842 - mae_t1: 0.0295 - val_loss: 1.3944 - val_mae: 0.4648 - val_mean_pred: 0.8373 - val_mae_t1: 0.0310\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.3372 - mae: 0.4457 - mean_pred: 0.7959 - mae_t1: 0.0297 - val_loss: 1.4009 - val_mae: 0.4670 - val_mean_pred: 0.7972 - val_mae_t1: 0.0311\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.3361 - mae: 0.4454 - mean_pred: 0.7474 - mae_t1: 0.0297 - val_loss: 1.4454 - val_mae: 0.4818 - val_mean_pred: 0.7327 - val_mae_t1: 0.0321\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.3125 - mae: 0.4375 - mean_pred: 0.7059 - mae_t1: 0.0292 - val_loss: 1.4161 - val_mae: 0.4720 - val_mean_pred: 0.7452 - val_mae_t1: 0.0315\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2824 - mae: 0.4275 - mean_pred: 0.7309 - mae_t1: 0.0285 - val_loss: 1.4196 - val_mae: 0.4732 - val_mean_pred: 0.7954 - val_mae_t1: 0.0315\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 1.2806 - mae: 0.4269 - mean_pred: 0.7793 - mae_t1: 0.0285 - val_loss: 1.3892 - val_mae: 0.4631 - val_mean_pred: 0.9036 - val_mae_t1: 0.0309\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 1.3125 - mae: 0.4375 - mean_pred: 0.8865 - mae_t1: 0.0292 - val_loss: 1.3584 - val_mae: 0.4528 - val_mean_pred: 0.9390 - val_mae_t1: 0.0302\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.3172 - mae: 0.4391 - mean_pred: 0.8680 - mae_t1: 0.0293 - val_loss: 1.3976 - val_mae: 0.4659 - val_mean_pred: 0.8303 - val_mae_t1: 0.0311\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2843 - mae: 0.4281 - mean_pred: 0.7985 - mae_t1: 0.0285 - val_loss: 1.3904 - val_mae: 0.4635 - val_mean_pred: 0.8373 - val_mae_t1: 0.0309\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2341 - mae: 0.4114 - mean_pred: 0.8036 - mae_t1: 0.0274 - val_loss: 1.5077 - val_mae: 0.5026 - val_mean_pred: 0.7395 - val_mae_t1: 0.0335\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2824 - mae: 0.4275 - mean_pred: 0.6814 - mae_t1: 0.0285 - val_loss: 1.6284 - val_mae: 0.5428 - val_mean_pred: 0.6091 - val_mae_t1: 0.0362\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.3536 - mae: 0.4512 - mean_pred: 0.5857 - mae_t1: 0.0301 - val_loss: 1.5531 - val_mae: 0.5177 - val_mean_pred: 0.6989 - val_mae_t1: 0.0345\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2563 - mae: 0.4188 - mean_pred: 0.6787 - mae_t1: 0.0279 - val_loss: 1.4758 - val_mae: 0.4919 - val_mean_pred: 0.9099 - val_mae_t1: 0.0328\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2593 - mae: 0.4198 - mean_pred: 0.8662 - mae_t1: 0.0280 - val_loss: 1.6340 - val_mae: 0.5447 - val_mean_pred: 0.9840 - val_mae_t1: 0.0363\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.3706 - mae: 0.4569 - mean_pred: 0.8924 - mae_t1: 0.0305 - val_loss: 1.6493 - val_mae: 0.5498 - val_mean_pred: 0.9219 - val_mae_t1: 0.0367\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.3483 - mae: 0.4494 - mean_pred: 0.8698 - mae_t1: 0.0300 - val_loss: 1.4781 - val_mae: 0.4927 - val_mean_pred: 0.9635 - val_mae_t1: 0.0328\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2866 - mae: 0.4289 - mean_pred: 0.9076 - mae_t1: 0.0286 - val_loss: 1.4191 - val_mae: 0.4730 - val_mean_pred: 0.9497 - val_mae_t1: 0.0315\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2785 - mae: 0.4262 - mean_pred: 0.8894 - mae_t1: 0.0284 - val_loss: 1.3961 - val_mae: 0.4654 - val_mean_pred: 0.8237 - val_mae_t1: 0.0310\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.2024 - mae: 0.4008 - mean_pred: 0.7511 - mae_t1: 0.0267 - val_loss: 1.4498 - val_mae: 0.4833 - val_mean_pred: 0.7029 - val_mae_t1: 0.0322\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2263 - mae: 0.4088 - mean_pred: 0.6639 - mae_t1: 0.0273 - val_loss: 1.3852 - val_mae: 0.4617 - val_mean_pred: 0.7491 - val_mae_t1: 0.0308\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 128us/sample - loss: 1.1737 - mae: 0.3912 - mean_pred: 0.7191 - mae_t1: 0.0261 - val_loss: 1.3385 - val_mae: 0.4462 - val_mean_pred: 0.9330 - val_mae_t1: 0.0297\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2679 - mae: 0.4226 - mean_pred: 0.9079 - mae_t1: 0.0282 - val_loss: 1.4653 - val_mae: 0.4884 - val_mean_pred: 1.0450 - val_mae_t1: 0.0326\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.3623 - mae: 0.4541 - mean_pred: 0.9533 - mae_t1: 0.0303 - val_loss: 1.4692 - val_mae: 0.4897 - val_mean_pred: 0.8926 - val_mae_t1: 0.0326\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2943 - mae: 0.4314 - mean_pred: 0.8050 - mae_t1: 0.0288 - val_loss: 1.3883 - val_mae: 0.4628 - val_mean_pred: 0.7641 - val_mae_t1: 0.0309\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.1703 - mae: 0.3901 - mean_pred: 0.7221 - mae_t1: 0.0260 - val_loss: 1.4172 - val_mae: 0.4724 - val_mean_pred: 0.7137 - val_mae_t1: 0.0315\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.2935 - mae: 0.4312 - mean_pred: 0.6863 - mae_t1: 0.0287 - val_loss: 1.4077 - val_mae: 0.4692 - val_mean_pred: 0.6705 - val_mae_t1: 0.0313\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.2232 - mae: 0.4077 - mean_pred: 0.6592 - mae_t1: 0.0272 - val_loss: 1.4128 - val_mae: 0.4709 - val_mean_pred: 0.7748 - val_mae_t1: 0.0314\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1777 - mae: 0.3926 - mean_pred: 0.7617 - mae_t1: 0.0262 - val_loss: 1.3890 - val_mae: 0.4630 - val_mean_pred: 0.9357 - val_mae_t1: 0.0309\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 103us/sample - loss: 1.1916 - mae: 0.3972 - mean_pred: 0.8896 - mae_t1: 0.0265 - val_loss: 1.3574 - val_mae: 0.4525 - val_mean_pred: 0.9454 - val_mae_t1: 0.0302\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 98us/sample - loss: 1.1323 - mae: 0.3774 - mean_pred: 0.8695 - mae_t1: 0.0252 - val_loss: 1.4066 - val_mae: 0.4689 - val_mean_pred: 0.8392 - val_mae_t1: 0.0313\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.1544 - mae: 0.3848 - mean_pred: 0.7672 - mae_t1: 0.0257 - val_loss: 1.4871 - val_mae: 0.4957 - val_mean_pred: 0.7251 - val_mae_t1: 0.0330\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.2188 - mae: 0.4063 - mean_pred: 0.6482 - mae_t1: 0.0271 - val_loss: 1.5241 - val_mae: 0.5080 - val_mean_pred: 0.6738 - val_mae_t1: 0.0339\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2130 - mae: 0.4043 - mean_pred: 0.6325 - mae_t1: 0.0270 - val_loss: 1.4424 - val_mae: 0.4808 - val_mean_pred: 0.8010 - val_mae_t1: 0.0321\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.1350 - mae: 0.3783 - mean_pred: 0.7562 - mae_t1: 0.0252 - val_loss: 1.4314 - val_mae: 0.4771 - val_mean_pred: 0.9560 - val_mae_t1: 0.0318\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.1667 - mae: 0.3889 - mean_pred: 0.8833 - mae_t1: 0.0259 - val_loss: 1.4733 - val_mae: 0.4911 - val_mean_pred: 1.0233 - val_mae_t1: 0.0327\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.2097 - mae: 0.4032 - mean_pred: 0.9353 - mae_t1: 0.0269 - val_loss: 1.4155 - val_mae: 0.4718 - val_mean_pred: 0.9651 - val_mae_t1: 0.0315\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1108 - mae: 0.3703 - mean_pred: 0.8629 - mae_t1: 0.0247 - val_loss: 1.3997 - val_mae: 0.4666 - val_mean_pred: 0.8382 - val_mae_t1: 0.0311\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0859 - mae: 0.3620 - mean_pred: 0.7833 - mae_t1: 0.0241 - val_loss: 1.3844 - val_mae: 0.4615 - val_mean_pred: 0.8247 - val_mae_t1: 0.0308\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.1225 - mae: 0.3742 - mean_pred: 0.7912 - mae_t1: 0.0249 - val_loss: 1.3592 - val_mae: 0.4531 - val_mean_pred: 0.8382 - val_mae_t1: 0.0302\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.0688 - mae: 0.3563 - mean_pred: 0.8039 - mae_t1: 0.0238 - val_loss: 1.3696 - val_mae: 0.4565 - val_mean_pred: 0.8454 - val_mae_t1: 0.0304\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.0601 - mae: 0.3534 - mean_pred: 0.7992 - mae_t1: 0.0236 - val_loss: 1.3486 - val_mae: 0.4495 - val_mean_pred: 0.8546 - val_mae_t1: 0.0300\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 1.0490 - mae: 0.3497 - mean_pred: 0.8073 - mae_t1: 0.0233 - val_loss: 1.3070 - val_mae: 0.4357 - val_mean_pred: 0.8737 - val_mae_t1: 0.0290\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 1.0503 - mae: 0.3501 - mean_pred: 0.8240 - mae_t1: 0.0233 - val_loss: 1.3048 - val_mae: 0.4349 - val_mean_pred: 0.8708 - val_mae_t1: 0.0290\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.0412 - mae: 0.3471 - mean_pred: 0.8140 - mae_t1: 0.0231 - val_loss: 1.3536 - val_mae: 0.4512 - val_mean_pred: 0.8716 - val_mae_t1: 0.0301\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0641 - mae: 0.3547 - mean_pred: 0.8077 - mae_t1: 0.0236 - val_loss: 1.3572 - val_mae: 0.4524 - val_mean_pred: 0.8878 - val_mae_t1: 0.0302\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 1.0642 - mae: 0.3547 - mean_pred: 0.8110 - mae_t1: 0.0236 - val_loss: 1.2841 - val_mae: 0.4280 - val_mean_pred: 0.9130 - val_mae_t1: 0.0285\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0668 - mae: 0.3556 - mean_pred: 0.8550 - mae_t1: 0.0237 - val_loss: 1.3142 - val_mae: 0.4381 - val_mean_pred: 0.9773 - val_mae_t1: 0.0292\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 1.1229 - mae: 0.3743 - mean_pred: 0.9078 - mae_t1: 0.0250 - val_loss: 1.2592 - val_mae: 0.4197 - val_mean_pred: 0.9065 - val_mae_t1: 0.0280\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0670 - mae: 0.3557 - mean_pred: 0.8169 - mae_t1: 0.0237 - val_loss: 1.3852 - val_mae: 0.4617 - val_mean_pred: 0.7183 - val_mae_t1: 0.0308\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1637 - mae: 0.3879 - mean_pred: 0.6507 - mae_t1: 0.0259 - val_loss: 1.5098 - val_mae: 0.5033 - val_mean_pred: 0.7020 - val_mae_t1: 0.0336\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.1785 - mae: 0.3928 - mean_pred: 0.6756 - mae_t1: 0.0262 - val_loss: 1.3574 - val_mae: 0.4525 - val_mean_pred: 0.9191 - val_mae_t1: 0.0302\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.0512 - mae: 0.3504 - mean_pred: 0.8837 - mae_t1: 0.0234 - val_loss: 1.4455 - val_mae: 0.4818 - val_mean_pred: 1.0203 - val_mae_t1: 0.0321\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1693 - mae: 0.3898 - mean_pred: 0.9379 - mae_t1: 0.0260 - val_loss: 1.3402 - val_mae: 0.4467 - val_mean_pred: 0.8717 - val_mae_t1: 0.0298\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.0510 - mae: 0.3503 - mean_pred: 0.7884 - mae_t1: 0.0234 - val_loss: 1.4956 - val_mae: 0.4985 - val_mean_pred: 0.6867 - val_mae_t1: 0.0332\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.1812 - mae: 0.3937 - mean_pred: 0.6273 - mae_t1: 0.0262 - val_loss: 1.5753 - val_mae: 0.5251 - val_mean_pred: 0.6389 - val_mae_t1: 0.0350\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2168 - mae: 0.4056 - mean_pred: 0.6134 - mae_t1: 0.0270 - val_loss: 1.5545 - val_mae: 0.5182 - val_mean_pred: 0.7129 - val_mae_t1: 0.0345\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 1.1395 - mae: 0.3798 - mean_pred: 0.6590 - mae_t1: 0.0253 - val_loss: 1.5240 - val_mae: 0.5080 - val_mean_pred: 0.7910 - val_mae_t1: 0.0339\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1128 - mae: 0.3709 - mean_pred: 0.7213 - mae_t1: 0.0247 - val_loss: 1.4706 - val_mae: 0.4902 - val_mean_pred: 0.9278 - val_mae_t1: 0.0327\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1306 - mae: 0.3769 - mean_pred: 0.8550 - mae_t1: 0.0251 - val_loss: 1.4311 - val_mae: 0.4770 - val_mean_pred: 1.0376 - val_mae_t1: 0.0318\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.1390 - mae: 0.3797 - mean_pred: 0.9414 - mae_t1: 0.0253 - val_loss: 1.3836 - val_mae: 0.4612 - val_mean_pred: 0.9788 - val_mae_t1: 0.0307\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.1123 - mae: 0.3708 - mean_pred: 0.8770 - mae_t1: 0.0247 - val_loss: 1.3288 - val_mae: 0.4429 - val_mean_pred: 0.8232 - val_mae_t1: 0.0295\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.0641 - mae: 0.3547 - mean_pred: 0.7168 - mae_t1: 0.0236 - val_loss: 1.5547 - val_mae: 0.5182 - val_mean_pred: 0.7364 - val_mae_t1: 0.0345\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.2945 - mae: 0.4315 - mean_pred: 0.6523 - mae_t1: 0.0288 - val_loss: 1.4621 - val_mae: 0.4874 - val_mean_pred: 0.7451 - val_mae_t1: 0.0325\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.1476 - mae: 0.3825 - mean_pred: 0.6734 - mae_t1: 0.0255 - val_loss: 1.2932 - val_mae: 0.4311 - val_mean_pred: 0.8261 - val_mae_t1: 0.0287\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0587 - mae: 0.3529 - mean_pred: 0.7902 - mae_t1: 0.0235 - val_loss: 1.3409 - val_mae: 0.4470 - val_mean_pred: 0.9909 - val_mae_t1: 0.0298\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 111us/sample - loss: 1.1298 - mae: 0.3766 - mean_pred: 0.9307 - mae_t1: 0.0251 - val_loss: 1.3402 - val_mae: 0.4467 - val_mean_pred: 0.9933 - val_mae_t1: 0.0298\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0906 - mae: 0.3635 - mean_pred: 0.8810 - mae_t1: 0.0242 - val_loss: 1.3741 - val_mae: 0.4580 - val_mean_pred: 0.8711 - val_mae_t1: 0.0305\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0906 - mae: 0.3635 - mean_pred: 0.7748 - mae_t1: 0.0242 - val_loss: 1.3667 - val_mae: 0.4556 - val_mean_pred: 0.8133 - val_mae_t1: 0.0304\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0363 - mae: 0.3454 - mean_pred: 0.7450 - mae_t1: 0.0230 - val_loss: 1.3283 - val_mae: 0.4428 - val_mean_pred: 0.8033 - val_mae_t1: 0.0295\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 1.0495 - mae: 0.3498 - mean_pred: 0.7533 - mae_t1: 0.0233 - val_loss: 1.2576 - val_mae: 0.4192 - val_mean_pred: 0.8411 - val_mae_t1: 0.0279\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 1.0803 - mae: 0.3601 - mean_pred: 0.8149 - mae_t1: 0.0240 - val_loss: 1.2567 - val_mae: 0.4189 - val_mean_pred: 0.9118 - val_mae_t1: 0.0279\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0375 - mae: 0.3458 - mean_pred: 0.8632 - mae_t1: 0.0231 - val_loss: 1.3837 - val_mae: 0.4612 - val_mean_pred: 0.8827 - val_mae_t1: 0.0307\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.0471 - mae: 0.3490 - mean_pred: 0.8127 - mae_t1: 0.0233 - val_loss: 1.3084 - val_mae: 0.4361 - val_mean_pred: 0.7921 - val_mae_t1: 0.0291\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.0217 - mae: 0.3406 - mean_pred: 0.7339 - mae_t1: 0.0227 - val_loss: 1.2904 - val_mae: 0.4301 - val_mean_pred: 0.7183 - val_mae_t1: 0.0287\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.1216 - mae: 0.3739 - mean_pred: 0.6905 - mae_t1: 0.0249 - val_loss: 1.3193 - val_mae: 0.4398 - val_mean_pred: 0.7046 - val_mae_t1: 0.0293\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.1165 - mae: 0.3722 - mean_pred: 0.6874 - mae_t1: 0.0248 - val_loss: 1.3201 - val_mae: 0.4400 - val_mean_pred: 0.7415 - val_mae_t1: 0.0293\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.0347 - mae: 0.3449 - mean_pred: 0.6956 - mae_t1: 0.0230 - val_loss: 1.3489 - val_mae: 0.4496 - val_mean_pred: 0.8246 - val_mae_t1: 0.0300\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 0.9979 - mae: 0.3326 - mean_pred: 0.7638 - mae_t1: 0.0222 - val_loss: 1.4832 - val_mae: 0.4944 - val_mean_pred: 1.0119 - val_mae_t1: 0.0330\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.0760 - mae: 0.3587 - mean_pred: 0.9169 - mae_t1: 0.0239 - val_loss: 1.5428 - val_mae: 0.5143 - val_mean_pred: 1.0888 - val_mae_t1: 0.0343\n",
      "Earliness...\n",
      "0.0020003318786621094\n",
      "____________________________________________________________\n",
      "Test MAE:      0.35723162946645315  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▃▂▂▃▂▂▁▂▂▁▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▃▂▂▃▂▂▁▂▂▁▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>mae_t1</td><td>█▅▄▄▃▃▃▃▃▃▂▂▂▂▃▂▂▃▂▂▁▂▂▁▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>mean_pred</td><td>▁▅▅▆▇▇▅▇▆▇▆▇▇▅▇▇▅█▆▆▇▅▇▆▇▇▇█▅█▅▆▇▅█▆▇▇▆█</td></tr><tr><td>val_loss</td><td>█▆▄▆▇▅▅▅▃▃▃▂▄▄▆▃▃▄▃▃▃▃▄▃▂▂▁▁▂▂▅▄▂▄▂▂▁▂▂▅</td></tr><tr><td>val_mae</td><td>█▆▄▆▇▅▅▅▃▃▃▂▄▄▆▃▃▄▃▃▃▃▄▃▂▂▁▁▂▂▅▄▂▄▂▂▁▂▂▅</td></tr><tr><td>val_mae_t1</td><td>█▆▄▆▇▅▅▅▃▃▃▂▄▄▆▃▃▄▃▃▃▃▄▃▂▂▁▁▂▂▅▄▂▄▂▂▁▂▂▅</td></tr><tr><td>val_mean_pred</td><td>▁▃▅▄█▃▅▆▅▅▅▆▄▆▆▅▄▆▄▆▅▅▇▅▅▆▆▆▆▆▄▆▅▄▇▅▆▅▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.53294</td></tr><tr><td>AE_2</td><td>0.34111</td></tr><tr><td>AE_3</td><td>0.30588</td></tr><tr><td>MAE</td><td>0.35723</td></tr><tr><td>best_epoch</td><td>91</td></tr><tr><td>best_val_loss</td><td>1.2567</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.07598</td></tr><tr><td>mae</td><td>0.35866</td></tr><tr><td>mae_t1</td><td>0.02391</td></tr><tr><td>mean_pred</td><td>0.9169</td></tr><tr><td>val_loss</td><td>1.5428</td></tr><tr><td>val_mae</td><td>0.51427</td></tr><tr><td>val_mae_t1</td><td>0.03428</td></tr><tr><td>val_mean_pred</td><td>1.08884</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">daily-mountain-94</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/334vbzg0\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/334vbzg0</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155900-334vbzg0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_155934-b31sbbsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/b31sbbsf\" target=\"_blank\">breezy-flower-95</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 880us/sample - loss: 3.9817 - mae: 0.8532 - mean_pred: 0.2650 - mae_t1: 0.0569 - val_loss: 2.3134 - val_mae: 0.4957 - val_mean_pred: 0.9661 - val_mae_t1: 0.0330\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 314us/sample - loss: 3.2032 - mae: 0.6864 - mean_pred: 1.0545 - mae_t1: 0.0458 - val_loss: 2.1340 - val_mae: 0.4573 - val_mean_pred: 0.8237 - val_mae_t1: 0.0305\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 247us/sample - loss: 2.8055 - mae: 0.6012 - mean_pred: 0.5730 - mae_t1: 0.0401 - val_loss: 2.6453 - val_mae: 0.5668 - val_mean_pred: 0.4757 - val_mae_t1: 0.0378\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 2.7816 - mae: 0.5961 - mean_pred: 0.4235 - mae_t1: 0.0397 - val_loss: 2.6882 - val_mae: 0.5760 - val_mean_pred: 0.6891 - val_mae_t1: 0.0384\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 2.6545 - mae: 0.5688 - mean_pred: 0.6565 - mae_t1: 0.0379 - val_loss: 2.2392 - val_mae: 0.4798 - val_mean_pred: 0.8463 - val_mae_t1: 0.0320\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 2.2267 - mae: 0.4772 - mean_pred: 0.7758 - mae_t1: 0.0318 - val_loss: 2.1655 - val_mae: 0.4640 - val_mean_pred: 0.8124 - val_mae_t1: 0.0309\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 202us/sample - loss: 2.2511 - mae: 0.4824 - mean_pred: 0.7255 - mae_t1: 0.0322 - val_loss: 2.1872 - val_mae: 0.4687 - val_mean_pred: 0.8149 - val_mae_t1: 0.0312\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.0720 - mae: 0.4440 - mean_pred: 0.8205 - mae_t1: 0.0296 - val_loss: 2.2531 - val_mae: 0.4828 - val_mean_pred: 0.8455 - val_mae_t1: 0.0322\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 2.0159 - mae: 0.4320 - mean_pred: 0.7395 - mae_t1: 0.0288 - val_loss: 2.5207 - val_mae: 0.5401 - val_mean_pred: 0.7071 - val_mae_t1: 0.0360\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.1888 - mae: 0.4690 - mean_pred: 0.6629 - mae_t1: 0.0313 - val_loss: 2.5504 - val_mae: 0.5465 - val_mean_pred: 0.7913 - val_mae_t1: 0.0364\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.9875 - mae: 0.4259 - mean_pred: 0.8047 - mae_t1: 0.0284 - val_loss: 2.4229 - val_mae: 0.5192 - val_mean_pred: 0.9132 - val_mae_t1: 0.0346\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.9813 - mae: 0.4246 - mean_pred: 0.7696 - mae_t1: 0.0283 - val_loss: 2.3245 - val_mae: 0.4981 - val_mean_pred: 0.7973 - val_mae_t1: 0.0332\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.9150 - mae: 0.4104 - mean_pred: 0.7612 - mae_t1: 0.0274 - val_loss: 2.2746 - val_mae: 0.4874 - val_mean_pred: 0.7536 - val_mae_t1: 0.0325\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.0312 - mae: 0.4353 - mean_pred: 0.6186 - mae_t1: 0.0290 - val_loss: 2.2248 - val_mae: 0.4768 - val_mean_pred: 0.7357 - val_mae_t1: 0.0318\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 231us/sample - loss: 1.9059 - mae: 0.4084 - mean_pred: 0.7619 - mae_t1: 0.0272 - val_loss: 2.1224 - val_mae: 0.4548 - val_mean_pred: 0.9263 - val_mae_t1: 0.0303\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.9425 - mae: 0.4163 - mean_pred: 0.8402 - mae_t1: 0.0278 - val_loss: 2.2797 - val_mae: 0.4885 - val_mean_pred: 0.7835 - val_mae_t1: 0.0326\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 1.8485 - mae: 0.3961 - mean_pred: 0.7537 - mae_t1: 0.0264 - val_loss: 2.2867 - val_mae: 0.4900 - val_mean_pred: 0.7647 - val_mae_t1: 0.0327\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.9517 - mae: 0.4182 - mean_pred: 0.7337 - mae_t1: 0.0279 - val_loss: 2.2581 - val_mae: 0.4839 - val_mean_pred: 0.7892 - val_mae_t1: 0.0323\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.7767 - mae: 0.3807 - mean_pred: 0.8000 - mae_t1: 0.0254 - val_loss: 2.2069 - val_mae: 0.4729 - val_mean_pred: 0.9704 - val_mae_t1: 0.0315\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.7762 - mae: 0.3806 - mean_pred: 0.8326 - mae_t1: 0.0254 - val_loss: 2.3362 - val_mae: 0.5006 - val_mean_pred: 0.7402 - val_mae_t1: 0.0334\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 1.9980 - mae: 0.4281 - mean_pred: 0.6192 - mae_t1: 0.0285 - val_loss: 2.0888 - val_mae: 0.4476 - val_mean_pred: 0.8040 - val_mae_t1: 0.0298\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 2.0335 - mae: 0.4357 - mean_pred: 0.8263 - mae_t1: 0.0290 - val_loss: 2.1523 - val_mae: 0.4612 - val_mean_pred: 0.9961 - val_mae_t1: 0.0307\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.1169 - mae: 0.4536 - mean_pred: 0.9213 - mae_t1: 0.0302 - val_loss: 2.3342 - val_mae: 0.5002 - val_mean_pred: 0.8286 - val_mae_t1: 0.0333\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 1.8767 - mae: 0.4021 - mean_pred: 0.6990 - mae_t1: 0.0268 - val_loss: 2.2959 - val_mae: 0.4920 - val_mean_pred: 0.8136 - val_mae_t1: 0.0328\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.9026 - mae: 0.4077 - mean_pred: 0.7687 - mae_t1: 0.0272 - val_loss: 2.3082 - val_mae: 0.4946 - val_mean_pred: 0.9463 - val_mae_t1: 0.0330\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.8742 - mae: 0.4016 - mean_pred: 0.8556 - mae_t1: 0.0268 - val_loss: 2.3222 - val_mae: 0.4976 - val_mean_pred: 0.9474 - val_mae_t1: 0.0332\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.6550 - mae: 0.3546 - mean_pred: 0.7613 - mae_t1: 0.0236 - val_loss: 2.2501 - val_mae: 0.4822 - val_mean_pred: 0.8018 - val_mae_t1: 0.0321\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 1.7061 - mae: 0.3656 - mean_pred: 0.7422 - mae_t1: 0.0244 - val_loss: 2.2299 - val_mae: 0.4778 - val_mean_pred: 0.9769 - val_mae_t1: 0.0319\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 1.7354 - mae: 0.3719 - mean_pred: 0.8722 - mae_t1: 0.0248 - val_loss: 2.0740 - val_mae: 0.4444 - val_mean_pred: 0.8923 - val_mae_t1: 0.0296\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 227us/sample - loss: 1.6428 - mae: 0.3520 - mean_pred: 0.7146 - mae_t1: 0.0235 - val_loss: 2.0415 - val_mae: 0.4375 - val_mean_pred: 0.8401 - val_mae_t1: 0.0292\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 221us/sample - loss: 1.6308 - mae: 0.3495 - mean_pred: 0.7924 - mae_t1: 0.0233 - val_loss: 2.0395 - val_mae: 0.4370 - val_mean_pred: 0.9131 - val_mae_t1: 0.0291\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 1.5841 - mae: 0.3395 - mean_pred: 0.7668 - mae_t1: 0.0226 - val_loss: 2.0324 - val_mae: 0.4355 - val_mean_pred: 0.8937 - val_mae_t1: 0.0290\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.5964 - mae: 0.3421 - mean_pred: 0.8601 - mae_t1: 0.0228 - val_loss: 2.0642 - val_mae: 0.4423 - val_mean_pred: 0.9341 - val_mae_t1: 0.0295\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.5396 - mae: 0.3299 - mean_pred: 0.7978 - mae_t1: 0.0220 - val_loss: 2.0563 - val_mae: 0.4406 - val_mean_pred: 0.8451 - val_mae_t1: 0.0294\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.5252 - mae: 0.3268 - mean_pred: 0.7956 - mae_t1: 0.0218 - val_loss: 2.1666 - val_mae: 0.4643 - val_mean_pred: 1.0028 - val_mae_t1: 0.0310\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.6310 - mae: 0.3495 - mean_pred: 0.8936 - mae_t1: 0.0233 - val_loss: 2.0748 - val_mae: 0.4446 - val_mean_pred: 0.8616 - val_mae_t1: 0.0296\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.5231 - mae: 0.3264 - mean_pred: 0.7413 - mae_t1: 0.0218 - val_loss: 2.0890 - val_mae: 0.4476 - val_mean_pred: 0.8204 - val_mae_t1: 0.0298\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.5001 - mae: 0.3215 - mean_pred: 0.7703 - mae_t1: 0.0214 - val_loss: 2.2600 - val_mae: 0.4843 - val_mean_pred: 1.0255 - val_mae_t1: 0.0323\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.5616 - mae: 0.3346 - mean_pred: 0.8789 - mae_t1: 0.0223 - val_loss: 2.0388 - val_mae: 0.4369 - val_mean_pred: 0.8338 - val_mae_t1: 0.0291\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 225us/sample - loss: 1.5405 - mae: 0.3301 - mean_pred: 0.7206 - mae_t1: 0.0220 - val_loss: 1.9965 - val_mae: 0.4278 - val_mean_pred: 0.8669 - val_mae_t1: 0.0285\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.5235 - mae: 0.3265 - mean_pred: 0.8263 - mae_t1: 0.0218 - val_loss: 2.3332 - val_mae: 0.5000 - val_mean_pred: 0.9102 - val_mae_t1: 0.0333\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 1.8244 - mae: 0.3909 - mean_pred: 0.7878 - mae_t1: 0.0261 - val_loss: 2.1390 - val_mae: 0.4583 - val_mean_pred: 0.8480 - val_mae_t1: 0.0306\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.8267 - mae: 0.3914 - mean_pred: 0.7907 - mae_t1: 0.0261 - val_loss: 2.1827 - val_mae: 0.4677 - val_mean_pred: 0.8198 - val_mae_t1: 0.0312\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 1.5738 - mae: 0.3372 - mean_pred: 0.7384 - mae_t1: 0.0225 - val_loss: 2.1796 - val_mae: 0.4671 - val_mean_pred: 0.9265 - val_mae_t1: 0.0311\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.4921 - mae: 0.3197 - mean_pred: 0.8363 - mae_t1: 0.0213 - val_loss: 2.0036 - val_mae: 0.4293 - val_mean_pred: 0.8695 - val_mae_t1: 0.0286\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.5514 - mae: 0.3324 - mean_pred: 0.7057 - mae_t1: 0.0222 - val_loss: 2.0664 - val_mae: 0.4428 - val_mean_pred: 0.8311 - val_mae_t1: 0.0295\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 203us/sample - loss: 1.5082 - mae: 0.3232 - mean_pred: 0.8302 - mae_t1: 0.0215 - val_loss: 2.2587 - val_mae: 0.4840 - val_mean_pred: 1.0471 - val_mae_t1: 0.0323\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 214us/sample - loss: 1.5978 - mae: 0.3424 - mean_pred: 0.8795 - mae_t1: 0.0228 - val_loss: 2.1165 - val_mae: 0.4535 - val_mean_pred: 0.7925 - val_mae_t1: 0.0302\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.7268 - mae: 0.3700 - mean_pred: 0.7012 - mae_t1: 0.0247 - val_loss: 2.0707 - val_mae: 0.4437 - val_mean_pred: 0.7942 - val_mae_t1: 0.0296\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.6112 - mae: 0.3453 - mean_pred: 0.7727 - mae_t1: 0.0230 - val_loss: 2.2179 - val_mae: 0.4753 - val_mean_pred: 0.9381 - val_mae_t1: 0.0317\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.5356 - mae: 0.3291 - mean_pred: 0.7745 - mae_t1: 0.0219 - val_loss: 2.0818 - val_mae: 0.4461 - val_mean_pred: 0.8165 - val_mae_t1: 0.0297\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.4009 - mae: 0.3002 - mean_pred: 0.7263 - mae_t1: 0.0200 - val_loss: 2.1105 - val_mae: 0.4522 - val_mean_pred: 0.9412 - val_mae_t1: 0.0301\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.4774 - mae: 0.3166 - mean_pred: 0.8485 - mae_t1: 0.0211 - val_loss: 2.1404 - val_mae: 0.4586 - val_mean_pred: 0.9324 - val_mae_t1: 0.0306\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 193us/sample - loss: 1.4585 - mae: 0.3125 - mean_pred: 0.7826 - mae_t1: 0.0208 - val_loss: 2.0985 - val_mae: 0.4497 - val_mean_pred: 0.9229 - val_mae_t1: 0.0300\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.4191 - mae: 0.3041 - mean_pred: 0.8234 - mae_t1: 0.0203 - val_loss: 2.2003 - val_mae: 0.4715 - val_mean_pred: 0.9971 - val_mae_t1: 0.0314\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.4037 - mae: 0.3008 - mean_pred: 0.8408 - mae_t1: 0.0201 - val_loss: 2.0604 - val_mae: 0.4415 - val_mean_pred: 0.8198 - val_mae_t1: 0.0294\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 1.4720 - mae: 0.3154 - mean_pred: 0.6879 - mae_t1: 0.0210 - val_loss: 2.0926 - val_mae: 0.4484 - val_mean_pred: 0.8131 - val_mae_t1: 0.0299\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.5669 - mae: 0.3358 - mean_pred: 0.7777 - mae_t1: 0.0224 - val_loss: 2.0314 - val_mae: 0.4353 - val_mean_pred: 0.8590 - val_mae_t1: 0.0290\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.5068 - mae: 0.3229 - mean_pred: 0.6885 - mae_t1: 0.0215 - val_loss: 2.0189 - val_mae: 0.4326 - val_mean_pred: 0.8259 - val_mae_t1: 0.0288\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 1.3857 - mae: 0.2969 - mean_pred: 0.7706 - mae_t1: 0.0198 - val_loss: 1.9766 - val_mae: 0.4236 - val_mean_pred: 0.9787 - val_mae_t1: 0.0282\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.3706 - mae: 0.2937 - mean_pred: 0.8154 - mae_t1: 0.0196 - val_loss: 2.0215 - val_mae: 0.4332 - val_mean_pred: 0.7731 - val_mae_t1: 0.0289\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.5400 - mae: 0.3300 - mean_pred: 0.6492 - mae_t1: 0.0220 - val_loss: 2.0376 - val_mae: 0.4366 - val_mean_pred: 0.7839 - val_mae_t1: 0.0291\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.4232 - mae: 0.3050 - mean_pred: 0.7745 - mae_t1: 0.0203 - val_loss: 2.1122 - val_mae: 0.4526 - val_mean_pred: 0.9963 - val_mae_t1: 0.0302\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 235us/sample - loss: 1.3931 - mae: 0.2985 - mean_pred: 0.8676 - mae_t1: 0.0199 - val_loss: 1.9240 - val_mae: 0.4123 - val_mean_pred: 0.8585 - val_mae_t1: 0.0275\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.3539 - mae: 0.2901 - mean_pred: 0.7468 - mae_t1: 0.0193 - val_loss: 1.9347 - val_mae: 0.4146 - val_mean_pred: 0.8787 - val_mae_t1: 0.0276\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.3259 - mae: 0.2841 - mean_pred: 0.8189 - mae_t1: 0.0189 - val_loss: 1.9919 - val_mae: 0.4268 - val_mean_pred: 0.9014 - val_mae_t1: 0.0285\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.3233 - mae: 0.2836 - mean_pred: 0.8035 - mae_t1: 0.0189 - val_loss: 2.0400 - val_mae: 0.4372 - val_mean_pred: 0.8946 - val_mae_t1: 0.0291\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.3926 - mae: 0.2984 - mean_pred: 0.7954 - mae_t1: 0.0199 - val_loss: 2.0083 - val_mae: 0.4304 - val_mean_pred: 0.8770 - val_mae_t1: 0.0287\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 234us/sample - loss: 1.4253 - mae: 0.3054 - mean_pred: 0.7686 - mae_t1: 0.0204 - val_loss: 1.8826 - val_mae: 0.4034 - val_mean_pred: 0.9151 - val_mae_t1: 0.0269\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 1.4056 - mae: 0.3012 - mean_pred: 0.7690 - mae_t1: 0.0201 - val_loss: 1.8438 - val_mae: 0.3951 - val_mean_pred: 0.8486 - val_mae_t1: 0.0263\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.3092 - mae: 0.2805 - mean_pred: 0.7931 - mae_t1: 0.0187 - val_loss: 2.2523 - val_mae: 0.4826 - val_mean_pred: 1.0549 - val_mae_t1: 0.0322\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.5218 - mae: 0.3261 - mean_pred: 0.9340 - mae_t1: 0.0217 - val_loss: 1.9887 - val_mae: 0.4262 - val_mean_pred: 0.9300 - val_mae_t1: 0.0284\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.3082 - mae: 0.2803 - mean_pred: 0.7961 - mae_t1: 0.0187 - val_loss: 2.0365 - val_mae: 0.4364 - val_mean_pred: 0.8098 - val_mae_t1: 0.0291\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.4055 - mae: 0.3012 - mean_pred: 0.7279 - mae_t1: 0.0201 - val_loss: 2.0851 - val_mae: 0.4468 - val_mean_pred: 0.9378 - val_mae_t1: 0.0298\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.3824 - mae: 0.2962 - mean_pred: 0.8380 - mae_t1: 0.0197 - val_loss: 2.1152 - val_mae: 0.4533 - val_mean_pred: 0.9049 - val_mae_t1: 0.0302\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.3356 - mae: 0.2862 - mean_pred: 0.7796 - mae_t1: 0.0191 - val_loss: 2.0119 - val_mae: 0.4311 - val_mean_pred: 0.9096 - val_mae_t1: 0.0287\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 1.2532 - mae: 0.2685 - mean_pred: 0.8220 - mae_t1: 0.0179 - val_loss: 1.9465 - val_mae: 0.4171 - val_mean_pred: 0.9200 - val_mae_t1: 0.0278\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 1.2439 - mae: 0.2665 - mean_pred: 0.8215 - mae_t1: 0.0178 - val_loss: 1.9196 - val_mae: 0.4113 - val_mean_pred: 0.9222 - val_mae_t1: 0.0274\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.2507 - mae: 0.2680 - mean_pred: 0.8337 - mae_t1: 0.0179 - val_loss: 2.0445 - val_mae: 0.4381 - val_mean_pred: 0.8840 - val_mae_t1: 0.0292\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.3774 - mae: 0.2952 - mean_pred: 0.7571 - mae_t1: 0.0197 - val_loss: 1.9614 - val_mae: 0.4203 - val_mean_pred: 0.8629 - val_mae_t1: 0.0280\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.3259 - mae: 0.2841 - mean_pred: 0.7441 - mae_t1: 0.0189 - val_loss: 1.9283 - val_mae: 0.4132 - val_mean_pred: 0.8731 - val_mae_t1: 0.0275\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.3015 - mae: 0.2789 - mean_pred: 0.8222 - mae_t1: 0.0186 - val_loss: 2.0026 - val_mae: 0.4291 - val_mean_pred: 0.9699 - val_mae_t1: 0.0286\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.2985 - mae: 0.2782 - mean_pred: 0.8457 - mae_t1: 0.0185 - val_loss: 2.0148 - val_mae: 0.4317 - val_mean_pred: 0.8480 - val_mae_t1: 0.0288\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 201us/sample - loss: 1.2311 - mae: 0.2638 - mean_pred: 0.7590 - mae_t1: 0.0176 - val_loss: 2.0561 - val_mae: 0.4406 - val_mean_pred: 0.9616 - val_mae_t1: 0.0294\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 1.3082 - mae: 0.2803 - mean_pred: 0.8679 - mae_t1: 0.0187 - val_loss: 2.0085 - val_mae: 0.4304 - val_mean_pred: 0.9132 - val_mae_t1: 0.0287\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.2452 - mae: 0.2668 - mean_pred: 0.7885 - mae_t1: 0.0178 - val_loss: 2.0784 - val_mae: 0.4454 - val_mean_pred: 0.8428 - val_mae_t1: 0.0297\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 200us/sample - loss: 1.2271 - mae: 0.2629 - mean_pred: 0.7911 - mae_t1: 0.0175 - val_loss: 1.9898 - val_mae: 0.4264 - val_mean_pred: 0.9367 - val_mae_t1: 0.0284\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2424 - mae: 0.2662 - mean_pred: 0.8294 - mae_t1: 0.0177 - val_loss: 1.9453 - val_mae: 0.4169 - val_mean_pred: 0.8565 - val_mae_t1: 0.0278\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2970 - mae: 0.2779 - mean_pred: 0.7360 - mae_t1: 0.0185 - val_loss: 1.9356 - val_mae: 0.4148 - val_mean_pred: 0.8624 - val_mae_t1: 0.0277\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 1.2635 - mae: 0.2707 - mean_pred: 0.7996 - mae_t1: 0.0180 - val_loss: 2.0330 - val_mae: 0.4356 - val_mean_pred: 0.9994 - val_mae_t1: 0.0290\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 204us/sample - loss: 1.2757 - mae: 0.2734 - mean_pred: 0.8658 - mae_t1: 0.0182 - val_loss: 1.9696 - val_mae: 0.4220 - val_mean_pred: 0.7858 - val_mae_t1: 0.0281\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.5219 - mae: 0.3261 - mean_pred: 0.6880 - mae_t1: 0.0217 - val_loss: 2.1776 - val_mae: 0.4666 - val_mean_pred: 0.6850 - val_mae_t1: 0.0311\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.4227 - mae: 0.3049 - mean_pred: 0.7034 - mae_t1: 0.0203 - val_loss: 2.2276 - val_mae: 0.4773 - val_mean_pred: 1.0318 - val_mae_t1: 0.0318\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 1.6260 - mae: 0.3484 - mean_pred: 0.9605 - mae_t1: 0.0232 - val_loss: 2.2090 - val_mae: 0.4734 - val_mean_pred: 1.0301 - val_mae_t1: 0.0316\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 205us/sample - loss: 1.3789 - mae: 0.2955 - mean_pred: 0.8547 - mae_t1: 0.0197 - val_loss: 2.0460 - val_mae: 0.4384 - val_mean_pred: 0.8521 - val_mae_t1: 0.0292\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 1.3078 - mae: 0.2802 - mean_pred: 0.7754 - mae_t1: 0.0187 - val_loss: 1.9039 - val_mae: 0.4080 - val_mean_pred: 0.8881 - val_mae_t1: 0.0272\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 195us/sample - loss: 1.2680 - mae: 0.2717 - mean_pred: 0.8172 - mae_t1: 0.0181 - val_loss: 2.1095 - val_mae: 0.4520 - val_mean_pred: 0.9954 - val_mae_t1: 0.0301\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.3143 - mae: 0.2816 - mean_pred: 0.9073 - mae_t1: 0.0188 - val_loss: 1.9684 - val_mae: 0.4218 - val_mean_pred: 0.9394 - val_mae_t1: 0.0281\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 1.2068 - mae: 0.2586 - mean_pred: 0.8238 - mae_t1: 0.0172 - val_loss: 2.2464 - val_mae: 0.4814 - val_mean_pred: 0.8669 - val_mae_t1: 0.0321\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 1.4845 - mae: 0.3181 - mean_pred: 0.8130 - mae_t1: 0.0212 - val_loss: 2.0645 - val_mae: 0.4424 - val_mean_pred: 0.9198 - val_mae_t1: 0.0295\n",
      "Earliness...\n",
      "0.002499103546142578\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3368636293392665  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▂▂▁▂</td></tr><tr><td>mae</td><td>█▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▂▂▁▂</td></tr><tr><td>mae_t1</td><td>█▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▂▂▁▂</td></tr><tr><td>mean_pred</td><td>▁▄▆▇▆▆▇▆▅█▇▆▆▆▇▇▇▆▅▅▆▆▇▅▇▇▆▆█▆▇▇▇▆▆▆▅█▇▇</td></tr><tr><td>val_loss</td><td>▅█▄▄▆▅▅▄▃▅▅▄▂▃▃▂▅▄▃▃▃▃▃▂▂▁▂▁▂▃▂▂▂▃▂▁▄▄▃▃</td></tr><tr><td>val_mae</td><td>▅█▄▄▆▅▅▄▃▅▅▄▂▃▃▂▅▄▃▃▃▃▃▂▂▁▂▁▂▃▂▂▂▃▂▁▄▄▃▃</td></tr><tr><td>val_mae_t1</td><td>▅█▄▄▆▅▅▄▃▅▅▄▂▃▃▂▅▄▃▃▃▃▃▂▂▁▂▁▂▃▂▂▂▃▂▁▄▄▃▃</td></tr><tr><td>val_mean_pred</td><td>▇▁▅▆▇▅▅▅▅▅▇▇▇▆▆▆▆▇▅▅▅▇▅▅▅▆▆▇▇▇▇▆▇▇▇▆▄██▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.36545</td></tr><tr><td>AE_2</td><td>0.33121</td></tr><tr><td>AE_3</td><td>0.30647</td></tr><tr><td>MAE</td><td>0.33686</td></tr><tr><td>best_epoch</td><td>69</td></tr><tr><td>best_val_loss</td><td>1.84377</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.48455</td></tr><tr><td>mae</td><td>0.31812</td></tr><tr><td>mae_t1</td><td>0.02121</td></tr><tr><td>mean_pred</td><td>0.81296</td></tr><tr><td>val_loss</td><td>2.06449</td></tr><tr><td>val_mae</td><td>0.44239</td></tr><tr><td>val_mae_t1</td><td>0.02949</td></tr><tr><td>val_mean_pred</td><td>0.91982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">breezy-flower-95</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/b31sbbsf\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/b31sbbsf</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_155934-b31sbbsf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_160002-1out8vkr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1out8vkr\" target=\"_blank\">denim-deluge-96</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 698us/sample - loss: 4.1544 - mae: 0.8902 - mean_pred: 0.2054 - mae_t1: 0.0593 - val_loss: 2.8748 - val_mae: 0.6160 - val_mean_pred: 0.3901 - val_mae_t1: 0.0411\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 213us/sample - loss: 3.0957 - mae: 0.6634 - mean_pred: 0.4851 - mae_t1: 0.0442 - val_loss: 2.6913 - val_mae: 0.5767 - val_mean_pred: 0.9639 - val_mae_t1: 0.0384\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 142us/sample - loss: 3.3148 - mae: 0.7103 - mean_pred: 0.9655 - mae_t1: 0.0474 - val_loss: 3.0210 - val_mae: 0.6474 - val_mean_pred: 1.0110 - val_mae_t1: 0.0432\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 133us/sample - loss: 2.9606 - mae: 0.6344 - mean_pred: 0.8546 - mae_t1: 0.0423 - val_loss: 2.7607 - val_mae: 0.5916 - val_mean_pred: 0.5785 - val_mae_t1: 0.0394\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 2.7396 - mae: 0.5871 - mean_pred: 0.4597 - mae_t1: 0.0391 - val_loss: 2.9599 - val_mae: 0.6343 - val_mean_pred: 0.4971 - val_mae_t1: 0.0423\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 144us/sample - loss: 2.7029 - mae: 0.5792 - mean_pred: 0.4767 - mae_t1: 0.0386 - val_loss: 2.4714 - val_mae: 0.5296 - val_mean_pred: 0.8323 - val_mae_t1: 0.0353\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.4008 - mae: 0.5145 - mean_pred: 0.8372 - mae_t1: 0.0343 - val_loss: 2.6149 - val_mae: 0.5603 - val_mean_pred: 1.0331 - val_mae_t1: 0.0374\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 132us/sample - loss: 2.5024 - mae: 0.5362 - mean_pred: 0.9279 - mae_t1: 0.0357 - val_loss: 2.3898 - val_mae: 0.5121 - val_mean_pred: 0.7361 - val_mae_t1: 0.0341\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 2.3314 - mae: 0.4996 - mean_pred: 0.6245 - mae_t1: 0.0333 - val_loss: 2.7780 - val_mae: 0.5953 - val_mean_pred: 0.5218 - val_mae_t1: 0.0397\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.6911 - mae: 0.5767 - mean_pred: 0.4670 - mae_t1: 0.0384 - val_loss: 2.6972 - val_mae: 0.5780 - val_mean_pred: 0.5588 - val_mae_t1: 0.0385\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.5548 - mae: 0.5475 - mean_pred: 0.5019 - mae_t1: 0.0365 - val_loss: 2.5041 - val_mae: 0.5366 - val_mean_pred: 0.8109 - val_mae_t1: 0.0358\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.4150 - mae: 0.5175 - mean_pred: 0.7569 - mae_t1: 0.0345 - val_loss: 2.8788 - val_mae: 0.6169 - val_mean_pred: 1.1071 - val_mae_t1: 0.0411\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.6763 - mae: 0.5735 - mean_pred: 0.9833 - mae_t1: 0.0382 - val_loss: 2.7893 - val_mae: 0.5977 - val_mean_pred: 1.1063 - val_mae_t1: 0.0398\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 2.5065 - mae: 0.5371 - mean_pred: 0.9461 - mae_t1: 0.0358 - val_loss: 2.3126 - val_mae: 0.4956 - val_mean_pred: 0.9128 - val_mae_t1: 0.0330\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.1667 - mae: 0.4643 - mean_pred: 0.7786 - mae_t1: 0.0310 - val_loss: 2.4068 - val_mae: 0.5157 - val_mean_pred: 0.8138 - val_mae_t1: 0.0344\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.2410 - mae: 0.4802 - mean_pred: 0.7367 - mae_t1: 0.0320 - val_loss: 2.3593 - val_mae: 0.5056 - val_mean_pred: 0.9135 - val_mae_t1: 0.0337\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.1464 - mae: 0.4599 - mean_pred: 0.8711 - mae_t1: 0.0307 - val_loss: 2.5316 - val_mae: 0.5425 - val_mean_pred: 0.9987 - val_mae_t1: 0.0362\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.2769 - mae: 0.4879 - mean_pred: 0.9175 - mae_t1: 0.0325 - val_loss: 2.4230 - val_mae: 0.5192 - val_mean_pred: 0.8323 - val_mae_t1: 0.0346\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.1340 - mae: 0.4573 - mean_pred: 0.7503 - mae_t1: 0.0305 - val_loss: 2.4335 - val_mae: 0.5215 - val_mean_pred: 0.6968 - val_mae_t1: 0.0348\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 2.1033 - mae: 0.4507 - mean_pred: 0.6630 - mae_t1: 0.0300 - val_loss: 2.3082 - val_mae: 0.4946 - val_mean_pred: 0.7720 - val_mae_t1: 0.0330\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 1.9663 - mae: 0.4213 - mean_pred: 0.7471 - mae_t1: 0.0281 - val_loss: 2.2405 - val_mae: 0.4801 - val_mean_pred: 0.8790 - val_mae_t1: 0.0320\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.9394 - mae: 0.4156 - mean_pred: 0.8121 - mae_t1: 0.0277 - val_loss: 2.2466 - val_mae: 0.4814 - val_mean_pred: 0.8185 - val_mae_t1: 0.0321\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.9439 - mae: 0.4166 - mean_pred: 0.7331 - mae_t1: 0.0278 - val_loss: 2.3276 - val_mae: 0.4988 - val_mean_pred: 0.7334 - val_mae_t1: 0.0333\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0076 - mae: 0.4302 - mean_pred: 0.6594 - mae_t1: 0.0287 - val_loss: 2.2600 - val_mae: 0.4843 - val_mean_pred: 0.7607 - val_mae_t1: 0.0323\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 1.9627 - mae: 0.4206 - mean_pred: 0.6975 - mae_t1: 0.0280 - val_loss: 2.1970 - val_mae: 0.4708 - val_mean_pred: 0.8715 - val_mae_t1: 0.0314\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.9597 - mae: 0.4199 - mean_pred: 0.7922 - mae_t1: 0.0280 - val_loss: 2.2272 - val_mae: 0.4772 - val_mean_pred: 0.8619 - val_mae_t1: 0.0318\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9223 - mae: 0.4119 - mean_pred: 0.7347 - mae_t1: 0.0275 - val_loss: 2.3944 - val_mae: 0.5131 - val_mean_pred: 0.7685 - val_mae_t1: 0.0342\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.1154 - mae: 0.4533 - mean_pred: 0.6745 - mae_t1: 0.0302 - val_loss: 2.4360 - val_mae: 0.5220 - val_mean_pred: 0.7861 - val_mae_t1: 0.0348\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.0034 - mae: 0.4293 - mean_pred: 0.6792 - mae_t1: 0.0286 - val_loss: 2.3122 - val_mae: 0.4955 - val_mean_pred: 0.7920 - val_mae_t1: 0.0330\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.9114 - mae: 0.4096 - mean_pred: 0.6973 - mae_t1: 0.0273 - val_loss: 2.2539 - val_mae: 0.4830 - val_mean_pred: 0.8188 - val_mae_t1: 0.0322\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.8694 - mae: 0.4006 - mean_pred: 0.7364 - mae_t1: 0.0267 - val_loss: 2.3142 - val_mae: 0.4959 - val_mean_pred: 0.7793 - val_mae_t1: 0.0331\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.9718 - mae: 0.4225 - mean_pred: 0.7109 - mae_t1: 0.0282 - val_loss: 2.3009 - val_mae: 0.4931 - val_mean_pred: 0.7161 - val_mae_t1: 0.0329\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 1.9490 - mae: 0.4176 - mean_pred: 0.6750 - mae_t1: 0.0278 - val_loss: 2.1916 - val_mae: 0.4696 - val_mean_pred: 0.8059 - val_mae_t1: 0.0313\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.8665 - mae: 0.4000 - mean_pred: 0.7916 - mae_t1: 0.0267 - val_loss: 2.2878 - val_mae: 0.4902 - val_mean_pred: 0.9571 - val_mae_t1: 0.0327\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 2.0274 - mae: 0.4344 - mean_pred: 0.8882 - mae_t1: 0.0290 - val_loss: 2.1798 - val_mae: 0.4671 - val_mean_pred: 0.8674 - val_mae_t1: 0.0311\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8381 - mae: 0.3939 - mean_pred: 0.7920 - mae_t1: 0.0263 - val_loss: 2.2749 - val_mae: 0.4875 - val_mean_pred: 0.7249 - val_mae_t1: 0.0325\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0649 - mae: 0.4425 - mean_pred: 0.6817 - mae_t1: 0.0295 - val_loss: 2.3392 - val_mae: 0.5013 - val_mean_pred: 0.6789 - val_mae_t1: 0.0334\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.9783 - mae: 0.4239 - mean_pred: 0.6586 - mae_t1: 0.0283 - val_loss: 2.1945 - val_mae: 0.4703 - val_mean_pred: 0.8020 - val_mae_t1: 0.0314\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8321 - mae: 0.3926 - mean_pred: 0.7542 - mae_t1: 0.0262 - val_loss: 2.2141 - val_mae: 0.4744 - val_mean_pred: 0.9124 - val_mae_t1: 0.0316\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.9432 - mae: 0.4164 - mean_pred: 0.8457 - mae_t1: 0.0278 - val_loss: 2.1897 - val_mae: 0.4692 - val_mean_pred: 0.8739 - val_mae_t1: 0.0313\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.8251 - mae: 0.3911 - mean_pred: 0.7820 - mae_t1: 0.0261 - val_loss: 2.3311 - val_mae: 0.4995 - val_mean_pred: 0.7365 - val_mae_t1: 0.0333\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.8389 - mae: 0.3940 - mean_pred: 0.6852 - mae_t1: 0.0263 - val_loss: 2.3274 - val_mae: 0.4987 - val_mean_pred: 0.7620 - val_mae_t1: 0.0332\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7912 - mae: 0.3838 - mean_pred: 0.7377 - mae_t1: 0.0256 - val_loss: 2.2603 - val_mae: 0.4844 - val_mean_pred: 0.8568 - val_mae_t1: 0.0323\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 1.8685 - mae: 0.4004 - mean_pred: 0.7940 - mae_t1: 0.0267 - val_loss: 2.2664 - val_mae: 0.4857 - val_mean_pred: 0.7942 - val_mae_t1: 0.0324\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.8387 - mae: 0.3940 - mean_pred: 0.7506 - mae_t1: 0.0263 - val_loss: 2.2698 - val_mae: 0.4864 - val_mean_pred: 0.7830 - val_mae_t1: 0.0324\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9943 - mae: 0.4273 - mean_pred: 0.7781 - mae_t1: 0.0285 - val_loss: 2.2628 - val_mae: 0.4849 - val_mean_pred: 0.8549 - val_mae_t1: 0.0323\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 123us/sample - loss: 1.9408 - mae: 0.4159 - mean_pred: 0.8307 - mae_t1: 0.0277 - val_loss: 2.1550 - val_mae: 0.4618 - val_mean_pred: 0.9023 - val_mae_t1: 0.0308\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7863 - mae: 0.3828 - mean_pred: 0.8455 - mae_t1: 0.0255 - val_loss: 2.1709 - val_mae: 0.4652 - val_mean_pred: 0.9129 - val_mae_t1: 0.0310\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7649 - mae: 0.3782 - mean_pred: 0.8250 - mae_t1: 0.0252 - val_loss: 2.2491 - val_mae: 0.4820 - val_mean_pred: 0.9058 - val_mae_t1: 0.0321\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.7717 - mae: 0.3796 - mean_pred: 0.7979 - mae_t1: 0.0253 - val_loss: 2.3472 - val_mae: 0.5030 - val_mean_pred: 0.7934 - val_mae_t1: 0.0335\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.8609 - mae: 0.3988 - mean_pred: 0.6674 - mae_t1: 0.0266 - val_loss: 2.3846 - val_mae: 0.5110 - val_mean_pred: 0.7068 - val_mae_t1: 0.0341\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 1.8967 - mae: 0.4064 - mean_pred: 0.6375 - mae_t1: 0.0271 - val_loss: 2.1489 - val_mae: 0.4605 - val_mean_pred: 0.8098 - val_mae_t1: 0.0307\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 1.7706 - mae: 0.3794 - mean_pred: 0.7200 - mae_t1: 0.0253 - val_loss: 2.0521 - val_mae: 0.4397 - val_mean_pred: 0.8368 - val_mae_t1: 0.0293\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7779 - mae: 0.3810 - mean_pred: 0.7329 - mae_t1: 0.0254 - val_loss: 2.1095 - val_mae: 0.4520 - val_mean_pred: 0.8559 - val_mae_t1: 0.0301\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.8602 - mae: 0.3986 - mean_pred: 0.7760 - mae_t1: 0.0266 - val_loss: 2.2337 - val_mae: 0.4786 - val_mean_pred: 0.9276 - val_mae_t1: 0.0319\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9364 - mae: 0.4149 - mean_pred: 0.8154 - mae_t1: 0.0277 - val_loss: 2.0983 - val_mae: 0.4496 - val_mean_pred: 0.8677 - val_mae_t1: 0.0300\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7300 - mae: 0.3707 - mean_pred: 0.7583 - mae_t1: 0.0247 - val_loss: 2.3979 - val_mae: 0.5138 - val_mean_pred: 0.8531 - val_mae_t1: 0.0343\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.9908 - mae: 0.4266 - mean_pred: 0.8008 - mae_t1: 0.0284 - val_loss: 2.2641 - val_mae: 0.4852 - val_mean_pred: 0.8994 - val_mae_t1: 0.0323\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7949 - mae: 0.3846 - mean_pred: 0.8584 - mae_t1: 0.0256 - val_loss: 2.2499 - val_mae: 0.4821 - val_mean_pred: 0.9121 - val_mae_t1: 0.0321\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.9531 - mae: 0.4185 - mean_pred: 0.8927 - mae_t1: 0.0279 - val_loss: 2.2507 - val_mae: 0.4823 - val_mean_pred: 0.9080 - val_mae_t1: 0.0322\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8488 - mae: 0.3962 - mean_pred: 0.8756 - mae_t1: 0.0264 - val_loss: 2.1997 - val_mae: 0.4714 - val_mean_pred: 0.8153 - val_mae_t1: 0.0314\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.7968 - mae: 0.3850 - mean_pred: 0.7831 - mae_t1: 0.0257 - val_loss: 2.2793 - val_mae: 0.4884 - val_mean_pred: 0.7214 - val_mae_t1: 0.0326\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.8736 - mae: 0.4015 - mean_pred: 0.6870 - mae_t1: 0.0268 - val_loss: 2.3186 - val_mae: 0.4968 - val_mean_pred: 0.7468 - val_mae_t1: 0.0331\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.8692 - mae: 0.4005 - mean_pred: 0.7230 - mae_t1: 0.0267 - val_loss: 2.0831 - val_mae: 0.4464 - val_mean_pred: 0.8256 - val_mae_t1: 0.0298\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 100us/sample - loss: 1.7400 - mae: 0.3729 - mean_pred: 0.7713 - mae_t1: 0.0249 - val_loss: 2.1097 - val_mae: 0.4521 - val_mean_pred: 0.8244 - val_mae_t1: 0.0301\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7587 - mae: 0.3769 - mean_pred: 0.7673 - mae_t1: 0.0251 - val_loss: 2.0708 - val_mae: 0.4437 - val_mean_pred: 0.8934 - val_mae_t1: 0.0296\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7142 - mae: 0.3673 - mean_pred: 0.8343 - mae_t1: 0.0245 - val_loss: 2.0889 - val_mae: 0.4476 - val_mean_pred: 0.9286 - val_mae_t1: 0.0298\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6836 - mae: 0.3608 - mean_pred: 0.8265 - mae_t1: 0.0241 - val_loss: 2.1181 - val_mae: 0.4539 - val_mean_pred: 0.8266 - val_mae_t1: 0.0303\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6348 - mae: 0.3503 - mean_pred: 0.7351 - mae_t1: 0.0234 - val_loss: 2.1731 - val_mae: 0.4657 - val_mean_pred: 0.8600 - val_mae_t1: 0.0310\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.6185 - mae: 0.3468 - mean_pred: 0.7829 - mae_t1: 0.0231 - val_loss: 2.3095 - val_mae: 0.4949 - val_mean_pred: 0.9591 - val_mae_t1: 0.0330\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 1.7040 - mae: 0.3651 - mean_pred: 0.8368 - mae_t1: 0.0243 - val_loss: 2.2469 - val_mae: 0.4815 - val_mean_pred: 0.8889 - val_mae_t1: 0.0321\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6778 - mae: 0.3595 - mean_pred: 0.7632 - mae_t1: 0.0240 - val_loss: 2.2091 - val_mae: 0.4734 - val_mean_pred: 0.8004 - val_mae_t1: 0.0316\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6856 - mae: 0.3612 - mean_pred: 0.7169 - mae_t1: 0.0241 - val_loss: 2.0611 - val_mae: 0.4417 - val_mean_pred: 0.8495 - val_mae_t1: 0.0294\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7362 - mae: 0.3720 - mean_pred: 0.8085 - mae_t1: 0.0248 - val_loss: 2.0624 - val_mae: 0.4420 - val_mean_pred: 0.9870 - val_mae_t1: 0.0295\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8439 - mae: 0.3951 - mean_pred: 0.9254 - mae_t1: 0.0263 - val_loss: 2.0682 - val_mae: 0.4432 - val_mean_pred: 0.9436 - val_mae_t1: 0.0295\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.7520 - mae: 0.3754 - mean_pred: 0.8256 - mae_t1: 0.0250 - val_loss: 2.3232 - val_mae: 0.4978 - val_mean_pred: 0.7520 - val_mae_t1: 0.0332\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.0038 - mae: 0.4294 - mean_pred: 0.6651 - mae_t1: 0.0286 - val_loss: 2.2100 - val_mae: 0.4736 - val_mean_pred: 0.7243 - val_mae_t1: 0.0316\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 1.8040 - mae: 0.3866 - mean_pred: 0.6529 - mae_t1: 0.0258 - val_loss: 2.1529 - val_mae: 0.4613 - val_mean_pred: 0.7920 - val_mae_t1: 0.0308\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8200 - mae: 0.3900 - mean_pred: 0.6952 - mae_t1: 0.0260 - val_loss: 2.1232 - val_mae: 0.4550 - val_mean_pred: 0.7948 - val_mae_t1: 0.0303\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7441 - mae: 0.3737 - mean_pred: 0.7019 - mae_t1: 0.0249 - val_loss: 2.1901 - val_mae: 0.4693 - val_mean_pred: 0.8609 - val_mae_t1: 0.0313\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 1.7575 - mae: 0.3766 - mean_pred: 0.7531 - mae_t1: 0.0251 - val_loss: 2.0351 - val_mae: 0.4361 - val_mean_pred: 0.9123 - val_mae_t1: 0.0291\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6729 - mae: 0.3585 - mean_pred: 0.8159 - mae_t1: 0.0239 - val_loss: 2.1667 - val_mae: 0.4643 - val_mean_pred: 0.9076 - val_mae_t1: 0.0310\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 116us/sample - loss: 1.8611 - mae: 0.3988 - mean_pred: 0.8089 - mae_t1: 0.0266 - val_loss: 1.9658 - val_mae: 0.4212 - val_mean_pred: 0.7869 - val_mae_t1: 0.0281\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.6816 - mae: 0.3604 - mean_pred: 0.6966 - mae_t1: 0.0240 - val_loss: 2.4139 - val_mae: 0.5173 - val_mean_pred: 0.7841 - val_mae_t1: 0.0345\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.9915 - mae: 0.4268 - mean_pred: 0.7348 - mae_t1: 0.0285 - val_loss: 2.3019 - val_mae: 0.4933 - val_mean_pred: 0.9179 - val_mae_t1: 0.0329\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8046 - mae: 0.3867 - mean_pred: 0.8600 - mae_t1: 0.0258 - val_loss: 2.1959 - val_mae: 0.4706 - val_mean_pred: 0.9460 - val_mae_t1: 0.0314\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.7694 - mae: 0.3792 - mean_pred: 0.8626 - mae_t1: 0.0253 - val_loss: 2.3540 - val_mae: 0.5044 - val_mean_pred: 0.8613 - val_mae_t1: 0.0336\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 1.7499 - mae: 0.3750 - mean_pred: 0.7815 - mae_t1: 0.0250 - val_loss: 2.3462 - val_mae: 0.5027 - val_mean_pred: 0.8583 - val_mae_t1: 0.0335\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.6731 - mae: 0.3585 - mean_pred: 0.7851 - mae_t1: 0.0239 - val_loss: 2.3308 - val_mae: 0.4995 - val_mean_pred: 0.9356 - val_mae_t1: 0.0333\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6559 - mae: 0.3548 - mean_pred: 0.8388 - mae_t1: 0.0237 - val_loss: 2.2290 - val_mae: 0.4776 - val_mean_pred: 0.9250 - val_mae_t1: 0.0318\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.5786 - mae: 0.3383 - mean_pred: 0.8109 - mae_t1: 0.0226 - val_loss: 2.1751 - val_mae: 0.4661 - val_mean_pred: 0.8241 - val_mae_t1: 0.0311\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 1.7355 - mae: 0.3719 - mean_pred: 0.7445 - mae_t1: 0.0248 - val_loss: 2.1689 - val_mae: 0.4648 - val_mean_pred: 0.7461 - val_mae_t1: 0.0310\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.7576 - mae: 0.3766 - mean_pred: 0.6928 - mae_t1: 0.0251 - val_loss: 2.1039 - val_mae: 0.4508 - val_mean_pred: 0.7779 - val_mae_t1: 0.0301\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 1.6230 - mae: 0.3478 - mean_pred: 0.7374 - mae_t1: 0.0232 - val_loss: 2.3239 - val_mae: 0.4980 - val_mean_pred: 0.9937 - val_mae_t1: 0.0332\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.8657 - mae: 0.3998 - mean_pred: 0.9443 - mae_t1: 0.0267 - val_loss: 2.6342 - val_mae: 0.5645 - val_mean_pred: 1.1278 - val_mae_t1: 0.0376\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.9784 - mae: 0.4239 - mean_pred: 1.0108 - mae_t1: 0.0283 - val_loss: 2.0257 - val_mae: 0.4341 - val_mean_pred: 0.9257 - val_mae_t1: 0.0289\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 1.6186 - mae: 0.3468 - mean_pred: 0.8133 - mae_t1: 0.0231 - val_loss: 2.0434 - val_mae: 0.4379 - val_mean_pred: 0.7642 - val_mae_t1: 0.0292\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 1.6522 - mae: 0.3540 - mean_pred: 0.6996 - mae_t1: 0.0236 - val_loss: 2.0237 - val_mae: 0.4337 - val_mean_pred: 0.8154 - val_mae_t1: 0.0289\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 1.5561 - mae: 0.3334 - mean_pred: 0.7477 - mae_t1: 0.0222 - val_loss: 1.9934 - val_mae: 0.4272 - val_mean_pred: 0.9033 - val_mae_t1: 0.0285\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 1.4872 - mae: 0.3187 - mean_pred: 0.8128 - mae_t1: 0.0212 - val_loss: 1.9610 - val_mae: 0.4202 - val_mean_pred: 0.8652 - val_mae_t1: 0.0280\n",
      "Earliness...\n",
      "0.002000570297241211\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3009204431351633  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▄▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▁▁▁</td></tr><tr><td>mae</td><td>█▆▄▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▁▁▁</td></tr><tr><td>mae_t1</td><td>█▆▄▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▁▁▁</td></tr><tr><td>mean_pred</td><td>▁█▃█▄█▆▇▆▆▆▅▆▆▆▆▆▆▆▇▅▆▆▇▇▆▇▆▆▆▅▅▆▅▇▆▆▆▆▆</td></tr><tr><td>val_loss</td><td>▇█▄▄▅▆▄▄▃▃▃▄▃▃▃▃▃▃▃▃▄▂▂▃▃▂▂▂▃▂▃▂▂▄▄▃▂▃▂▁</td></tr><tr><td>val_mae</td><td>▇█▄▄▅▆▄▄▃▃▃▄▃▃▃▃▃▃▃▃▄▂▂▃▃▂▂▂▃▂▃▂▂▄▄▃▂▃▂▁</td></tr><tr><td>val_mae_t1</td><td>▇█▄▄▅▆▄▄▃▃▃▄▃▃▃▃▃▃▃▃▄▂▂▃▃▂▂▂▃▂▃▂▂▄▄▃▂▃▂▁</td></tr><tr><td>val_mean_pred</td><td>▁▇▅▄▅█▆▅▆▄▆▅▅▇▄▆▄▅▆▆▄▆▆▆▅▅▆▆▅▇▄▅▆▅▆▆▄▇▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.35304</td></tr><tr><td>AE_2</td><td>0.30107</td></tr><tr><td>AE_3</td><td>0.2728</td></tr><tr><td>MAE</td><td>0.30092</td></tr><tr><td>best_epoch</td><td>99</td></tr><tr><td>best_val_loss</td><td>1.96098</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.48715</td></tr><tr><td>mae</td><td>0.31868</td></tr><tr><td>mae_t1</td><td>0.02125</td></tr><tr><td>mean_pred</td><td>0.81281</td></tr><tr><td>val_loss</td><td>1.96098</td></tr><tr><td>val_mae</td><td>0.42021</td></tr><tr><td>val_mae_t1</td><td>0.02801</td></tr><tr><td>val_mean_pred</td><td>0.86519</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">denim-deluge-96</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1out8vkr\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1out8vkr</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_160002-1out8vkr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_160027-2sd967md</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sd967md\" target=\"_blank\">scarlet-sun-97</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 878us/sample - loss: 4.4492 - mae: 0.7025 - mean_pred: 0.3451 - mae_t1: 0.0468 - val_loss: 3.3749 - val_mae: 0.5329 - val_mean_pred: 0.9905 - val_mae_t1: 0.0355\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 343us/sample - loss: 4.0054 - mae: 0.6324 - mean_pred: 0.9433 - mae_t1: 0.0422 - val_loss: 2.7311 - val_mae: 0.4312 - val_mean_pred: 0.8064 - val_mae_t1: 0.0287\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 247us/sample - loss: 3.3466 - mae: 0.5284 - mean_pred: 0.6881 - mae_t1: 0.0352 - val_loss: 3.0155 - val_mae: 0.4761 - val_mean_pred: 0.7355 - val_mae_t1: 0.0317\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 3.1218 - mae: 0.4929 - mean_pred: 0.6493 - mae_t1: 0.0329 - val_loss: 3.1774 - val_mae: 0.5017 - val_mean_pred: 0.7734 - val_mae_t1: 0.0334\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 197us/sample - loss: 2.9598 - mae: 0.4673 - mean_pred: 0.7219 - mae_t1: 0.0312 - val_loss: 3.0099 - val_mae: 0.4753 - val_mean_pred: 0.8963 - val_mae_t1: 0.0317\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.8961 - mae: 0.4573 - mean_pred: 0.8353 - mae_t1: 0.0305 - val_loss: 3.1899 - val_mae: 0.5037 - val_mean_pred: 0.7480 - val_mae_t1: 0.0336\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.8305 - mae: 0.4469 - mean_pred: 0.6783 - mae_t1: 0.0298 - val_loss: 3.2853 - val_mae: 0.5187 - val_mean_pred: 0.7563 - val_mae_t1: 0.0346\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.8537 - mae: 0.4506 - mean_pred: 0.6945 - mae_t1: 0.0300 - val_loss: 3.2265 - val_mae: 0.5094 - val_mean_pred: 0.7682 - val_mae_t1: 0.0340\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.9498 - mae: 0.4658 - mean_pred: 0.6210 - mae_t1: 0.0311 - val_loss: 2.8344 - val_mae: 0.4475 - val_mean_pred: 0.7980 - val_mae_t1: 0.0298\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.8724 - mae: 0.4535 - mean_pred: 0.8787 - mae_t1: 0.0302 - val_loss: 2.8549 - val_mae: 0.4508 - val_mean_pred: 0.7715 - val_mae_t1: 0.0301\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0011 - mae: 0.4739 - mean_pred: 0.6068 - mae_t1: 0.0316 - val_loss: 3.3668 - val_mae: 0.5316 - val_mean_pred: 0.6216 - val_mae_t1: 0.0354\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.7698 - mae: 0.4373 - mean_pred: 0.6917 - mae_t1: 0.0292 - val_loss: 3.3296 - val_mae: 0.5257 - val_mean_pred: 1.0105 - val_mae_t1: 0.0350\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 3.0994 - mae: 0.4894 - mean_pred: 0.9440 - mae_t1: 0.0326 - val_loss: 3.3650 - val_mae: 0.5313 - val_mean_pred: 0.9364 - val_mae_t1: 0.0354\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8554 - mae: 0.4509 - mean_pred: 0.8044 - mae_t1: 0.0301 - val_loss: 3.4824 - val_mae: 0.5499 - val_mean_pred: 0.6146 - val_mae_t1: 0.0367\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.8788 - mae: 0.4546 - mean_pred: 0.5823 - mae_t1: 0.0303 - val_loss: 3.0973 - val_mae: 0.4890 - val_mean_pred: 0.7901 - val_mae_t1: 0.0326\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.6919 - mae: 0.4250 - mean_pred: 0.8582 - mae_t1: 0.0283 - val_loss: 2.9399 - val_mae: 0.4642 - val_mean_pred: 0.9193 - val_mae_t1: 0.0309\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.7050 - mae: 0.4271 - mean_pred: 0.7363 - mae_t1: 0.0285 - val_loss: 3.2285 - val_mae: 0.5098 - val_mean_pred: 0.7504 - val_mae_t1: 0.0340\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.8670 - mae: 0.4527 - mean_pred: 0.7605 - mae_t1: 0.0302 - val_loss: 3.1736 - val_mae: 0.5011 - val_mean_pred: 0.8118 - val_mae_t1: 0.0334\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.6300 - mae: 0.4153 - mean_pred: 0.7109 - mae_t1: 0.0277 - val_loss: 3.1314 - val_mae: 0.4944 - val_mean_pred: 0.8567 - val_mae_t1: 0.0330\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.5248 - mae: 0.3987 - mean_pred: 0.7977 - mae_t1: 0.0266 - val_loss: 3.1317 - val_mae: 0.4945 - val_mean_pred: 0.9062 - val_mae_t1: 0.0330\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.3409 - mae: 0.3696 - mean_pred: 0.7911 - mae_t1: 0.0246 - val_loss: 3.1197 - val_mae: 0.4926 - val_mean_pred: 0.8349 - val_mae_t1: 0.0328\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.4118 - mae: 0.3808 - mean_pred: 0.6899 - mae_t1: 0.0254 - val_loss: 2.9788 - val_mae: 0.4703 - val_mean_pred: 0.8760 - val_mae_t1: 0.0314\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3873 - mae: 0.3769 - mean_pred: 0.8007 - mae_t1: 0.0251 - val_loss: 2.9400 - val_mae: 0.4642 - val_mean_pred: 0.9370 - val_mae_t1: 0.0309\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 211us/sample - loss: 2.4052 - mae: 0.3798 - mean_pred: 0.7799 - mae_t1: 0.0253 - val_loss: 2.7059 - val_mae: 0.4272 - val_mean_pred: 0.8621 - val_mae_t1: 0.0285\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.3396 - mae: 0.3694 - mean_pred: 0.7822 - mae_t1: 0.0246 - val_loss: 2.6423 - val_mae: 0.4172 - val_mean_pred: 0.8822 - val_mae_t1: 0.0278\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2051 - mae: 0.3482 - mean_pred: 0.7643 - mae_t1: 0.0232 - val_loss: 2.7036 - val_mae: 0.4269 - val_mean_pred: 0.8597 - val_mae_t1: 0.0285\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2005 - mae: 0.3474 - mean_pred: 0.8025 - mae_t1: 0.0232 - val_loss: 2.9313 - val_mae: 0.4628 - val_mean_pred: 0.9293 - val_mae_t1: 0.0309\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.3518 - mae: 0.3713 - mean_pred: 0.8201 - mae_t1: 0.0248 - val_loss: 2.9798 - val_mae: 0.4705 - val_mean_pred: 0.7579 - val_mae_t1: 0.0314\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.7578 - mae: 0.4354 - mean_pred: 0.6773 - mae_t1: 0.0290 - val_loss: 2.9955 - val_mae: 0.4730 - val_mean_pred: 0.6848 - val_mae_t1: 0.0315\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.4796 - mae: 0.3915 - mean_pred: 0.7739 - mae_t1: 0.0261 - val_loss: 2.8405 - val_mae: 0.4485 - val_mean_pred: 0.9337 - val_mae_t1: 0.0299\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.2908 - mae: 0.3617 - mean_pred: 0.7767 - mae_t1: 0.0241 - val_loss: 2.8658 - val_mae: 0.4525 - val_mean_pred: 0.8354 - val_mae_t1: 0.0302\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3668 - mae: 0.3737 - mean_pred: 0.7399 - mae_t1: 0.0249 - val_loss: 3.1554 - val_mae: 0.4982 - val_mean_pred: 0.8217 - val_mae_t1: 0.0332\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.7025 - mae: 0.4267 - mean_pred: 0.7152 - mae_t1: 0.0284 - val_loss: 2.8579 - val_mae: 0.4513 - val_mean_pred: 0.9387 - val_mae_t1: 0.0301\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.3017 - mae: 0.3634 - mean_pred: 0.9026 - mae_t1: 0.0242 - val_loss: 2.9630 - val_mae: 0.4678 - val_mean_pred: 0.8907 - val_mae_t1: 0.0312\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.4275 - mae: 0.3833 - mean_pred: 0.8024 - mae_t1: 0.0256 - val_loss: 3.0367 - val_mae: 0.4795 - val_mean_pred: 0.7790 - val_mae_t1: 0.0320\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1112 - mae: 0.3333 - mean_pred: 0.7930 - mae_t1: 0.0222 - val_loss: 2.8914 - val_mae: 0.4565 - val_mean_pred: 0.9652 - val_mae_t1: 0.0304\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1487 - mae: 0.3393 - mean_pred: 0.8155 - mae_t1: 0.0226 - val_loss: 2.9130 - val_mae: 0.4600 - val_mean_pred: 0.8212 - val_mae_t1: 0.0307\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1592 - mae: 0.3409 - mean_pred: 0.7172 - mae_t1: 0.0227 - val_loss: 2.7861 - val_mae: 0.4399 - val_mean_pred: 0.9778 - val_mae_t1: 0.0293\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.3176 - mae: 0.3659 - mean_pred: 0.8884 - mae_t1: 0.0244 - val_loss: 2.8441 - val_mae: 0.4491 - val_mean_pred: 0.8307 - val_mae_t1: 0.0299\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.3707 - mae: 0.3743 - mean_pred: 0.6462 - mae_t1: 0.0250 - val_loss: 2.9828 - val_mae: 0.4710 - val_mean_pred: 0.8215 - val_mae_t1: 0.0314\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2184 - mae: 0.3503 - mean_pred: 0.8048 - mae_t1: 0.0234 - val_loss: 3.0077 - val_mae: 0.4749 - val_mean_pred: 0.9420 - val_mae_t1: 0.0317\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.2216 - mae: 0.3508 - mean_pred: 0.7380 - mae_t1: 0.0234 - val_loss: 3.1202 - val_mae: 0.4927 - val_mean_pred: 0.7889 - val_mae_t1: 0.0328\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.5295 - mae: 0.3994 - mean_pred: 0.7345 - mae_t1: 0.0266 - val_loss: 3.1102 - val_mae: 0.4911 - val_mean_pred: 0.7830 - val_mae_t1: 0.0327\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.7267 - mae: 0.4305 - mean_pred: 0.6262 - mae_t1: 0.0287 - val_loss: 3.0020 - val_mae: 0.4740 - val_mean_pred: 0.8046 - val_mae_t1: 0.0316\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.6989 - mae: 0.4261 - mean_pred: 0.8254 - mae_t1: 0.0284 - val_loss: 3.1570 - val_mae: 0.4985 - val_mean_pred: 1.0198 - val_mae_t1: 0.0332\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.3199 - mae: 0.3663 - mean_pred: 0.8421 - mae_t1: 0.0244 - val_loss: 3.1802 - val_mae: 0.5021 - val_mean_pred: 0.7971 - val_mae_t1: 0.0335\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.4129 - mae: 0.3810 - mean_pred: 0.6691 - mae_t1: 0.0254 - val_loss: 2.7078 - val_mae: 0.4275 - val_mean_pred: 0.8585 - val_mae_t1: 0.0285\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.6848 - mae: 0.4239 - mean_pred: 0.8355 - mae_t1: 0.0283 - val_loss: 3.1780 - val_mae: 0.5018 - val_mean_pred: 1.0032 - val_mae_t1: 0.0335\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.7622 - mae: 0.4361 - mean_pred: 0.8869 - mae_t1: 0.0291 - val_loss: 2.9509 - val_mae: 0.4659 - val_mean_pred: 0.8808 - val_mae_t1: 0.0311\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.1776 - mae: 0.3438 - mean_pred: 0.7183 - mae_t1: 0.0229 - val_loss: 2.8984 - val_mae: 0.4576 - val_mean_pred: 0.7400 - val_mae_t1: 0.0305\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.1453 - mae: 0.3387 - mean_pred: 0.7128 - mae_t1: 0.0226 - val_loss: 2.8501 - val_mae: 0.4500 - val_mean_pred: 0.8966 - val_mae_t1: 0.0300\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.1545 - mae: 0.3402 - mean_pred: 0.8125 - mae_t1: 0.0227 - val_loss: 2.8380 - val_mae: 0.4481 - val_mean_pred: 0.9609 - val_mae_t1: 0.0299\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.2602 - mae: 0.3569 - mean_pred: 0.8951 - mae_t1: 0.0238 - val_loss: 2.6848 - val_mae: 0.4239 - val_mean_pred: 0.9147 - val_mae_t1: 0.0283\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 1.9925 - mae: 0.3146 - mean_pred: 0.7728 - mae_t1: 0.0210 - val_loss: 2.4913 - val_mae: 0.3934 - val_mean_pred: 0.8872 - val_mae_t1: 0.0262\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 1.8773 - mae: 0.2964 - mean_pred: 0.8178 - mae_t1: 0.0198 - val_loss: 2.7272 - val_mae: 0.4306 - val_mean_pred: 0.9031 - val_mae_t1: 0.0287\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.0039 - mae: 0.3164 - mean_pred: 0.7680 - mae_t1: 0.0211 - val_loss: 2.6217 - val_mae: 0.4140 - val_mean_pred: 0.8046 - val_mae_t1: 0.0276\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.8812 - mae: 0.2970 - mean_pred: 0.7506 - mae_t1: 0.0198 - val_loss: 2.5968 - val_mae: 0.4100 - val_mean_pred: 0.9600 - val_mae_t1: 0.0273\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 168us/sample - loss: 1.8846 - mae: 0.2976 - mean_pred: 0.8740 - mae_t1: 0.0198 - val_loss: 2.5540 - val_mae: 0.4033 - val_mean_pred: 0.8489 - val_mae_t1: 0.0269\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9770 - mae: 0.3122 - mean_pred: 0.7371 - mae_t1: 0.0208 - val_loss: 2.6295 - val_mae: 0.4152 - val_mean_pred: 0.8557 - val_mae_t1: 0.0277\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0350 - mae: 0.3213 - mean_pred: 0.8509 - mae_t1: 0.0214 - val_loss: 3.1516 - val_mae: 0.4976 - val_mean_pred: 1.0522 - val_mae_t1: 0.0332\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 169us/sample - loss: 2.0310 - mae: 0.3207 - mean_pred: 0.8451 - mae_t1: 0.0214 - val_loss: 2.9629 - val_mae: 0.4678 - val_mean_pred: 0.9084 - val_mae_t1: 0.0312\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.9926 - mae: 0.3146 - mean_pred: 0.7762 - mae_t1: 0.0210 - val_loss: 3.1743 - val_mae: 0.5012 - val_mean_pred: 1.0424 - val_mae_t1: 0.0334\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.1080 - mae: 0.3328 - mean_pred: 0.8920 - mae_t1: 0.0222 - val_loss: 2.8746 - val_mae: 0.4539 - val_mean_pred: 0.9450 - val_mae_t1: 0.0303\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 1.7694 - mae: 0.2794 - mean_pred: 0.8159 - mae_t1: 0.0186 - val_loss: 2.6378 - val_mae: 0.4165 - val_mean_pred: 0.8363 - val_mae_t1: 0.0278\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.7771 - mae: 0.2806 - mean_pred: 0.7741 - mae_t1: 0.0187 - val_loss: 2.5491 - val_mae: 0.4025 - val_mean_pred: 0.9308 - val_mae_t1: 0.0268\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 210us/sample - loss: 1.8072 - mae: 0.2854 - mean_pred: 0.8610 - mae_t1: 0.0190 - val_loss: 2.4754 - val_mae: 0.3909 - val_mean_pred: 0.8073 - val_mae_t1: 0.0261\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 1.9866 - mae: 0.3137 - mean_pred: 0.6901 - mae_t1: 0.0209 - val_loss: 2.6019 - val_mae: 0.4108 - val_mean_pred: 0.7546 - val_mae_t1: 0.0274\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9227 - mae: 0.3036 - mean_pred: 0.7482 - mae_t1: 0.0202 - val_loss: 2.8759 - val_mae: 0.4541 - val_mean_pred: 0.9610 - val_mae_t1: 0.0303\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.0093 - mae: 0.3173 - mean_pred: 0.8224 - mae_t1: 0.0212 - val_loss: 2.4508 - val_mae: 0.3870 - val_mean_pred: 0.8272 - val_mae_t1: 0.0258\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0452 - mae: 0.3229 - mean_pred: 0.7198 - mae_t1: 0.0215 - val_loss: 2.6819 - val_mae: 0.4235 - val_mean_pred: 0.8215 - val_mae_t1: 0.0282\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.7831 - mae: 0.2815 - mean_pred: 0.7810 - mae_t1: 0.0188 - val_loss: 2.7595 - val_mae: 0.4357 - val_mean_pred: 0.9719 - val_mae_t1: 0.0290\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9693 - mae: 0.3109 - mean_pred: 0.8255 - mae_t1: 0.0207 - val_loss: 2.7005 - val_mae: 0.4264 - val_mean_pred: 0.8100 - val_mae_t1: 0.0284\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 583us/sample - loss: 1.8473 - mae: 0.2917 - mean_pred: 0.7736 - mae_t1: 0.0194 - val_loss: 2.5278 - val_mae: 0.3991 - val_mean_pred: 0.8913 - val_mae_t1: 0.0266\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 1.7946 - mae: 0.2834 - mean_pred: 0.8006 - mae_t1: 0.0189 - val_loss: 2.8230 - val_mae: 0.4457 - val_mean_pred: 0.9030 - val_mae_t1: 0.0297\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.7314 - mae: 0.2734 - mean_pred: 0.7966 - mae_t1: 0.0182 - val_loss: 2.8981 - val_mae: 0.4576 - val_mean_pred: 0.9393 - val_mae_t1: 0.0305\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.7053 - mae: 0.2693 - mean_pred: 0.7934 - mae_t1: 0.0180 - val_loss: 2.7314 - val_mae: 0.4313 - val_mean_pred: 0.8366 - val_mae_t1: 0.0288\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.7646 - mae: 0.2786 - mean_pred: 0.6987 - mae_t1: 0.0186 - val_loss: 2.6593 - val_mae: 0.4199 - val_mean_pred: 0.8744 - val_mae_t1: 0.0280\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.8215 - mae: 0.2876 - mean_pred: 0.8381 - mae_t1: 0.0192 - val_loss: 2.6757 - val_mae: 0.4225 - val_mean_pred: 0.9161 - val_mae_t1: 0.0282\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.6558 - mae: 0.2614 - mean_pred: 0.7740 - mae_t1: 0.0174 - val_loss: 2.6876 - val_mae: 0.4244 - val_mean_pred: 0.8107 - val_mae_t1: 0.0283\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.8967 - mae: 0.2995 - mean_pred: 0.7845 - mae_t1: 0.0200 - val_loss: 2.7655 - val_mae: 0.4367 - val_mean_pred: 0.8489 - val_mae_t1: 0.0291\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7700 - mae: 0.2795 - mean_pred: 0.7429 - mae_t1: 0.0186 - val_loss: 2.6272 - val_mae: 0.4148 - val_mean_pred: 0.8302 - val_mae_t1: 0.0277\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7492 - mae: 0.2762 - mean_pred: 0.7994 - mae_t1: 0.0184 - val_loss: 2.8481 - val_mae: 0.4497 - val_mean_pred: 1.0268 - val_mae_t1: 0.0300\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 1.8873 - mae: 0.2980 - mean_pred: 0.8656 - mae_t1: 0.0199 - val_loss: 2.5140 - val_mae: 0.3970 - val_mean_pred: 0.8422 - val_mae_t1: 0.0265\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.7047 - mae: 0.2692 - mean_pred: 0.7128 - mae_t1: 0.0179 - val_loss: 2.6091 - val_mae: 0.4120 - val_mean_pred: 0.8912 - val_mae_t1: 0.0275\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6419 - mae: 0.2592 - mean_pred: 0.8185 - mae_t1: 0.0173 - val_loss: 2.8189 - val_mae: 0.4451 - val_mean_pred: 0.8957 - val_mae_t1: 0.0297\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6732 - mae: 0.2642 - mean_pred: 0.7700 - mae_t1: 0.0176 - val_loss: 2.6571 - val_mae: 0.4195 - val_mean_pred: 0.8489 - val_mae_t1: 0.0280\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.5798 - mae: 0.2494 - mean_pred: 0.8088 - mae_t1: 0.0166 - val_loss: 2.6545 - val_mae: 0.4191 - val_mean_pred: 0.9630 - val_mae_t1: 0.0279\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 1.6629 - mae: 0.2626 - mean_pred: 0.8183 - mae_t1: 0.0175 - val_loss: 2.6696 - val_mae: 0.4215 - val_mean_pred: 0.8920 - val_mae_t1: 0.0281\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.6793 - mae: 0.2651 - mean_pred: 0.7826 - mae_t1: 0.0177 - val_loss: 2.8424 - val_mae: 0.4488 - val_mean_pred: 0.9372 - val_mae_t1: 0.0299\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.7898 - mae: 0.2826 - mean_pred: 0.7643 - mae_t1: 0.0188 - val_loss: 2.8058 - val_mae: 0.4430 - val_mean_pred: 0.9129 - val_mae_t1: 0.0295\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.7398 - mae: 0.2747 - mean_pred: 0.8058 - mae_t1: 0.0183 - val_loss: 2.8158 - val_mae: 0.4446 - val_mean_pred: 0.9458 - val_mae_t1: 0.0296\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 1.6751 - mae: 0.2645 - mean_pred: 0.7986 - mae_t1: 0.0176 - val_loss: 2.6138 - val_mae: 0.4127 - val_mean_pred: 0.8909 - val_mae_t1: 0.0275\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 1.5491 - mae: 0.2446 - mean_pred: 0.7783 - mae_t1: 0.0163 - val_loss: 2.7549 - val_mae: 0.4350 - val_mean_pred: 0.9022 - val_mae_t1: 0.0290\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 1.4706 - mae: 0.2322 - mean_pred: 0.7997 - mae_t1: 0.0155 - val_loss: 2.6258 - val_mae: 0.4146 - val_mean_pred: 0.9224 - val_mae_t1: 0.0276\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 1.4737 - mae: 0.2327 - mean_pred: 0.7988 - mae_t1: 0.0155 - val_loss: 2.6267 - val_mae: 0.4147 - val_mean_pred: 0.9129 - val_mae_t1: 0.0276\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.6005 - mae: 0.2527 - mean_pred: 0.8093 - mae_t1: 0.0168 - val_loss: 2.5724 - val_mae: 0.4062 - val_mean_pred: 0.9372 - val_mae_t1: 0.0271\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.5034 - mae: 0.2374 - mean_pred: 0.8206 - mae_t1: 0.0158 - val_loss: 2.6086 - val_mae: 0.4119 - val_mean_pred: 0.9178 - val_mae_t1: 0.0275\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.4873 - mae: 0.2348 - mean_pred: 0.7822 - mae_t1: 0.0157 - val_loss: 3.1565 - val_mae: 0.4984 - val_mean_pred: 1.0048 - val_mae_t1: 0.0332\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.0692 - mae: 0.3267 - mean_pred: 0.9302 - mae_t1: 0.0218 - val_loss: 3.1269 - val_mae: 0.4937 - val_mean_pred: 0.9609 - val_mae_t1: 0.0329\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 1.7498 - mae: 0.2763 - mean_pred: 0.7382 - mae_t1: 0.0184 - val_loss: 2.9643 - val_mae: 0.4681 - val_mean_pred: 0.7543 - val_mae_t1: 0.0312\n",
      "Earliness...\n",
      "0.0019702911376953125\n",
      "____________________________________________________________\n",
      "Test MAE:      0.30448342198749284  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▅▅▄▄▃▃▃▃▃▃▃▃▃▄▃▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>mae</td><td>█▅▄▄▅▅▄▄▃▃▃▃▃▃▃▃▃▄▃▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>mae_t1</td><td>█▅▄▄▅▅▄▄▃▃▃▃▃▃▃▃▃▄▃▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂</td></tr><tr><td>mean_pred</td><td>▁▅▇▅▄█▇▆▆▆▆▇▆█▆▇▆▄▇▇▅▆▆▆▇▇▅▇▇▆▅▆▆▅▆▆▆▆▇▆</td></tr><tr><td>val_loss</td><td>█▅▇▇██▅▆▆▅▃▅▄▅▄▄▅▅▇▅▄▁▂▂▅▂▂▁▃▄▃▃▄▂▃▄▂▂▂▅</td></tr><tr><td>val_mae</td><td>█▅▇▇██▅▆▆▅▃▅▄▅▄▄▅▅▇▅▄▁▂▂▅▂▂▁▃▄▃▃▄▂▃▄▂▂▂▅</td></tr><tr><td>val_mae_t1</td><td>█▅▇▇██▅▆▆▅▃▅▄▅▄▄▅▅▇▅▄▁▂▂▅▂▂▁▃▄▃▃▄▂▃▄▂▂▂▅</td></tr><tr><td>val_mean_pred</td><td>▇▃▃▄▁▆▆▄▅▆▅▃▅▆▇▅▇▄▄▅▆▆▄▅▆▅▃▅▄▆▅▄█▆▇▆▆▆▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.38731</td></tr><tr><td>AE_2</td><td>0.29661</td></tr><tr><td>AE_3</td><td>0.29197</td></tr><tr><td>MAE</td><td>0.30448</td></tr><tr><td>best_epoch</td><td>68</td></tr><tr><td>best_val_loss</td><td>2.45077</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>1.74983</td></tr><tr><td>mae</td><td>0.27629</td></tr><tr><td>mae_t1</td><td>0.01842</td></tr><tr><td>mean_pred</td><td>0.7382</td></tr><tr><td>val_loss</td><td>2.96434</td></tr><tr><td>val_mae</td><td>0.46805</td></tr><tr><td>val_mae_t1</td><td>0.0312</td></tr><tr><td>val_mean_pred</td><td>0.75434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">scarlet-sun-97</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sd967md\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2sd967md</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_160027-2sd967md\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_160059-1z71s6bc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1z71s6bc\" target=\"_blank\">comfy-fire-98</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 729us/sample - loss: 5.6426 - mae: 0.8909 - mean_pred: 0.1556 - mae_t1: 0.0594 - val_loss: 3.2318 - val_mae: 0.5103 - val_mean_pred: 0.6735 - val_mae_t1: 0.0340\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 147us/sample - loss: 3.9782 - mae: 0.6281 - mean_pred: 0.7499 - mae_t1: 0.0419 - val_loss: 3.5870 - val_mae: 0.5664 - val_mean_pred: 1.0780 - val_mae_t1: 0.0378\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 148us/sample - loss: 3.8316 - mae: 0.6050 - mean_pred: 0.9806 - mae_t1: 0.0403 - val_loss: 3.6590 - val_mae: 0.5777 - val_mean_pred: 0.6501 - val_mae_t1: 0.0385\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 139us/sample - loss: 3.7580 - mae: 0.5934 - mean_pred: 0.5535 - mae_t1: 0.0396 - val_loss: 4.3496 - val_mae: 0.6868 - val_mean_pred: 0.3397 - val_mae_t1: 0.0458\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 121us/sample - loss: 4.0389 - mae: 0.6377 - mean_pred: 0.3504 - mae_t1: 0.0425 - val_loss: 4.0663 - val_mae: 0.6420 - val_mean_pred: 0.5309 - val_mae_t1: 0.0428\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 4.1855 - mae: 0.6609 - mean_pred: 0.5825 - mae_t1: 0.0441 - val_loss: 4.2673 - val_mae: 0.6738 - val_mean_pred: 0.7267 - val_mae_t1: 0.0449\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 4.2818 - mae: 0.6761 - mean_pred: 0.6924 - mae_t1: 0.0451 - val_loss: 3.4978 - val_mae: 0.5523 - val_mean_pred: 0.6192 - val_mae_t1: 0.0368\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 3.4636 - mae: 0.5469 - mean_pred: 0.5639 - mae_t1: 0.0365 - val_loss: 3.4752 - val_mae: 0.5487 - val_mean_pred: 0.5704 - val_mae_t1: 0.0366\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 137us/sample - loss: 3.2675 - mae: 0.5159 - mean_pred: 0.5682 - mae_t1: 0.0344 - val_loss: 3.0354 - val_mae: 0.4793 - val_mean_pred: 0.8100 - val_mae_t1: 0.0320\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.0879 - mae: 0.4876 - mean_pred: 0.8021 - mae_t1: 0.0325 - val_loss: 3.5719 - val_mae: 0.5640 - val_mean_pred: 1.0693 - val_mae_t1: 0.0376\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.5360 - mae: 0.5583 - mean_pred: 1.0042 - mae_t1: 0.0372 - val_loss: 3.1483 - val_mae: 0.4971 - val_mean_pred: 0.8720 - val_mae_t1: 0.0331\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.0008 - mae: 0.4738 - mean_pred: 0.7180 - mae_t1: 0.0316 - val_loss: 3.8632 - val_mae: 0.6100 - val_mean_pred: 0.5267 - val_mae_t1: 0.0407\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.3434 - mae: 0.5279 - mean_pred: 0.4847 - mae_t1: 0.0352 - val_loss: 3.4793 - val_mae: 0.5494 - val_mean_pred: 0.6726 - val_mae_t1: 0.0366\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.9897 - mae: 0.4721 - mean_pred: 0.6914 - mae_t1: 0.0315 - val_loss: 3.6437 - val_mae: 0.5753 - val_mean_pred: 1.0403 - val_mae_t1: 0.0384\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.3092 - mae: 0.5225 - mean_pred: 1.0074 - mae_t1: 0.0348 - val_loss: 3.3345 - val_mae: 0.5265 - val_mean_pred: 1.0138 - val_mae_t1: 0.0351\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 104us/sample - loss: 2.9774 - mae: 0.4701 - mean_pred: 0.8934 - mae_t1: 0.0313 - val_loss: 3.4097 - val_mae: 0.5384 - val_mean_pred: 0.6788 - val_mae_t1: 0.0359\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.0703 - mae: 0.4848 - mean_pred: 0.5882 - mae_t1: 0.0323 - val_loss: 3.8247 - val_mae: 0.6039 - val_mean_pred: 0.5701 - val_mae_t1: 0.0403\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.5542 - mae: 0.5612 - mean_pred: 0.5547 - mae_t1: 0.0374 - val_loss: 3.8735 - val_mae: 0.6116 - val_mean_pred: 0.6917 - val_mae_t1: 0.0408\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 3.4160 - mae: 0.5394 - mean_pred: 0.6420 - mae_t1: 0.0360 - val_loss: 3.2720 - val_mae: 0.5166 - val_mean_pred: 0.8088 - val_mae_t1: 0.0344\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.8381 - mae: 0.4481 - mean_pred: 0.7360 - mae_t1: 0.0299 - val_loss: 3.1026 - val_mae: 0.4899 - val_mean_pred: 0.9119 - val_mae_t1: 0.0327\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 127us/sample - loss: 2.7955 - mae: 0.4414 - mean_pred: 0.8372 - mae_t1: 0.0294 - val_loss: 2.8682 - val_mae: 0.4529 - val_mean_pred: 0.9090 - val_mae_t1: 0.0302\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 95us/sample - loss: 2.6924 - mae: 0.4251 - mean_pred: 0.8541 - mae_t1: 0.0283 - val_loss: 2.8970 - val_mae: 0.4574 - val_mean_pred: 0.7013 - val_mae_t1: 0.0305\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 93us/sample - loss: 2.8401 - mae: 0.4484 - mean_pred: 0.6560 - mae_t1: 0.0299 - val_loss: 3.3964 - val_mae: 0.5363 - val_mean_pred: 0.5305 - val_mae_t1: 0.0358\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.1754 - mae: 0.5014 - mean_pred: 0.5644 - mae_t1: 0.0334 - val_loss: 3.0120 - val_mae: 0.4756 - val_mean_pred: 0.7253 - val_mae_t1: 0.0317\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8212 - mae: 0.4454 - mean_pred: 0.7742 - mae_t1: 0.0297 - val_loss: 3.2118 - val_mae: 0.5071 - val_mean_pred: 0.9567 - val_mae_t1: 0.0338\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0399 - mae: 0.4800 - mean_pred: 0.9165 - mae_t1: 0.0320 - val_loss: 3.1748 - val_mae: 0.5013 - val_mean_pred: 0.7700 - val_mae_t1: 0.0334\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.7525 - mae: 0.4346 - mean_pred: 0.6882 - mae_t1: 0.0290 - val_loss: 3.4319 - val_mae: 0.5419 - val_mean_pred: 0.6529 - val_mae_t1: 0.0361\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.7610 - mae: 0.4359 - mean_pred: 0.6263 - mae_t1: 0.0291 - val_loss: 3.3797 - val_mae: 0.5336 - val_mean_pred: 0.8430 - val_mae_t1: 0.0356\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3273 - mae: 0.5254 - mean_pred: 0.8325 - mae_t1: 0.0350 - val_loss: 3.4156 - val_mae: 0.5393 - val_mean_pred: 0.9756 - val_mae_t1: 0.0360\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0312 - mae: 0.4786 - mean_pred: 0.8861 - mae_t1: 0.0319 - val_loss: 2.9494 - val_mae: 0.4657 - val_mean_pred: 0.8476 - val_mae_t1: 0.0310\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 126us/sample - loss: 2.6027 - mae: 0.4110 - mean_pred: 0.7299 - mae_t1: 0.0274 - val_loss: 2.8488 - val_mae: 0.4498 - val_mean_pred: 0.7794 - val_mae_t1: 0.0300\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 2.6024 - mae: 0.4109 - mean_pred: 0.7237 - mae_t1: 0.0274 - val_loss: 2.7473 - val_mae: 0.4338 - val_mean_pred: 0.8739 - val_mae_t1: 0.0289\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.6083 - mae: 0.4118 - mean_pred: 0.8004 - mae_t1: 0.0275 - val_loss: 2.7999 - val_mae: 0.4421 - val_mean_pred: 0.8526 - val_mae_t1: 0.0295\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6431 - mae: 0.4173 - mean_pred: 0.7784 - mae_t1: 0.0278 - val_loss: 3.0902 - val_mae: 0.4879 - val_mean_pred: 0.7758 - val_mae_t1: 0.0325\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.7213 - mae: 0.4297 - mean_pred: 0.6888 - mae_t1: 0.0286 - val_loss: 3.2169 - val_mae: 0.5079 - val_mean_pred: 0.7744 - val_mae_t1: 0.0339\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6410 - mae: 0.4170 - mean_pred: 0.7380 - mae_t1: 0.0278 - val_loss: 3.1860 - val_mae: 0.5031 - val_mean_pred: 0.9304 - val_mae_t1: 0.0335\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6871 - mae: 0.4243 - mean_pred: 0.8729 - mae_t1: 0.0283 - val_loss: 3.2953 - val_mae: 0.5203 - val_mean_pred: 0.9831 - val_mae_t1: 0.0347\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.7675 - mae: 0.4370 - mean_pred: 0.9306 - mae_t1: 0.0291 - val_loss: 3.1256 - val_mae: 0.4935 - val_mean_pred: 0.9344 - val_mae_t1: 0.0329\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.5380 - mae: 0.4007 - mean_pred: 0.8757 - mae_t1: 0.0267 - val_loss: 3.0521 - val_mae: 0.4819 - val_mean_pred: 0.7953 - val_mae_t1: 0.0321\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.4621 - mae: 0.3888 - mean_pred: 0.7551 - mae_t1: 0.0259 - val_loss: 2.9538 - val_mae: 0.4664 - val_mean_pred: 0.7663 - val_mae_t1: 0.0311\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.5456 - mae: 0.4019 - mean_pred: 0.7561 - mae_t1: 0.0268 - val_loss: 2.9071 - val_mae: 0.4590 - val_mean_pred: 0.8562 - val_mae_t1: 0.0306\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.4542 - mae: 0.3875 - mean_pred: 0.8302 - mae_t1: 0.0258 - val_loss: 3.0636 - val_mae: 0.4837 - val_mean_pred: 0.8668 - val_mae_t1: 0.0322\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.5441 - mae: 0.4017 - mean_pred: 0.7861 - mae_t1: 0.0268 - val_loss: 2.9310 - val_mae: 0.4628 - val_mean_pred: 0.8507 - val_mae_t1: 0.0309\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 2.5056 - mae: 0.3956 - mean_pred: 0.8259 - mae_t1: 0.0264 - val_loss: 2.7440 - val_mae: 0.4333 - val_mean_pred: 0.9060 - val_mae_t1: 0.0289\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.6438 - mae: 0.4174 - mean_pred: 0.8381 - mae_t1: 0.0278 - val_loss: 2.8137 - val_mae: 0.4443 - val_mean_pred: 0.8027 - val_mae_t1: 0.0296\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.6603 - mae: 0.4200 - mean_pred: 0.7533 - mae_t1: 0.0280 - val_loss: 2.8518 - val_mae: 0.4503 - val_mean_pred: 0.7726 - val_mae_t1: 0.0300\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.4792 - mae: 0.3915 - mean_pred: 0.6997 - mae_t1: 0.0261 - val_loss: 3.0080 - val_mae: 0.4749 - val_mean_pred: 0.8280 - val_mae_t1: 0.0317\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5684 - mae: 0.4055 - mean_pred: 0.7352 - mae_t1: 0.0270 - val_loss: 2.9626 - val_mae: 0.4678 - val_mean_pred: 0.9761 - val_mae_t1: 0.0312\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5727 - mae: 0.4062 - mean_pred: 0.8966 - mae_t1: 0.0271 - val_loss: 2.9423 - val_mae: 0.4646 - val_mean_pred: 1.0171 - val_mae_t1: 0.0310\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.5021 - mae: 0.3951 - mean_pred: 0.8876 - mae_t1: 0.0263 - val_loss: 2.8223 - val_mae: 0.4456 - val_mean_pred: 0.8022 - val_mae_t1: 0.0297\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.4157 - mae: 0.3814 - mean_pred: 0.7209 - mae_t1: 0.0254 - val_loss: 3.0164 - val_mae: 0.4763 - val_mean_pred: 0.6586 - val_mae_t1: 0.0318\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.6567 - mae: 0.4195 - mean_pred: 0.6047 - mae_t1: 0.0280 - val_loss: 3.0543 - val_mae: 0.4823 - val_mean_pred: 0.6285 - val_mae_t1: 0.0322\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5870 - mae: 0.4085 - mean_pred: 0.6051 - mae_t1: 0.0272 - val_loss: 2.7917 - val_mae: 0.4408 - val_mean_pred: 0.7730 - val_mae_t1: 0.0294\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.4486 - mae: 0.3866 - mean_pred: 0.7387 - mae_t1: 0.0258 - val_loss: 3.2156 - val_mae: 0.5077 - val_mean_pred: 1.0204 - val_mae_t1: 0.0338\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.8098 - mae: 0.4437 - mean_pred: 0.9589 - mae_t1: 0.0296 - val_loss: 3.1665 - val_mae: 0.5000 - val_mean_pred: 1.0803 - val_mae_t1: 0.0333\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7195 - mae: 0.4294 - mean_pred: 0.9658 - mae_t1: 0.0286 - val_loss: 2.8021 - val_mae: 0.4424 - val_mean_pred: 0.8439 - val_mae_t1: 0.0295\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.2684 - mae: 0.3582 - mean_pred: 0.7608 - mae_t1: 0.0239 - val_loss: 3.1409 - val_mae: 0.4959 - val_mean_pred: 0.7527 - val_mae_t1: 0.0331\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.3870 - mae: 0.3769 - mean_pred: 0.6892 - mae_t1: 0.0251 - val_loss: 3.0626 - val_mae: 0.4836 - val_mean_pred: 0.8584 - val_mae_t1: 0.0322\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 109us/sample - loss: 2.2260 - mae: 0.3515 - mean_pred: 0.7995 - mae_t1: 0.0234 - val_loss: 3.4485 - val_mae: 0.5445 - val_mean_pred: 0.9955 - val_mae_t1: 0.0363\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.6802 - mae: 0.4232 - mean_pred: 0.8857 - mae_t1: 0.0282 - val_loss: 3.3956 - val_mae: 0.5361 - val_mean_pred: 0.9683 - val_mae_t1: 0.0357\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.5006 - mae: 0.3948 - mean_pred: 0.8377 - mae_t1: 0.0263 - val_loss: 3.0621 - val_mae: 0.4835 - val_mean_pred: 0.8348 - val_mae_t1: 0.0322\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.4283 - mae: 0.3834 - mean_pred: 0.7068 - mae_t1: 0.0256 - val_loss: 2.9249 - val_mae: 0.4618 - val_mean_pred: 0.7349 - val_mae_t1: 0.0308\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 113us/sample - loss: 2.4333 - mae: 0.3842 - mean_pred: 0.6724 - mae_t1: 0.0256 - val_loss: 2.7328 - val_mae: 0.4315 - val_mean_pred: 0.7755 - val_mae_t1: 0.0288\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3292 - mae: 0.3678 - mean_pred: 0.7196 - mae_t1: 0.0245 - val_loss: 2.8431 - val_mae: 0.4489 - val_mean_pred: 0.8610 - val_mae_t1: 0.0299\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3912 - mae: 0.3776 - mean_pred: 0.8133 - mae_t1: 0.0252 - val_loss: 2.9154 - val_mae: 0.4603 - val_mean_pred: 0.9692 - val_mae_t1: 0.0307\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 2.4837 - mae: 0.3922 - mean_pred: 0.9075 - mae_t1: 0.0261 - val_loss: 2.9897 - val_mae: 0.4721 - val_mean_pred: 0.8360 - val_mae_t1: 0.0315\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.7726 - mae: 0.4378 - mean_pred: 0.7608 - mae_t1: 0.0292 - val_loss: 3.3099 - val_mae: 0.5226 - val_mean_pred: 0.6022 - val_mae_t1: 0.0348\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8567 - mae: 0.4511 - mean_pred: 0.5763 - mae_t1: 0.0301 - val_loss: 3.2687 - val_mae: 0.5161 - val_mean_pred: 0.6296 - val_mae_t1: 0.0344\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.4770 - mae: 0.3911 - mean_pred: 0.6037 - mae_t1: 0.0261 - val_loss: 2.9468 - val_mae: 0.4653 - val_mean_pred: 0.9041 - val_mae_t1: 0.0310\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3348 - mae: 0.3687 - mean_pred: 0.8608 - mae_t1: 0.0246 - val_loss: 3.2577 - val_mae: 0.5144 - val_mean_pred: 1.0597 - val_mae_t1: 0.0343\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7035 - mae: 0.4269 - mean_pred: 0.9527 - mae_t1: 0.0285 - val_loss: 3.0734 - val_mae: 0.4853 - val_mean_pred: 0.8264 - val_mae_t1: 0.0324\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.5224 - mae: 0.3983 - mean_pred: 0.7178 - mae_t1: 0.0266 - val_loss: 3.4124 - val_mae: 0.5388 - val_mean_pred: 0.5714 - val_mae_t1: 0.0359\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.9375 - mae: 0.4638 - mean_pred: 0.4983 - mae_t1: 0.0309 - val_loss: 3.6843 - val_mae: 0.5817 - val_mean_pred: 0.6058 - val_mae_t1: 0.0388\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0153 - mae: 0.4761 - mean_pred: 0.5425 - mae_t1: 0.0317 - val_loss: 2.9460 - val_mae: 0.4652 - val_mean_pred: 0.8343 - val_mae_t1: 0.0310\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2627 - mae: 0.3573 - mean_pred: 0.7740 - mae_t1: 0.0238 - val_loss: 3.2341 - val_mae: 0.5107 - val_mean_pred: 1.0512 - val_mae_t1: 0.0340\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.4853 - mae: 0.3924 - mean_pred: 0.9193 - mae_t1: 0.0262 - val_loss: 2.9448 - val_mae: 0.4650 - val_mean_pred: 0.9640 - val_mae_t1: 0.0310\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.1629 - mae: 0.3415 - mean_pred: 0.8213 - mae_t1: 0.0228 - val_loss: 3.0116 - val_mae: 0.4755 - val_mean_pred: 0.8084 - val_mae_t1: 0.0317\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.2131 - mae: 0.3494 - mean_pred: 0.7004 - mae_t1: 0.0233 - val_loss: 3.1538 - val_mae: 0.4980 - val_mean_pred: 0.7728 - val_mae_t1: 0.0332\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 97us/sample - loss: 2.4338 - mae: 0.3843 - mean_pred: 0.7210 - mae_t1: 0.0256 - val_loss: 3.1721 - val_mae: 0.5009 - val_mean_pred: 0.8263 - val_mae_t1: 0.0334\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.3299 - mae: 0.3679 - mean_pred: 0.7619 - mae_t1: 0.0245 - val_loss: 3.1584 - val_mae: 0.4987 - val_mean_pred: 0.8364 - val_mae_t1: 0.0332\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 2.2959 - mae: 0.3625 - mean_pred: 0.7576 - mae_t1: 0.0242 - val_loss: 3.4269 - val_mae: 0.5411 - val_mean_pred: 0.8920 - val_mae_t1: 0.0361\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6863 - mae: 0.4241 - mean_pred: 0.8095 - mae_t1: 0.0283 - val_loss: 3.0864 - val_mae: 0.4873 - val_mean_pred: 0.9285 - val_mae_t1: 0.0325\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.3089 - mae: 0.3646 - mean_pred: 0.8477 - mae_t1: 0.0243 - val_loss: 2.8339 - val_mae: 0.4475 - val_mean_pred: 0.8694 - val_mae_t1: 0.0298\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.7737 - mae: 0.4380 - mean_pred: 0.8437 - mae_t1: 0.0292 - val_loss: 2.7423 - val_mae: 0.4330 - val_mean_pred: 0.8264 - val_mae_t1: 0.0289\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 122us/sample - loss: 2.4503 - mae: 0.3869 - mean_pred: 0.7920 - mae_t1: 0.0258 - val_loss: 2.7040 - val_mae: 0.4270 - val_mean_pred: 0.8334 - val_mae_t1: 0.0285\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2377 - mae: 0.3533 - mean_pred: 0.7539 - mae_t1: 0.0236 - val_loss: 2.7604 - val_mae: 0.4359 - val_mean_pred: 0.8958 - val_mae_t1: 0.0291\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.1804 - mae: 0.3443 - mean_pred: 0.7860 - mae_t1: 0.0230 - val_loss: 2.9959 - val_mae: 0.4730 - val_mean_pred: 0.9368 - val_mae_t1: 0.0315\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.5270 - mae: 0.3990 - mean_pred: 0.8175 - mae_t1: 0.0266 - val_loss: 3.4508 - val_mae: 0.5449 - val_mean_pred: 0.8681 - val_mae_t1: 0.0363\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.7676 - mae: 0.4370 - mean_pred: 0.7211 - mae_t1: 0.0291 - val_loss: 3.3892 - val_mae: 0.5351 - val_mean_pred: 0.7878 - val_mae_t1: 0.0357\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.8035 - mae: 0.4427 - mean_pred: 0.6616 - mae_t1: 0.0295 - val_loss: 3.6585 - val_mae: 0.5777 - val_mean_pred: 0.9415 - val_mae_t1: 0.0385\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.8474 - mae: 0.4496 - mean_pred: 0.7816 - mae_t1: 0.0300 - val_loss: 3.3932 - val_mae: 0.5358 - val_mean_pred: 1.0473 - val_mae_t1: 0.0357\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.5506 - mae: 0.4027 - mean_pred: 0.8864 - mae_t1: 0.0268 - val_loss: 3.3170 - val_mae: 0.5237 - val_mean_pred: 0.9724 - val_mae_t1: 0.0349\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.5462 - mae: 0.4020 - mean_pred: 0.8256 - mae_t1: 0.0268 - val_loss: 3.0492 - val_mae: 0.4815 - val_mean_pred: 0.7727 - val_mae_t1: 0.0321\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.4109 - mae: 0.3807 - mean_pred: 0.6676 - mae_t1: 0.0254 - val_loss: 3.2845 - val_mae: 0.5186 - val_mean_pred: 0.6734 - val_mae_t1: 0.0346\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.4866 - mae: 0.3926 - mean_pred: 0.6108 - mae_t1: 0.0262 - val_loss: 3.1756 - val_mae: 0.5014 - val_mean_pred: 0.8514 - val_mae_t1: 0.0334\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2131 - mae: 0.3494 - mean_pred: 0.7809 - mae_t1: 0.0233 - val_loss: 3.0862 - val_mae: 0.4873 - val_mean_pred: 1.0338 - val_mae_t1: 0.0325\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.2316 - mae: 0.3524 - mean_pred: 0.8927 - mae_t1: 0.0235 - val_loss: 2.8987 - val_mae: 0.4577 - val_mean_pred: 0.9282 - val_mae_t1: 0.0305\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.1047 - mae: 0.3323 - mean_pred: 0.8016 - mae_t1: 0.0222 - val_loss: 2.9881 - val_mae: 0.4718 - val_mean_pred: 0.7823 - val_mae_t1: 0.0315\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.2287 - mae: 0.3519 - mean_pred: 0.6896 - mae_t1: 0.0235 - val_loss: 3.1012 - val_mae: 0.4897 - val_mean_pred: 0.7512 - val_mae_t1: 0.0326\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.7079 - mae: 0.4276 - mean_pred: 0.7260 - mae_t1: 0.0285 - val_loss: 3.0296 - val_mae: 0.4784 - val_mean_pred: 0.8095 - val_mae_t1: 0.0319\n",
      "Earliness...\n",
      "0.0014998912811279297\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3203181896246895  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.998340…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▅▄▄▃▃▄▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▃▁▂▂▂▁▂▂▁▁▂</td></tr><tr><td>mae</td><td>█▄▅▄▄▃▃▄▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▃▁▂▂▂▁▂▂▁▁▂</td></tr><tr><td>mae_t1</td><td>█▄▅▄▄▃▃▄▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▃▁▂▂▂▁▂▂▁▁▂</td></tr><tr><td>mean_pred</td><td>▁█▅▄█▄▇▄▇▅▇▅▆▆▆▇▆▇▆▇▆▆█▆▇▆▆▅▆▄▆▆▆▇▆▆▇▅▇▆</td></tr><tr><td>val_loss</td><td>▃▅█▄▃▄▄▆▂▄▃▄▁▃▃▂▂▁▂▂▂▃▁▄▂▁▄▂▄▂▂▃▃▁▂▄▄▃▂▂</td></tr><tr><td>val_mae</td><td>▃▅█▄▃▄▄▆▂▄▃▄▁▃▃▂▂▁▂▂▂▃▁▄▂▁▄▂▄▂▂▃▃▁▂▄▄▃▂▂</td></tr><tr><td>val_mae_t1</td><td>▃▅█▄▃▄▄▆▂▄▃▄▁▃▃▂▂▁▂▂▂▃▁▄▂▁▄▂▄▂▂▃▃▁▂▄▄▃▂▂</td></tr><tr><td>val_mean_pred</td><td>▃▃▄▂▆▃▃▃▆▁▄▅▅▅▇▅▆▆▄█▃█▅█▅▆▂▆▂▅▅▅▇▅▇▅▇▃▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.32199</td></tr><tr><td>AE_2</td><td>0.30944</td></tr><tr><td>AE_3</td><td>0.30955</td></tr><tr><td>MAE</td><td>0.32032</td></tr><tr><td>best_epoch</td><td>84</td></tr><tr><td>best_val_loss</td><td>2.70402</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.70789</td></tr><tr><td>mae</td><td>0.42756</td></tr><tr><td>mae_t1</td><td>0.0285</td></tr><tr><td>mean_pred</td><td>0.72602</td></tr><tr><td>val_loss</td><td>3.0296</td></tr><tr><td>val_mae</td><td>0.47836</td></tr><tr><td>val_mae_t1</td><td>0.03189</td></tr><tr><td>val_mean_pred</td><td>0.80954</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">comfy-fire-98</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1z71s6bc\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1z71s6bc</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_160059-1z71s6bc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_160122-1rpsyg2b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1rpsyg2b\" target=\"_blank\">youthful-donkey-99</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 859us/sample - loss: 6.0952 - mae: 0.7619 - mean_pred: 0.4277 - mae_t1: 0.0508 - val_loss: 4.4452 - val_mae: 0.5557 - val_mean_pred: 1.0544 - val_mae_t1: 0.0370\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 4.7225 - mae: 0.5903 - mean_pred: 0.7462 - mae_t1: 0.0394 - val_loss: 4.4096 - val_mae: 0.5512 - val_mean_pred: 0.5835 - val_mae_t1: 0.0367\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 223us/sample - loss: 4.3456 - mae: 0.5432 - mean_pred: 0.7918 - mae_t1: 0.0362 - val_loss: 3.8650 - val_mae: 0.4831 - val_mean_pred: 1.0124 - val_mae_t1: 0.0322\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 3.9563 - mae: 0.4945 - mean_pred: 0.7747 - mae_t1: 0.0330 - val_loss: 4.1292 - val_mae: 0.5162 - val_mean_pred: 0.5981 - val_mae_t1: 0.0344\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 206us/sample - loss: 3.9744 - mae: 0.4968 - mean_pred: 0.6386 - mae_t1: 0.0331 - val_loss: 3.5924 - val_mae: 0.4491 - val_mean_pred: 0.7809 - val_mae_t1: 0.0299\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 3.7644 - mae: 0.4706 - mean_pred: 0.7500 - mae_t1: 0.0314 - val_loss: 3.9697 - val_mae: 0.4962 - val_mean_pred: 0.6969 - val_mae_t1: 0.0331\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 3.7030 - mae: 0.4629 - mean_pred: 0.6166 - mae_t1: 0.0309 - val_loss: 4.4155 - val_mae: 0.5519 - val_mean_pred: 0.7320 - val_mae_t1: 0.0368\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 3.5707 - mae: 0.4463 - mean_pred: 0.6957 - mae_t1: 0.0298 - val_loss: 4.0310 - val_mae: 0.5039 - val_mean_pred: 0.9247 - val_mae_t1: 0.0336\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 217us/sample - loss: 4.1437 - mae: 0.5180 - mean_pred: 0.8944 - mae_t1: 0.0345 - val_loss: 3.4832 - val_mae: 0.4354 - val_mean_pred: 0.8818 - val_mae_t1: 0.0290\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.7261 - mae: 0.4658 - mean_pred: 0.7509 - mae_t1: 0.0311 - val_loss: 3.4906 - val_mae: 0.4363 - val_mean_pred: 0.7434 - val_mae_t1: 0.0291\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 196us/sample - loss: 3.4751 - mae: 0.4344 - mean_pred: 0.7955 - mae_t1: 0.0290 - val_loss: 3.5431 - val_mae: 0.4429 - val_mean_pred: 0.8407 - val_mae_t1: 0.0295\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.4110 - mae: 0.4264 - mean_pred: 0.7379 - mae_t1: 0.0284 - val_loss: 3.8351 - val_mae: 0.4794 - val_mean_pred: 0.9229 - val_mae_t1: 0.0320\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.5892 - mae: 0.4487 - mean_pred: 0.9382 - mae_t1: 0.0299 - val_loss: 4.6450 - val_mae: 0.5806 - val_mean_pred: 0.6289 - val_mae_t1: 0.0387\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 4.5850 - mae: 0.5731 - mean_pred: 0.4257 - mae_t1: 0.0382 - val_loss: 4.7044 - val_mae: 0.5880 - val_mean_pred: 0.5293 - val_mae_t1: 0.0392\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 3.8243 - mae: 0.4780 - mean_pred: 0.6868 - mae_t1: 0.0319 - val_loss: 4.5523 - val_mae: 0.5690 - val_mean_pred: 1.1629 - val_mae_t1: 0.0379\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 3.8057 - mae: 0.4757 - mean_pred: 0.9753 - mae_t1: 0.0317 - val_loss: 4.3257 - val_mae: 0.5407 - val_mean_pred: 0.6623 - val_mae_t1: 0.0360\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.8341 - mae: 0.4793 - mean_pred: 0.5334 - mae_t1: 0.0320 - val_loss: 3.9627 - val_mae: 0.4953 - val_mean_pred: 0.7340 - val_mae_t1: 0.0330\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 3.4219 - mae: 0.4277 - mean_pred: 0.7821 - mae_t1: 0.0285 - val_loss: 3.6622 - val_mae: 0.4578 - val_mean_pred: 0.9427 - val_mae_t1: 0.0305\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 3.4174 - mae: 0.4272 - mean_pred: 0.7128 - mae_t1: 0.0285 - val_loss: 4.0795 - val_mae: 0.5099 - val_mean_pred: 0.6315 - val_mae_t1: 0.0340\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 3.2864 - mae: 0.4108 - mean_pred: 0.6608 - mae_t1: 0.0274 - val_loss: 3.7555 - val_mae: 0.4694 - val_mean_pred: 1.0673 - val_mae_t1: 0.0313\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 226us/sample - loss: 3.8405 - mae: 0.4801 - mean_pred: 1.0218 - mae_t1: 0.0320 - val_loss: 3.4189 - val_mae: 0.4274 - val_mean_pred: 0.7971 - val_mae_t1: 0.0285\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 3.5849 - mae: 0.4481 - mean_pred: 0.5762 - mae_t1: 0.0299 - val_loss: 4.2523 - val_mae: 0.5315 - val_mean_pred: 0.6556 - val_mae_t1: 0.0354\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.1432 - mae: 0.3929 - mean_pred: 0.7186 - mae_t1: 0.0262 - val_loss: 4.4124 - val_mae: 0.5516 - val_mean_pred: 1.1329 - val_mae_t1: 0.0368\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 3.3712 - mae: 0.4214 - mean_pred: 0.9546 - mae_t1: 0.0281 - val_loss: 4.3755 - val_mae: 0.5469 - val_mean_pred: 0.9084 - val_mae_t1: 0.0365\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 3.8251 - mae: 0.4781 - mean_pred: 0.7653 - mae_t1: 0.0319 - val_loss: 4.0560 - val_mae: 0.5070 - val_mean_pred: 0.7510 - val_mae_t1: 0.0338\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 4.0070 - mae: 0.5009 - mean_pred: 0.7136 - mae_t1: 0.0334 - val_loss: 3.7569 - val_mae: 0.4696 - val_mean_pred: 0.7635 - val_mae_t1: 0.0313\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 3.2118 - mae: 0.4015 - mean_pred: 0.7121 - mae_t1: 0.0268 - val_loss: 4.5579 - val_mae: 0.5697 - val_mean_pred: 1.0330 - val_mae_t1: 0.0380\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.2544 - mae: 0.4068 - mean_pred: 0.9008 - mae_t1: 0.0271 - val_loss: 3.9460 - val_mae: 0.4933 - val_mean_pred: 0.8588 - val_mae_t1: 0.0329\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 3.1700 - mae: 0.3962 - mean_pred: 0.6823 - mae_t1: 0.0264 - val_loss: 3.7290 - val_mae: 0.4661 - val_mean_pred: 0.7997 - val_mae_t1: 0.0311\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.6985 - mae: 0.3373 - mean_pred: 0.7665 - mae_t1: 0.0225 - val_loss: 3.6269 - val_mae: 0.4534 - val_mean_pred: 0.9954 - val_mae_t1: 0.0302\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.6381 - mae: 0.3298 - mean_pred: 0.8361 - mae_t1: 0.0220 - val_loss: 3.6146 - val_mae: 0.4518 - val_mean_pred: 0.8618 - val_mae_t1: 0.0301\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 220us/sample - loss: 2.6036 - mae: 0.3254 - mean_pred: 0.7765 - mae_t1: 0.0217 - val_loss: 3.3463 - val_mae: 0.4183 - val_mean_pred: 0.9339 - val_mae_t1: 0.0279\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.6082 - mae: 0.3260 - mean_pred: 0.8462 - mae_t1: 0.0217 - val_loss: 3.3764 - val_mae: 0.4221 - val_mean_pred: 0.9094 - val_mae_t1: 0.0281\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.4676 - mae: 0.3085 - mean_pred: 0.8045 - mae_t1: 0.0206 - val_loss: 3.4361 - val_mae: 0.4295 - val_mean_pred: 0.8722 - val_mae_t1: 0.0286\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.5160 - mae: 0.3145 - mean_pred: 0.7825 - mae_t1: 0.0210 - val_loss: 3.8889 - val_mae: 0.4861 - val_mean_pred: 0.8815 - val_mae_t1: 0.0324\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 188us/sample - loss: 2.7630 - mae: 0.3454 - mean_pred: 0.7275 - mae_t1: 0.0230 - val_loss: 3.6422 - val_mae: 0.4553 - val_mean_pred: 0.8909 - val_mae_t1: 0.0304\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.8639 - mae: 0.3580 - mean_pred: 0.8420 - mae_t1: 0.0239 - val_loss: 3.6097 - val_mae: 0.4512 - val_mean_pred: 0.9658 - val_mae_t1: 0.0301\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.7419 - mae: 0.3427 - mean_pred: 0.7558 - mae_t1: 0.0228 - val_loss: 3.5868 - val_mae: 0.4483 - val_mean_pred: 0.7866 - val_mae_t1: 0.0299\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.7089 - mae: 0.3386 - mean_pred: 0.7123 - mae_t1: 0.0226 - val_loss: 3.6833 - val_mae: 0.4604 - val_mean_pred: 0.9639 - val_mae_t1: 0.0307\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 180us/sample - loss: 2.4680 - mae: 0.3085 - mean_pred: 0.8642 - mae_t1: 0.0206 - val_loss: 3.6122 - val_mae: 0.4515 - val_mean_pred: 0.8432 - val_mae_t1: 0.0301\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 207us/sample - loss: 2.5423 - mae: 0.3178 - mean_pred: 0.7154 - mae_t1: 0.0212 - val_loss: 3.2814 - val_mae: 0.4102 - val_mean_pred: 0.8244 - val_mae_t1: 0.0273\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 2.7877 - mae: 0.3485 - mean_pred: 0.7852 - mae_t1: 0.0232 - val_loss: 4.1009 - val_mae: 0.5126 - val_mean_pred: 0.9386 - val_mae_t1: 0.0342\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.9770 - mae: 0.3721 - mean_pred: 0.8195 - mae_t1: 0.0248 - val_loss: 3.7632 - val_mae: 0.4704 - val_mean_pred: 0.8798 - val_mae_t1: 0.0314\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 3.7001 - mae: 0.4625 - mean_pred: 0.7492 - mae_t1: 0.0308 - val_loss: 3.6996 - val_mae: 0.4624 - val_mean_pred: 0.7283 - val_mae_t1: 0.0308\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.7852 - mae: 0.3482 - mean_pred: 0.6766 - mae_t1: 0.0232 - val_loss: 3.5966 - val_mae: 0.4496 - val_mean_pred: 0.9177 - val_mae_t1: 0.0300\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.3261 - mae: 0.2908 - mean_pred: 0.7925 - mae_t1: 0.0194 - val_loss: 3.4361 - val_mae: 0.4295 - val_mean_pred: 0.8564 - val_mae_t1: 0.0286\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.2154 - mae: 0.2769 - mean_pred: 0.7793 - mae_t1: 0.0185 - val_loss: 3.3542 - val_mae: 0.4193 - val_mean_pred: 0.9003 - val_mae_t1: 0.0280\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.3148 - mae: 0.2894 - mean_pred: 0.8058 - mae_t1: 0.0193 - val_loss: 3.5114 - val_mae: 0.4389 - val_mean_pred: 0.8380 - val_mae_t1: 0.0293\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 208us/sample - loss: 2.5230 - mae: 0.3154 - mean_pred: 0.7310 - mae_t1: 0.0210 - val_loss: 3.1791 - val_mae: 0.3974 - val_mean_pred: 0.8663 - val_mae_t1: 0.0265\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.3227 - mae: 0.2903 - mean_pred: 0.8387 - mae_t1: 0.0194 - val_loss: 3.4443 - val_mae: 0.4305 - val_mean_pred: 0.8843 - val_mae_t1: 0.0287\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.3359 - mae: 0.2920 - mean_pred: 0.7793 - mae_t1: 0.0195 - val_loss: 3.5292 - val_mae: 0.4412 - val_mean_pred: 0.8582 - val_mae_t1: 0.0294\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 174us/sample - loss: 2.3457 - mae: 0.2932 - mean_pred: 0.7912 - mae_t1: 0.0195 - val_loss: 3.9350 - val_mae: 0.4919 - val_mean_pred: 1.0314 - val_mae_t1: 0.0328\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.5774 - mae: 0.3222 - mean_pred: 0.8971 - mae_t1: 0.0215 - val_loss: 3.5621 - val_mae: 0.4453 - val_mean_pred: 0.8761 - val_mae_t1: 0.0297\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.4019 - mae: 0.3002 - mean_pred: 0.7990 - mae_t1: 0.0200 - val_loss: 3.2943 - val_mae: 0.4118 - val_mean_pred: 0.9244 - val_mae_t1: 0.0275\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 175us/sample - loss: 2.3396 - mae: 0.2924 - mean_pred: 0.8312 - mae_t1: 0.0195 - val_loss: 3.3216 - val_mae: 0.4152 - val_mean_pred: 0.8750 - val_mae_t1: 0.0277\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 170us/sample - loss: 2.3436 - mae: 0.2929 - mean_pred: 0.7890 - mae_t1: 0.0195 - val_loss: 3.3064 - val_mae: 0.4133 - val_mean_pred: 0.9318 - val_mae_t1: 0.0276\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 2.4781 - mae: 0.3098 - mean_pred: 0.8616 - mae_t1: 0.0207 - val_loss: 3.4522 - val_mae: 0.4315 - val_mean_pred: 0.8085 - val_mae_t1: 0.0288\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 2.7147 - mae: 0.3393 - mean_pred: 0.6942 - mae_t1: 0.0226 - val_loss: 3.5134 - val_mae: 0.4392 - val_mean_pred: 0.8308 - val_mae_t1: 0.0293\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.5085 - mae: 0.3136 - mean_pred: 0.8534 - mae_t1: 0.0209 - val_loss: 3.6126 - val_mae: 0.4516 - val_mean_pred: 0.9819 - val_mae_t1: 0.0301\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 190us/sample - loss: 2.4738 - mae: 0.3092 - mean_pred: 0.7871 - mae_t1: 0.0206 - val_loss: 3.7784 - val_mae: 0.4723 - val_mean_pred: 0.7403 - val_mae_t1: 0.0315\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.8129 - mae: 0.3516 - mean_pred: 0.7085 - mae_t1: 0.0234 - val_loss: 3.5558 - val_mae: 0.4445 - val_mean_pred: 0.9520 - val_mae_t1: 0.0296\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 2.6552 - mae: 0.3319 - mean_pred: 0.8856 - mae_t1: 0.0221 - val_loss: 3.5986 - val_mae: 0.4498 - val_mean_pred: 0.8678 - val_mae_t1: 0.0300\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 209us/sample - loss: 2.5038 - mae: 0.3130 - mean_pred: 0.7262 - mae_t1: 0.0209 - val_loss: 3.1046 - val_mae: 0.3881 - val_mean_pred: 0.8059 - val_mae_t1: 0.0259\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.3319 - mae: 0.2915 - mean_pred: 0.8289 - mae_t1: 0.0194 - val_loss: 3.5136 - val_mae: 0.4392 - val_mean_pred: 0.9080 - val_mae_t1: 0.0293\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 171us/sample - loss: 2.1931 - mae: 0.2741 - mean_pred: 0.7803 - mae_t1: 0.0183 - val_loss: 3.5662 - val_mae: 0.4458 - val_mean_pred: 0.8287 - val_mae_t1: 0.0297\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.2610 - mae: 0.2826 - mean_pred: 0.8000 - mae_t1: 0.0188 - val_loss: 3.5548 - val_mae: 0.4443 - val_mean_pred: 0.9546 - val_mae_t1: 0.0296\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 2.2107 - mae: 0.2763 - mean_pred: 0.8463 - mae_t1: 0.0184 - val_loss: 3.7440 - val_mae: 0.4680 - val_mean_pred: 0.8585 - val_mae_t1: 0.0312\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 192us/sample - loss: 2.5852 - mae: 0.3231 - mean_pred: 0.7476 - mae_t1: 0.0215 - val_loss: 3.2571 - val_mae: 0.4071 - val_mean_pred: 0.8337 - val_mae_t1: 0.0271\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.1903 - mae: 0.2738 - mean_pred: 0.8213 - mae_t1: 0.0183 - val_loss: 3.2837 - val_mae: 0.4105 - val_mean_pred: 0.9035 - val_mae_t1: 0.0274\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.1407 - mae: 0.2676 - mean_pred: 0.7688 - mae_t1: 0.0178 - val_loss: 3.4404 - val_mae: 0.4301 - val_mean_pred: 0.8927 - val_mae_t1: 0.0287\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 2.2991 - mae: 0.2874 - mean_pred: 0.8404 - mae_t1: 0.0192 - val_loss: 3.4629 - val_mae: 0.4329 - val_mean_pred: 0.9810 - val_mae_t1: 0.0289\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.1409 - mae: 0.2676 - mean_pred: 0.8330 - mae_t1: 0.0178 - val_loss: 3.6476 - val_mae: 0.4559 - val_mean_pred: 0.8525 - val_mae_t1: 0.0304\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.4540 - mae: 0.3067 - mean_pred: 0.7580 - mae_t1: 0.0204 - val_loss: 3.3804 - val_mae: 0.4225 - val_mean_pred: 0.8074 - val_mae_t1: 0.0282\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.3270 - mae: 0.2909 - mean_pred: 0.7701 - mae_t1: 0.0194 - val_loss: 3.7066 - val_mae: 0.4633 - val_mean_pred: 0.9424 - val_mae_t1: 0.0309\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 191us/sample - loss: 2.1988 - mae: 0.2749 - mean_pred: 0.8423 - mae_t1: 0.0183 - val_loss: 3.4558 - val_mae: 0.4320 - val_mean_pred: 0.9036 - val_mae_t1: 0.0288\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.1317 - mae: 0.2665 - mean_pred: 0.7712 - mae_t1: 0.0178 - val_loss: 3.2767 - val_mae: 0.4096 - val_mean_pred: 0.8360 - val_mae_t1: 0.0273\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.0665 - mae: 0.2583 - mean_pred: 0.7806 - mae_t1: 0.0172 - val_loss: 3.3311 - val_mae: 0.4164 - val_mean_pred: 0.9026 - val_mae_t1: 0.0278\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9766 - mae: 0.2471 - mean_pred: 0.7925 - mae_t1: 0.0165 - val_loss: 3.3598 - val_mae: 0.4200 - val_mean_pred: 0.9142 - val_mae_t1: 0.0280\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.9779 - mae: 0.2472 - mean_pred: 0.8285 - mae_t1: 0.0165 - val_loss: 3.5404 - val_mae: 0.4425 - val_mean_pred: 0.8922 - val_mae_t1: 0.0295\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 1.9493 - mae: 0.2437 - mean_pred: 0.7743 - mae_t1: 0.0162 - val_loss: 3.4161 - val_mae: 0.4270 - val_mean_pred: 0.8915 - val_mae_t1: 0.0285\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 172us/sample - loss: 1.9127 - mae: 0.2391 - mean_pred: 0.7989 - mae_t1: 0.0159 - val_loss: 3.9181 - val_mae: 0.4898 - val_mean_pred: 0.9832 - val_mae_t1: 0.0327\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 189us/sample - loss: 2.2980 - mae: 0.2872 - mean_pred: 0.8385 - mae_t1: 0.0191 - val_loss: 3.8801 - val_mae: 0.4850 - val_mean_pred: 0.9070 - val_mae_t1: 0.0323\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 177us/sample - loss: 2.1493 - mae: 0.2687 - mean_pred: 0.7589 - mae_t1: 0.0179 - val_loss: 3.6941 - val_mae: 0.4618 - val_mean_pred: 0.9062 - val_mae_t1: 0.0308\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.1732 - mae: 0.2717 - mean_pred: 0.8222 - mae_t1: 0.0181 - val_loss: 3.5817 - val_mae: 0.4477 - val_mean_pred: 0.8563 - val_mae_t1: 0.0298\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 178us/sample - loss: 2.1325 - mae: 0.2666 - mean_pred: 0.7377 - mae_t1: 0.0178 - val_loss: 3.7032 - val_mae: 0.4629 - val_mean_pred: 0.8518 - val_mae_t1: 0.0309\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.0397 - mae: 0.2550 - mean_pred: 0.8270 - mae_t1: 0.0170 - val_loss: 3.5705 - val_mae: 0.4463 - val_mean_pred: 0.9663 - val_mae_t1: 0.0298\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 179us/sample - loss: 1.9042 - mae: 0.2380 - mean_pred: 0.8118 - mae_t1: 0.0159 - val_loss: 3.5224 - val_mae: 0.4403 - val_mean_pred: 0.8598 - val_mae_t1: 0.0294\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 181us/sample - loss: 1.9659 - mae: 0.2457 - mean_pred: 0.7790 - mae_t1: 0.0164 - val_loss: 3.7688 - val_mae: 0.4711 - val_mean_pred: 1.0166 - val_mae_t1: 0.0314\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 173us/sample - loss: 2.1606 - mae: 0.2701 - mean_pred: 0.8814 - mae_t1: 0.0180 - val_loss: 3.5823 - val_mae: 0.4478 - val_mean_pred: 0.8263 - val_mae_t1: 0.0299\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 183us/sample - loss: 2.2789 - mae: 0.2849 - mean_pred: 0.7028 - mae_t1: 0.0190 - val_loss: 3.9537 - val_mae: 0.4942 - val_mean_pred: 0.8830 - val_mae_t1: 0.0329\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 186us/sample - loss: 2.2432 - mae: 0.2804 - mean_pred: 0.8468 - mae_t1: 0.0187 - val_loss: 3.6029 - val_mae: 0.4504 - val_mean_pred: 0.9890 - val_mae_t1: 0.0300\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 182us/sample - loss: 2.0454 - mae: 0.2557 - mean_pred: 0.8311 - mae_t1: 0.0170 - val_loss: 3.8978 - val_mae: 0.4872 - val_mean_pred: 0.8944 - val_mae_t1: 0.0325\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 199us/sample - loss: 2.1456 - mae: 0.2682 - mean_pred: 0.7814 - mae_t1: 0.0179 - val_loss: 3.5616 - val_mae: 0.4452 - val_mean_pred: 0.9752 - val_mae_t1: 0.0297\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 185us/sample - loss: 2.0652 - mae: 0.2582 - mean_pred: 0.8164 - mae_t1: 0.0172 - val_loss: 3.4394 - val_mae: 0.4299 - val_mean_pred: 0.8843 - val_mae_t1: 0.0287\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.1248 - mae: 0.2656 - mean_pred: 0.7935 - mae_t1: 0.0177 - val_loss: 3.9152 - val_mae: 0.4894 - val_mean_pred: 0.9128 - val_mae_t1: 0.0326\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 184us/sample - loss: 2.4592 - mae: 0.3074 - mean_pred: 0.7967 - mae_t1: 0.0205 - val_loss: 3.7703 - val_mae: 0.4713 - val_mean_pred: 0.9558 - val_mae_t1: 0.0314\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 176us/sample - loss: 2.3278 - mae: 0.2910 - mean_pred: 0.8356 - mae_t1: 0.0194 - val_loss: 3.7804 - val_mae: 0.4726 - val_mean_pred: 0.9352 - val_mae_t1: 0.0315\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 187us/sample - loss: 2.0957 - mae: 0.2620 - mean_pred: 0.7887 - mae_t1: 0.0175 - val_loss: 3.6672 - val_mae: 0.4584 - val_mean_pred: 1.0168 - val_mae_t1: 0.0306\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 198us/sample - loss: 2.3797 - mae: 0.2975 - mean_pred: 0.9132 - mae_t1: 0.0198 - val_loss: 4.1329 - val_mae: 0.5166 - val_mean_pred: 0.9697 - val_mae_t1: 0.0344\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 194us/sample - loss: 2.5389 - mae: 0.3174 - mean_pred: 0.8038 - mae_t1: 0.0212 - val_loss: 3.6895 - val_mae: 0.4612 - val_mean_pred: 0.9147 - val_mae_t1: 0.0307\n",
      "Earliness...\n",
      "0.0025000572204589844\n",
      "____________________________________________________________\n",
      "Test MAE:      0.3023309603865029  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▄▄▄▄▃▅▃▂▂▂▂▂▄▂▂▂▂▂▂▃▂▂▁▁▂▁▁▂▁▁▁▁▁▂▂</td></tr><tr><td>mae</td><td>█▅▄▄▄▄▄▄▄▃▅▃▂▂▂▂▂▄▂▂▂▂▂▂▃▂▂▁▁▂▁▁▂▁▁▁▁▁▂▂</td></tr><tr><td>mae_t1</td><td>█▅▄▄▄▄▄▄▄▃▅▃▂▂▂▂▂▄▂▂▂▂▂▂▃▂▂▁▁▂▁▁▂▁▁▁▁▁▂▂</td></tr><tr><td>mean_pred</td><td>▁▅▅▄▅▇▇▅█▄▄▇▆▅▅▄▄▅▅▅▅▅▅▆▄▆▆▆▆▅▅▆▆▆▆▆▆▆▆▅</td></tr><tr><td>val_loss</td><td>▇▄▅▅▃█▆▃▂▇▄▅▃▂▃▃▁▃▂▁▃▂▂▃▃▃▄▁▃▄▂▃▄▃▃▃▄▂▄▃</td></tr><tr><td>val_mae</td><td>▇▄▅▅▃█▆▃▂▇▄▅▃▂▃▃▁▃▂▁▃▂▂▃▃▃▄▁▃▄▂▃▄▃▃▃▄▂▄▃</td></tr><tr><td>val_mae_t1</td><td>▇▄▅▅▃█▆▃▂▇▄▅▃▂▃▃▁▃▂▁▃▂▂▃▃▃▄▁▃▄▂▃▄▃▃▃▄▂▄▃</td></tr><tr><td>val_mean_pred</td><td>▇▆▂▅▄▁▁▅▃█▃▄▄▄▅▆▄▂▄▄▄▅▅▆▅▅▄▅▄▅▅▅▅▄▄▄▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.33562</td></tr><tr><td>AE_2</td><td>0.24347</td></tr><tr><td>AE_3</td><td>0.3004</td></tr><tr><td>MAE</td><td>0.30233</td></tr><tr><td>best_epoch</td><td>62</td></tr><tr><td>best_val_loss</td><td>3.10461</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.53885</td></tr><tr><td>mae</td><td>0.31736</td></tr><tr><td>mae_t1</td><td>0.02116</td></tr><tr><td>mean_pred</td><td>0.80376</td></tr><tr><td>val_loss</td><td>3.6895</td></tr><tr><td>val_mae</td><td>0.46119</td></tr><tr><td>val_mae_t1</td><td>0.03075</td></tr><tr><td>val_mean_pred</td><td>0.91468</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">youthful-donkey-99</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1rpsyg2b\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/1rpsyg2b</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_160122-1rpsyg2b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:/wandb/wandb\\run-20220508_160149-2i3hn7dr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2i3hn7dr\" target=\"_blank\">worldly-feather-100</a></strong> to <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: (646, 15, 18)\n",
      "Input data shape: (15, 18)\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                13800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 13,851\n",
      "Trainable params: 13,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "y_train mean: -7.38138e-10\n",
      "x_train mean: 3.992270846490644e-18\n",
      "y_test mean: 6.274173e-09\n",
      "x_test mean: 8.3104413539193e-18\n",
      "Train on 516 samples, validate on 130 samples\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 0s 716us/sample - loss: 5.6801 - mae: 0.7100 - mean_pred: 0.3672 - mae_t1: 0.0473 - val_loss: 4.1170 - val_mae: 0.5146 - val_mean_pred: 0.9125 - val_mae_t1: 0.0343\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 0s 165us/sample - loss: 5.2861 - mae: 0.6608 - mean_pred: 0.9473 - mae_t1: 0.0441 - val_loss: 4.7833 - val_mae: 0.5979 - val_mean_pred: 0.9665 - val_mae_t1: 0.0399\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 0s 131us/sample - loss: 4.9358 - mae: 0.6170 - mean_pred: 0.8107 - mae_t1: 0.0411 - val_loss: 4.9470 - val_mae: 0.6184 - val_mean_pred: 0.4738 - val_mae_t1: 0.0412\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 0s 139us/sample - loss: 5.1280 - mae: 0.6410 - mean_pred: 0.3666 - mae_t1: 0.0427 - val_loss: 5.0647 - val_mae: 0.6331 - val_mean_pred: 0.5048 - val_mae_t1: 0.0422\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 0s 112us/sample - loss: 4.6736 - mae: 0.5842 - mean_pred: 0.5253 - mae_t1: 0.0389 - val_loss: 5.2279 - val_mae: 0.6535 - val_mean_pred: 1.1045 - val_mae_t1: 0.0436\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 0s 105us/sample - loss: 5.0729 - mae: 0.6341 - mean_pred: 1.0701 - mae_t1: 0.0423 - val_loss: 4.4253 - val_mae: 0.5532 - val_mean_pred: 1.0096 - val_mae_t1: 0.0369\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 0s 108us/sample - loss: 4.1977 - mae: 0.5247 - mean_pred: 0.8103 - mae_t1: 0.0350 - val_loss: 5.3284 - val_mae: 0.6660 - val_mean_pred: 0.4513 - val_mae_t1: 0.0444\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 0s 107us/sample - loss: 4.9895 - mae: 0.6237 - mean_pred: 0.4077 - mae_t1: 0.0416 - val_loss: 5.2689 - val_mae: 0.6586 - val_mean_pred: 0.4973 - val_mae_t1: 0.0439\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 4.9311 - mae: 0.6164 - mean_pred: 0.5058 - mae_t1: 0.0411 - val_loss: 5.2099 - val_mae: 0.6512 - val_mean_pred: 0.8028 - val_mae_t1: 0.0434\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 4.7489 - mae: 0.5936 - mean_pred: 0.7822 - mae_t1: 0.0396 - val_loss: 4.2151 - val_mae: 0.5269 - val_mean_pred: 0.9121 - val_mae_t1: 0.0351\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.2665 - mae: 0.5333 - mean_pred: 0.8379 - mae_t1: 0.0356 - val_loss: 5.1060 - val_mae: 0.6382 - val_mean_pred: 1.0216 - val_mae_t1: 0.0425\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 5.7519 - mae: 0.7190 - mean_pred: 1.0045 - mae_t1: 0.0479 - val_loss: 4.8653 - val_mae: 0.6082 - val_mean_pred: 1.1487 - val_mae_t1: 0.0405\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.6624 - mae: 0.5828 - mean_pred: 1.0428 - mae_t1: 0.0389 - val_loss: 4.5139 - val_mae: 0.5642 - val_mean_pred: 0.9196 - val_mae_t1: 0.0376\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 0s 101us/sample - loss: 4.0375 - mae: 0.5047 - mean_pred: 0.7580 - mae_t1: 0.0336 - val_loss: 4.6857 - val_mae: 0.5857 - val_mean_pred: 0.5389 - val_mae_t1: 0.0390\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 4.5912 - mae: 0.5739 - mean_pred: 0.4324 - mae_t1: 0.0383 - val_loss: 4.8319 - val_mae: 0.6040 - val_mean_pred: 0.5083 - val_mae_t1: 0.0403\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 4.4121 - mae: 0.5515 - mean_pred: 0.4891 - mae_t1: 0.0368 - val_loss: 4.1336 - val_mae: 0.5167 - val_mean_pred: 0.9964 - val_mae_t1: 0.0344\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 4.2122 - mae: 0.5265 - mean_pred: 0.9539 - mae_t1: 0.0351 - val_loss: 5.0019 - val_mae: 0.6252 - val_mean_pred: 1.1952 - val_mae_t1: 0.0417\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 0s 134us/sample - loss: 4.2564 - mae: 0.5320 - mean_pred: 0.9948 - mae_t1: 0.0355 - val_loss: 3.9399 - val_mae: 0.4925 - val_mean_pred: 0.8125 - val_mae_t1: 0.0328\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.7054 - mae: 0.4632 - mean_pred: 0.6739 - mae_t1: 0.0309 - val_loss: 4.4011 - val_mae: 0.5501 - val_mean_pred: 0.6541 - val_mae_t1: 0.0367\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.0373 - mae: 0.5047 - mean_pred: 0.5508 - mae_t1: 0.0336 - val_loss: 4.5147 - val_mae: 0.5643 - val_mean_pred: 0.6673 - val_mae_t1: 0.0376\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.8966 - mae: 0.4871 - mean_pred: 0.5994 - mae_t1: 0.0325 - val_loss: 4.0575 - val_mae: 0.5072 - val_mean_pred: 0.9011 - val_mae_t1: 0.0338\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6815 - mae: 0.4602 - mean_pred: 0.8531 - mae_t1: 0.0307 - val_loss: 4.1933 - val_mae: 0.5242 - val_mean_pred: 1.0593 - val_mae_t1: 0.0349\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.9607 - mae: 0.4951 - mean_pred: 0.9564 - mae_t1: 0.0330 - val_loss: 4.1004 - val_mae: 0.5126 - val_mean_pred: 0.9406 - val_mae_t1: 0.0342\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.7649 - mae: 0.4706 - mean_pred: 0.8254 - mae_t1: 0.0314 - val_loss: 3.9994 - val_mae: 0.4999 - val_mean_pred: 0.7557 - val_mae_t1: 0.0333\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.5822 - mae: 0.4478 - mean_pred: 0.6541 - mae_t1: 0.0299 - val_loss: 4.0959 - val_mae: 0.5120 - val_mean_pred: 0.6604 - val_mae_t1: 0.0341\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.6745 - mae: 0.4593 - mean_pred: 0.6317 - mae_t1: 0.0306 - val_loss: 4.2795 - val_mae: 0.5349 - val_mean_pred: 0.7033 - val_mae_t1: 0.0357\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.8218 - mae: 0.4777 - mean_pred: 0.6674 - mae_t1: 0.0318 - val_loss: 4.0414 - val_mae: 0.5052 - val_mean_pred: 0.7316 - val_mae_t1: 0.0337\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 0s 125us/sample - loss: 3.3662 - mae: 0.4208 - mean_pred: 0.7290 - mae_t1: 0.0281 - val_loss: 3.8944 - val_mae: 0.4868 - val_mean_pred: 0.8094 - val_mae_t1: 0.0325\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 0s 115us/sample - loss: 3.3321 - mae: 0.4165 - mean_pred: 0.7530 - mae_t1: 0.0278 - val_loss: 3.7824 - val_mae: 0.4728 - val_mean_pred: 0.7812 - val_mae_t1: 0.0315\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 0s 114us/sample - loss: 3.3289 - mae: 0.4161 - mean_pred: 0.7669 - mae_t1: 0.0277 - val_loss: 3.6548 - val_mae: 0.4569 - val_mean_pred: 0.8367 - val_mae_t1: 0.0305\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 3.2509 - mae: 0.4064 - mean_pred: 0.7749 - mae_t1: 0.0271 - val_loss: 3.5451 - val_mae: 0.4431 - val_mean_pred: 0.8074 - val_mae_t1: 0.0295\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.2700 - mae: 0.4087 - mean_pred: 0.7633 - mae_t1: 0.0272 - val_loss: 3.6296 - val_mae: 0.4537 - val_mean_pred: 0.7741 - val_mae_t1: 0.0302\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.3568 - mae: 0.4196 - mean_pred: 0.7191 - mae_t1: 0.0280 - val_loss: 3.7685 - val_mae: 0.4711 - val_mean_pred: 0.8908 - val_mae_t1: 0.0314\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3153 - mae: 0.4144 - mean_pred: 0.8554 - mae_t1: 0.0276 - val_loss: 3.8802 - val_mae: 0.4850 - val_mean_pred: 1.0238 - val_mae_t1: 0.0323\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 3.2249 - mae: 0.4031 - mean_pred: 0.9023 - mae_t1: 0.0269 - val_loss: 4.0453 - val_mae: 0.5057 - val_mean_pred: 0.7483 - val_mae_t1: 0.0337\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.3488 - mae: 0.4186 - mean_pred: 0.6317 - mae_t1: 0.0279 - val_loss: 4.5467 - val_mae: 0.5683 - val_mean_pred: 0.6323 - val_mae_t1: 0.0379\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.6251 - mae: 0.4531 - mean_pred: 0.5942 - mae_t1: 0.0302 - val_loss: 4.1770 - val_mae: 0.5221 - val_mean_pred: 0.8705 - val_mae_t1: 0.0348\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2504 - mae: 0.4063 - mean_pred: 0.8446 - mae_t1: 0.0271 - val_loss: 4.4416 - val_mae: 0.5552 - val_mean_pred: 1.0888 - val_mae_t1: 0.0370\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.6462 - mae: 0.4558 - mean_pred: 0.9711 - mae_t1: 0.0304 - val_loss: 4.0435 - val_mae: 0.5054 - val_mean_pred: 0.8359 - val_mae_t1: 0.0337\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3916 - mae: 0.4239 - mean_pred: 0.7447 - mae_t1: 0.0283 - val_loss: 4.5570 - val_mae: 0.5696 - val_mean_pred: 0.6517 - val_mae_t1: 0.0380\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7936 - mae: 0.4742 - mean_pred: 0.6140 - mae_t1: 0.0316 - val_loss: 4.2100 - val_mae: 0.5263 - val_mean_pred: 0.6925 - val_mae_t1: 0.0351\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 0s 99us/sample - loss: 3.3371 - mae: 0.4171 - mean_pred: 0.6861 - mae_t1: 0.0278 - val_loss: 3.9497 - val_mae: 0.4937 - val_mean_pred: 0.9805 - val_mae_t1: 0.0329\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.3649 - mae: 0.4206 - mean_pred: 0.9311 - mae_t1: 0.0280 - val_loss: 3.7208 - val_mae: 0.4651 - val_mean_pred: 0.9853 - val_mae_t1: 0.0310\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0331 - mae: 0.3791 - mean_pred: 0.8523 - mae_t1: 0.0253 - val_loss: 3.7563 - val_mae: 0.4695 - val_mean_pred: 0.8130 - val_mae_t1: 0.0313\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.2943 - mae: 0.4118 - mean_pred: 0.7405 - mae_t1: 0.0275 - val_loss: 3.6729 - val_mae: 0.4591 - val_mean_pred: 0.9261 - val_mae_t1: 0.0306\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.1918 - mae: 0.3990 - mean_pred: 0.8482 - mae_t1: 0.0266 - val_loss: 4.2660 - val_mae: 0.5333 - val_mean_pred: 1.0805 - val_mae_t1: 0.0356\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 4.1079 - mae: 0.5135 - mean_pred: 1.0018 - mae_t1: 0.0342 - val_loss: 3.8825 - val_mae: 0.4853 - val_mean_pred: 1.0150 - val_mae_t1: 0.0324\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.1926 - mae: 0.3991 - mean_pred: 0.8712 - mae_t1: 0.0266 - val_loss: 4.5765 - val_mae: 0.5721 - val_mean_pred: 0.6969 - val_mae_t1: 0.0381\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 0s 102us/sample - loss: 3.7672 - mae: 0.4709 - mean_pred: 0.6156 - mae_t1: 0.0314 - val_loss: 4.3672 - val_mae: 0.5459 - val_mean_pred: 0.6547 - val_mae_t1: 0.0364\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.2640 - mae: 0.4080 - mean_pred: 0.6385 - mae_t1: 0.0272 - val_loss: 3.8541 - val_mae: 0.4818 - val_mean_pred: 0.8420 - val_mae_t1: 0.0321\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.6290 - mae: 0.4536 - mean_pred: 0.7984 - mae_t1: 0.0302 - val_loss: 3.8721 - val_mae: 0.4840 - val_mean_pred: 0.7412 - val_mae_t1: 0.0323\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.3170 - mae: 0.4146 - mean_pred: 0.6400 - mae_t1: 0.0276 - val_loss: 4.7927 - val_mae: 0.5991 - val_mean_pred: 0.6461 - val_mae_t1: 0.0399\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 4.1059 - mae: 0.5132 - mean_pred: 0.5762 - mae_t1: 0.0342 - val_loss: 4.2182 - val_mae: 0.5273 - val_mean_pred: 0.8119 - val_mae_t1: 0.0352\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1193 - mae: 0.3899 - mean_pred: 0.7411 - mae_t1: 0.0260 - val_loss: 4.2691 - val_mae: 0.5336 - val_mean_pred: 1.0938 - val_mae_t1: 0.0356\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7241 - mae: 0.4655 - mean_pred: 0.9931 - mae_t1: 0.0310 - val_loss: 4.2456 - val_mae: 0.5307 - val_mean_pred: 1.0536 - val_mae_t1: 0.0354\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.1185 - mae: 0.3898 - mean_pred: 0.8911 - mae_t1: 0.0260 - val_loss: 4.3844 - val_mae: 0.5481 - val_mean_pred: 0.8111 - val_mae_t1: 0.0365\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.7227 - mae: 0.4653 - mean_pred: 0.7005 - mae_t1: 0.0310 - val_loss: 4.7271 - val_mae: 0.5909 - val_mean_pred: 0.8123 - val_mae_t1: 0.0394\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.7317 - mae: 0.4665 - mean_pred: 0.7167 - mae_t1: 0.0311 - val_loss: 4.3609 - val_mae: 0.5451 - val_mean_pred: 0.8605 - val_mae_t1: 0.0363\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.4285 - mae: 0.4286 - mean_pred: 0.7752 - mae_t1: 0.0286 - val_loss: 5.2128 - val_mae: 0.6516 - val_mean_pred: 0.9705 - val_mae_t1: 0.0434\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 4.1065 - mae: 0.5133 - mean_pred: 0.8927 - mae_t1: 0.0342 - val_loss: 4.3522 - val_mae: 0.5440 - val_mean_pred: 1.0124 - val_mae_t1: 0.0363\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1051 - mae: 0.3881 - mean_pred: 0.8863 - mae_t1: 0.0259 - val_loss: 4.3780 - val_mae: 0.5472 - val_mean_pred: 0.9036 - val_mae_t1: 0.0365\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.4790 - mae: 0.4349 - mean_pred: 0.8013 - mae_t1: 0.0290 - val_loss: 4.0413 - val_mae: 0.5052 - val_mean_pred: 0.7636 - val_mae_t1: 0.0337\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.1434 - mae: 0.3929 - mean_pred: 0.6786 - mae_t1: 0.0262 - val_loss: 3.6639 - val_mae: 0.4580 - val_mean_pred: 0.7494 - val_mae_t1: 0.0305\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 0s 117us/sample - loss: 3.0715 - mae: 0.3839 - mean_pred: 0.7069 - mae_t1: 0.0256 - val_loss: 3.4350 - val_mae: 0.4294 - val_mean_pred: 0.9230 - val_mae_t1: 0.0286\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.8923 - mae: 0.3615 - mean_pred: 0.8242 - mae_t1: 0.0241 - val_loss: 4.2423 - val_mae: 0.5303 - val_mean_pred: 1.0245 - val_mae_t1: 0.0354\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.3985 - mae: 0.4248 - mean_pred: 0.9104 - mae_t1: 0.0283 - val_loss: 3.8180 - val_mae: 0.4772 - val_mean_pred: 0.9867 - val_mae_t1: 0.0318\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.9498 - mae: 0.3687 - mean_pred: 0.8667 - mae_t1: 0.0246 - val_loss: 3.4993 - val_mae: 0.4374 - val_mean_pred: 0.8447 - val_mae_t1: 0.0292\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 2.8327 - mae: 0.3541 - mean_pred: 0.7708 - mae_t1: 0.0236 - val_loss: 3.5937 - val_mae: 0.4492 - val_mean_pred: 0.8953 - val_mae_t1: 0.0299\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8781 - mae: 0.3598 - mean_pred: 0.8525 - mae_t1: 0.0240 - val_loss: 3.5339 - val_mae: 0.4417 - val_mean_pred: 0.9031 - val_mae_t1: 0.0294\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.7225 - mae: 0.3403 - mean_pred: 0.7707 - mae_t1: 0.0227 - val_loss: 4.0220 - val_mae: 0.5027 - val_mean_pred: 0.7532 - val_mae_t1: 0.0335\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.0131 - mae: 0.3766 - mean_pred: 0.6591 - mae_t1: 0.0251 - val_loss: 4.2498 - val_mae: 0.5312 - val_mean_pred: 0.7621 - val_mae_t1: 0.0354\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 3.0751 - mae: 0.3844 - mean_pred: 0.6393 - mae_t1: 0.0256 - val_loss: 4.0756 - val_mae: 0.5094 - val_mean_pred: 0.8019 - val_mae_t1: 0.0340\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 2.8789 - mae: 0.3599 - mean_pred: 0.7153 - mae_t1: 0.0240 - val_loss: 4.0682 - val_mae: 0.5085 - val_mean_pred: 0.9861 - val_mae_t1: 0.0339\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0880 - mae: 0.3860 - mean_pred: 0.8824 - mae_t1: 0.0257 - val_loss: 4.6774 - val_mae: 0.5847 - val_mean_pred: 1.0653 - val_mae_t1: 0.0390\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 0s 90us/sample - loss: 3.8689 - mae: 0.4836 - mean_pred: 0.9224 - mae_t1: 0.0322 - val_loss: 4.6508 - val_mae: 0.5814 - val_mean_pred: 0.9975 - val_mae_t1: 0.0388\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.4999 - mae: 0.4375 - mean_pred: 0.8588 - mae_t1: 0.0292 - val_loss: 3.7189 - val_mae: 0.4649 - val_mean_pred: 0.8504 - val_mae_t1: 0.0310\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.2584 - mae: 0.4073 - mean_pred: 0.7601 - mae_t1: 0.0272 - val_loss: 4.3136 - val_mae: 0.5392 - val_mean_pred: 0.8286 - val_mae_t1: 0.0359\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 0s 91us/sample - loss: 3.7121 - mae: 0.4640 - mean_pred: 0.7533 - mae_t1: 0.0309 - val_loss: 3.8039 - val_mae: 0.4755 - val_mean_pred: 0.8415 - val_mae_t1: 0.0317\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9625 - mae: 0.3703 - mean_pred: 0.7277 - mae_t1: 0.0247 - val_loss: 4.2678 - val_mae: 0.5335 - val_mean_pred: 0.9683 - val_mae_t1: 0.0356\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.4446 - mae: 0.4306 - mean_pred: 0.8565 - mae_t1: 0.0287 - val_loss: 3.8573 - val_mae: 0.4822 - val_mean_pred: 1.0129 - val_mae_t1: 0.0321\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.9576 - mae: 0.3697 - mean_pred: 0.8520 - mae_t1: 0.0246 - val_loss: 3.5321 - val_mae: 0.4415 - val_mean_pred: 0.9016 - val_mae_t1: 0.0294\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8853 - mae: 0.3607 - mean_pred: 0.8036 - mae_t1: 0.0240 - val_loss: 3.5129 - val_mae: 0.4391 - val_mean_pred: 0.8516 - val_mae_t1: 0.0293\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9963 - mae: 0.3745 - mean_pred: 0.7640 - mae_t1: 0.0250 - val_loss: 4.1003 - val_mae: 0.5125 - val_mean_pred: 0.7523 - val_mae_t1: 0.0342\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.3489 - mae: 0.4186 - mean_pred: 0.6886 - mae_t1: 0.0279 - val_loss: 3.4795 - val_mae: 0.4349 - val_mean_pred: 0.8234 - val_mae_t1: 0.0290\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 0s 92us/sample - loss: 2.9640 - mae: 0.3705 - mean_pred: 0.8261 - mae_t1: 0.0247 - val_loss: 4.0424 - val_mae: 0.5053 - val_mean_pred: 1.0932 - val_mae_t1: 0.0337\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4137 - mae: 0.4267 - mean_pred: 1.0138 - mae_t1: 0.0284 - val_loss: 4.1185 - val_mae: 0.5148 - val_mean_pred: 1.0686 - val_mae_t1: 0.0343\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.2960 - mae: 0.4120 - mean_pred: 0.9170 - mae_t1: 0.0275 - val_loss: 4.3315 - val_mae: 0.5414 - val_mean_pred: 0.7822 - val_mae_t1: 0.0361\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.3968 - mae: 0.4246 - mean_pred: 0.6588 - mae_t1: 0.0283 - val_loss: 3.9663 - val_mae: 0.4958 - val_mean_pred: 0.7066 - val_mae_t1: 0.0331\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0622 - mae: 0.3828 - mean_pred: 0.6650 - mae_t1: 0.0255 - val_loss: 3.7643 - val_mae: 0.4705 - val_mean_pred: 0.9508 - val_mae_t1: 0.0314\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.6427 - mae: 0.3303 - mean_pred: 0.8622 - mae_t1: 0.0220 - val_loss: 3.8691 - val_mae: 0.4836 - val_mean_pred: 0.9390 - val_mae_t1: 0.0322\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 2.6668 - mae: 0.3333 - mean_pred: 0.7746 - mae_t1: 0.0222 - val_loss: 3.7089 - val_mae: 0.4636 - val_mean_pred: 0.7135 - val_mae_t1: 0.0309\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.9203 - mae: 0.3650 - mean_pred: 0.6401 - mae_t1: 0.0243 - val_loss: 3.4841 - val_mae: 0.4355 - val_mean_pred: 0.8196 - val_mae_t1: 0.0290\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 0s 96us/sample - loss: 2.8600 - mae: 0.3575 - mean_pred: 0.7675 - mae_t1: 0.0238 - val_loss: 3.7797 - val_mae: 0.4725 - val_mean_pred: 1.0296 - val_mae_t1: 0.0315\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 0s 88us/sample - loss: 3.0267 - mae: 0.3783 - mean_pred: 0.9359 - mae_t1: 0.0252 - val_loss: 3.6874 - val_mae: 0.4609 - val_mean_pred: 0.9847 - val_mae_t1: 0.0307\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 0s 89us/sample - loss: 2.8139 - mae: 0.3517 - mean_pred: 0.8217 - mae_t1: 0.0234 - val_loss: 3.5018 - val_mae: 0.4377 - val_mean_pred: 0.7099 - val_mae_t1: 0.0292\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 0s 86us/sample - loss: 2.9135 - mae: 0.3642 - mean_pred: 0.6131 - mae_t1: 0.0243 - val_loss: 3.9306 - val_mae: 0.4913 - val_mean_pred: 0.6962 - val_mae_t1: 0.0328\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.9471 - mae: 0.4934 - mean_pred: 0.6722 - mae_t1: 0.0329 - val_loss: 3.9083 - val_mae: 0.4885 - val_mean_pred: 0.7139 - val_mae_t1: 0.0326\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.4223 - mae: 0.4278 - mean_pred: 0.6439 - mae_t1: 0.0285 - val_loss: 3.9037 - val_mae: 0.4880 - val_mean_pred: 0.6975 - val_mae_t1: 0.0325\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 3.0663 - mae: 0.3833 - mean_pred: 0.6280 - mae_t1: 0.0256 - val_loss: 3.5447 - val_mae: 0.4431 - val_mean_pred: 0.8700 - val_mae_t1: 0.0295\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 0s 87us/sample - loss: 2.5326 - mae: 0.3166 - mean_pred: 0.7950 - mae_t1: 0.0211 - val_loss: 3.6469 - val_mae: 0.4559 - val_mean_pred: 1.0335 - val_mae_t1: 0.0304\n",
      "Earliness...\n",
      "0.0014996528625488281\n",
      "____________________________________________________________\n",
      "Test MAE:      0.34116301817271166  (days)\n",
      "================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.134 MB of 0.134 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>▁</td></tr><tr><td>AE_2</td><td>▁</td></tr><tr><td>AE_3</td><td>▁</td></tr><tr><td>MAE</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▇▆▅▆▅▅▄▄▄▃▃▃▃▃▄▂▂▄▃▂▂▃▂▂▂▂▂▂▃▂▂▃▃▂▂▂▄▁</td></tr><tr><td>mae</td><td>█▆▇▆▅▆▅▅▄▄▄▃▃▃▃▃▄▂▂▄▃▂▂▃▂▂▂▂▂▂▃▂▂▃▃▂▂▂▄▁</td></tr><tr><td>mae_t1</td><td>█▆▇▆▅▆▅▅▄▄▄▃▃▃▃▃▄▂▂▄▃▂▂▃▂▂▂▂▂▂▃▂▂▃▃▂▂▂▄▁</td></tr><tr><td>mean_pred</td><td>▁▅█▁▆█▂▇▃▇▄▅▅▆▄▇▃▆▆▃▅▅▆▅▆▄▆▆▄▆▅▅▅▄▆▄▄▇▄▅</td></tr><tr><td>val_loss</td><td>▄▇▅█▇▅▄▃▃▄▄▃▁▃▅▃▄▂▄▅▃▄▅█▅▁▁▁▃▆▄▄▁▁▄▂▁▂▃▂</td></tr><tr><td>val_mae</td><td>▄▇▅█▇▅▄▃▃▄▄▃▁▃▅▃▄▂▄▅▃▄▅█▅▁▁▁▃▆▄▄▁▁▄▂▁▂▃▂</td></tr><tr><td>val_mae_t1</td><td>▄▇▅█▇▅▄▃▃▄▄▃▁▃▅▃▄▂▄▅▃▄▅█▅▁▁▁▃▆▄▄▁▁▄▂▁▂▃▂</td></tr><tr><td>val_mean_pred</td><td>▆▁▇▁▇▆▇▅▆▆▄▅▅▇▃▅▃▅█▃▄█▅▇▆▆▅▆▅█▅▇▅▅▄▆▅▇▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AE_1</td><td>0.38897</td></tr><tr><td>AE_2</td><td>0.31852</td></tr><tr><td>AE_3</td><td>0.31031</td></tr><tr><td>MAE</td><td>0.34116</td></tr><tr><td>best_epoch</td><td>63</td></tr><tr><td>best_val_loss</td><td>3.43504</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>2.53261</td></tr><tr><td>mae</td><td>0.31658</td></tr><tr><td>mae_t1</td><td>0.02111</td></tr><tr><td>mean_pred</td><td>0.79504</td></tr><tr><td>val_loss</td><td>3.64689</td></tr><tr><td>val_mae</td><td>0.45586</td></tr><tr><td>val_mae_t1</td><td>0.03039</td></tr><tr><td>val_mean_pred</td><td>1.03348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">worldly-feather-100</strong>: <a href=\"https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2i3hn7dr\" target=\"_blank\">https://wandb.ai/mikeriess/sim_play_pfx_5x5x2/runs/2i3hn7dr</a><br/>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>D:/wandb/wandb\\run-20220508_160149-2i3hn7dr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "inference_tables = []\n",
    "\n",
    "for run_no in df.index:\n",
    "    #print(run)\n",
    "    #print(df.loc[run])\n",
    "    \n",
    "    \"\"\"\n",
    "    Settings from experiments\n",
    "    \"\"\"\n",
    "    curr_settings = df.loc[run_no]\n",
    "    \n",
    "    \"\"\"\n",
    "    ##### Add settings to W&B\n",
    "    \"\"\"\n",
    "    \n",
    "    run = wandb.init(project=project_name, \n",
    "                     entity=\"mikeriess\", \n",
    "                     config=df.loc[run_no].to_dict(),\n",
    "                     dir=\"D:/wandb/\")\n",
    "    \n",
    "    \n",
    "    # wandb.config = {\n",
    "    #   \"learning_rate\": 0.001,\n",
    "    #   \"epochs\": 100,\n",
    "    #   \"batch_size\": 128\n",
    "    # }\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Prepare data (transformation) (FROZEN)\n",
    "    \"\"\"\n",
    "\n",
    "    # part two\n",
    "    data_objects = prepare_dataset_from_memory(Input_data, \n",
    "                                               sample=1.0, \n",
    "                                               transform=curr_settings[\"y_transformation\"][0], \n",
    "                                               first_state=False)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    settings for modelling\n",
    "    \"\"\"\n",
    "     \n",
    "    # Generate the architecture:    \n",
    "    modelparams = {\"FC_BLOCK1\":int(curr_settings[\"num_units\"]),#np.random.choice([25, 50, 75, 100, 125, 150, 175, 200], 1)[0],\n",
    "                   \"FC_BLOCK2\":int(curr_settings[\"num_units\"]),#np.random.choice([25, 50, 75, 100, 125, 150, 175, 200], 1)[0],\n",
    "                   \"FC_BLOCK3\":int(curr_settings[\"num_units\"]),#np.random.choice([25, 50, 75, 100, 125, 150, 175, 200], 1)[0],\n",
    "                   \"FC_BLOCK4\":int(curr_settings[\"num_units\"]),#np.random.choice([25, 50, 75, 100, 125, 150, 175, 200], 1)[0],\n",
    "\n",
    "                   \"BLOCK_LAYERS\":int(curr_settings[\"num_blocks\"]),#np.random.choice([1,2], 1)[0],\n",
    "\n",
    "                   \"DROPOUT_RATE\":curr_settings[\"dropout\"], #np.random.choice([0.05, 0.1, 0.2, 0.3, 0.4], 1)[0],\n",
    "\n",
    "                   ### CNN HPO:\n",
    "                   \"FULLY_CONNECTED\":int(curr_settings[\"num_units\"]),#np.random.choice([25, 50, 75, 100, 125, 150, 175, 200], 1)[0],\n",
    "\n",
    "                   ### common:\n",
    "                   \"batch_size\":curr_settings[\"batch_size\"], #\n",
    "                   \"learningrate\":curr_settings[\"learningrate\"], #np.random.choice(, 1)[0], #np.random.choice(np.linspace(start=0.001,stop=0.25, num=100,endpoint=False), 1)[0]\n",
    "                   \"optimizer\":curr_settings[\"optimizer\"], #\"Adam\"\n",
    "                   \"epochs\":int(curr_settings[\"epochs\"]),\n",
    "\n",
    "                   #misc\n",
    "                   \"lossfunction\":curr_settings[\"loss_function\"],\n",
    "                   \"y_transformation\":curr_settings[\"y_transformation\"],\n",
    "\n",
    "                    # Get the loss function parameters:\n",
    "                    \"alpha\":curr_settings[\"alpha\"],\n",
    "                    \"beta\":curr_settings[\"beta\"],\n",
    "                    \"gamma\":curr_settings[\"gamma\"],\n",
    "\n",
    "                   }\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Train a model\n",
    "    \"\"\"\n",
    "    data_objects = train_model(data_objects, modelparams, curr_settings)\n",
    "    \n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    data_objects = evaluate_rt_model(data_objects, curr_settings)\n",
    "    \n",
    "    \"\"\"\n",
    "    Store the results\n",
    "    \"\"\"\n",
    "    curr_settings[\"RES_num_events\"] = len(log)\n",
    "    \n",
    "    curr_settings[\"MAE\"] = data_objects[\"report\"].loc[0][\"MAE\"]\n",
    "    curr_settings[\"Time\"] = data_objects[\"report\"].loc[0][\"Time\"]\n",
    "    curr_settings[\"Traintime\"] = data_objects[\"report\"].loc[0][\"Traintime\"]\n",
    "    \n",
    "    #get multiple cols from prefix performance\n",
    "    prefix_perf = data_objects[\"prefix_performance\"].loc[0]\n",
    "    \n",
    "    #add them to table\n",
    "    curr_settings = pd.concat([curr_settings,prefix_perf])\n",
    "    \n",
    "    out = pd.DataFrame(curr_settings.T)\n",
    "        \n",
    "    results.append(out)\n",
    "\n",
    "    inference_i = data_objects[\"Inference_test\"][[\"caseid\",\"prefix_number\",\"prefixes\",\"distinct_events\",\"y\",\"y_pred\",\"AE\"]]\n",
    "    inference_i[\"RUN\"] = run_no\n",
    "    inference_i[\"process_memory\"] = curr_settings[\"process_type\"]\n",
    "    inference_tables.append(inference_i)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ##### W&B end the run\n",
    "    \"\"\"\n",
    "    \n",
    "    wandb.log({\"MAE\":curr_settings[\"MAE\"]})\n",
    "    \n",
    "    wandb.log({\"AE_1\":curr_settings[\"AE_1\"]})\n",
    "    wandb.log({\"AE_2\":curr_settings[\"AE_2\"]})\n",
    "    wandb.log({\"AE_3\":curr_settings[\"AE_3\"]})\n",
    "    \n",
    "    \"\"\"\n",
    "    Test: prefix performance plot\n",
    "    \"\"\"\n",
    "    \n",
    "    pfx_mae = inference_i.pivot_table(index='prefix_number', aggfunc={'AE':np.mean})\n",
    "    pfx_mae[\"prefix\"] = pfx_mae.index\n",
    "    pfx_mae.index = list(range(0,len(pfx_mae)))\n",
    "    \n",
    "    pfx_mae = wandb.Table(dataframe=pfx_mae) #, columns=[\"prefix\",\"MAE_t\"]\n",
    "    pfx_mae_plot = wandb.plot.line(pfx_mae, x=\"prefix\", y=\"AE\", title=\"MAE_t per prefix\")\n",
    "    \n",
    "    wandb.log({\"prefix_MAE_t\":pfx_mae_plot})\n",
    "    \n",
    "    \n",
    "    pfx_sd = inference_i.pivot_table(index='prefix_number', aggfunc={'AE':np.std})\n",
    "    pfx_sd[\"prefix\"] = pfx_sd.index\n",
    "    pfx_sd.index = list(range(0,len(pfx_sd)))\n",
    "    \n",
    "    pfx_sd = wandb.Table(dataframe=pfx_sd) #, columns=[\"prefix\",\"MAE_t\"]\n",
    "    pfx_sd_plot = wandb.plot.line(pfx_sd, x=\"prefix\", y=\"AE\", title=\"SD_t per prefix\")\n",
    "    \n",
    "    wandb.log({\"prefix_sd_t\":pfx_sd_plot})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #wandb.log({\"bar_prefix_MAE_t\" : wandb.plot.bar(pfxdata, \"prefix\", \"AE\", title=\"MAE_t Bar Chart\")})\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    run.join()\n",
    "    \n",
    "    \n",
    "# Make final table\n",
    "#results_table = pd.concat(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063debee-7776-40cd-b990-cb3031c24976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_i\n",
    "\n",
    "#inference_i.pivot_table(index='prefix_number', aggfunc={'AE':np.mean})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba2fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Store results tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e522cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final table\n",
    "results_table = pd.concat(results, axis=1)\n",
    "results_table = results_table.T\n",
    "results_table.index = list(range(0,len(results_table)))\n",
    "#results_table.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41aca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_all = pd.concat(inference_tables, axis=0, ignore_index=True)\n",
    "inference_table_all.index = list(range(0,len(inference_table_all)))\n",
    "#inference_table_all.to_csv(\"inference_table_all.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28daa6bc",
   "metadata": {},
   "source": [
    "# Inspect results tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae12cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learningrate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>dropout</th>\n",
       "      <th>y_transformation</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>gamma</th>\n",
       "      <th>...</th>\n",
       "      <th>repetition</th>\n",
       "      <th>RUN</th>\n",
       "      <th>RES_num_events</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "      <th>Traintime</th>\n",
       "      <th>AE_1</th>\n",
       "      <th>AE_2</th>\n",
       "      <th>AE_3</th>\n",
       "      <th>AE_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>822</td>\n",
       "      <td>0.358021</td>\n",
       "      <td>2022/05/08, 15:40:51</td>\n",
       "      <td>8.989201</td>\n",
       "      <td>0.399212</td>\n",
       "      <td>0.373171</td>\n",
       "      <td>0.370554</td>\n",
       "      <td>0.329822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>822</td>\n",
       "      <td>0.316761</td>\n",
       "      <td>2022/05/08, 15:41:18</td>\n",
       "      <td>13.263919</td>\n",
       "      <td>0.308696</td>\n",
       "      <td>0.293006</td>\n",
       "      <td>0.292217</td>\n",
       "      <td>0.300938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>822</td>\n",
       "      <td>0.356384</td>\n",
       "      <td>2022/05/08, 15:41:43</td>\n",
       "      <td>8.967124</td>\n",
       "      <td>0.361603</td>\n",
       "      <td>0.290779</td>\n",
       "      <td>0.334557</td>\n",
       "      <td>0.332782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>822</td>\n",
       "      <td>0.368142</td>\n",
       "      <td>2022/05/08, 15:42:08</td>\n",
       "      <td>13.375228</td>\n",
       "      <td>0.419116</td>\n",
       "      <td>0.409769</td>\n",
       "      <td>0.316663</td>\n",
       "      <td>0.332109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>822</td>\n",
       "      <td>0.331886</td>\n",
       "      <td>2022/05/08, 15:42:33</td>\n",
       "      <td>8.717787</td>\n",
       "      <td>0.323346</td>\n",
       "      <td>0.281521</td>\n",
       "      <td>0.334016</td>\n",
       "      <td>0.34126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>822</td>\n",
       "      <td>0.485406</td>\n",
       "      <td>2022/05/08, 15:43:02</td>\n",
       "      <td>14.585893</td>\n",
       "      <td>0.453163</td>\n",
       "      <td>0.36626</td>\n",
       "      <td>0.421903</td>\n",
       "      <td>0.485689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>822</td>\n",
       "      <td>0.392773</td>\n",
       "      <td>2022/05/08, 15:43:30</td>\n",
       "      <td>9.793743</td>\n",
       "      <td>0.393227</td>\n",
       "      <td>0.387804</td>\n",
       "      <td>0.363374</td>\n",
       "      <td>0.414232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>822</td>\n",
       "      <td>0.472515</td>\n",
       "      <td>2022/05/08, 15:44:05</td>\n",
       "      <td>13.045527</td>\n",
       "      <td>0.485183</td>\n",
       "      <td>0.421899</td>\n",
       "      <td>0.48466</td>\n",
       "      <td>0.404837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>822</td>\n",
       "      <td>0.322713</td>\n",
       "      <td>2022/05/08, 15:44:27</td>\n",
       "      <td>8.81156</td>\n",
       "      <td>0.316682</td>\n",
       "      <td>0.281978</td>\n",
       "      <td>0.309257</td>\n",
       "      <td>0.401633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>822</td>\n",
       "      <td>0.370417</td>\n",
       "      <td>2022/05/08, 15:44:54</td>\n",
       "      <td>14.318308</td>\n",
       "      <td>0.453964</td>\n",
       "      <td>0.339276</td>\n",
       "      <td>0.308794</td>\n",
       "      <td>0.325693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>822</td>\n",
       "      <td>0.353665</td>\n",
       "      <td>2022/05/08, 15:45:18</td>\n",
       "      <td>9.006847</td>\n",
       "      <td>0.473836</td>\n",
       "      <td>0.343159</td>\n",
       "      <td>0.311077</td>\n",
       "      <td>0.270085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>822</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>2022/05/08, 15:45:47</td>\n",
       "      <td>13.043032</td>\n",
       "      <td>0.365272</td>\n",
       "      <td>0.290896</td>\n",
       "      <td>0.303176</td>\n",
       "      <td>0.305255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341271</td>\n",
       "      <td>2022/05/08, 15:46:15</td>\n",
       "      <td>9.009533</td>\n",
       "      <td>0.403598</td>\n",
       "      <td>0.296567</td>\n",
       "      <td>0.331714</td>\n",
       "      <td>0.368743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>822</td>\n",
       "      <td>0.331149</td>\n",
       "      <td>2022/05/08, 15:46:42</td>\n",
       "      <td>12.998656</td>\n",
       "      <td>0.404477</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>0.309329</td>\n",
       "      <td>0.259126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>822</td>\n",
       "      <td>0.317151</td>\n",
       "      <td>2022/05/08, 15:47:06</td>\n",
       "      <td>9.45688</td>\n",
       "      <td>0.364505</td>\n",
       "      <td>0.307303</td>\n",
       "      <td>0.324404</td>\n",
       "      <td>0.297551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>822</td>\n",
       "      <td>0.334406</td>\n",
       "      <td>2022/05/08, 15:47:35</td>\n",
       "      <td>13.010383</td>\n",
       "      <td>0.337774</td>\n",
       "      <td>0.324463</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>0.303673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "      <td>822</td>\n",
       "      <td>0.326766</td>\n",
       "      <td>2022/05/08, 15:47:57</td>\n",
       "      <td>8.797354</td>\n",
       "      <td>0.4255</td>\n",
       "      <td>0.328419</td>\n",
       "      <td>0.306548</td>\n",
       "      <td>0.287978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "      <td>822</td>\n",
       "      <td>0.319884</td>\n",
       "      <td>2022/05/08, 15:48:25</td>\n",
       "      <td>13.086464</td>\n",
       "      <td>0.39037</td>\n",
       "      <td>0.28437</td>\n",
       "      <td>0.376036</td>\n",
       "      <td>0.258464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>822</td>\n",
       "      <td>0.36051</td>\n",
       "      <td>2022/05/08, 15:48:54</td>\n",
       "      <td>8.920228</td>\n",
       "      <td>0.336853</td>\n",
       "      <td>0.350465</td>\n",
       "      <td>0.298924</td>\n",
       "      <td>0.286462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>822</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>2022/05/08, 15:49:21</td>\n",
       "      <td>13.000605</td>\n",
       "      <td>0.400807</td>\n",
       "      <td>0.299861</td>\n",
       "      <td>0.264516</td>\n",
       "      <td>0.285987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>822</td>\n",
       "      <td>0.377498</td>\n",
       "      <td>2022/05/08, 15:49:43</td>\n",
       "      <td>9.459116</td>\n",
       "      <td>0.561406</td>\n",
       "      <td>0.358077</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.30325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>822</td>\n",
       "      <td>0.304884</td>\n",
       "      <td>2022/05/08, 15:50:10</td>\n",
       "      <td>13.290977</td>\n",
       "      <td>0.33488</td>\n",
       "      <td>0.303462</td>\n",
       "      <td>0.32059</td>\n",
       "      <td>0.314135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341068</td>\n",
       "      <td>2022/05/08, 15:50:33</td>\n",
       "      <td>9.096085</td>\n",
       "      <td>0.333905</td>\n",
       "      <td>0.325235</td>\n",
       "      <td>0.316052</td>\n",
       "      <td>0.303541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "      <td>822</td>\n",
       "      <td>0.318021</td>\n",
       "      <td>2022/05/08, 15:51:00</td>\n",
       "      <td>13.132267</td>\n",
       "      <td>0.378323</td>\n",
       "      <td>0.229717</td>\n",
       "      <td>0.307506</td>\n",
       "      <td>0.3527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>822</td>\n",
       "      <td>0.299139</td>\n",
       "      <td>2022/05/08, 15:51:21</td>\n",
       "      <td>8.989135</td>\n",
       "      <td>0.353176</td>\n",
       "      <td>0.294607</td>\n",
       "      <td>0.288522</td>\n",
       "      <td>0.266007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>822</td>\n",
       "      <td>0.337355</td>\n",
       "      <td>2022/05/08, 15:51:56</td>\n",
       "      <td>13.225945</td>\n",
       "      <td>0.411795</td>\n",
       "      <td>0.306611</td>\n",
       "      <td>0.28078</td>\n",
       "      <td>0.25379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>822</td>\n",
       "      <td>0.320961</td>\n",
       "      <td>2022/05/08, 15:52:18</td>\n",
       "      <td>9.097372</td>\n",
       "      <td>0.387906</td>\n",
       "      <td>0.25213</td>\n",
       "      <td>0.262803</td>\n",
       "      <td>0.305349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>822</td>\n",
       "      <td>0.348844</td>\n",
       "      <td>2022/05/08, 15:52:49</td>\n",
       "      <td>13.766808</td>\n",
       "      <td>0.377832</td>\n",
       "      <td>0.323053</td>\n",
       "      <td>0.347102</td>\n",
       "      <td>0.33694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>822</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>2022/05/08, 15:53:14</td>\n",
       "      <td>9.145357</td>\n",
       "      <td>0.394325</td>\n",
       "      <td>0.307704</td>\n",
       "      <td>0.345167</td>\n",
       "      <td>0.324182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30</td>\n",
       "      <td>822</td>\n",
       "      <td>0.340073</td>\n",
       "      <td>2022/05/08, 15:53:42</td>\n",
       "      <td>13.029304</td>\n",
       "      <td>0.437775</td>\n",
       "      <td>0.307514</td>\n",
       "      <td>0.310447</td>\n",
       "      <td>0.309602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>822</td>\n",
       "      <td>0.377306</td>\n",
       "      <td>2022/05/08, 15:54:05</td>\n",
       "      <td>9.842166</td>\n",
       "      <td>0.511826</td>\n",
       "      <td>0.360754</td>\n",
       "      <td>0.271005</td>\n",
       "      <td>0.322914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>822</td>\n",
       "      <td>0.327099</td>\n",
       "      <td>2022/05/08, 15:54:32</td>\n",
       "      <td>13.204188</td>\n",
       "      <td>0.367453</td>\n",
       "      <td>0.33066</td>\n",
       "      <td>0.30963</td>\n",
       "      <td>0.309675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>822</td>\n",
       "      <td>0.316543</td>\n",
       "      <td>2022/05/08, 15:54:55</td>\n",
       "      <td>9.608252</td>\n",
       "      <td>0.40538</td>\n",
       "      <td>0.281143</td>\n",
       "      <td>0.328623</td>\n",
       "      <td>0.283622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "      <td>822</td>\n",
       "      <td>0.310723</td>\n",
       "      <td>2022/05/08, 15:55:20</td>\n",
       "      <td>13.14671</td>\n",
       "      <td>0.335371</td>\n",
       "      <td>0.303588</td>\n",
       "      <td>0.30006</td>\n",
       "      <td>0.283879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>822</td>\n",
       "      <td>0.303345</td>\n",
       "      <td>2022/05/08, 15:55:44</td>\n",
       "      <td>9.924299</td>\n",
       "      <td>0.350278</td>\n",
       "      <td>0.293565</td>\n",
       "      <td>0.275322</td>\n",
       "      <td>0.276828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36</td>\n",
       "      <td>822</td>\n",
       "      <td>0.350509</td>\n",
       "      <td>2022/05/08, 15:56:10</td>\n",
       "      <td>13.214521</td>\n",
       "      <td>0.439592</td>\n",
       "      <td>0.390317</td>\n",
       "      <td>0.295267</td>\n",
       "      <td>0.340893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>822</td>\n",
       "      <td>0.365439</td>\n",
       "      <td>2022/05/08, 15:56:33</td>\n",
       "      <td>9.967258</td>\n",
       "      <td>0.433456</td>\n",
       "      <td>0.301648</td>\n",
       "      <td>0.375876</td>\n",
       "      <td>0.385029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>822</td>\n",
       "      <td>0.293821</td>\n",
       "      <td>2022/05/08, 15:57:01</td>\n",
       "      <td>14.367573</td>\n",
       "      <td>0.335666</td>\n",
       "      <td>0.287244</td>\n",
       "      <td>0.299678</td>\n",
       "      <td>0.271733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>822</td>\n",
       "      <td>0.338342</td>\n",
       "      <td>2022/05/08, 15:57:24</td>\n",
       "      <td>9.863151</td>\n",
       "      <td>0.357844</td>\n",
       "      <td>0.276592</td>\n",
       "      <td>0.368158</td>\n",
       "      <td>0.329502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>822</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>2022/05/08, 15:57:53</td>\n",
       "      <td>13.486431</td>\n",
       "      <td>0.531936</td>\n",
       "      <td>0.342114</td>\n",
       "      <td>0.304741</td>\n",
       "      <td>0.303843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41</td>\n",
       "      <td>822</td>\n",
       "      <td>0.348359</td>\n",
       "      <td>2022/05/08, 15:58:18</td>\n",
       "      <td>9.331533</td>\n",
       "      <td>0.473232</td>\n",
       "      <td>0.323886</td>\n",
       "      <td>0.290402</td>\n",
       "      <td>0.304391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42</td>\n",
       "      <td>822</td>\n",
       "      <td>0.340013</td>\n",
       "      <td>2022/05/08, 15:58:50</td>\n",
       "      <td>14.34408</td>\n",
       "      <td>0.380046</td>\n",
       "      <td>0.320033</td>\n",
       "      <td>0.326045</td>\n",
       "      <td>0.31863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43</td>\n",
       "      <td>822</td>\n",
       "      <td>0.357232</td>\n",
       "      <td>2022/05/08, 15:59:12</td>\n",
       "      <td>8.951424</td>\n",
       "      <td>0.532942</td>\n",
       "      <td>0.341109</td>\n",
       "      <td>0.30588</td>\n",
       "      <td>0.279556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44</td>\n",
       "      <td>822</td>\n",
       "      <td>0.336864</td>\n",
       "      <td>2022/05/08, 15:59:52</td>\n",
       "      <td>14.989444</td>\n",
       "      <td>0.365446</td>\n",
       "      <td>0.331207</td>\n",
       "      <td>0.306473</td>\n",
       "      <td>0.328298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45</td>\n",
       "      <td>822</td>\n",
       "      <td>0.30092</td>\n",
       "      <td>2022/05/08, 16:00:15</td>\n",
       "      <td>8.965955</td>\n",
       "      <td>0.353039</td>\n",
       "      <td>0.301073</td>\n",
       "      <td>0.272802</td>\n",
       "      <td>0.284072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>822</td>\n",
       "      <td>0.304483</td>\n",
       "      <td>2022/05/08, 16:00:44</td>\n",
       "      <td>13.362375</td>\n",
       "      <td>0.387309</td>\n",
       "      <td>0.296611</td>\n",
       "      <td>0.291969</td>\n",
       "      <td>0.277323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47</td>\n",
       "      <td>822</td>\n",
       "      <td>0.320318</td>\n",
       "      <td>2022/05/08, 16:01:12</td>\n",
       "      <td>8.860831</td>\n",
       "      <td>0.321986</td>\n",
       "      <td>0.309435</td>\n",
       "      <td>0.309549</td>\n",
       "      <td>0.280101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48</td>\n",
       "      <td>822</td>\n",
       "      <td>0.302331</td>\n",
       "      <td>2022/05/08, 16:01:39</td>\n",
       "      <td>13.586211</td>\n",
       "      <td>0.335622</td>\n",
       "      <td>0.243469</td>\n",
       "      <td>0.3004</td>\n",
       "      <td>0.300824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.2</td>\n",
       "      <td>standard</td>\n",
       "      <td>MAE_td</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_units num_blocks epochs batch_size learningrate optimizer dropout  \\\n",
       "0       50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "1       50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "2       50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "3       50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "4       50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "5       50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "6       50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "7       50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "8       50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "9       50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "10      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "11      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "12      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "13      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "14      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "15      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "16      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "17      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "18      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "19      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "20      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "21      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "22      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "23      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "24      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "25      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "26      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "27      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "28      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "29      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "30      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "31      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "32      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "33      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "34      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "35      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "36      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "37      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "38      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "39      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "40      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "41      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "42      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "43      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "44      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "45      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "46      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "47      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "48      50.0        1.0  100.0      128.0         0.01       SGD     0.2   \n",
       "49      50.0        1.0  100.0      256.0         0.01       SGD     0.2   \n",
       "\n",
       "   y_transformation loss_function gamma  ... repetition RUN RES_num_events  \\\n",
       "0          standard        MAE_td   0.0  ...        1.0   0            822   \n",
       "1          standard        MAE_td   0.0  ...        1.0   1            822   \n",
       "2          standard        MAE_td   0.0  ...        1.0   2            822   \n",
       "3          standard        MAE_td   0.0  ...        1.0   3            822   \n",
       "4          standard        MAE_td   0.0  ...        1.0   4            822   \n",
       "5          standard        MAE_td   0.0  ...        1.0   5            822   \n",
       "6          standard        MAE_td   0.0  ...        1.0   6            822   \n",
       "7          standard        MAE_td   0.0  ...        1.0   7            822   \n",
       "8          standard        MAE_td   0.0  ...        1.0   8            822   \n",
       "9          standard        MAE_td   0.0  ...        1.0   9            822   \n",
       "10         standard        MAE_td   0.0  ...        1.0  10            822   \n",
       "11         standard        MAE_td   0.0  ...        1.0  11            822   \n",
       "12         standard        MAE_td   0.0  ...        1.0  12            822   \n",
       "13         standard        MAE_td   0.0  ...        1.0  13            822   \n",
       "14         standard        MAE_td   0.0  ...        1.0  14            822   \n",
       "15         standard        MAE_td   0.0  ...        1.0  15            822   \n",
       "16         standard        MAE_td   0.0  ...        1.0  16            822   \n",
       "17         standard        MAE_td   0.0  ...        1.0  17            822   \n",
       "18         standard        MAE_td   0.0  ...        1.0  18            822   \n",
       "19         standard        MAE_td   0.0  ...        1.0  19            822   \n",
       "20         standard        MAE_td   0.0  ...        1.0  20            822   \n",
       "21         standard        MAE_td   0.0  ...        1.0  21            822   \n",
       "22         standard        MAE_td   0.0  ...        1.0  22            822   \n",
       "23         standard        MAE_td   0.0  ...        1.0  23            822   \n",
       "24         standard        MAE_td   0.0  ...        1.0  24            822   \n",
       "25         standard        MAE_td   0.0  ...        1.0  25            822   \n",
       "26         standard        MAE_td   0.0  ...        1.0  26            822   \n",
       "27         standard        MAE_td   0.0  ...        1.0  27            822   \n",
       "28         standard        MAE_td   0.0  ...        1.0  28            822   \n",
       "29         standard        MAE_td   0.0  ...        1.0  29            822   \n",
       "30         standard        MAE_td   0.0  ...        1.0  30            822   \n",
       "31         standard        MAE_td   0.0  ...        1.0  31            822   \n",
       "32         standard        MAE_td   0.0  ...        1.0  32            822   \n",
       "33         standard        MAE_td   0.0  ...        1.0  33            822   \n",
       "34         standard        MAE_td   0.0  ...        1.0  34            822   \n",
       "35         standard        MAE_td   0.0  ...        1.0  35            822   \n",
       "36         standard        MAE_td   0.0  ...        1.0  36            822   \n",
       "37         standard        MAE_td   0.0  ...        1.0  37            822   \n",
       "38         standard        MAE_td   0.0  ...        1.0  38            822   \n",
       "39         standard        MAE_td   0.0  ...        1.0  39            822   \n",
       "40         standard        MAE_td   0.0  ...        1.0  40            822   \n",
       "41         standard        MAE_td   0.0  ...        1.0  41            822   \n",
       "42         standard        MAE_td   0.0  ...        1.0  42            822   \n",
       "43         standard        MAE_td   0.0  ...        1.0  43            822   \n",
       "44         standard        MAE_td   0.0  ...        1.0  44            822   \n",
       "45         standard        MAE_td   0.0  ...        1.0  45            822   \n",
       "46         standard        MAE_td   0.0  ...        1.0  46            822   \n",
       "47         standard        MAE_td   0.0  ...        1.0  47            822   \n",
       "48         standard        MAE_td   0.0  ...        1.0  48            822   \n",
       "49         standard        MAE_td   0.0  ...        1.0  49            822   \n",
       "\n",
       "         MAE                  Time  Traintime      AE_1      AE_2      AE_3  \\\n",
       "0   0.324216  2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601   \n",
       "1   0.358021  2022/05/08, 15:40:51   8.989201  0.399212  0.373171  0.370554   \n",
       "2   0.316761  2022/05/08, 15:41:18  13.263919  0.308696  0.293006  0.292217   \n",
       "3   0.356384  2022/05/08, 15:41:43   8.967124  0.361603  0.290779  0.334557   \n",
       "4   0.368142  2022/05/08, 15:42:08  13.375228  0.419116  0.409769  0.316663   \n",
       "5   0.331886  2022/05/08, 15:42:33   8.717787  0.323346  0.281521  0.334016   \n",
       "6   0.485406  2022/05/08, 15:43:02  14.585893  0.453163   0.36626  0.421903   \n",
       "7   0.392773  2022/05/08, 15:43:30   9.793743  0.393227  0.387804  0.363374   \n",
       "8   0.472515  2022/05/08, 15:44:05  13.045527  0.485183  0.421899   0.48466   \n",
       "9   0.322713  2022/05/08, 15:44:27    8.81156  0.316682  0.281978  0.309257   \n",
       "10  0.370417  2022/05/08, 15:44:54  14.318308  0.453964  0.339276  0.308794   \n",
       "11  0.353665  2022/05/08, 15:45:18   9.006847  0.473836  0.343159  0.311077   \n",
       "12  0.317788  2022/05/08, 15:45:47  13.043032  0.365272  0.290896  0.303176   \n",
       "13  0.341271  2022/05/08, 15:46:15   9.009533  0.403598  0.296567  0.331714   \n",
       "14  0.331149  2022/05/08, 15:46:42  12.998656  0.404477  0.322786  0.309329   \n",
       "15  0.317151  2022/05/08, 15:47:06    9.45688  0.364505  0.307303  0.324404   \n",
       "16  0.334406  2022/05/08, 15:47:35  13.010383  0.337774  0.324463  0.352875   \n",
       "17  0.326766  2022/05/08, 15:47:57   8.797354    0.4255  0.328419  0.306548   \n",
       "18  0.319884  2022/05/08, 15:48:25  13.086464   0.39037   0.28437  0.376036   \n",
       "19   0.36051  2022/05/08, 15:48:54   8.920228  0.336853  0.350465  0.298924   \n",
       "20  0.313458  2022/05/08, 15:49:21  13.000605  0.400807  0.299861  0.264516   \n",
       "21  0.377498  2022/05/08, 15:49:43   9.459116  0.561406  0.358077  0.286934   \n",
       "22  0.304884  2022/05/08, 15:50:10  13.290977   0.33488  0.303462   0.32059   \n",
       "23  0.341068  2022/05/08, 15:50:33   9.096085  0.333905  0.325235  0.316052   \n",
       "24  0.318021  2022/05/08, 15:51:00  13.132267  0.378323  0.229717  0.307506   \n",
       "25  0.299139  2022/05/08, 15:51:21   8.989135  0.353176  0.294607  0.288522   \n",
       "26  0.337355  2022/05/08, 15:51:56  13.225945  0.411795  0.306611   0.28078   \n",
       "27  0.320961  2022/05/08, 15:52:18   9.097372  0.387906   0.25213  0.262803   \n",
       "28  0.348844  2022/05/08, 15:52:49  13.766808  0.377832  0.323053  0.347102   \n",
       "29  0.350096  2022/05/08, 15:53:14   9.145357  0.394325  0.307704  0.345167   \n",
       "30  0.340073  2022/05/08, 15:53:42  13.029304  0.437775  0.307514  0.310447   \n",
       "31  0.377306  2022/05/08, 15:54:05   9.842166  0.511826  0.360754  0.271005   \n",
       "32  0.327099  2022/05/08, 15:54:32  13.204188  0.367453   0.33066   0.30963   \n",
       "33  0.316543  2022/05/08, 15:54:55   9.608252   0.40538  0.281143  0.328623   \n",
       "34  0.310723  2022/05/08, 15:55:20   13.14671  0.335371  0.303588   0.30006   \n",
       "35  0.303345  2022/05/08, 15:55:44   9.924299  0.350278  0.293565  0.275322   \n",
       "36  0.350509  2022/05/08, 15:56:10  13.214521  0.439592  0.390317  0.295267   \n",
       "37  0.365439  2022/05/08, 15:56:33   9.967258  0.433456  0.301648  0.375876   \n",
       "38  0.293821  2022/05/08, 15:57:01  14.367573  0.335666  0.287244  0.299678   \n",
       "39  0.338342  2022/05/08, 15:57:24   9.863151  0.357844  0.276592  0.368158   \n",
       "40  0.377571  2022/05/08, 15:57:53  13.486431  0.531936  0.342114  0.304741   \n",
       "41  0.348359  2022/05/08, 15:58:18   9.331533  0.473232  0.323886  0.290402   \n",
       "42  0.340013  2022/05/08, 15:58:50   14.34408  0.380046  0.320033  0.326045   \n",
       "43  0.357232  2022/05/08, 15:59:12   8.951424  0.532942  0.341109   0.30588   \n",
       "44  0.336864  2022/05/08, 15:59:52  14.989444  0.365446  0.331207  0.306473   \n",
       "45   0.30092  2022/05/08, 16:00:15   8.965955  0.353039  0.301073  0.272802   \n",
       "46  0.304483  2022/05/08, 16:00:44  13.362375  0.387309  0.296611  0.291969   \n",
       "47  0.320318  2022/05/08, 16:01:12   8.860831  0.321986  0.309435  0.309549   \n",
       "48  0.302331  2022/05/08, 16:01:39  13.586211  0.335622  0.243469    0.3004   \n",
       "49  0.341163  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309   \n",
       "\n",
       "        AE_4  \n",
       "0   0.301537  \n",
       "1   0.329822  \n",
       "2   0.300938  \n",
       "3   0.332782  \n",
       "4   0.332109  \n",
       "5    0.34126  \n",
       "6   0.485689  \n",
       "7   0.414232  \n",
       "8   0.404837  \n",
       "9   0.401633  \n",
       "10  0.325693  \n",
       "11  0.270085  \n",
       "12  0.305255  \n",
       "13  0.368743  \n",
       "14  0.259126  \n",
       "15  0.297551  \n",
       "16  0.303673  \n",
       "17  0.287978  \n",
       "18  0.258464  \n",
       "19  0.286462  \n",
       "20  0.285987  \n",
       "21   0.30325  \n",
       "22  0.314135  \n",
       "23  0.303541  \n",
       "24    0.3527  \n",
       "25  0.266007  \n",
       "26   0.25379  \n",
       "27  0.305349  \n",
       "28   0.33694  \n",
       "29  0.324182  \n",
       "30  0.309602  \n",
       "31  0.322914  \n",
       "32  0.309675  \n",
       "33  0.283622  \n",
       "34  0.283879  \n",
       "35  0.276828  \n",
       "36  0.340893  \n",
       "37  0.385029  \n",
       "38  0.271733  \n",
       "39  0.329502  \n",
       "40  0.303843  \n",
       "41  0.304391  \n",
       "42   0.31863  \n",
       "43  0.279556  \n",
       "44  0.328298  \n",
       "45  0.284072  \n",
       "46  0.277323  \n",
       "47  0.280101  \n",
       "48  0.300824  \n",
       "49  0.315507  \n",
       "\n",
       "[50 rows x 33 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#results_table[\"RUN\"] = list(range(0,len(results_table)))\n",
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c04879b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_units</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learningrate</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>y_transformation</th>\n",
       "      <th>process_entropy</th>\n",
       "      <th>number_of_traces</th>\n",
       "      <th>statespace_size</th>\n",
       "      <th>process_type</th>\n",
       "      <th>process_memory</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RUN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.293821</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.299139</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.30092</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.302331</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.303345</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.304483</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.304884</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.310723</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.316543</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.316761</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.317151</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.317788</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.318021</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.319884</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.320318</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.320961</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.322713</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.326766</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.327099</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.331149</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.331886</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.334406</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.336864</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.337355</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.338342</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.340013</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.340073</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.341068</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.341271</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.348359</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.348844</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.350096</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.350509</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.353665</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.356384</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.357232</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.358021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.36051</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.365439</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.368142</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.370417</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.377306</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.377498</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.377571</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.392773</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.472515</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>standard</td>\n",
       "      <td>med_entropy</td>\n",
       "      <td>100.0</td>\n",
       "      <td>12</td>\n",
       "      <td>memory</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.485406</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_units num_blocks epochs batch_size learningrate optimizer  \\\n",
       "38      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "25      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "45      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "48      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "35      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "46      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "22      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "34      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "20      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "33      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "2       50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "15      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "12      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "24      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "18      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "47      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "27      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "9       50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "0       50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "17      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "32      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "14      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "5       50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "16      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "44      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "26      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "39      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "42      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "30      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "23      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "49      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "13      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "41      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "28      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "29      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "36      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "11      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "3       50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "43      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "1       50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "19      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "37      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "4       50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "10      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "31      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "21      50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "40      50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "7       50.0        1.0  100.0      256.0         0.01       SGD   \n",
       "8       50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "6       50.0        1.0  100.0      128.0         0.01       SGD   \n",
       "\n",
       "   y_transformation process_entropy number_of_traces statespace_size  \\\n",
       "38         standard     med_entropy            100.0              12   \n",
       "25         standard     med_entropy            100.0              12   \n",
       "45         standard     med_entropy            100.0              12   \n",
       "48         standard     med_entropy            100.0              12   \n",
       "35         standard     med_entropy            100.0              12   \n",
       "46         standard     med_entropy            100.0              12   \n",
       "22         standard     med_entropy            100.0              12   \n",
       "34         standard     med_entropy            100.0              12   \n",
       "20         standard     med_entropy            100.0              12   \n",
       "33         standard     med_entropy            100.0              12   \n",
       "2          standard     med_entropy            100.0              12   \n",
       "15         standard     med_entropy            100.0              12   \n",
       "12         standard     med_entropy            100.0              12   \n",
       "24         standard     med_entropy            100.0              12   \n",
       "18         standard     med_entropy            100.0              12   \n",
       "47         standard     med_entropy            100.0              12   \n",
       "27         standard     med_entropy            100.0              12   \n",
       "9          standard     med_entropy            100.0              12   \n",
       "0          standard     med_entropy            100.0              12   \n",
       "17         standard     med_entropy            100.0              12   \n",
       "32         standard     med_entropy            100.0              12   \n",
       "14         standard     med_entropy            100.0              12   \n",
       "5          standard     med_entropy            100.0              12   \n",
       "16         standard     med_entropy            100.0              12   \n",
       "44         standard     med_entropy            100.0              12   \n",
       "26         standard     med_entropy            100.0              12   \n",
       "39         standard     med_entropy            100.0              12   \n",
       "42         standard     med_entropy            100.0              12   \n",
       "30         standard     med_entropy            100.0              12   \n",
       "23         standard     med_entropy            100.0              12   \n",
       "49         standard     med_entropy            100.0              12   \n",
       "13         standard     med_entropy            100.0              12   \n",
       "41         standard     med_entropy            100.0              12   \n",
       "28         standard     med_entropy            100.0              12   \n",
       "29         standard     med_entropy            100.0              12   \n",
       "36         standard     med_entropy            100.0              12   \n",
       "11         standard     med_entropy            100.0              12   \n",
       "3          standard     med_entropy            100.0              12   \n",
       "43         standard     med_entropy            100.0              12   \n",
       "1          standard     med_entropy            100.0              12   \n",
       "19         standard     med_entropy            100.0              12   \n",
       "37         standard     med_entropy            100.0              12   \n",
       "4          standard     med_entropy            100.0              12   \n",
       "10         standard     med_entropy            100.0              12   \n",
       "31         standard     med_entropy            100.0              12   \n",
       "21         standard     med_entropy            100.0              12   \n",
       "40         standard     med_entropy            100.0              12   \n",
       "7          standard     med_entropy            100.0              12   \n",
       "8          standard     med_entropy            100.0              12   \n",
       "6          standard     med_entropy            100.0              12   \n",
       "\n",
       "   process_type process_memory       MAE RUN  \n",
       "38       memory           15.0  0.293821  38  \n",
       "25       memory           15.0  0.299139  25  \n",
       "45       memory           15.0   0.30092  45  \n",
       "48       memory           15.0  0.302331  48  \n",
       "35       memory           15.0  0.303345  35  \n",
       "46       memory           15.0  0.304483  46  \n",
       "22       memory           15.0  0.304884  22  \n",
       "34       memory           15.0  0.310723  34  \n",
       "20       memory           15.0  0.313458  20  \n",
       "33       memory           15.0  0.316543  33  \n",
       "2        memory           15.0  0.316761   2  \n",
       "15       memory           15.0  0.317151  15  \n",
       "12       memory           15.0  0.317788  12  \n",
       "24       memory           15.0  0.318021  24  \n",
       "18       memory           15.0  0.319884  18  \n",
       "47       memory           15.0  0.320318  47  \n",
       "27       memory           15.0  0.320961  27  \n",
       "9        memory           15.0  0.322713   9  \n",
       "0        memory           15.0  0.324216   0  \n",
       "17       memory           15.0  0.326766  17  \n",
       "32       memory           15.0  0.327099  32  \n",
       "14       memory           15.0  0.331149  14  \n",
       "5        memory           15.0  0.331886   5  \n",
       "16       memory           15.0  0.334406  16  \n",
       "44       memory           15.0  0.336864  44  \n",
       "26       memory           15.0  0.337355  26  \n",
       "39       memory           15.0  0.338342  39  \n",
       "42       memory           15.0  0.340013  42  \n",
       "30       memory           15.0  0.340073  30  \n",
       "23       memory           15.0  0.341068  23  \n",
       "49       memory           15.0  0.341163  49  \n",
       "13       memory           15.0  0.341271  13  \n",
       "41       memory           15.0  0.348359  41  \n",
       "28       memory           15.0  0.348844  28  \n",
       "29       memory           15.0  0.350096  29  \n",
       "36       memory           15.0  0.350509  36  \n",
       "11       memory           15.0  0.353665  11  \n",
       "3        memory           15.0  0.356384   3  \n",
       "43       memory           15.0  0.357232  43  \n",
       "1        memory           15.0  0.358021   1  \n",
       "19       memory           15.0   0.36051  19  \n",
       "37       memory           15.0  0.365439  37  \n",
       "4        memory           15.0  0.368142   4  \n",
       "10       memory           15.0  0.370417  10  \n",
       "31       memory           15.0  0.377306  31  \n",
       "21       memory           15.0  0.377498  21  \n",
       "40       memory           15.0  0.377571  40  \n",
       "7        memory           15.0  0.392773   7  \n",
       "8        memory           15.0  0.472515   8  \n",
       "6        memory           15.0  0.485406   6  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table[['num_units', 'num_blocks', 'epochs', 'batch_size', 'learningrate',\n",
    "       'optimizer', 'y_transformation', 'process_entropy', 'number_of_traces',\n",
    "       'statespace_size', 'process_type', 'process_memory','MAE', 'RUN']].sort_values(\"MAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf2d7aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>prefix_number</th>\n",
       "      <th>prefixes</th>\n",
       "      <th>distinct_events</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>AE</th>\n",
       "      <th>RUN</th>\n",
       "      <th>process_memory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>92772.406250</td>\n",
       "      <td>6372.406250</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>74946.984375</td>\n",
       "      <td>11453.015625</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>73621.0</td>\n",
       "      <td>63827.546875</td>\n",
       "      <td>9793.453125</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>60093.0</td>\n",
       "      <td>52198.070312</td>\n",
       "      <td>7894.929688</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>55571.0</td>\n",
       "      <td>42842.269531</td>\n",
       "      <td>12728.730469</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12587.0</td>\n",
       "      <td>-1392.807373</td>\n",
       "      <td>13979.807373</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>78563.0</td>\n",
       "      <td>94553.765625</td>\n",
       "      <td>15990.765625</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8538.0</td>\n",
       "      <td>14428.172852</td>\n",
       "      <td>5890.172852</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8538.0</td>\n",
       "      <td>17327.326172</td>\n",
       "      <td>8789.326172</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8445.0</td>\n",
       "      <td>12644.540039</td>\n",
       "      <td>4199.540039</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      caseid  prefix_number  prefixes  distinct_events        y        y_pred  \\\n",
       "0          3              2         6                7  86400.0  92772.406250   \n",
       "1          3              3         6                7  86400.0  74946.984375   \n",
       "2          3              4         6                7  73621.0  63827.546875   \n",
       "3          3              5         6                7  60093.0  52198.070312   \n",
       "4          3              6         6                7  55571.0  42842.269531   \n",
       "...      ...            ...       ...              ...      ...           ...   \n",
       "3795      85              9         8                6  12587.0  -1392.807373   \n",
       "3796      86              2         4                4  78563.0  94553.765625   \n",
       "3797      86              3         4                4   8538.0  14428.172852   \n",
       "3798      86              4         4                4   8538.0  17327.326172   \n",
       "3799      86              5         4                4   8445.0  12644.540039   \n",
       "\n",
       "                AE  RUN process_memory  \n",
       "0      6372.406250    0         memory  \n",
       "1     11453.015625    0         memory  \n",
       "2      9793.453125    0         memory  \n",
       "3      7894.929688    0         memory  \n",
       "4     12728.730469    0         memory  \n",
       "...            ...  ...            ...  \n",
       "3795  13979.807373   49         memory  \n",
       "3796  15990.765625   49         memory  \n",
       "3797   5890.172852   49         memory  \n",
       "3798   8789.326172   49         memory  \n",
       "3799   4199.540039   49         memory  \n",
       "\n",
       "[3800 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inference_table_all[\"RUN\"] = inference_table_all[\"RUN\"] +1\n",
    "inference_table_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "273a345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make inference table with experiment settings shown\n",
    "Inference = pd.merge(left=inference_table_all, right=results_table, on=\"RUN\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e39710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caseid</th>\n",
       "      <th>prefix_number</th>\n",
       "      <th>prefixes</th>\n",
       "      <th>distinct_events</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>AE</th>\n",
       "      <th>RUN</th>\n",
       "      <th>process_memory_x</th>\n",
       "      <th>num_units</th>\n",
       "      <th>...</th>\n",
       "      <th>activity_duration_lambda_range</th>\n",
       "      <th>repetition</th>\n",
       "      <th>RES_num_events</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Time</th>\n",
       "      <th>Traintime</th>\n",
       "      <th>AE_1</th>\n",
       "      <th>AE_2</th>\n",
       "      <th>AE_3</th>\n",
       "      <th>AE_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>92772.406250</td>\n",
       "      <td>6372.406250</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>74946.984375</td>\n",
       "      <td>11453.015625</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>73621.0</td>\n",
       "      <td>63827.546875</td>\n",
       "      <td>9793.453125</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>60093.0</td>\n",
       "      <td>52198.070312</td>\n",
       "      <td>7894.929688</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>55571.0</td>\n",
       "      <td>42842.269531</td>\n",
       "      <td>12728.730469</td>\n",
       "      <td>0</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.324216</td>\n",
       "      <td>2022/05/08, 15:40:29</td>\n",
       "      <td>62.046877</td>\n",
       "      <td>0.403866</td>\n",
       "      <td>0.324451</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.301537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12587.0</td>\n",
       "      <td>-1392.807373</td>\n",
       "      <td>13979.807373</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>78563.0</td>\n",
       "      <td>94553.765625</td>\n",
       "      <td>15990.765625</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8538.0</td>\n",
       "      <td>14428.172852</td>\n",
       "      <td>5890.172852</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8538.0</td>\n",
       "      <td>17327.326172</td>\n",
       "      <td>8789.326172</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8445.0</td>\n",
       "      <td>12644.540039</td>\n",
       "      <td>4199.540039</td>\n",
       "      <td>49</td>\n",
       "      <td>memory</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>822</td>\n",
       "      <td>0.341163</td>\n",
       "      <td>2022/05/08, 16:02:02</td>\n",
       "      <td>9.159774</td>\n",
       "      <td>0.388972</td>\n",
       "      <td>0.318521</td>\n",
       "      <td>0.310309</td>\n",
       "      <td>0.315507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3800 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      caseid  prefix_number  prefixes  distinct_events        y        y_pred  \\\n",
       "0          3              2         6                7  86400.0  92772.406250   \n",
       "1          3              3         6                7  86400.0  74946.984375   \n",
       "2          3              4         6                7  73621.0  63827.546875   \n",
       "3          3              5         6                7  60093.0  52198.070312   \n",
       "4          3              6         6                7  55571.0  42842.269531   \n",
       "...      ...            ...       ...              ...      ...           ...   \n",
       "3795      85              9         8                6  12587.0  -1392.807373   \n",
       "3796      86              2         4                4  78563.0  94553.765625   \n",
       "3797      86              3         4                4   8538.0  14428.172852   \n",
       "3798      86              4         4                4   8538.0  17327.326172   \n",
       "3799      86              5         4                4   8445.0  12644.540039   \n",
       "\n",
       "                AE RUN process_memory_x num_units  ...  \\\n",
       "0      6372.406250   0           memory      50.0  ...   \n",
       "1     11453.015625   0           memory      50.0  ...   \n",
       "2      9793.453125   0           memory      50.0  ...   \n",
       "3      7894.929688   0           memory      50.0  ...   \n",
       "4     12728.730469   0           memory      50.0  ...   \n",
       "...            ...  ..              ...       ...  ...   \n",
       "3795  13979.807373  49           memory      50.0  ...   \n",
       "3796  15990.765625  49           memory      50.0  ...   \n",
       "3797   5890.172852  49           memory      50.0  ...   \n",
       "3798   8789.326172  49           memory      50.0  ...   \n",
       "3799   4199.540039  49           memory      50.0  ...   \n",
       "\n",
       "     activity_duration_lambda_range repetition RES_num_events       MAE  \\\n",
       "0                               0.5        1.0            822  0.324216   \n",
       "1                               0.5        1.0            822  0.324216   \n",
       "2                               0.5        1.0            822  0.324216   \n",
       "3                               0.5        1.0            822  0.324216   \n",
       "4                               0.5        1.0            822  0.324216   \n",
       "...                             ...        ...            ...       ...   \n",
       "3795                            0.5        1.0            822  0.341163   \n",
       "3796                            0.5        1.0            822  0.341163   \n",
       "3797                            0.5        1.0            822  0.341163   \n",
       "3798                            0.5        1.0            822  0.341163   \n",
       "3799                            0.5        1.0            822  0.341163   \n",
       "\n",
       "                      Time  Traintime      AE_1      AE_2      AE_3      AE_4  \n",
       "0     2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601  0.301537  \n",
       "1     2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601  0.301537  \n",
       "2     2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601  0.301537  \n",
       "3     2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601  0.301537  \n",
       "4     2022/05/08, 15:40:29  62.046877  0.403866  0.324451  0.293601  0.301537  \n",
       "...                    ...        ...       ...       ...       ...       ...  \n",
       "3795  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309  0.315507  \n",
       "3796  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309  0.315507  \n",
       "3797  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309  0.315507  \n",
       "3798  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309  0.315507  \n",
       "3799  2022/05/08, 16:02:02   9.159774  0.388972  0.318521  0.310309  0.315507  \n",
       "\n",
       "[3800 rows x 41 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference = Inference.loc[Inference.RUN < 5]\n",
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f033e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['caseid', 'prefix_number', 'prefixes', 'distinct_events', 'y', 'y_pred',\n",
       "       'AE', 'RUN', 'process_memory_x', 'num_units', 'num_blocks', 'epochs',\n",
       "       'batch_size', 'learningrate', 'optimizer', 'dropout',\n",
       "       'y_transformation', 'loss_function', 'gamma', 'beta', 'alpha',\n",
       "       'number_of_traces', 'statespace_size', 'process_entropy',\n",
       "       'process_type', 'process_memory_y', 'inter_arrival_time',\n",
       "       'process_stability_scale', 'resource_availability_p',\n",
       "       'resource_availability_n', 'resource_availability_m',\n",
       "       'activity_duration_lambda_range', 'repetition', 'RES_num_events', 'MAE',\n",
       "       'Time', 'Traintime', 'AE_1', 'AE_2', 'AE_3', 'AE_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inference.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43399a6b",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92ef33f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"prefix_number\", \n",
    "            y=\"AE\",\n",
    "            hue=\"loss_function\",#\"loss_function\",#\"y_transformation\", \n",
    "            #palette=[\"m\", \"g\"],\n",
    "            data=Inference)#.loc[(Inference.process_memory_x == \"memoryless\") & (Inference.beta < 0.02)])\n",
    "\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc8d9d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"prefix_number\", \n",
    "            y=\"AE\",\n",
    "            hue=\"beta\",#\"loss_function\",#\"y_transformation\", \n",
    "            #palette=[\"m\", \"g\"],\n",
    "            data=Inference)#.loc[Inference.process_memory_x == \"memoryless\"])\n",
    "\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f14e9122-2806-4016-ae30-5e854fa70eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Draw a nested boxplot to show bills by day and time\n",
    "sns.boxplot(x=\"prefix_number\", \n",
    "            y=\"AE\",\n",
    "            hue=\"RUN\",#\"loss_function\",#\"y_transformation\", \n",
    "            #palette=[\"m\", \"g\"],\n",
    "            data=Inference)#.loc[Inference.process_memory_x == \"memoryless\"])\n",
    "\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5cefb0",
   "metadata": {},
   "source": [
    "# Factorplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d1761d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_units': [50],\n",
       " 'num_blocks': [1],\n",
       " 'epochs': [100],\n",
       " 'batch_size': [128, 256],\n",
       " 'learningrate': [0.01],\n",
       " 'optimizer': ['SGD'],\n",
       " 'dropout': [0.2],\n",
       " 'y_transformation': ['standard'],\n",
       " 'loss_function': ['MAE_td'],\n",
       " 'gamma': [0],\n",
       " 'beta': array([0.25  , 0.5625, 0.875 , 1.1875, 1.5   ]),\n",
       " 'alpha': array([0.25  , 0.5625, 0.875 , 1.1875, 1.5   ]),\n",
       " 'number_of_traces': [100],\n",
       " 'statespace_size': [12],\n",
       " 'process_entropy': ['med_entropy'],\n",
       " 'process_type': ['memory'],\n",
       " 'process_memory': [15],\n",
       " 'inter_arrival_time': [1.5],\n",
       " 'process_stability_scale': [0.1],\n",
       " 'resource_availability_p': [0.25],\n",
       " 'resource_availability_n': [3],\n",
       " 'resource_availability_m': [0.041],\n",
       " 'activity_duration_lambda_range': [0.5],\n",
       " 'repetition': [1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4823c906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['caseid', 'prefix_number', 'prefixes', 'distinct_events', 'y', 'y_pred',\n",
       "       'AE', 'RUN', 'process_memory_x', 'num_units', 'num_blocks', 'epochs',\n",
       "       'batch_size', 'learningrate', 'optimizer', 'dropout',\n",
       "       'y_transformation', 'loss_function', 'gamma', 'beta', 'alpha',\n",
       "       'number_of_traces', 'statespace_size', 'process_entropy',\n",
       "       'process_type', 'process_memory_y', 'inter_arrival_time',\n",
       "       'process_stability_scale', 'resource_availability_p',\n",
       "       'resource_availability_n', 'resource_availability_m',\n",
       "       'activity_duration_lambda_range', 'repetition', 'RES_num_events', 'MAE',\n",
       "       'Time', 'Traintime', 'AE_1', 'AE_2', 'AE_3', 'AE_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Inference.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6430d6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1059.85x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.factorplot(x=\"prefix_number\", \n",
    "                   y='AE',\n",
    "                   hue='RUN', \n",
    "                   col='batch_size',\n",
    "                   data=Inference,\n",
    "                   size=7, \n",
    "                   aspect=1, \n",
    "                   col_wrap=2, \n",
    "                   legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "565b3cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1081.35x1512 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.factorplot(x=\"prefix_number\", \n",
    "                   y='AE',\n",
    "                   hue='beta', \n",
    "                   col='alpha',\n",
    "                   data=Inference, \n",
    "                   size=7, \n",
    "                   aspect=1, \n",
    "                   col_wrap=2, \n",
    "                   legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38a15444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1086.85x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.factorplot(x=\"prefix_number\", \n",
    "                   y='AE',\n",
    "                   hue='loss_function', \n",
    "                   col='statespace_size',\n",
    "                   data=Inference, \n",
    "                   size=7, \n",
    "                   aspect=1, \n",
    "                   col_wrap=2, \n",
    "                   legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9308da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # g = sns.FacetGrid(pd.melt(Inference, id_vars=['loss_function','AE', 'prefix_number',\"beta\"]), \n",
    "# #                   col='loss_function',\n",
    "# #                   size=5,\n",
    "# #                   legend_out=False)\n",
    "\n",
    "# # #g.figure(figsize=(16, 6))\n",
    "# # g.map(sns.boxplot, 'prefix_number','AE',\"beta\")\n",
    "\n",
    "\n",
    "# df_results = Inference.melt(id_vars=['prefix_number','process_stability_scale'], \n",
    "#                             #col='loss_function',\n",
    "#                             value_vars=['AE'])\n",
    "\n",
    "# #df_results['new_var'] = df_results[col] + ' - ' + df_results['loss_function']\n",
    "\n",
    "# g = sns.factorplot(x=\"prefix_number\", y='AE',\n",
    "#                            hue='loss_function', col='process_stability_scale',\n",
    "#                            data=df_results, #kind=kind,\n",
    "#                            size=5, aspect=1, col_wrap=2, legend=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43af9a",
   "metadata": {},
   "source": [
    "## Need further simplification and tweakability!\n",
    "\n",
    "- get functions out of the notebook (halfway there)\n",
    "- put more control over training process (done)\n",
    "- make sure transformations work as intended (done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345f3bf",
   "metadata": {},
   "source": [
    "# Need to figure out why prefix_number start from 2?!?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db68d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcde6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
